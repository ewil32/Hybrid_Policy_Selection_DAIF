{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run The Agent on Mountain Car"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from vae_recurrent import VAE, create_decoder, create_encoder\n",
    "from transition_gru import TransitionGRU\n",
    "from recurrent_agent import DAIFAgentRecurrent\n",
    "from prior_model import PriorModelBellman\n",
    "from habitual_action_network import HabitualAction, compute_discounted_cumulative_reward\n",
    "from ddpg import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations, test_policy, habit_policy\n",
    "from train_agent import train_single_agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# from identity_vae import IdentityVAE, identity_encoder, identity_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with no prior model FEEF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "pln_hrzn = 5\n",
    "latent_dim = 2\n",
    "obs_dim = 3\n",
    "\n",
    "# make the VAE\n",
    "enc = create_encoder(obs_dim, latent_dim, [20])\n",
    "dec = create_decoder(latent_dim, obs_dim, [20])\n",
    "vae = VAE(enc, dec, latent_dim,  [0]*latent_dim, [0.3]*latent_dim, train_epochs=2, show_training=True)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the TRANSITION\n",
    "tran = TransitionGRU(latent_dim, 1, 2*pln_hrzn*latent_dim, 2, train_epochs=2, show_training=True)\n",
    "tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the HABIT ACTION NET\n",
    "habit_net = HabitualAction(latent_dim, 1, [16, 16], train_epochs=2, show_training=False)\n",
    "habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# # # make the HABIT ACTION NET\n",
    "# actor_model = get_actor(2, 1)\n",
    "# critic_model = get_critic(2, 1)\n",
    "# target_actor = get_actor(2, 1)\n",
    "# target_critic = get_critic(2, 1)\n",
    "#\n",
    "# # Making the weights equal initially\n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "# critic_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "# actor_optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "# habit_net = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau=0.005, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)\n",
    "\n",
    "\n",
    "# make the PRIOR NET\n",
    "prior_model = PriorModelBellman(latent_dim, output_dim=1, scaling_factor=1, show_training=False, use_tanh_on_output=False)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0, 0, 0]\n",
    "prior_stddev = [1, 1, 1]\n",
    "\n",
    "observation_max = np.array([1, 1, 8])\n",
    "observation_min = np.array([-1, -1, -8])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0,0])  # no noise on prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           habit_net,\n",
    "                           planning_horizon=pln_hrzn,\n",
    "                           use_kl_extrinsic=True,  # maybe this works\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           train_habit_net=True,\n",
    "                           train_prior_model=True,\n",
    "                           train_tran=True,\n",
    "                           train_after_exploring=True,\n",
    "                           train_with_replay=True,\n",
    "                           use_fast_thinking=True,\n",
    "                           habit_model_type=\"PG\",\n",
    "                           uncertainty_tolerance=0.1,\n",
    "                           min_rewards_needed_to_train_prior=-30)\n",
    "\n",
    "\n",
    "daifa.train_prior = True\n",
    "daifa.prior_model.show_training = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.0795482], dtype=float32)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "env.action_space.sample()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[ 0.46496177  0.88533074 -0.2899176 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 1s 525ms/step - kl_loss: 0.3682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 300.3416 - reconstruction_loss: 292.7230 - kl_loss: 7.6186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 370.9782 - reconstruction_loss: 363.4541 - kl_loss: 7.5241\n",
      "WARNING:tensorflow:5 out of the last 10199 calls to <function Model.make_train_function.<locals>.train_function at 0x17fbc2820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4107\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3980\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 222.8383 - reconstruction_loss: 214.0977 - kl_loss: 8.7406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 280.5497 - reconstruction_loss: 271.7922 - kl_loss: 8.7574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4099\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 351.5280 - reconstruction_loss: 342.5060 - kl_loss: 9.0220\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.8343 - reconstruction_loss: 164.8707 - kl_loss: 8.9636\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4917\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4770\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 232.1961 - reconstruction_loss: 222.3430 - kl_loss: 9.8531\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 145.2799 - reconstruction_loss: 135.4469 - kl_loss: 9.8331\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5433\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 243.4968 - reconstruction_loss: 235.9993 - kl_loss: 7.4975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 286.1794 - reconstruction_loss: 278.6922 - kl_loss: 7.4872\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2199\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2118\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 270.4360 - reconstruction_loss: 262.0047 - kl_loss: 8.4313\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 335.6098 - reconstruction_loss: 327.2090 - kl_loss: 8.4008\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3531\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3446\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 236.2422 - reconstruction_loss: 226.7677 - kl_loss: 9.4745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 216.8926 - reconstruction_loss: 207.4300 - kl_loss: 9.4627\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2808\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2713\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 304.3321 - reconstruction_loss: 296.3484 - kl_loss: 7.9836\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 359.2697 - reconstruction_loss: 351.3128 - kl_loss: 7.9569\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3302\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3256\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 265.7792 - reconstruction_loss: 257.2844 - kl_loss: 8.4948\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 150.4740 - reconstruction_loss: 141.9890 - kl_loss: 8.4850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7212\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7144\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 234.8704 - reconstruction_loss: 227.4876 - kl_loss: 7.3828\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 398.7808 - reconstruction_loss: 391.4300 - kl_loss: 7.3508\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7064\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6877\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 215.3904 - reconstruction_loss: 208.9015 - kl_loss: 6.4889\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 243.4675 - reconstruction_loss: 237.0219 - kl_loss: 6.4456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7980\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7683\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 226.2339 - reconstruction_loss: 215.9098 - kl_loss: 10.3241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 236.9691 - reconstruction_loss: 226.6448 - kl_loss: 10.3242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8468\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 231.2260 - reconstruction_loss: 223.2991 - kl_loss: 7.9269\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 292.1177 - reconstruction_loss: 284.2095 - kl_loss: 7.9082\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8987\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 191.5867 - reconstruction_loss: 184.9887 - kl_loss: 6.5981\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 202.1831 - reconstruction_loss: 195.6077 - kl_loss: 6.5754\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3502\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 203.0282 - reconstruction_loss: 194.3332 - kl_loss: 8.6950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 247.8789 - reconstruction_loss: 239.1918 - kl_loss: 8.6871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3467\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2889\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 248.2177 - reconstruction_loss: 239.5689 - kl_loss: 8.6488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 267.2327 - reconstruction_loss: 258.5925 - kl_loss: 8.6402\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 291ms/step - kl_loss: 0.4099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 249.8103 - reconstruction_loss: 241.7791 - kl_loss: 8.0312\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 278.8207 - reconstruction_loss: 270.8116 - kl_loss: 8.0091\n",
      "Success in episode 1 at time step 200 with reward -200.04056739100673\n",
      "Episode 2\n",
      "[-0.12845826  0.9917149   0.581576  ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1009\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 251.8763 - reconstruction_loss: 245.2157 - kl_loss: 6.6606\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 360.2597 - reconstruction_loss: 353.6337 - kl_loss: 6.6260\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4579\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 172.0310 - reconstruction_loss: 161.3762 - kl_loss: 10.6547\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 213.6711 - reconstruction_loss: 203.0151 - kl_loss: 10.6560\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6542\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6305\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 297.3740 - reconstruction_loss: 290.5933 - kl_loss: 6.7807\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 189.7369 - reconstruction_loss: 182.9946 - kl_loss: 6.7422\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1356\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1331\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 193.3723 - reconstruction_loss: 187.3501 - kl_loss: 6.0222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 867us/step - loss: 199.4161 - reconstruction_loss: 193.4280 - kl_loss: 5.9881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 157.3723 - reconstruction_loss: 149.5543 - kl_loss: 7.8180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 215.3907 - reconstruction_loss: 207.5830 - kl_loss: 7.8077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6161\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5991\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 217.4704 - reconstruction_loss: 210.9499 - kl_loss: 6.5204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 289.0786 - reconstruction_loss: 282.5793 - kl_loss: 6.4994\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2255\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 212.4876 - reconstruction_loss: 206.9672 - kl_loss: 5.5204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.9900 - reconstruction_loss: 168.4911 - kl_loss: 5.4989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2007\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1993\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 138.3570 - reconstruction_loss: 131.0127 - kl_loss: 7.3443\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 184.4525 - reconstruction_loss: 177.1165 - kl_loss: 7.3360\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0832\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 172.5029 - reconstruction_loss: 166.0946 - kl_loss: 6.4083\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 172.5128 - reconstruction_loss: 166.1149 - kl_loss: 6.3978\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0755\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 203.4192 - reconstruction_loss: 196.9055 - kl_loss: 6.5136\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 226.8265 - reconstruction_loss: 220.3184 - kl_loss: 6.5081\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2716\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2701\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 207.4859 - reconstruction_loss: 200.8745 - kl_loss: 6.6113\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 157.3775 - reconstruction_loss: 150.7745 - kl_loss: 6.6030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6388\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6318\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 204.4574 - reconstruction_loss: 198.2445 - kl_loss: 6.2128\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 390.2328 - reconstruction_loss: 384.0261 - kl_loss: 6.2067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2220\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 185.1633 - reconstruction_loss: 179.8603 - kl_loss: 5.3030\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 212.6075 - reconstruction_loss: 207.3243 - kl_loss: 5.2832\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2329\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 168.7825 - reconstruction_loss: 162.3510 - kl_loss: 6.4315\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 182.6639 - reconstruction_loss: 176.2453 - kl_loss: 6.4186\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0537\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0532\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 267.8704 - reconstruction_loss: 261.9425 - kl_loss: 5.9279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 226.7369 - reconstruction_loss: 220.8342 - kl_loss: 5.9027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1410\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1402\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 171.7265 - reconstruction_loss: 166.5220 - kl_loss: 5.2045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 162.0273 - reconstruction_loss: 156.8408 - kl_loss: 5.1865\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2170\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 205.3160 - reconstruction_loss: 198.8933 - kl_loss: 6.4226\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 178.6730 - reconstruction_loss: 172.2660 - kl_loss: 6.4070\n",
      "Success in episode 2 at time step 200 with reward -221.84669109606827\n",
      "Episode 3\n",
      "[-0.5546827   0.83206195  0.59899807]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2039\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2011\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 182.8716 - reconstruction_loss: 177.2531 - kl_loss: 5.6185\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 241.7074 - reconstruction_loss: 236.1040 - kl_loss: 5.6034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1563\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1543\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 218.1310 - reconstruction_loss: 212.1065 - kl_loss: 6.0245\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 178.0462 - reconstruction_loss: 172.0334 - kl_loss: 6.0129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0698\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 151.4448 - reconstruction_loss: 145.6090 - kl_loss: 5.8358\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 326.5275 - reconstruction_loss: 320.6950 - kl_loss: 5.8324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2202\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 134.3871 - reconstruction_loss: 128.7366 - kl_loss: 5.6505\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 138.8090 - reconstruction_loss: 133.1693 - kl_loss: 5.6397\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5187\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5125\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 168.1053 - reconstruction_loss: 162.3243 - kl_loss: 5.7810\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 140.2549 - reconstruction_loss: 134.4751 - kl_loss: 5.7798\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2679\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 162.7159 - reconstruction_loss: 157.6925 - kl_loss: 5.0234\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 157.5838 - reconstruction_loss: 152.5619 - kl_loss: 5.0219\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2420\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2391\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 146.4499 - reconstruction_loss: 140.3890 - kl_loss: 6.0609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 155.2630 - reconstruction_loss: 149.2010 - kl_loss: 6.0620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0930\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 236.6173 - reconstruction_loss: 230.7923 - kl_loss: 5.8250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 179.3314 - reconstruction_loss: 173.5029 - kl_loss: 5.8285\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1398\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 171.0557 - reconstruction_loss: 166.1264 - kl_loss: 4.9292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 179.2599 - reconstruction_loss: 174.3317 - kl_loss: 4.9283\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3478\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 152.6653 - reconstruction_loss: 146.4279 - kl_loss: 6.2375\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 139.9973 - reconstruction_loss: 133.7599 - kl_loss: 6.2373\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1477\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1447\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 133.2431 - reconstruction_loss: 127.7077 - kl_loss: 5.5354\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 137.6525 - reconstruction_loss: 132.1141 - kl_loss: 5.5384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1264\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 195.0788 - reconstruction_loss: 190.0834 - kl_loss: 4.9954\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 136.1142 - reconstruction_loss: 131.1201 - kl_loss: 4.9941\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 147.7703 - reconstruction_loss: 141.8882 - kl_loss: 5.8822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 161.9874 - reconstruction_loss: 156.1051 - kl_loss: 5.8822\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1651\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1611\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 209.3204 - reconstruction_loss: 203.9326 - kl_loss: 5.3879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 256.6605 - reconstruction_loss: 251.2731 - kl_loss: 5.3874\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1586\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 141.1866 - reconstruction_loss: 136.2606 - kl_loss: 4.9260\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 166.3026 - reconstruction_loss: 161.3890 - kl_loss: 4.9136\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2439\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 111.7404 - reconstruction_loss: 106.1497 - kl_loss: 5.5907\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.1459 - reconstruction_loss: 126.5605 - kl_loss: 5.5854\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1860\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 162.1498 - reconstruction_loss: 156.6178 - kl_loss: 5.5320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 174.5190 - reconstruction_loss: 169.0004 - kl_loss: 5.5187\n",
      "Success in episode 3 at time step 200 with reward -254.7590569092662\n",
      "Episode 4\n",
      "[-0.9699535   0.2432905   0.33345345]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1472\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1413\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 115.9337 - reconstruction_loss: 110.7738 - kl_loss: 5.1599\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.8859 - reconstruction_loss: 123.7433 - kl_loss: 5.1426\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0904\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0835\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 127.0072 - reconstruction_loss: 121.7546 - kl_loss: 5.2525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 844us/step - loss: 130.8156 - reconstruction_loss: 125.5696 - kl_loss: 5.2460\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0950\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 162.7954 - reconstruction_loss: 157.9769 - kl_loss: 4.8186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 143.0210 - reconstruction_loss: 138.2107 - kl_loss: 4.8103\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1220\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 137.9427 - reconstruction_loss: 132.6953 - kl_loss: 5.2474\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 156.0656 - reconstruction_loss: 150.8257 - kl_loss: 5.2398\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0700\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0653\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 133.9532 - reconstruction_loss: 128.8497 - kl_loss: 5.1034\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 127.8249 - reconstruction_loss: 122.7258 - kl_loss: 5.0990\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1008\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0972\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.3268 - reconstruction_loss: 107.4545 - kl_loss: 4.8723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 152.7607 - reconstruction_loss: 147.8939 - kl_loss: 4.8669\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0946\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.6498 - reconstruction_loss: 114.2674 - kl_loss: 5.3825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.5280 - reconstruction_loss: 61.1499 - kl_loss: 5.3780\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0601\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0568\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 169.3170 - reconstruction_loss: 164.3043 - kl_loss: 5.0128\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.6185 - reconstruction_loss: 96.6068 - kl_loss: 5.0117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.5403 - reconstruction_loss: 126.5181 - kl_loss: 5.0223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.0199 - reconstruction_loss: 106.0013 - kl_loss: 5.0186\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0501\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 128.9347 - reconstruction_loss: 123.7262 - kl_loss: 5.2085\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.5462 - reconstruction_loss: 99.3383 - kl_loss: 5.2080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0386\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0366\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 110.0110 - reconstruction_loss: 105.1512 - kl_loss: 4.8598\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.9372 - reconstruction_loss: 90.0774 - kl_loss: 4.8598\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0775\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 97.6137 - reconstruction_loss: 92.5184 - kl_loss: 5.0952\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.9053 - reconstruction_loss: 116.8106 - kl_loss: 5.0948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0459\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 115.7832 - reconstruction_loss: 110.6448 - kl_loss: 5.1385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.0274 - reconstruction_loss: 116.8888 - kl_loss: 5.1387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0368\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0354\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 118.6812 - reconstruction_loss: 113.9929 - kl_loss: 4.6883\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.9885 - reconstruction_loss: 87.3024 - kl_loss: 4.6861\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0605\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0585\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 111.2798 - reconstruction_loss: 106.0697 - kl_loss: 5.2101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.6200 - reconstruction_loss: 105.4120 - kl_loss: 5.2080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0280\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0267\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 123.1982 - reconstruction_loss: 118.1530 - kl_loss: 5.0451\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.6870 - reconstruction_loss: 116.6419 - kl_loss: 5.0452\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0506\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0478\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 97.5547 - reconstruction_loss: 92.5377 - kl_loss: 5.0170\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.2390 - reconstruction_loss: 107.2128 - kl_loss: 5.0262\n",
      "Success in episode 4 at time step 200 with reward -287.24242059741056\n",
      "Episode 5\n",
      "[ 0.5882155  -0.80870426  0.5805888 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2552\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2537\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 294.8042 - reconstruction_loss: 286.1444 - kl_loss: 8.6597\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 993us/step - loss: 240.4464 - reconstruction_loss: 231.7814 - kl_loss: 8.6651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5676\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5707\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 374.2680 - reconstruction_loss: 362.3161 - kl_loss: 11.9519\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 343.1668 - reconstruction_loss: 331.2126 - kl_loss: 11.9542\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2659\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2655\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 334.4954 - reconstruction_loss: 324.5906 - kl_loss: 9.9047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 368.6206 - reconstruction_loss: 358.7147 - kl_loss: 9.9059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3513\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 537.4297 - reconstruction_loss: 524.5008 - kl_loss: 12.9289\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 397.9633 - reconstruction_loss: 385.1194 - kl_loss: 12.8439\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4436\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 310.9136 - reconstruction_loss: 303.2968 - kl_loss: 7.6168\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 282.4815 - reconstruction_loss: 274.8895 - kl_loss: 7.5921\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2505\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2496\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 397.8207 - reconstruction_loss: 386.0509 - kl_loss: 11.7698\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 391.4790 - reconstruction_loss: 379.7799 - kl_loss: 11.6991\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5533\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 319.9607 - reconstruction_loss: 309.9216 - kl_loss: 10.0392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 338.5114 - reconstruction_loss: 328.5097 - kl_loss: 10.0017\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5269\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 266.0992 - reconstruction_loss: 257.1739 - kl_loss: 8.9253\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 875us/step - loss: 433.6389 - reconstruction_loss: 424.7559 - kl_loss: 8.8830\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5499\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5470\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 389.7269 - reconstruction_loss: 378.4230 - kl_loss: 11.3039\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 277.3459 - reconstruction_loss: 266.0725 - kl_loss: 11.2735\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2682\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 266.0074 - reconstruction_loss: 260.4501 - kl_loss: 5.5574\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 244.0895 - reconstruction_loss: 238.5229 - kl_loss: 5.5666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1965\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1957\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 238.7076 - reconstruction_loss: 229.9036 - kl_loss: 8.8040\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 203.9362 - reconstruction_loss: 195.1498 - kl_loss: 8.7864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5938\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5928\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 259.4388 - reconstruction_loss: 250.8849 - kl_loss: 8.5539\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 932us/step - loss: 250.9899 - reconstruction_loss: 242.4562 - kl_loss: 8.5337\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2962\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2955\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 376.6197 - reconstruction_loss: 363.7932 - kl_loss: 12.8266\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 380.0795 - reconstruction_loss: 367.3105 - kl_loss: 12.7691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5003\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4982\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 324.4797 - reconstruction_loss: 316.1705 - kl_loss: 8.3092\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 317.2244 - reconstruction_loss: 308.9390 - kl_loss: 8.2854\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1816\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1809\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 394.1934 - reconstruction_loss: 384.0171 - kl_loss: 10.1764\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 394.5164 - reconstruction_loss: 384.3816 - kl_loss: 10.1348\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4026\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 370.1134 - reconstruction_loss: 360.1302 - kl_loss: 9.9832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 469.4506 - reconstruction_loss: 459.5128 - kl_loss: 9.9377\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3640\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3609\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 329.7902 - reconstruction_loss: 320.4226 - kl_loss: 9.3676\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 337.1202 - reconstruction_loss: 327.8426 - kl_loss: 9.2776\n",
      "Success in episode 5 at time step 200 with reward -202.54405751911673\n",
      "Episode 6\n",
      "[ 0.46869573  0.8833597  -0.81347346]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1812\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1827\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 233.3124 - reconstruction_loss: 227.0185 - kl_loss: 6.2939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 257.4261 - reconstruction_loss: 251.0866 - kl_loss: 6.3395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4220\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4204\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 305.0752 - reconstruction_loss: 292.9390 - kl_loss: 12.1362\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 335.6803 - reconstruction_loss: 323.5965 - kl_loss: 12.0838\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1374\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1381\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 386.8539 - reconstruction_loss: 380.2006 - kl_loss: 6.6534\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 313.1165 - reconstruction_loss: 306.4207 - kl_loss: 6.6958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4137\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 423.8256 - reconstruction_loss: 412.5147 - kl_loss: 11.3108\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 362.3305 - reconstruction_loss: 351.0364 - kl_loss: 11.2940\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3041\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 370.4168 - reconstruction_loss: 361.9274 - kl_loss: 8.4894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 427.4886 - reconstruction_loss: 418.9888 - kl_loss: 8.4999\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 316.2875 - reconstruction_loss: 308.1935 - kl_loss: 8.0940\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 398.9507 - reconstruction_loss: 390.8639 - kl_loss: 8.0868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3063\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3050\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 384.2410 - reconstruction_loss: 373.7788 - kl_loss: 10.4622\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 326.2603 - reconstruction_loss: 315.8081 - kl_loss: 10.4522\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1762\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1758\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 260.5341 - reconstruction_loss: 249.4497 - kl_loss: 11.0844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 415.0332 - reconstruction_loss: 403.9963 - kl_loss: 11.0368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5856\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5839\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 201.0218 - reconstruction_loss: 193.3544 - kl_loss: 7.6674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 188.0046 - reconstruction_loss: 180.3447 - kl_loss: 7.6599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 198.2927 - reconstruction_loss: 190.7266 - kl_loss: 7.5661\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 226.1638 - reconstruction_loss: 218.5919 - kl_loss: 7.5719\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1932\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1940\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 212.1491 - reconstruction_loss: 205.4215 - kl_loss: 6.7276\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 251.1873 - reconstruction_loss: 244.4489 - kl_loss: 6.7385\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1309\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1305\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 316.7817 - reconstruction_loss: 305.9312 - kl_loss: 10.8506\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 263.8746 - reconstruction_loss: 253.0378 - kl_loss: 10.8368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4157\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4131\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 360.9942 - reconstruction_loss: 351.0118 - kl_loss: 9.9825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 207.8936 - reconstruction_loss: 197.9308 - kl_loss: 9.9628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3773\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3750\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 225.5690 - reconstruction_loss: 218.9853 - kl_loss: 6.5837\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 145.8852 - reconstruction_loss: 139.3003 - kl_loss: 6.5849\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2446\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2438\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 258.4734 - reconstruction_loss: 251.6452 - kl_loss: 6.8283\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 215.7791 - reconstruction_loss: 208.9410 - kl_loss: 6.8380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1712\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1711\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 141.0530 - reconstruction_loss: 131.7335 - kl_loss: 9.3196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 288.4146 - reconstruction_loss: 279.0909 - kl_loss: 9.3237\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2965\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 257.7592 - reconstruction_loss: 248.9372 - kl_loss: 8.8219\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 277.3232 - reconstruction_loss: 268.4816 - kl_loss: 8.8416\n",
      "Success in episode 6 at time step 200 with reward -167.44956550527695\n",
      "Episode 7\n",
      "[-0.999323   0.0367897 -0.3944452]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4895\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.7437 - reconstruction_loss: 113.1316 - kl_loss: 5.6121\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.7940 - reconstruction_loss: 77.1782 - kl_loss: 5.6158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5806\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5726\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51.8742 - reconstruction_loss: 46.1178 - kl_loss: 5.7565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 98.1832 - reconstruction_loss: 92.4162 - kl_loss: 5.7669\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5277\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.9446 - reconstruction_loss: 113.1597 - kl_loss: 5.7849\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 137.6401 - reconstruction_loss: 131.8443 - kl_loss: 5.7958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4493\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.1805 - reconstruction_loss: 128.4116 - kl_loss: 5.7689\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.2331 - reconstruction_loss: 113.4548 - kl_loss: 5.7783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4456\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4313\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 174.2687 - reconstruction_loss: 168.3164 - kl_loss: 5.9523\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 143.3754 - reconstruction_loss: 137.4159 - kl_loss: 5.9595\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3676\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3530\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 89.4597 - reconstruction_loss: 83.8761 - kl_loss: 5.5836\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.9511 - reconstruction_loss: 89.3629 - kl_loss: 5.5882\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4324\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4179\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 158.7134 - reconstruction_loss: 152.6120 - kl_loss: 6.1014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.6912 - reconstruction_loss: 98.5805 - kl_loss: 6.1108\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3267\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3105\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.3665 - reconstruction_loss: 78.3881 - kl_loss: 5.9784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 99.8472 - reconstruction_loss: 93.8551 - kl_loss: 5.9921\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1243\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1185\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 137.6179 - reconstruction_loss: 131.9172 - kl_loss: 5.7008\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.8516 - reconstruction_loss: 77.1393 - kl_loss: 5.7123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2139\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.8267 - reconstruction_loss: 109.2300 - kl_loss: 5.5967\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 139.7447 - reconstruction_loss: 134.1358 - kl_loss: 5.6089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3175\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.7014 - reconstruction_loss: 113.4101 - kl_loss: 6.2912\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 167.2303 - reconstruction_loss: 160.9236 - kl_loss: 6.3067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1871\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1770\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.3909 - reconstruction_loss: 86.4368 - kl_loss: 5.9541\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 996us/step - loss: 129.5621 - reconstruction_loss: 123.5964 - kl_loss: 5.9658\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1715\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1662\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 161.3190 - reconstruction_loss: 155.6792 - kl_loss: 5.6398\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 924us/step - loss: 131.8102 - reconstruction_loss: 126.1619 - kl_loss: 5.6483\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2220\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2131\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.5474 - reconstruction_loss: 63.2078 - kl_loss: 6.3397\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.3627 - reconstruction_loss: 76.0087 - kl_loss: 6.3540\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1065\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1012\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 155.6161 - reconstruction_loss: 149.5528 - kl_loss: 6.0633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 994us/step - loss: 157.2671 - reconstruction_loss: 151.1920 - kl_loss: 6.0751\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0272\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0255\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 129.0845 - reconstruction_loss: 123.5162 - kl_loss: 5.5684\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 945us/step - loss: 109.2500 - reconstruction_loss: 103.6759 - kl_loss: 5.5741\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 100.0836 - reconstruction_loss: 94.0461 - kl_loss: 6.0375\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.9628 - reconstruction_loss: 100.9007 - kl_loss: 6.0621\n",
      "Success in episode 7 at time step 200 with reward -290.85073954739306\n",
      "Episode 8\n",
      "[-0.7393225  0.6733515  0.7115626]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3478\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3383\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 144.9822 - reconstruction_loss: 138.7870 - kl_loss: 6.1952\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 133.9714 - reconstruction_loss: 127.7531 - kl_loss: 6.2183\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3615\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3584\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 152.7563 - reconstruction_loss: 145.5518 - kl_loss: 7.2045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 156.4616 - reconstruction_loss: 149.2219 - kl_loss: 7.2397\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3374\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3348\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 174.6986 - reconstruction_loss: 166.8135 - kl_loss: 7.8852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 141.3352 - reconstruction_loss: 133.4518 - kl_loss: 7.8834\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2183\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 152.6639 - reconstruction_loss: 146.5533 - kl_loss: 6.1106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: 211.4150 - reconstruction_loss: 205.2927 - kl_loss: 6.1223\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4830\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 175.9862 - reconstruction_loss: 168.1752 - kl_loss: 7.8110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 105.3066 - reconstruction_loss: 97.4692 - kl_loss: 7.8375\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0842\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 148.3611 - reconstruction_loss: 142.3976 - kl_loss: 5.9635\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.2378 - reconstruction_loss: 68.2714 - kl_loss: 5.9664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1267\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1264\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.7514 - reconstruction_loss: 109.7386 - kl_loss: 6.0128\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 157.4367 - reconstruction_loss: 151.4129 - kl_loss: 6.0238\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1619\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.6739 - reconstruction_loss: 59.3830 - kl_loss: 6.2909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.1025 - reconstruction_loss: 76.8003 - kl_loss: 6.3022\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1013\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1016\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 107.3640 - reconstruction_loss: 101.1687 - kl_loss: 6.1954\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 986us/step - loss: 85.0821 - reconstruction_loss: 78.8786 - kl_loss: 6.2035\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0594\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0573\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.6975 - reconstruction_loss: 86.5627 - kl_loss: 7.1348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 987us/step - loss: 112.6707 - reconstruction_loss: 105.5195 - kl_loss: 7.1512\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0306\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.5579 - reconstruction_loss: 83.1906 - kl_loss: 6.3673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.3225 - reconstruction_loss: 76.9446 - kl_loss: 6.3779\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1291\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1270\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 101.5166 - reconstruction_loss: 95.4082 - kl_loss: 6.1083\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.7302 - reconstruction_loss: 84.6103 - kl_loss: 6.1199\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0265\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0259\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.4256 - reconstruction_loss: 101.2581 - kl_loss: 7.1676\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 44.2154 - reconstruction_loss: 37.0282 - kl_loss: 7.1871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0364\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0365\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.3923 - reconstruction_loss: 52.1872 - kl_loss: 6.2051\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.6001 - reconstruction_loss: 48.3810 - kl_loss: 6.2191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1165\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 86.0905 - reconstruction_loss: 79.5111 - kl_loss: 6.5794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.5205 - reconstruction_loss: 76.9234 - kl_loss: 6.5972\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0489\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0513\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.4206 - reconstruction_loss: 94.1595 - kl_loss: 7.2611\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.8141 - reconstruction_loss: 75.5297 - kl_loss: 7.2844\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1661\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 97.1853 - reconstruction_loss: 90.3353 - kl_loss: 6.8500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.5472 - reconstruction_loss: 97.6628 - kl_loss: 6.8844\n",
      "Success in episode 8 at time step 200 with reward -259.541079954914\n",
      "Episode 9\n",
      "[-0.7725726  -0.63492644  0.3519897 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2851\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2796\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.0675 - reconstruction_loss: 50.5751 - kl_loss: 6.4924\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.5006 - reconstruction_loss: 46.9711 - kl_loss: 6.5295\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1201\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1168\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.5157 - reconstruction_loss: 100.8656 - kl_loss: 6.6501\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 158.5518 - reconstruction_loss: 151.8831 - kl_loss: 6.6687\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2031\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.7471 - reconstruction_loss: 111.1799 - kl_loss: 6.5672\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 116.1090 - reconstruction_loss: 109.5136 - kl_loss: 6.5954\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3569\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 172.5521 - reconstruction_loss: 164.9918 - kl_loss: 7.5604\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.2212 - reconstruction_loss: 59.6330 - kl_loss: 7.5882\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1019\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 123.8246 - reconstruction_loss: 117.8306 - kl_loss: 5.9940\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 129.1026 - reconstruction_loss: 123.1007 - kl_loss: 6.0019\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2413\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2358\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.4089 - reconstruction_loss: 65.3659 - kl_loss: 7.0430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.8475 - reconstruction_loss: 62.7846 - kl_loss: 7.0629\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2592\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2502\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 67.6780 - reconstruction_loss: 60.6059 - kl_loss: 7.0720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86.2404 - reconstruction_loss: 79.1534 - kl_loss: 7.0869\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1953\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 137.5264 - reconstruction_loss: 130.8018 - kl_loss: 6.7246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 105.8339 - reconstruction_loss: 99.0995 - kl_loss: 6.7344\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1859\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1790\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.6173 - reconstruction_loss: 114.0755 - kl_loss: 7.5418\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.6350 - reconstruction_loss: 40.0752 - kl_loss: 7.5598\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1995\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.0893 - reconstruction_loss: 93.1471 - kl_loss: 6.9422\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 904us/step - loss: 103.2181 - reconstruction_loss: 96.2653 - kl_loss: 6.9527\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1749\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1740\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.5270 - reconstruction_loss: 71.3507 - kl_loss: 6.1763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.6072 - reconstruction_loss: 94.4199 - kl_loss: 6.1873\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 104.6760 - reconstruction_loss: 96.7644 - kl_loss: 7.9116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 130.8866 - reconstruction_loss: 122.9578 - kl_loss: 7.9288\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2228\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2214\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.7781 - reconstruction_loss: 59.7526 - kl_loss: 7.0255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 95.2854 - reconstruction_loss: 88.2490 - kl_loss: 7.0365\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1926\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75.7473 - reconstruction_loss: 69.3061 - kl_loss: 6.4411\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.1884 - reconstruction_loss: 51.7353 - kl_loss: 6.4531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1380\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1364\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63.0036 - reconstruction_loss: 55.1352 - kl_loss: 7.8684\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.9496 - reconstruction_loss: 50.0661 - kl_loss: 7.8835\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2165\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2154\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.9607 - reconstruction_loss: 109.9370 - kl_loss: 7.0237\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 146.7757 - reconstruction_loss: 139.7402 - kl_loss: 7.0355\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1826\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 100.4290 - reconstruction_loss: 93.2570 - kl_loss: 7.1720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.6881 - reconstruction_loss: 78.4952 - kl_loss: 7.1929\n",
      "Success in episode 9 at time step 200 with reward -249.12668169787358\n",
      "Episode 10\n",
      "[ 0.598574   -0.80106753  0.37401924]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6160\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6088\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 198.9895 - reconstruction_loss: 189.6537 - kl_loss: 9.3358\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 951us/step - loss: 185.8429 - reconstruction_loss: 176.4773 - kl_loss: 9.3657\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7121\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7115\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 468.1787 - reconstruction_loss: 458.2649 - kl_loss: 9.9139\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 445.5797 - reconstruction_loss: 435.6467 - kl_loss: 9.9330\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7166\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 367.4112 - reconstruction_loss: 357.8979 - kl_loss: 9.5133\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 257.0472 - reconstruction_loss: 247.5340 - kl_loss: 9.5133\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6397\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 326.6247 - reconstruction_loss: 317.0003 - kl_loss: 9.6244\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 394.7878 - reconstruction_loss: 385.1771 - kl_loss: 9.6107\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9555\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 267.7674 - reconstruction_loss: 259.1826 - kl_loss: 8.5848\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 249.5994 - reconstruction_loss: 241.0233 - kl_loss: 8.5761\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5907\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5890\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 373.4406 - reconstruction_loss: 363.2071 - kl_loss: 10.2335\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 384.2400 - reconstruction_loss: 374.0253 - kl_loss: 10.2148\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6802\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6723\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 355.2597 - reconstruction_loss: 346.4225 - kl_loss: 8.8371\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 275.9644 - reconstruction_loss: 267.1425 - kl_loss: 8.8219\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5624\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 318.6229 - reconstruction_loss: 309.3014 - kl_loss: 9.3215\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 413.6375 - reconstruction_loss: 404.3557 - kl_loss: 9.2817\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7256\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7187\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 373.5836 - reconstruction_loss: 364.7870 - kl_loss: 8.7967\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 477.7715 - reconstruction_loss: 468.9918 - kl_loss: 8.7797\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0071\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 329.1456 - reconstruction_loss: 320.1706 - kl_loss: 8.9750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 960us/step - loss: 339.2069 - reconstruction_loss: 330.2666 - kl_loss: 8.9403\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5780\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 329.8195 - reconstruction_loss: 320.6384 - kl_loss: 9.1810\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 355.2841 - reconstruction_loss: 346.1390 - kl_loss: 9.1451\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5676\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5596\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 254.6412 - reconstruction_loss: 245.9654 - kl_loss: 8.6758\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 299.0469 - reconstruction_loss: 290.3915 - kl_loss: 8.6554\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7630\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7506\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 352.5096 - reconstruction_loss: 343.6948 - kl_loss: 8.8148\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 973us/step - loss: 478.1307 - reconstruction_loss: 469.3705 - kl_loss: 8.7602\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6945\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 210.4850 - reconstruction_loss: 202.5323 - kl_loss: 7.9527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: 420.2786 - reconstruction_loss: 412.3371 - kl_loss: 7.9415\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4523\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4488\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 291.2926 - reconstruction_loss: 282.2378 - kl_loss: 9.0548\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 401.7408 - reconstruction_loss: 392.7252 - kl_loss: 9.0156\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6726\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6649\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 383.0925 - reconstruction_loss: 374.4454 - kl_loss: 8.6471\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 980us/step - loss: 447.4130 - reconstruction_loss: 438.8062 - kl_loss: 8.6068\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6390\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 300.6728 - reconstruction_loss: 291.9309 - kl_loss: 8.7419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 313.7597 - reconstruction_loss: 305.0736 - kl_loss: 8.6860\n",
      "Success in episode 10 at time step 200 with reward -223.48339286583533\n",
      "Episode 11\n",
      "[0.5683167  0.8228099  0.30183274]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 222.4407 - reconstruction_loss: 213.8700 - kl_loss: 8.5707\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 981us/step - loss: 257.9885 - reconstruction_loss: 249.3456 - kl_loss: 8.6430\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4852\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 154.9341 - reconstruction_loss: 146.0657 - kl_loss: 8.8684\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 178.5048 - reconstruction_loss: 169.6494 - kl_loss: 8.8554\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8574\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8447\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 212.2483 - reconstruction_loss: 202.9114 - kl_loss: 9.3369\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 197.7512 - reconstruction_loss: 188.4142 - kl_loss: 9.3370\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7007\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6945\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 299.6757 - reconstruction_loss: 289.6229 - kl_loss: 10.0528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 261.1631 - reconstruction_loss: 251.1469 - kl_loss: 10.0162\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6179\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 341.9249 - reconstruction_loss: 332.4275 - kl_loss: 9.4974\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 278.0964 - reconstruction_loss: 268.6265 - kl_loss: 9.4699\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4863\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4780\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 319.0684 - reconstruction_loss: 310.8560 - kl_loss: 8.2124\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 989us/step - loss: 370.3177 - reconstruction_loss: 362.1163 - kl_loss: 8.2014\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1666\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1524\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 227.3558 - reconstruction_loss: 219.2839 - kl_loss: 8.0719\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 866us/step - loss: 228.3919 - reconstruction_loss: 220.3347 - kl_loss: 8.0572\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9086\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8968\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 303.2239 - reconstruction_loss: 295.0572 - kl_loss: 8.1667\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 290.9142 - reconstruction_loss: 282.7629 - kl_loss: 8.1512\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4065\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4017\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.3492 - reconstruction_loss: 146.4970 - kl_loss: 8.8522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 178.5497 - reconstruction_loss: 169.6899 - kl_loss: 8.8598\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6900\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6802\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 301.0677 - reconstruction_loss: 292.9009 - kl_loss: 8.1668\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 374.2379 - reconstruction_loss: 366.0787 - kl_loss: 8.1591\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4845\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 264.3416 - reconstruction_loss: 256.4240 - kl_loss: 7.9176\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 247.4451 - reconstruction_loss: 239.5299 - kl_loss: 7.9152\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8163\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 273.2949 - reconstruction_loss: 266.1248 - kl_loss: 7.1700\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 838us/step - loss: 262.0750 - reconstruction_loss: 254.9115 - kl_loss: 7.1634\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0230\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 365.3120 - reconstruction_loss: 356.1598 - kl_loss: 9.1522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 261.8408 - reconstruction_loss: 252.6917 - kl_loss: 9.1491\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3831\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3821\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 241.7325 - reconstruction_loss: 233.8532 - kl_loss: 7.8793\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 151.1635 - reconstruction_loss: 143.2725 - kl_loss: 7.8909\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6946\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6848\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 182.4204 - reconstruction_loss: 174.8052 - kl_loss: 7.6153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 315.3204 - reconstruction_loss: 307.7088 - kl_loss: 7.6117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5136\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5081\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 271.1779 - reconstruction_loss: 263.1522 - kl_loss: 8.0257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 255.3563 - reconstruction_loss: 247.3214 - kl_loss: 8.0350\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6998\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 238.1472 - reconstruction_loss: 229.5198 - kl_loss: 8.6274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 244.2592 - reconstruction_loss: 235.6205 - kl_loss: 8.6387\n",
      "Success in episode 11 at time step 200 with reward -220.74567454015283\n",
      "Episode 12\n",
      "[ 0.9423352  -0.33467054 -0.7353602 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6115\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.8922 - reconstruction_loss: 126.8618 - kl_loss: 8.0304\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 169.4336 - reconstruction_loss: 161.4164 - kl_loss: 8.0172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 195.0511 - reconstruction_loss: 186.9952 - kl_loss: 8.0559\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 279.1910 - reconstruction_loss: 271.0973 - kl_loss: 8.0937\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6488\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 258.6512 - reconstruction_loss: 251.6787 - kl_loss: 6.9725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 190.2347 - reconstruction_loss: 183.2657 - kl_loss: 6.9690\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7307\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7086\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 307.7232 - reconstruction_loss: 300.2535 - kl_loss: 7.4696\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 218.4665 - reconstruction_loss: 210.9917 - kl_loss: 7.4748\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5194\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5119\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 152.5638 - reconstruction_loss: 144.1855 - kl_loss: 8.3783\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 201.8904 - reconstruction_loss: 193.4813 - kl_loss: 8.4091\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 201.6862 - reconstruction_loss: 193.1784 - kl_loss: 8.5078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 207.1983 - reconstruction_loss: 198.6870 - kl_loss: 8.5113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4289\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 234.8195 - reconstruction_loss: 225.7438 - kl_loss: 9.0757\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 204.2513 - reconstruction_loss: 195.1431 - kl_loss: 9.1082\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5110\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 178.8481 - reconstruction_loss: 171.0424 - kl_loss: 7.8057\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 217.5622 - reconstruction_loss: 209.7491 - kl_loss: 7.8132\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7671\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 311.2263 - reconstruction_loss: 303.4767 - kl_loss: 7.7496\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 262.2073 - reconstruction_loss: 254.4558 - kl_loss: 7.7516\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4775\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4712\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 184.0341 - reconstruction_loss: 175.3578 - kl_loss: 8.6763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 171.3936 - reconstruction_loss: 162.7072 - kl_loss: 8.6863\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8052\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7810\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 222.9017 - reconstruction_loss: 214.9368 - kl_loss: 7.9650\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.7577 - reconstruction_loss: 151.7817 - kl_loss: 7.9759\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5322\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 270.7702 - reconstruction_loss: 261.7774 - kl_loss: 8.9928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 258.7372 - reconstruction_loss: 249.7325 - kl_loss: 9.0047\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3497\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3429\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 147.3079 - reconstruction_loss: 139.0969 - kl_loss: 8.2110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 126.3506 - reconstruction_loss: 118.1376 - kl_loss: 8.2130\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 203.9628 - reconstruction_loss: 196.7958 - kl_loss: 7.1670\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 263.6032 - reconstruction_loss: 256.4343 - kl_loss: 7.1689\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4451\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4392\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 207.5018 - reconstruction_loss: 198.9207 - kl_loss: 8.5811\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 206.6672 - reconstruction_loss: 198.0720 - kl_loss: 8.5951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7826\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 190.5045 - reconstruction_loss: 182.1908 - kl_loss: 8.3137\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 213.2312 - reconstruction_loss: 204.9184 - kl_loss: 8.3127\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5149\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4952\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 200.7816 - reconstruction_loss: 192.4258 - kl_loss: 8.3558\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 205.7496 - reconstruction_loss: 197.3698 - kl_loss: 8.3798\n",
      "Success in episode 12 at time step 200 with reward -231.0411085185199\n",
      "Episode 13\n",
      "[-0.9012274  -0.43334648 -0.849676  ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1270\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1563\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.5189 - reconstruction_loss: 83.2290 - kl_loss: 9.2899\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.4751 - reconstruction_loss: 79.1160 - kl_loss: 9.3591\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7854\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7733\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.2768 - reconstruction_loss: 79.7321 - kl_loss: 9.5447\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.1670 - reconstruction_loss: 122.5541 - kl_loss: 9.6128\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0160\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9841\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 160.4114 - reconstruction_loss: 150.7936 - kl_loss: 9.6178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 103.4632 - reconstruction_loss: 93.7873 - kl_loss: 9.6759\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0339\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9623\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.6209 - reconstruction_loss: 64.0158 - kl_loss: 9.6051\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.6475 - reconstruction_loss: 57.9912 - kl_loss: 9.6562\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 131.7414 - reconstruction_loss: 122.0537 - kl_loss: 9.6876\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 148.0435 - reconstruction_loss: 138.3030 - kl_loss: 9.7405\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.8951 - reconstruction_loss: 64.5131 - kl_loss: 10.3820\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 140.4219 - reconstruction_loss: 129.9948 - kl_loss: 10.4271\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.2485 - reconstruction_loss: 88.6090 - kl_loss: 9.6395\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.4035 - reconstruction_loss: 83.7295 - kl_loss: 9.6740\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9908\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9140\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 142.4951 - reconstruction_loss: 132.5925 - kl_loss: 9.9027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.9684 - reconstruction_loss: 115.0356 - kl_loss: 9.9328\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4831\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.0118 - reconstruction_loss: 57.7485 - kl_loss: 11.2633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.7631 - reconstruction_loss: 54.4633 - kl_loss: 11.2997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3009\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.5468 - reconstruction_loss: 100.8069 - kl_loss: 9.7399\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 973us/step - loss: 78.0953 - reconstruction_loss: 68.3371 - kl_loss: 9.7582\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4672\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.4104 - reconstruction_loss: 111.8725 - kl_loss: 10.5380\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 149.3612 - reconstruction_loss: 138.8064 - kl_loss: 10.5548\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2953\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2729\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.7286 - reconstruction_loss: 31.2294 - kl_loss: 10.4992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 946us/step - loss: 42.1419 - reconstruction_loss: 31.6288 - kl_loss: 10.5131\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0629\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0764\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.7428 - reconstruction_loss: 57.7934 - kl_loss: 8.9494\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.1080 - reconstruction_loss: 70.1492 - kl_loss: 8.9588\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4898\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4346\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 108.3839 - reconstruction_loss: 98.0867 - kl_loss: 10.2972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 140.5861 - reconstruction_loss: 130.2922 - kl_loss: 10.2939\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2554\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.9854 - reconstruction_loss: 46.3935 - kl_loss: 10.5919\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.1866 - reconstruction_loss: 34.5885 - kl_loss: 10.5980\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1254\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.7420 - reconstruction_loss: 64.1080 - kl_loss: 10.6339\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 987us/step - loss: 124.5990 - reconstruction_loss: 113.9548 - kl_loss: 10.6442\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3546\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.0223 - reconstruction_loss: 72.6706 - kl_loss: 10.3517\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.3586 - reconstruction_loss: 64.9500 - kl_loss: 10.4086\n",
      "Success in episode 13 at time step 200 with reward -281.91426484788894\n",
      "Episode 14\n",
      "[ 0.9997291   0.02327538 -0.71118593]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6382\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6272\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 246.8883 - reconstruction_loss: 236.0010 - kl_loss: 10.8872\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 854us/step - loss: 186.6609 - reconstruction_loss: 175.7229 - kl_loss: 10.9380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 162.8993 - reconstruction_loss: 154.3703 - kl_loss: 8.5289\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 181.1439 - reconstruction_loss: 172.5768 - kl_loss: 8.5671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6777\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6721\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 289.2311 - reconstruction_loss: 279.4070 - kl_loss: 9.8242\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 156.2161 - reconstruction_loss: 146.3630 - kl_loss: 9.8531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1918\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 158.0067 - reconstruction_loss: 146.9682 - kl_loss: 11.0385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 139.8192 - reconstruction_loss: 128.7622 - kl_loss: 11.0570\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0117\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0007\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 178.8408 - reconstruction_loss: 168.8547 - kl_loss: 9.9860\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 404.5981 - reconstruction_loss: 394.5695 - kl_loss: 10.0286\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9446\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 139.5975 - reconstruction_loss: 128.8206 - kl_loss: 10.7769\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.7530 - reconstruction_loss: 148.9679 - kl_loss: 10.7851\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9179\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9171\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.7245 - reconstruction_loss: 112.7225 - kl_loss: 10.0020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 207.0806 - reconstruction_loss: 197.0357 - kl_loss: 10.0450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8602\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8360\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 105.7980 - reconstruction_loss: 94.9242 - kl_loss: 10.8738\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 206.8529 - reconstruction_loss: 195.9646 - kl_loss: 10.8883\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6391\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 171.2338 - reconstruction_loss: 162.8490 - kl_loss: 8.3847\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 113.0842 - reconstruction_loss: 104.6796 - kl_loss: 8.4046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9044\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8826\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.3660 - reconstruction_loss: 102.7155 - kl_loss: 9.6505\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: 95.9034 - reconstruction_loss: 86.2316 - kl_loss: 9.6718\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7859\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7760\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 232.8011 - reconstruction_loss: 223.3716 - kl_loss: 9.4295\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 975us/step - loss: 122.7487 - reconstruction_loss: 113.3056 - kl_loss: 9.4431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 138.8217 - reconstruction_loss: 128.7913 - kl_loss: 10.0304\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 220.7892 - reconstruction_loss: 210.7180 - kl_loss: 10.0712\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7964\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7744\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 128.1871 - reconstruction_loss: 118.3011 - kl_loss: 9.8860\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 164.2012 - reconstruction_loss: 154.3236 - kl_loss: 9.8776\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0056\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0020\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.5038 - reconstruction_loss: 69.2888 - kl_loss: 10.2151\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 255.1679 - reconstruction_loss: 244.9342 - kl_loss: 10.2337\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4974\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4819\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 137.3798 - reconstruction_loss: 126.4295 - kl_loss: 10.9503\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 161.6133 - reconstruction_loss: 150.6535 - kl_loss: 10.9598\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6631\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6509\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 179.3673 - reconstruction_loss: 170.2144 - kl_loss: 9.1530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 855us/step - loss: 162.4771 - reconstruction_loss: 153.3099 - kl_loss: 9.1673\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8397\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 173.9489 - reconstruction_loss: 163.5342 - kl_loss: 10.4146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 188.1172 - reconstruction_loss: 177.6451 - kl_loss: 10.4721\n",
      "Success in episode 14 at time step 200 with reward -206.79284984220558\n",
      "Episode 15\n",
      "[ 0.04014215 -0.99919397  0.13937366]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9200\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.3008 - reconstruction_loss: 80.6527 - kl_loss: 9.6481\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 139.7632 - reconstruction_loss: 130.1406 - kl_loss: 9.6226\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8935\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8065\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.6288 - reconstruction_loss: 26.8721 - kl_loss: 14.7567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.9798 - reconstruction_loss: 48.1306 - kl_loss: 14.8492\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6539\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 228.2432 - reconstruction_loss: 213.7642 - kl_loss: 14.4790\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 199.3868 - reconstruction_loss: 184.8215 - kl_loss: 14.5653\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0396\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0433\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.0423 - reconstruction_loss: 88.5137 - kl_loss: 9.5287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.7056 - reconstruction_loss: 83.1569 - kl_loss: 9.5486\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7764\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7270\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.0929 - reconstruction_loss: 31.4534 - kl_loss: 11.6395\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.6508 - reconstruction_loss: 53.9858 - kl_loss: 11.6650\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.5055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.3370 - reconstruction_loss: 119.3464 - kl_loss: 14.9906\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.5987 - reconstruction_loss: 96.5437 - kl_loss: 15.0549\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9236\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9303\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 177.7289 - reconstruction_loss: 165.6082 - kl_loss: 12.1207\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 330.2237 - reconstruction_loss: 318.0789 - kl_loss: 12.1448\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7193\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 163.8575 - reconstruction_loss: 154.2505 - kl_loss: 9.6070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.9835 - reconstruction_loss: 103.3626 - kl_loss: 9.6209\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0439\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.1864 - reconstruction_loss: 63.4640 - kl_loss: 16.7225\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.7195 - reconstruction_loss: 74.9174 - kl_loss: 16.8021\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8675\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8429\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 142.5641 - reconstruction_loss: 129.3459 - kl_loss: 13.2183\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 163.8042 - reconstruction_loss: 150.5452 - kl_loss: 13.2591\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2693\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.4755 - reconstruction_loss: 112.7107 - kl_loss: 9.7648\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 152.4810 - reconstruction_loss: 142.6876 - kl_loss: 9.7933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2885\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 63.8677 - reconstruction_loss: 51.8801 - kl_loss: 11.9877\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.7308 - reconstruction_loss: 67.7082 - kl_loss: 12.0226\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6860\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.7280 - reconstruction_loss: 77.2980 - kl_loss: 16.4300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.3111 - reconstruction_loss: 39.7936 - kl_loss: 16.5176\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0220\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 142.3775 - reconstruction_loss: 130.3684 - kl_loss: 12.0092\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 133.2191 - reconstruction_loss: 121.1926 - kl_loss: 12.0265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4383\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4217\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.1329 - reconstruction_loss: 109.9740 - kl_loss: 10.1589\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 81.7652 - reconstruction_loss: 71.5836 - kl_loss: 10.1817\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3132\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.7902 - reconstruction_loss: 79.1105 - kl_loss: 14.6798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.2308 - reconstruction_loss: 55.4907 - kl_loss: 14.7401\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3051\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 116.1632 - reconstruction_loss: 102.8481 - kl_loss: 13.3150\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.3725 - reconstruction_loss: 108.9956 - kl_loss: 13.3769\n",
      "Success in episode 15 at time step 200 with reward -185.29333293601016\n",
      "Episode 16\n",
      "[ 0.54513985 -0.8383451   0.52609897]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3885\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3634\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.0852 - reconstruction_loss: 106.9025 - kl_loss: 9.1827\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 136.1801 - reconstruction_loss: 126.9353 - kl_loss: 9.2448\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0362\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.8489 - reconstruction_loss: 105.9091 - kl_loss: 14.9398\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.0555 - reconstruction_loss: 56.0386 - kl_loss: 15.0169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0455\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.4093 - reconstruction_loss: 89.2530 - kl_loss: 10.1563\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 160.0309 - reconstruction_loss: 149.8352 - kl_loss: 10.1957\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8317\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.6733 - reconstruction_loss: 97.5873 - kl_loss: 14.0860\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 288.9004 - reconstruction_loss: 274.7704 - kl_loss: 14.1300\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8277\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 169.0273 - reconstruction_loss: 158.4959 - kl_loss: 10.5314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 183.9541 - reconstruction_loss: 173.3753 - kl_loss: 10.5788\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5736\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5719\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.7523 - reconstruction_loss: 69.1969 - kl_loss: 11.5553\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 157.3293 - reconstruction_loss: 145.7368 - kl_loss: 11.5924\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3791\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3567\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103.8034 - reconstruction_loss: 91.8640 - kl_loss: 11.9394\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 126.6275 - reconstruction_loss: 114.6827 - kl_loss: 11.9449\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2692\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2359\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 114.0170 - reconstruction_loss: 102.6989 - kl_loss: 11.3181\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.7377 - reconstruction_loss: 154.3891 - kl_loss: 11.3486\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7201\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.8388 - reconstruction_loss: 82.4726 - kl_loss: 11.3662\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.0830 - reconstruction_loss: 110.7046 - kl_loss: 11.3783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6870\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.1685 - reconstruction_loss: 123.2755 - kl_loss: 8.8930\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.2692 - reconstruction_loss: 123.3663 - kl_loss: 8.9029\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7183\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.9106 - reconstruction_loss: 96.1671 - kl_loss: 14.7435\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.2332 - reconstruction_loss: 85.4475 - kl_loss: 14.7857\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0272\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9909\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 107.8957 - reconstruction_loss: 95.7452 - kl_loss: 12.1505\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.6253 - reconstruction_loss: 89.4660 - kl_loss: 12.1593\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3889\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.0415 - reconstruction_loss: 108.6388 - kl_loss: 13.4027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.3624 - reconstruction_loss: 84.9456 - kl_loss: 13.4168\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8811\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.6898 - reconstruction_loss: 112.8471 - kl_loss: 11.8427\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 163.6061 - reconstruction_loss: 151.7550 - kl_loss: 11.8511\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7114\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6588\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 130.6720 - reconstruction_loss: 119.8212 - kl_loss: 10.8509\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 109.8924 - reconstruction_loss: 99.0228 - kl_loss: 10.8696\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.3615 - reconstruction_loss: 78.8292 - kl_loss: 12.5324\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.7814 - reconstruction_loss: 87.2232 - kl_loss: 12.5582\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4737\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4189\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 129.9142 - reconstruction_loss: 117.6608 - kl_loss: 12.2533\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 138.9630 - reconstruction_loss: 126.6802 - kl_loss: 12.2828\n",
      "Success in episode 16 at time step 200 with reward -214.2208972801054\n",
      "Episode 17\n",
      "[ 0.943784   -0.33056274  0.00605112]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8780\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.6260 - reconstruction_loss: 45.8229 - kl_loss: 13.8031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 943us/step - loss: 314.4003 - reconstruction_loss: 300.4390 - kl_loss: 13.9612\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1074\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 108.3251 - reconstruction_loss: 96.4987 - kl_loss: 11.8264\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.1189 - reconstruction_loss: 81.3496 - kl_loss: 11.7693\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1176\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1297\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.9061 - reconstruction_loss: 142.2657 - kl_loss: 17.6403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 85.6880 - reconstruction_loss: 68.0885 - kl_loss: 17.5994\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0373\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.0349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 473.7883 - reconstruction_loss: 458.6862 - kl_loss: 15.1022\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 268.8000 - reconstruction_loss: 253.6788 - kl_loss: 15.1211\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5085\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5141\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 271.2773 - reconstruction_loss: 255.0935 - kl_loss: 16.1839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 307.4614 - reconstruction_loss: 291.2709 - kl_loss: 16.1906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5718\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.5312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 241.8160 - reconstruction_loss: 228.3400 - kl_loss: 13.4760\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 169.8066 - reconstruction_loss: 156.2966 - kl_loss: 13.5100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9252\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9288\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102.8357 - reconstruction_loss: 92.0281 - kl_loss: 10.8076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.9531 - reconstruction_loss: 73.1115 - kl_loss: 10.8416\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0581\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58.7162 - reconstruction_loss: 42.0783 - kl_loss: 16.6379\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 136.8394 - reconstruction_loss: 120.2016 - kl_loss: 16.6378\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3365\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 147.8022 - reconstruction_loss: 139.7961 - kl_loss: 8.0060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 153.9616 - reconstruction_loss: 145.9391 - kl_loss: 8.0225\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1624\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1413\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.6870 - reconstruction_loss: 75.0164 - kl_loss: 13.6706\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 163.9332 - reconstruction_loss: 150.2473 - kl_loss: 13.6860\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9707\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 113.4855 - reconstruction_loss: 101.4222 - kl_loss: 12.0633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 181.8151 - reconstruction_loss: 169.7298 - kl_loss: 12.0853\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0841\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0684\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 99.7495 - reconstruction_loss: 87.6626 - kl_loss: 12.0869\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 953us/step - loss: 133.0646 - reconstruction_loss: 120.9635 - kl_loss: 12.1011\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1054\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0670\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.7298 - reconstruction_loss: 78.3552 - kl_loss: 16.3745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.8666 - reconstruction_loss: 73.4325 - kl_loss: 16.4341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8757\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.0820 - reconstruction_loss: 116.9808 - kl_loss: 11.1012\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 123.1740 - reconstruction_loss: 112.0586 - kl_loss: 11.1154\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.2298 - reconstruction_loss: 61.4211 - kl_loss: 13.8087\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.8958 - reconstruction_loss: 59.0518 - kl_loss: 13.8440\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2845\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2636\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 88.1008 - reconstruction_loss: 76.6714 - kl_loss: 11.4294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.6830 - reconstruction_loss: 104.2197 - kl_loss: 11.4633\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3782\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 137.0167 - reconstruction_loss: 123.1321 - kl_loss: 13.8846\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.7961 - reconstruction_loss: 110.8463 - kl_loss: 13.9498\n",
      "Success in episode 17 at time step 200 with reward -207.1301127500055\n",
      "Episode 18\n",
      "[-0.98009473  0.19853032  0.5449126 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 5.0111\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.9855\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 105.2373 - reconstruction_loss: 90.7925 - kl_loss: 14.4448\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.4727 - reconstruction_loss: 64.0021 - kl_loss: 14.4706\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.6328\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.0883 - reconstruction_loss: 20.9877 - kl_loss: 13.1006\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.0322 - reconstruction_loss: 5.9093 - kl_loss: 13.1230\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.0819\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.9374 - reconstruction_loss: 53.7816 - kl_loss: 13.1557\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.8397 - reconstruction_loss: 45.6372 - kl_loss: 13.2025\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6660\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5364\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.8736 - reconstruction_loss: 102.1743 - kl_loss: 14.6993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.4446 - reconstruction_loss: 43.7255 - kl_loss: 14.7191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2509\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.3114 - reconstruction_loss: 16.4370 - kl_loss: 12.8745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 933us/step - loss: 36.6939 - reconstruction_loss: 23.7960 - kl_loss: 12.8979\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.8683\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58.1546 - reconstruction_loss: 43.9509 - kl_loss: 14.2037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.1601 - reconstruction_loss: 60.9287 - kl_loss: 14.2314\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0643\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9445\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.7576 - reconstruction_loss: 61.9170 - kl_loss: 12.8406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.0246 - reconstruction_loss: 49.1682 - kl_loss: 12.8564\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2777\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51.8832 - reconstruction_loss: 38.8296 - kl_loss: 13.0537\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.6796 - reconstruction_loss: 36.6033 - kl_loss: 13.0763\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3309\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0784\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.2903 - reconstruction_loss: 19.5844 - kl_loss: 13.7060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.6212 - reconstruction_loss: 45.8945 - kl_loss: 13.7266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3451\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.1372 - reconstruction_loss: 52.9689 - kl_loss: 15.1683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.3712 - reconstruction_loss: 95.1816 - kl_loss: 15.1896\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3866\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2978\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.1455 - reconstruction_loss: 24.1483 - kl_loss: 13.9973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 870us/step - loss: 32.4287 - reconstruction_loss: 18.4290 - kl_loss: 13.9997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0969\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9365\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.6505 - reconstruction_loss: 42.6819 - kl_loss: 13.9685\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.6172 - reconstruction_loss: 14.6256 - kl_loss: 13.9915\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3132\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2476\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.6436 - reconstruction_loss: 46.0858 - kl_loss: 13.5578\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.9419 - reconstruction_loss: 20.3661 - kl_loss: 13.5758\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7889\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8536\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.7349 - reconstruction_loss: 34.8148 - kl_loss: 13.9202\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.9287 - reconstruction_loss: 35.0141 - kl_loss: 13.9147\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2063\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2328\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.8583 - reconstruction_loss: 14.1728 - kl_loss: 13.6855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.3593 - reconstruction_loss: 25.6751 - kl_loss: 13.6842\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.5841 - reconstruction_loss: 46.7227 - kl_loss: 14.8613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.7712 - reconstruction_loss: 31.9007 - kl_loss: 14.8705\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7936\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7751\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 47.3659 - reconstruction_loss: 33.4149 - kl_loss: 13.9511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.5169 - reconstruction_loss: 33.5237 - kl_loss: 13.9932\n",
      "Success in episode 18 at time step 200 with reward -290.65966482803066\n",
      "Episode 19\n",
      "[ 0.7218434  0.6920564 -0.7608177]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8161\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7708\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 199.2244 - reconstruction_loss: 180.2052 - kl_loss: 19.0192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 284.1270 - reconstruction_loss: 265.0570 - kl_loss: 19.0701\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1624\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 144.1448 - reconstruction_loss: 133.2053 - kl_loss: 10.9395\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.2602 - reconstruction_loss: 114.3138 - kl_loss: 10.9464\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3668\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3525\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.1013 - reconstruction_loss: 63.8306 - kl_loss: 14.2707\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 954us/step - loss: 80.2582 - reconstruction_loss: 65.9514 - kl_loss: 14.3068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8259\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7475\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.6272 - reconstruction_loss: 68.1938 - kl_loss: 21.4334\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.6136 - reconstruction_loss: 63.1964 - kl_loss: 21.4172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 195.8380 - reconstruction_loss: 180.4045 - kl_loss: 15.4335\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 242.4592 - reconstruction_loss: 226.9827 - kl_loss: 15.4765\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3383\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3147\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73.0196 - reconstruction_loss: 62.4439 - kl_loss: 10.5757\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.5890 - reconstruction_loss: 67.9847 - kl_loss: 10.6043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.6631\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.9031 - reconstruction_loss: 47.6753 - kl_loss: 18.2278\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.8912 - reconstruction_loss: 50.6571 - kl_loss: 18.2341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3139\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2821\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 156.2935 - reconstruction_loss: 137.8854 - kl_loss: 18.4081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.1110 - reconstruction_loss: 103.6920 - kl_loss: 18.4189\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4248\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3895\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 144.4708 - reconstruction_loss: 132.5393 - kl_loss: 11.9315\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86.9606 - reconstruction_loss: 74.9942 - kl_loss: 11.9664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5751\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5322\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.4691 - reconstruction_loss: 57.3182 - kl_loss: 13.1509\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 95.9659 - reconstruction_loss: 82.8030 - kl_loss: 13.1629\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0401\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.9009\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.0450 - reconstruction_loss: 76.1317 - kl_loss: 19.9133\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 822us/step - loss: 69.3554 - reconstruction_loss: 49.5017 - kl_loss: 19.8537\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5451\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5470\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.3046 - reconstruction_loss: 149.0046 - kl_loss: 16.3000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 162.4088 - reconstruction_loss: 146.0837 - kl_loss: 16.3251\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1537\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 82.3434 - reconstruction_loss: 72.0117 - kl_loss: 10.3317\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.0701 - reconstruction_loss: 121.7053 - kl_loss: 10.3648\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4090\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.3261\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.2667 - reconstruction_loss: 73.5282 - kl_loss: 14.7385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.9353 - reconstruction_loss: 75.2099 - kl_loss: 14.7254\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9501\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 145.1231 - reconstruction_loss: 128.3508 - kl_loss: 16.7723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 894us/step - loss: 241.6439 - reconstruction_loss: 224.9294 - kl_loss: 16.7145\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2819\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.0885 - reconstruction_loss: 135.9305 - kl_loss: 19.1580\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 135.4015 - reconstruction_loss: 116.2684 - kl_loss: 19.1331\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4815\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 109.5982 - reconstruction_loss: 93.8690 - kl_loss: 15.7292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.4793 - reconstruction_loss: 88.7417 - kl_loss: 15.7375\n",
      "Success in episode 19 at time step 200 with reward -173.55389153086836\n",
      "Episode 20\n",
      "[ 0.97374743 -0.22763118  0.98507905]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7601\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.0007 - reconstruction_loss: 99.7695 - kl_loss: 16.2312\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 151.2134 - reconstruction_loss: 135.0626 - kl_loss: 16.1508\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6755\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6260\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 152.6678 - reconstruction_loss: 133.4655 - kl_loss: 19.2023\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 186.3680 - reconstruction_loss: 167.0994 - kl_loss: 19.2687\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4059\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.1105 - reconstruction_loss: 36.2482 - kl_loss: 11.8623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.8940 - reconstruction_loss: 90.9426 - kl_loss: 11.9513\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3370\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.9003 - reconstruction_loss: 104.8680 - kl_loss: 14.0323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.6931 - reconstruction_loss: 94.6810 - kl_loss: 14.0121\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4946\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.4612\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 168.2522 - reconstruction_loss: 151.6982 - kl_loss: 16.5540\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: 96.6710 - reconstruction_loss: 80.1450 - kl_loss: 16.5260\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2611\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 168.4532 - reconstruction_loss: 158.2993 - kl_loss: 10.1539\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 142.8099 - reconstruction_loss: 132.6162 - kl_loss: 10.1937\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0891\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0822\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.5578 - reconstruction_loss: 25.0549 - kl_loss: 19.5029\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.0943 - reconstruction_loss: 40.5677 - kl_loss: 19.5266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0691\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0782\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.1018 - reconstruction_loss: 113.1324 - kl_loss: 11.9694\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 123.0377 - reconstruction_loss: 111.0858 - kl_loss: 11.9518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9167\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8466\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.6574 - reconstruction_loss: 98.3964 - kl_loss: 14.2610\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.0369 - reconstruction_loss: 105.7656 - kl_loss: 14.2712\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7740\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6922\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 97.5771 - reconstruction_loss: 82.6734 - kl_loss: 14.9037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 146.4735 - reconstruction_loss: 131.5611 - kl_loss: 14.9124\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4978\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4588\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 170.2744 - reconstruction_loss: 159.2699 - kl_loss: 11.0045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.9723 - reconstruction_loss: 148.9622 - kl_loss: 11.0101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7327\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.5954\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.9854 - reconstruction_loss: 60.2990 - kl_loss: 17.6864\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.2542 - reconstruction_loss: 48.5554 - kl_loss: 17.6989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8237\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7718\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.3525 - reconstruction_loss: 118.8149 - kl_loss: 13.5377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.7875 - reconstruction_loss: 104.2585 - kl_loss: 13.5290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1321\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.9540 - reconstruction_loss: 86.8946 - kl_loss: 15.0595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.9098 - reconstruction_loss: 109.8613 - kl_loss: 15.0485\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9288\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.9587 - reconstruction_loss: 72.7012 - kl_loss: 17.2574\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 226.6834 - reconstruction_loss: 209.4071 - kl_loss: 17.2763\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.7690\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 160.0008 - reconstruction_loss: 149.1559 - kl_loss: 10.8449\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.0340 - reconstruction_loss: 154.2026 - kl_loss: 10.8314\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5316\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4498\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 124.3060 - reconstruction_loss: 109.4783 - kl_loss: 14.8276\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.3088 - reconstruction_loss: 107.4585 - kl_loss: 14.8502\n",
      "Success in episode 20 at time step 200 with reward -203.28402567881204\n",
      "Episode 21\n",
      "[-0.6359533   0.7717275  -0.16549388]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2901\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.8568 - reconstruction_loss: 31.3430 - kl_loss: 15.5138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.3062 - reconstruction_loss: 32.8002 - kl_loss: 15.5060\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9474\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9673\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.8274 - reconstruction_loss: 43.0053 - kl_loss: 13.8222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.5180 - reconstruction_loss: 21.7366 - kl_loss: 13.7815\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4413\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.1269 - reconstruction_loss: 33.2007 - kl_loss: 14.9262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.9350 - reconstruction_loss: 35.0284 - kl_loss: 14.9067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2616\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2555\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.8634 - reconstruction_loss: 67.2073 - kl_loss: 15.6562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 827us/step - loss: 103.5057 - reconstruction_loss: 87.8554 - kl_loss: 15.6503\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2991\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.6425 - reconstruction_loss: 14.8628 - kl_loss: 12.7796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.6946 - reconstruction_loss: 34.9165 - kl_loss: 12.7782\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9843\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.6570 - reconstruction_loss: 53.1894 - kl_loss: 13.4675\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.2102 - reconstruction_loss: 45.7450 - kl_loss: 13.4652\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3058\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2885\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.9758 - reconstruction_loss: 96.0705 - kl_loss: 20.9054\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 863us/step - loss: 102.7148 - reconstruction_loss: 81.7865 - kl_loss: 20.9282\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0914\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.0681\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 140.7865 - reconstruction_loss: 124.7524 - kl_loss: 16.0341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 969us/step - loss: 116.6183 - reconstruction_loss: 100.5751 - kl_loss: 16.0432\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1637\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1609\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.4096 - reconstruction_loss: 77.7285 - kl_loss: 9.6811\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.2575 - reconstruction_loss: 81.5764 - kl_loss: 9.6810\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1899\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.4803 - reconstruction_loss: 46.5777 - kl_loss: 17.9025\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.6965 - reconstruction_loss: 26.7587 - kl_loss: 17.9378\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9966\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.9099\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 84.8199 - reconstruction_loss: 66.8736 - kl_loss: 17.9463\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.2899 - reconstruction_loss: 19.3593 - kl_loss: 17.9306\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8170\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8035\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.7968 - reconstruction_loss: 89.3697 - kl_loss: 13.4271\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.3151 - reconstruction_loss: 100.8722 - kl_loss: 13.4429\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8760\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8790\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.9171 - reconstruction_loss: 101.5860 - kl_loss: 14.3311\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.2702 - reconstruction_loss: 109.9425 - kl_loss: 14.3278\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.8973 - reconstruction_loss: 52.3378 - kl_loss: 17.5595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 980us/step - loss: 76.9418 - reconstruction_loss: 59.3441 - kl_loss: 17.5978\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7225\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 149.3075 - reconstruction_loss: 138.8610 - kl_loss: 10.4465\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 150.0405 - reconstruction_loss: 139.5914 - kl_loss: 10.4491\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.4073\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 72.0423 - reconstruction_loss: 51.8463 - kl_loss: 20.1961\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.3393 - reconstruction_loss: 47.1207 - kl_loss: 20.2186\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3916\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 89.7095 - reconstruction_loss: 74.5365 - kl_loss: 15.1730\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.1250 - reconstruction_loss: 63.9535 - kl_loss: 15.1715\n",
      "Success in episode 21 at time step 200 with reward -220.59241842190855\n",
      "Episode 22\n",
      "[-0.283417  -0.9589968 -0.2795074]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9260\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.6951 - reconstruction_loss: 28.1873 - kl_loss: 10.5078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.1474 - reconstruction_loss: 33.6333 - kl_loss: 10.5141\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.9500\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.7380 - reconstruction_loss: 45.8829 - kl_loss: 16.8552\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.7862 - reconstruction_loss: 35.9017 - kl_loss: 16.8846\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5119\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.1776 - reconstruction_loss: 34.8770 - kl_loss: 13.3005\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.0268 - reconstruction_loss: 42.7301 - kl_loss: 13.2967\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2632\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.6438 - reconstruction_loss: 7.2039 - kl_loss: 12.4399\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.4476 - reconstruction_loss: 11.9899 - kl_loss: 12.4576\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2510\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.3422 - reconstruction_loss: 38.1974 - kl_loss: 16.1449\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.9347 - reconstruction_loss: 20.7678 - kl_loss: 16.1669\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.3457 - reconstruction_loss: 27.1282 - kl_loss: 16.2175\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.2738 - reconstruction_loss: 19.0519 - kl_loss: 16.2219\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3665\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.6351 - reconstruction_loss: 25.7113 - kl_loss: 13.9239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.8669 - reconstruction_loss: 14.9237 - kl_loss: 13.9433\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3407\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.9674 - reconstruction_loss: 33.4821 - kl_loss: 17.4853\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.9391 - reconstruction_loss: 29.4467 - kl_loss: 17.4924\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7324\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.4977 - reconstruction_loss: 6.6656 - kl_loss: 12.8320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.7892 - reconstruction_loss: 11.9609 - kl_loss: 12.8283\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4361\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3029\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.4803 - reconstruction_loss: 40.9926 - kl_loss: 14.4877\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.4239 - reconstruction_loss: 20.9103 - kl_loss: 14.5135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2018\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2125\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.8493 - reconstruction_loss: 49.2095 - kl_loss: 16.6397\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.7577 - reconstruction_loss: 25.1077 - kl_loss: 16.6500\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3535\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3255\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.9462 - reconstruction_loss: 32.6221 - kl_loss: 14.3241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.3367 - reconstruction_loss: 15.0023 - kl_loss: 14.3344\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.1426 - reconstruction_loss: 15.6967 - kl_loss: 15.4459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.0304 - reconstruction_loss: 36.5550 - kl_loss: 15.4755\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0364\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0720\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.2172 - reconstruction_loss: 14.1028 - kl_loss: 17.1144\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51.9063 - reconstruction_loss: 34.7838 - kl_loss: 17.1225\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8216\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7782\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.9294 - reconstruction_loss: 10.1738 - kl_loss: 14.7556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.3905 - reconstruction_loss: 16.6245 - kl_loss: 14.7660\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.6301 - reconstruction_loss: 14.0040 - kl_loss: 16.6261\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.3562 - reconstruction_loss: 11.7091 - kl_loss: 16.6471\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.2795 - reconstruction_loss: 24.1451 - kl_loss: 15.1344\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.5765 - reconstruction_loss: 22.4364 - kl_loss: 15.1402\n",
      "Success in episode 22 at time step 200 with reward -264.51520045925787\n",
      "Episode 23\n",
      "[ 0.40757334 -0.9131725  -0.81314456]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4543\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.8884 - reconstruction_loss: 74.9488 - kl_loss: 9.9397\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 95.4447 - reconstruction_loss: 85.4864 - kl_loss: 9.9583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9460\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9200\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.0461 - reconstruction_loss: 31.5872 - kl_loss: 18.4588\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.9740 - reconstruction_loss: 20.4911 - kl_loss: 18.4830\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7982\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7607\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.6506 - reconstruction_loss: 85.7873 - kl_loss: 13.8633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.6921 - reconstruction_loss: 65.8009 - kl_loss: 13.8913\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8767\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102.5175 - reconstruction_loss: 87.1166 - kl_loss: 15.4010\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.9227 - reconstruction_loss: 102.5109 - kl_loss: 15.4117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6380\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.0478 - reconstruction_loss: 98.6374 - kl_loss: 16.4103\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.8464 - reconstruction_loss: 94.4291 - kl_loss: 16.4173\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.6184\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.3468 - reconstruction_loss: 35.3493 - kl_loss: 20.9975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 952us/step - loss: 72.5877 - reconstruction_loss: 51.5840 - kl_loss: 21.0037\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.3528\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 126.1508 - reconstruction_loss: 113.1716 - kl_loss: 12.9792\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.4210 - reconstruction_loss: 112.4469 - kl_loss: 12.9742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0343\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 71.7519 - reconstruction_loss: 55.0830 - kl_loss: 16.6689\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.2506 - reconstruction_loss: 68.6027 - kl_loss: 16.6479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6455\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6124\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.1132 - reconstruction_loss: 78.0517 - kl_loss: 14.0614\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.5163 - reconstruction_loss: 104.4742 - kl_loss: 14.0421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.4155 - reconstruction_loss: 143.3226 - kl_loss: 12.0928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.3669 - reconstruction_loss: 116.2940 - kl_loss: 12.0729\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2059\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0910\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.5063 - reconstruction_loss: 38.1569 - kl_loss: 20.3494\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 885us/step - loss: 60.8869 - reconstruction_loss: 40.5672 - kl_loss: 20.3197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7442\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.6673 - reconstruction_loss: 97.5067 - kl_loss: 14.1605\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.4372 - reconstruction_loss: 103.2900 - kl_loss: 14.1472\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0468\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.7107 - reconstruction_loss: 67.5912 - kl_loss: 13.1194\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.3274 - reconstruction_loss: 66.2321 - kl_loss: 13.0954\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5251\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4917\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.9671 - reconstruction_loss: 115.0308 - kl_loss: 16.9362\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.5864 - reconstruction_loss: 52.6702 - kl_loss: 16.9162\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1149\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0156\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 144.7327 - reconstruction_loss: 131.6514 - kl_loss: 13.0813\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 155.0975 - reconstruction_loss: 142.0377 - kl_loss: 13.0598\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3498\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.6887 - reconstruction_loss: 35.1449 - kl_loss: 18.5438\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.4950 - reconstruction_loss: 34.9682 - kl_loss: 18.5268\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6531\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5875\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 86.8994 - reconstruction_loss: 71.9277 - kl_loss: 14.9716\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.9406 - reconstruction_loss: 77.0048 - kl_loss: 14.9359\n",
      "Success in episode 23 at time step 200 with reward -235.00583627900374\n",
      "Episode 24\n",
      "[ 0.61061484  0.7919277  -0.7446009 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5482\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.5637\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 166.9162 - reconstruction_loss: 145.8853 - kl_loss: 21.0309\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 249.0297 - reconstruction_loss: 228.0810 - kl_loss: 20.9487\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 307.9224 - reconstruction_loss: 292.8091 - kl_loss: 15.1133\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 401.5554 - reconstruction_loss: 386.4693 - kl_loss: 15.0861\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1231\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0700\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 183.5993 - reconstruction_loss: 161.6075 - kl_loss: 21.9918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 247.3832 - reconstruction_loss: 225.4076 - kl_loss: 21.9756\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.3262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0567\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.1972 - reconstruction_loss: 64.0499 - kl_loss: 17.1473\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.6594 - reconstruction_loss: 67.5551 - kl_loss: 17.1043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1437\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1656\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.4130 - reconstruction_loss: 109.3480 - kl_loss: 13.0650\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.6476 - reconstruction_loss: 91.5946 - kl_loss: 13.0531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0955\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.1125 - reconstruction_loss: 75.2231 - kl_loss: 16.8894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.8230 - reconstruction_loss: 54.9220 - kl_loss: 16.9010\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9521\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8982\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.8461 - reconstruction_loss: 88.8736 - kl_loss: 12.9726\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 954us/step - loss: 93.3580 - reconstruction_loss: 80.3858 - kl_loss: 12.9722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5593\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.0837 - reconstruction_loss: 78.0915 - kl_loss: 13.9922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.4037 - reconstruction_loss: 92.4187 - kl_loss: 13.9849\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6585\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6559\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 43.2047 - reconstruction_loss: 23.6704 - kl_loss: 19.5343\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.8733 - reconstruction_loss: 39.3408 - kl_loss: 19.5325\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4942\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.0738 - reconstruction_loss: 92.7805 - kl_loss: 13.2933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.8804 - reconstruction_loss: 88.5986 - kl_loss: 13.2818\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0868\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 87.1917 - reconstruction_loss: 71.3695 - kl_loss: 15.8222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.6861 - reconstruction_loss: 74.8877 - kl_loss: 15.7984\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5263\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.5975 - reconstruction_loss: 63.4822 - kl_loss: 17.1153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 934us/step - loss: 76.1197 - reconstruction_loss: 59.0291 - kl_loss: 17.0906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3910\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3642\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 127.4402 - reconstruction_loss: 114.2491 - kl_loss: 13.1912\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 924us/step - loss: 122.1836 - reconstruction_loss: 109.0128 - kl_loss: 13.1708\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7622\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7516\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.8333 - reconstruction_loss: 16.5632 - kl_loss: 20.2701\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.9692 - reconstruction_loss: 34.7342 - kl_loss: 20.2351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4702\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.2309 - reconstruction_loss: 81.9457 - kl_loss: 14.2852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102.6171 - reconstruction_loss: 88.3440 - kl_loss: 14.2731\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7263\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6801\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.0234 - reconstruction_loss: 73.2526 - kl_loss: 16.7708\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.2365 - reconstruction_loss: 56.4825 - kl_loss: 16.7540\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 124.7662 - reconstruction_loss: 108.6543 - kl_loss: 16.1119\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.2018 - reconstruction_loss: 100.1029 - kl_loss: 16.0989\n",
      "Success in episode 24 at time step 200 with reward -229.61443046668597\n",
      "Episode 25\n",
      "[ 0.8151177  -0.57929534  0.13617884]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.8424\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.3161 - reconstruction_loss: 55.4885 - kl_loss: 11.8276\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.3457 - reconstruction_loss: 62.5280 - kl_loss: 11.8176\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8187\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8321\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.5559 - reconstruction_loss: 37.1560 - kl_loss: 17.3999\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.6109 - reconstruction_loss: 53.1984 - kl_loss: 17.4124\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0543\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0672\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 144.6186 - reconstruction_loss: 133.7430 - kl_loss: 10.8756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 130.3315 - reconstruction_loss: 119.4562 - kl_loss: 10.8753\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9407\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6985\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.4893 - reconstruction_loss: 61.3864 - kl_loss: 12.1029\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.5239 - reconstruction_loss: 62.4305 - kl_loss: 12.0933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0401\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0516\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.8587 - reconstruction_loss: 49.0080 - kl_loss: 16.8507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.3700 - reconstruction_loss: 26.5101 - kl_loss: 16.8599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4822\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 105.5232 - reconstruction_loss: 94.7668 - kl_loss: 10.7563\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.2789 - reconstruction_loss: 95.5272 - kl_loss: 10.7518\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2926\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2533\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 85.4056 - reconstruction_loss: 72.8629 - kl_loss: 12.5427\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 981us/step - loss: 91.2244 - reconstruction_loss: 78.6876 - kl_loss: 12.5368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7774\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7734\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.8696 - reconstruction_loss: 26.7441 - kl_loss: 21.1255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.9405 - reconstruction_loss: 26.8275 - kl_loss: 21.1130\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3151\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.1833 - reconstruction_loss: 81.9061 - kl_loss: 12.2772\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.7179 - reconstruction_loss: 80.4470 - kl_loss: 12.2709\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7889\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7777\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86.2141 - reconstruction_loss: 75.2779 - kl_loss: 10.9362\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.6221 - reconstruction_loss: 79.6897 - kl_loss: 10.9324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7840\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5905\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.3317 - reconstruction_loss: 16.6684 - kl_loss: 19.6634\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.4689 - reconstruction_loss: 15.7933 - kl_loss: 19.6756\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1764\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1775\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.2029 - reconstruction_loss: 67.9551 - kl_loss: 14.2477\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.4597 - reconstruction_loss: 75.2213 - kl_loss: 14.2384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4514\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.9362 - reconstruction_loss: 59.0814 - kl_loss: 14.8548\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.1403 - reconstruction_loss: 65.2835 - kl_loss: 14.8568\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1230\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0916\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.0336 - reconstruction_loss: 29.0892 - kl_loss: 16.9443\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.0244 - reconstruction_loss: 34.0764 - kl_loss: 16.9480\n",
      "training on full data\n",
      "3 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8663\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8370\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73.1411 - reconstruction_loss: 58.6003 - kl_loss: 14.5407\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.3753 - reconstruction_loss: 65.8142 - kl_loss: 14.5611\n",
      "Success in episode 25 at time step 200 with reward -229.34799982737758\n",
      "Episode 26\n",
      "[-0.5068721  -0.86202127  0.5118711 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1319\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9873\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.7595 - reconstruction_loss: 16.4167 - kl_loss: 9.3428\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.4048 - reconstruction_loss: 26.0712 - kl_loss: 9.3337\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.7490\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.5308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.5363 - reconstruction_loss: 17.2214 - kl_loss: 18.3149\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.8860 - reconstruction_loss: 13.5772 - kl_loss: 18.3088\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9187\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 152.4291 - reconstruction_loss: 137.1034 - kl_loss: 15.3257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 154.9509 - reconstruction_loss: 139.6278 - kl_loss: 15.3231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5770\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5249\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.5581 - reconstruction_loss: 42.4878 - kl_loss: 10.0703\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.9593 - reconstruction_loss: 24.8895 - kl_loss: 10.0697\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7354\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.5881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.0592 - reconstruction_loss: 43.0103 - kl_loss: 16.0490\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.3342 - reconstruction_loss: 27.2633 - kl_loss: 16.0709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3990\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 97.0574 - reconstruction_loss: 80.1887 - kl_loss: 16.8688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.1773 - reconstruction_loss: 91.3064 - kl_loss: 16.8709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0088\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.0541\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 246.3337 - reconstruction_loss: 228.6337 - kl_loss: 17.7000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 890us/step - loss: 205.1002 - reconstruction_loss: 187.3773 - kl_loss: 17.7229\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.2207\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1315\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.0130 - reconstruction_loss: 37.9822 - kl_loss: 13.0307\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.2907 - reconstruction_loss: 47.2406 - kl_loss: 13.0501\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1095\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.8361 - reconstruction_loss: 48.4691 - kl_loss: 13.3670\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 945us/step - loss: 63.3236 - reconstruction_loss: 49.9526 - kl_loss: 13.3710\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3809\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3420\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.2588 - reconstruction_loss: 23.4323 - kl_loss: 17.8264\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 812us/step - loss: 39.1661 - reconstruction_loss: 21.3420 - kl_loss: 17.8241\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6573\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.9691 - reconstruction_loss: 56.2444 - kl_loss: 13.7247\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.1915 - reconstruction_loss: 58.4755 - kl_loss: 13.7159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9367\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62.3161 - reconstruction_loss: 46.4551 - kl_loss: 15.8609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.6550 - reconstruction_loss: 46.7957 - kl_loss: 15.8594\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8589\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.1650 - reconstruction_loss: 29.0209 - kl_loss: 19.1441\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.5635 - reconstruction_loss: 20.4566 - kl_loss: 19.1069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9685\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9855\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 95.3818 - reconstruction_loss: 83.2595 - kl_loss: 12.1224\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.6645 - reconstruction_loss: 82.5492 - kl_loss: 12.1153\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.8058 - reconstruction_loss: 38.3297 - kl_loss: 15.4761\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.7844 - reconstruction_loss: 26.3197 - kl_loss: 15.4648\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5710\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5301\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.3974 - reconstruction_loss: 68.5169 - kl_loss: 14.8805\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.8415 - reconstruction_loss: 59.9770 - kl_loss: 14.8645\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 5.6690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6339\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.7947 - reconstruction_loss: 65.7278 - kl_loss: 15.0669\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.0555 - reconstruction_loss: 68.0153 - kl_loss: 15.0402\n",
      "Success in episode 26 at time step 200 with reward -203.97734989094923\n",
      "Episode 27\n",
      "[ 0.62568504 -0.7800758   0.8491868 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2431\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9052\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 88.3799 - reconstruction_loss: 76.6297 - kl_loss: 11.7502\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.8280 - reconstruction_loss: 37.0439 - kl_loss: 11.7841\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8470\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7413\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54.0696 - reconstruction_loss: 36.7162 - kl_loss: 17.3534\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 878us/step - loss: 47.0791 - reconstruction_loss: 29.7811 - kl_loss: 17.2980\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1998\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.6693 - reconstruction_loss: 84.1926 - kl_loss: 10.4767\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.9570 - reconstruction_loss: 89.4661 - kl_loss: 10.4909\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4021\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4402\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.1995 - reconstruction_loss: 17.5284 - kl_loss: 18.6711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.4036 - reconstruction_loss: 10.7908 - kl_loss: 18.6128\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5452\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.5843 - reconstruction_loss: 60.7625 - kl_loss: 11.8219\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.0221 - reconstruction_loss: 60.2115 - kl_loss: 11.8107\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9296\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.6384 - reconstruction_loss: 66.1450 - kl_loss: 11.4934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 898us/step - loss: 79.3833 - reconstruction_loss: 67.8939 - kl_loss: 11.4894\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 50.1007 - reconstruction_loss: 35.4804 - kl_loss: 14.6203\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.0628 - reconstruction_loss: 58.4637 - kl_loss: 14.5991\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5982\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.8223 - reconstruction_loss: 54.6079 - kl_loss: 14.2144\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.8048 - reconstruction_loss: 58.5921 - kl_loss: 14.2127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3524\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2916\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.3766 - reconstruction_loss: 70.1756 - kl_loss: 13.2009\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.8453 - reconstruction_loss: 69.6510 - kl_loss: 13.1942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4510\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3579\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.8948 - reconstruction_loss: 16.9038 - kl_loss: 20.9910\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.9136 - reconstruction_loss: 21.9312 - kl_loss: 20.9824\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7569\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7378\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.4931 - reconstruction_loss: 52.4620 - kl_loss: 13.0311\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.9921 - reconstruction_loss: 53.9740 - kl_loss: 13.0181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2607\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.0896 - reconstruction_loss: 56.0223 - kl_loss: 14.0673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 989us/step - loss: 70.7801 - reconstruction_loss: 56.7316 - kl_loss: 14.0485\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9012\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8840\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.9084 - reconstruction_loss: 53.4735 - kl_loss: 16.4350\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.4694 - reconstruction_loss: 50.0356 - kl_loss: 16.4339\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5901\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5188\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.9844 - reconstruction_loss: 98.6382 - kl_loss: 13.3462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.2143 - reconstruction_loss: 82.8908 - kl_loss: 13.3235\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9100\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.1161 - reconstruction_loss: 52.3698 - kl_loss: 14.7464\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.5545 - reconstruction_loss: 49.8377 - kl_loss: 14.7168\n",
      "training on full data\n",
      "2 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2178\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.7708 - reconstruction_loss: 51.0389 - kl_loss: 14.7318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.1705 - reconstruction_loss: 47.4598 - kl_loss: 14.7108\n",
      "Success in episode 27 at time step 200 with reward -228.85689115113036\n",
      "Episode 28\n",
      "[ 0.9814184  -0.19188015  0.4430433 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9800\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 126.5565 - reconstruction_loss: 114.6400 - kl_loss: 11.9165\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.6914 - reconstruction_loss: 122.7829 - kl_loss: 11.9086\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8112\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.6198\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.3034 - reconstruction_loss: 97.4696 - kl_loss: 14.8338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 103.3902 - reconstruction_loss: 88.5433 - kl_loss: 14.8468\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9245\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7588\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.1795 - reconstruction_loss: 95.3140 - kl_loss: 12.8655\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.4046 - reconstruction_loss: 45.5426 - kl_loss: 12.8620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5652\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5612\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.0511 - reconstruction_loss: 45.5820 - kl_loss: 19.4691\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.4946 - reconstruction_loss: 37.9872 - kl_loss: 19.5075\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.5227 - reconstruction_loss: 59.1684 - kl_loss: 11.3543\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 991us/step - loss: 81.7335 - reconstruction_loss: 70.3696 - kl_loss: 11.3639\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4658\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.7356 - reconstruction_loss: 33.1575 - kl_loss: 14.5781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 937us/step - loss: 48.4045 - reconstruction_loss: 33.8344 - kl_loss: 14.5702\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6164\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6130\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.5420 - reconstruction_loss: 41.4224 - kl_loss: 14.1196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.9789 - reconstruction_loss: 39.8608 - kl_loss: 14.1181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5611\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.8215 - reconstruction_loss: 47.0858 - kl_loss: 12.7357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.7876 - reconstruction_loss: 47.0588 - kl_loss: 12.7288\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7375\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.0044 - reconstruction_loss: 3.9567 - kl_loss: 20.0477\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.4167 - reconstruction_loss: 7.3942 - kl_loss: 20.0225\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6386\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6415\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62.7780 - reconstruction_loss: 52.1844 - kl_loss: 10.5937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.0618 - reconstruction_loss: 45.4686 - kl_loss: 10.5931\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0625\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73.5490 - reconstruction_loss: 63.3851 - kl_loss: 10.1639\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.9088 - reconstruction_loss: 61.7558 - kl_loss: 10.1530\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7558\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6343\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.3717 - reconstruction_loss: 29.0474 - kl_loss: 18.3242\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.8108 - reconstruction_loss: 18.5289 - kl_loss: 18.2819\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3194\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3162\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.9148 - reconstruction_loss: 60.8208 - kl_loss: 14.0940\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.9276 - reconstruction_loss: 45.8492 - kl_loss: 14.0784\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4539\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3726\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.4227 - reconstruction_loss: 34.7619 - kl_loss: 11.6608\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 81.9698 - reconstruction_loss: 70.3217 - kl_loss: 11.6481\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2621\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2639\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.7759 - reconstruction_loss: 18.8740 - kl_loss: 18.9019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54.5860 - reconstruction_loss: 35.7358 - kl_loss: 18.8502\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8965\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63.5938 - reconstruction_loss: 49.4723 - kl_loss: 14.1215\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.2878 - reconstruction_loss: 51.1623 - kl_loss: 14.1255\n",
      "Success in episode 28 at time step 200 with reward -209.7387208329663\n",
      "Episode 29\n",
      "[-0.7705558 -0.6373726 -0.1707581]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5281\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3814\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.9755 - reconstruction_loss: 22.5228 - kl_loss: 12.4527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.3857 - reconstruction_loss: 21.9244 - kl_loss: 12.4613\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7158\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.3395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 76.9469 - reconstruction_loss: 59.4019 - kl_loss: 17.5450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 78.6536 - reconstruction_loss: 61.0918 - kl_loss: 17.5619\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8988\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8491\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 114.2072 - reconstruction_loss: 99.2120 - kl_loss: 14.9952\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 991us/step - loss: 135.1234 - reconstruction_loss: 120.1459 - kl_loss: 14.9774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1439\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0306\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.1019 - reconstruction_loss: 16.6582 - kl_loss: 11.4437\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.6569 - reconstruction_loss: 10.2294 - kl_loss: 11.4275\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9508\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.7091\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 86.8573 - reconstruction_loss: 70.2111 - kl_loss: 16.6462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.8179 - reconstruction_loss: 62.1458 - kl_loss: 16.6720\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0075\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0331\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 219.3345 - reconstruction_loss: 203.0588 - kl_loss: 16.2757\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 194.6265 - reconstruction_loss: 178.3653 - kl_loss: 16.2613\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0757\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0357\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.6901 - reconstruction_loss: 49.9506 - kl_loss: 9.7395\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 956us/step - loss: 45.8845 - reconstruction_loss: 36.1310 - kl_loss: 9.7535\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5296\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.2890 - reconstruction_loss: 24.7643 - kl_loss: 15.5247\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.6652 - reconstruction_loss: 29.1299 - kl_loss: 15.5353\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7473\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.7342\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 251.8257 - reconstruction_loss: 231.5799 - kl_loss: 20.2458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 245.7245 - reconstruction_loss: 225.4862 - kl_loss: 20.2383\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.2868\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.1054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 150.2038 - reconstruction_loss: 136.3960 - kl_loss: 13.8079\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 941us/step - loss: 131.8103 - reconstruction_loss: 117.9864 - kl_loss: 13.8239\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.0721\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54.3131 - reconstruction_loss: 45.6666 - kl_loss: 8.6465\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.3056 - reconstruction_loss: 45.6448 - kl_loss: 8.6609\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8766\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7226\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.0492 - reconstruction_loss: 30.9583 - kl_loss: 17.0909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 933us/step - loss: 52.2189 - reconstruction_loss: 35.1048 - kl_loss: 17.1140\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3467\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.9456 - reconstruction_loss: 32.5594 - kl_loss: 12.3862\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 874us/step - loss: 57.4188 - reconstruction_loss: 45.0167 - kl_loss: 12.4021\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3780\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.4006 - reconstruction_loss: 5.2231 - kl_loss: 19.1775\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.9794 - reconstruction_loss: 23.7626 - kl_loss: 19.2168\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68.6191 - reconstruction_loss: 57.1806 - kl_loss: 11.4385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.2019 - reconstruction_loss: 52.7541 - kl_loss: 11.4478\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0086\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.4636 - reconstruction_loss: 51.8266 - kl_loss: 14.6371\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 872us/step - loss: 69.3096 - reconstruction_loss: 54.6550 - kl_loss: 14.6547\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7930\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7363\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.6755 - reconstruction_loss: 67.2087 - kl_loss: 14.4669\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.2785 - reconstruction_loss: 64.8117 - kl_loss: 14.4668\n",
      "Success in episode 29 at time step 200 with reward -222.48001634741445\n",
      "Episode 30\n",
      "[0.92102116 0.38951248 0.9841462 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7590\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7232\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 317.1145 - reconstruction_loss: 297.6833 - kl_loss: 19.4312\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 323.9891 - reconstruction_loss: 304.5681 - kl_loss: 19.4210\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.1914\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.0780\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 271.4454 - reconstruction_loss: 257.9169 - kl_loss: 13.5285\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 975us/step - loss: 284.5271 - reconstruction_loss: 270.9864 - kl_loss: 13.5407\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8982\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8405\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 283.3142 - reconstruction_loss: 266.6692 - kl_loss: 16.6451\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 848us/step - loss: 326.0079 - reconstruction_loss: 309.3538 - kl_loss: 16.6541\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9721\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 187.9915 - reconstruction_loss: 173.4213 - kl_loss: 14.5702\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.4338 - reconstruction_loss: 150.8299 - kl_loss: 14.6039\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7735\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6474\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.3487 - reconstruction_loss: 38.8116 - kl_loss: 14.5371\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.9354 - reconstruction_loss: 94.3749 - kl_loss: 14.5605\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8278\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8328\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62.8052 - reconstruction_loss: 47.5444 - kl_loss: 15.2608\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.9680 - reconstruction_loss: 45.6888 - kl_loss: 15.2793\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7958\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.7879 - reconstruction_loss: 51.3619 - kl_loss: 14.4260\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.2905 - reconstruction_loss: 47.8512 - kl_loss: 14.4393\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2418\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1146\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.7693 - reconstruction_loss: 2.3483 - kl_loss: 20.4210\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.3791 - reconstruction_loss: 10.9494 - kl_loss: 20.4297\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1980\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1418\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.6681 - reconstruction_loss: 50.3742 - kl_loss: 14.2939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 63.8210 - reconstruction_loss: 49.5403 - kl_loss: 14.2807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5363\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2471\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.8688 - reconstruction_loss: 57.2025 - kl_loss: 14.6663\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.1592 - reconstruction_loss: 55.4813 - kl_loss: 14.6779\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1085\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1102\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.3328 - reconstruction_loss: 28.7353 - kl_loss: 17.5975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.3983 - reconstruction_loss: 23.8308 - kl_loss: 17.5675\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6436\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.5353\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.2245 - reconstruction_loss: 80.7173 - kl_loss: 12.5072\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.3065 - reconstruction_loss: 78.8026 - kl_loss: 12.5039\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7225\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.7412\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 44.4365 - reconstruction_loss: 24.4869 - kl_loss: 19.9496\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.5450 - reconstruction_loss: 6.6224 - kl_loss: 19.9226\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7130\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6633\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.0128 - reconstruction_loss: 46.6135 - kl_loss: 12.3993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.7721 - reconstruction_loss: 45.3843 - kl_loss: 12.3878\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6626\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5905\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.2321 - reconstruction_loss: 51.5692 - kl_loss: 13.6630\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.7590 - reconstruction_loss: 56.1159 - kl_loss: 13.6431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4993\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.2134 - reconstruction_loss: 32.0820 - kl_loss: 17.1314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.8064 - reconstruction_loss: 37.6992 - kl_loss: 17.1072\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 7.4498\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 106.5867 - reconstruction_loss: 91.1168 - kl_loss: 15.4698\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.1681 - reconstruction_loss: 96.7432 - kl_loss: 15.4249\n",
      "Success in episode 30 at time step 200 with reward -215.96398427356755\n",
      "Episode 31\n",
      "[ 0.9996272 -0.0273035  0.9367339]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.5060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 16.4822\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 214.1349 - reconstruction_loss: 195.2350 - kl_loss: 18.8999\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 217.3160 - reconstruction_loss: 198.4348 - kl_loss: 18.8811\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7650\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 13.6682\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 270.3181 - reconstruction_loss: 252.9776 - kl_loss: 17.3405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 241.5665 - reconstruction_loss: 224.2577 - kl_loss: 17.3088\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.6066\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.6488 - reconstruction_loss: 64.4027 - kl_loss: 16.2461\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.0050 - reconstruction_loss: 43.7886 - kl_loss: 16.2164\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58.6483 - reconstruction_loss: 47.3149 - kl_loss: 11.3335\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.4095 - reconstruction_loss: 46.0771 - kl_loss: 11.3324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9862\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0264\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.3007 - reconstruction_loss: 18.8748 - kl_loss: 16.4259\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.2624 - reconstruction_loss: 39.8704 - kl_loss: 16.3921\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2216\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.2464 - reconstruction_loss: 51.7054 - kl_loss: 13.5410\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.0914 - reconstruction_loss: 56.5388 - kl_loss: 13.5526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.5329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.2282 - reconstruction_loss: 50.1898 - kl_loss: 14.0384\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.5606 - reconstruction_loss: 43.5411 - kl_loss: 14.0195\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2269\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.0292 - reconstruction_loss: 21.4906 - kl_loss: 18.5386\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.1587 - reconstruction_loss: 30.6339 - kl_loss: 18.5249\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7587\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86.2632 - reconstruction_loss: 74.2213 - kl_loss: 12.0419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.3736 - reconstruction_loss: 71.3450 - kl_loss: 12.0286\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6240\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.5226 - reconstruction_loss: 32.5857 - kl_loss: 14.9369\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.1671 - reconstruction_loss: 29.2320 - kl_loss: 14.9351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0409\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.2128 - reconstruction_loss: 51.1387 - kl_loss: 13.0741\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.5301 - reconstruction_loss: 43.4676 - kl_loss: 13.0625\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7263\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4184\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.5039 - reconstruction_loss: 59.2990 - kl_loss: 12.2049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.2760 - reconstruction_loss: 55.0822 - kl_loss: 12.1937\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4579\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3516\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.2486 - reconstruction_loss: 16.6021 - kl_loss: 18.6465\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.7160 - reconstruction_loss: 24.1000 - kl_loss: 18.6160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.1783 - reconstruction_loss: 46.5988 - kl_loss: 12.5794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.8005 - reconstruction_loss: 47.2543 - kl_loss: 12.5462\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7809\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62.9441 - reconstruction_loss: 48.4748 - kl_loss: 14.4693\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.6891 - reconstruction_loss: 51.2323 - kl_loss: 14.4568\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4156\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.5996 - reconstruction_loss: 49.5309 - kl_loss: 16.0687\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.5450 - reconstruction_loss: 40.5133 - kl_loss: 16.0317\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 78.7899 - reconstruction_loss: 64.1336 - kl_loss: 14.6563\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.4621 - reconstruction_loss: 63.8149 - kl_loss: 14.6471\n",
      "Success in episode 31 at time step 200 with reward -216.43192040804874\n",
      "Episode 32\n",
      "[ 0.8312784   0.55585635 -0.12270769]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9118\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 294.1159 - reconstruction_loss: 274.1225 - kl_loss: 19.9934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 919us/step - loss: 329.5422 - reconstruction_loss: 309.5778 - kl_loss: 19.9644\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.7342\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.7514\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 283.7049 - reconstruction_loss: 270.1535 - kl_loss: 13.5514\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 286.6956 - reconstruction_loss: 273.1328 - kl_loss: 13.5628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.5464\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 210.0426 - reconstruction_loss: 192.7834 - kl_loss: 17.2592\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 313.8976 - reconstruction_loss: 296.6299 - kl_loss: 17.2677\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0260\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.8046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 222.0730 - reconstruction_loss: 205.1394 - kl_loss: 16.9337\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 181.6431 - reconstruction_loss: 164.6823 - kl_loss: 16.9608\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9281\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.1084 - reconstruction_loss: 47.3397 - kl_loss: 11.7687\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.5746 - reconstruction_loss: 27.7548 - kl_loss: 11.8198\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2561\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 57.3992 - reconstruction_loss: 42.6491 - kl_loss: 14.7501\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.9854 - reconstruction_loss: 27.2234 - kl_loss: 14.7620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0398\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0883\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.5221 - reconstruction_loss: 75.2603 - kl_loss: 13.2618\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.9816 - reconstruction_loss: 71.6945 - kl_loss: 13.2871\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.4159 - reconstruction_loss: 41.0300 - kl_loss: 12.3859\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 931us/step - loss: 53.1319 - reconstruction_loss: 40.7354 - kl_loss: 12.3965\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0695\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.3432 - reconstruction_loss: 23.2884 - kl_loss: 16.0548\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 850us/step - loss: 36.7299 - reconstruction_loss: 20.6697 - kl_loss: 16.0603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9810\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9343\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.6962 - reconstruction_loss: 63.2334 - kl_loss: 9.4627\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 961us/step - loss: 69.9455 - reconstruction_loss: 60.4788 - kl_loss: 9.4667\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7872\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7527\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.8703 - reconstruction_loss: 17.6508 - kl_loss: 20.2196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.3923 - reconstruction_loss: 10.1854 - kl_loss: 20.2069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5448\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5468\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.3962 - reconstruction_loss: 43.2405 - kl_loss: 12.1557\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.6165 - reconstruction_loss: 44.4685 - kl_loss: 12.1479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6415\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5857\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.2674 - reconstruction_loss: 35.2437 - kl_loss: 12.0237\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.4994 - reconstruction_loss: 38.4775 - kl_loss: 12.0219\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3152\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2965\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.6181 - reconstruction_loss: 26.6975 - kl_loss: 14.9206\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.3569 - reconstruction_loss: 23.4372 - kl_loss: 14.9197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7403\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 79.9571 - reconstruction_loss: 68.5984 - kl_loss: 11.3588\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.4398 - reconstruction_loss: 68.0885 - kl_loss: 11.3514\n",
      "fast thinking\n",
      "training on full data\n",
      "2 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1946\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.5232 - reconstruction_loss: 78.7787 - kl_loss: 14.7445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.7973 - reconstruction_loss: 76.0365 - kl_loss: 14.7608\n",
      "Success in episode 32 at time step 200 with reward -203.44292549555124\n",
      "Episode 33\n",
      "[-0.16992722  0.9854566  -0.82888055]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0157\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9790\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 213.4646 - reconstruction_loss: 198.2143 - kl_loss: 15.2503\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 921us/step - loss: 230.7249 - reconstruction_loss: 215.4828 - kl_loss: 15.2420\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4325\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.5931 - reconstruction_loss: 33.8727 - kl_loss: 14.7204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.6873 - reconstruction_loss: 58.9604 - kl_loss: 14.7268\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2258\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.1080 - reconstruction_loss: 42.9853 - kl_loss: 14.1226\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.8803 - reconstruction_loss: 39.7538 - kl_loss: 14.1265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0815\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.7404 - reconstruction_loss: 27.7719 - kl_loss: 14.9685\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 896us/step - loss: 37.4609 - reconstruction_loss: 22.4714 - kl_loss: 14.9894\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7228\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.2166 - reconstruction_loss: 39.1727 - kl_loss: 13.0440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.5250 - reconstruction_loss: 44.4774 - kl_loss: 13.0475\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3647\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.6203 - reconstruction_loss: 42.3274 - kl_loss: 14.2929\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.6723 - reconstruction_loss: 46.3717 - kl_loss: 14.3006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4715\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.9914 - reconstruction_loss: 15.0771 - kl_loss: 18.9143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.7506 - reconstruction_loss: 17.8364 - kl_loss: 18.9142\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8601\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.2830 - reconstruction_loss: 49.6521 - kl_loss: 11.6309\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.6015 - reconstruction_loss: 50.9829 - kl_loss: 11.6185\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3154\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 71.0883 - reconstruction_loss: 59.0722 - kl_loss: 12.0161\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.3962 - reconstruction_loss: 56.3933 - kl_loss: 12.0030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6841\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4151\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 43.2754 - reconstruction_loss: 26.8704 - kl_loss: 16.4050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.2027 - reconstruction_loss: 15.8386 - kl_loss: 16.3641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1334\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0781\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.2933 - reconstruction_loss: 44.0638 - kl_loss: 12.2294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.1958 - reconstruction_loss: 42.9697 - kl_loss: 12.2262\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9170\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.3832 - reconstruction_loss: 45.2386 - kl_loss: 14.1445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 938us/step - loss: 65.8814 - reconstruction_loss: 51.7607 - kl_loss: 14.1207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.5111 - reconstruction_loss: 33.8174 - kl_loss: 15.6937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.8681 - reconstruction_loss: 34.2050 - kl_loss: 15.6631\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5772\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5494\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 85.6447 - reconstruction_loss: 74.4863 - kl_loss: 11.1584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.0206 - reconstruction_loss: 78.8785 - kl_loss: 11.1421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5315\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5562\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.3902 - reconstruction_loss: 18.5354 - kl_loss: 18.8548\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.2141 - reconstruction_loss: 8.4266 - kl_loss: 18.7875\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0767\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0376\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62.7564 - reconstruction_loss: 48.5329 - kl_loss: 14.2235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 61.2299 - reconstruction_loss: 47.0533 - kl_loss: 14.1766\n",
      "Success in episode 33 at time step 200 with reward -225.64652584813206\n",
      "Episode 34\n",
      "[ 0.9883972  -0.15189147  0.9495038 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.7560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 20.8048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 232.1633 - reconstruction_loss: 216.3047 - kl_loss: 15.8585\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 205.6470 - reconstruction_loss: 189.8607 - kl_loss: 15.7863\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5295\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 306.8371 - reconstruction_loss: 289.6717 - kl_loss: 17.1654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 320.2468 - reconstruction_loss: 303.1057 - kl_loss: 17.1411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.8422\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.7959\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 245.3869 - reconstruction_loss: 228.9839 - kl_loss: 16.4030\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 298.4402 - reconstruction_loss: 282.0666 - kl_loss: 16.3736\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1184\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0309\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 308.9895 - reconstruction_loss: 292.4405 - kl_loss: 16.5490\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 325.9779 - reconstruction_loss: 309.4323 - kl_loss: 16.5456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.3180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.1022\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 250.9240 - reconstruction_loss: 235.5466 - kl_loss: 15.3774\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 205.7625 - reconstruction_loss: 190.3748 - kl_loss: 15.3876\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5827\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4584\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 318.0115 - reconstruction_loss: 300.5255 - kl_loss: 17.4861\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 992us/step - loss: 421.1399 - reconstruction_loss: 403.6286 - kl_loss: 17.5113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.7025\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.3974\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 238.2685 - reconstruction_loss: 223.7044 - kl_loss: 14.5641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 213.9061 - reconstruction_loss: 199.3201 - kl_loss: 14.5860\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 304.9697 - reconstruction_loss: 288.1024 - kl_loss: 16.8673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 299.4761 - reconstruction_loss: 282.5599 - kl_loss: 16.9161\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.9193\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 21.4920\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 183.4156 - reconstruction_loss: 169.9571 - kl_loss: 13.4585\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 174.9554 - reconstruction_loss: 161.4806 - kl_loss: 13.4748\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9978\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8987\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 311.9297 - reconstruction_loss: 294.4265 - kl_loss: 17.5032\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 306.7701 - reconstruction_loss: 289.2225 - kl_loss: 17.5477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.4255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.1096\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 289.2365 - reconstruction_loss: 273.8304 - kl_loss: 15.4061\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 257.2075 - reconstruction_loss: 241.7492 - kl_loss: 15.4583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 350.5317 - reconstruction_loss: 332.4974 - kl_loss: 18.0343\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 828us/step - loss: 294.4103 - reconstruction_loss: 276.3112 - kl_loss: 18.0992\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7499\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6911\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 333.4604 - reconstruction_loss: 318.0895 - kl_loss: 15.3709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 378.7986 - reconstruction_loss: 363.4020 - kl_loss: 15.3966\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5577\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.3048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 286.9939 - reconstruction_loss: 273.3903 - kl_loss: 13.6036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: 262.8401 - reconstruction_loss: 249.1755 - kl_loss: 13.6646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 336.7292 - reconstruction_loss: 318.2272 - kl_loss: 18.5019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 854us/step - loss: 353.9322 - reconstruction_loss: 335.3971 - kl_loss: 18.5351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.6208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.3542\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 295.7846 - reconstruction_loss: 278.3036 - kl_loss: 17.4810\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 938us/step - loss: 256.9854 - reconstruction_loss: 239.4352 - kl_loss: 17.5501\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 13.4343\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0200\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 251.8291 - reconstruction_loss: 234.9051 - kl_loss: 16.9240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 248.3182 - reconstruction_loss: 231.2655 - kl_loss: 17.0527\n",
      "Success in episode 34 at time step 200 with reward -212.48941874341645\n",
      "Episode 35\n",
      "[ 0.13745509 -0.990508   -0.32890382]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9969\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0185\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 32.4113 - reconstruction_loss: 21.7326 - kl_loss: 10.6787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.5507 - reconstruction_loss: 12.7837 - kl_loss: 10.7671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1662\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 85.0502 - reconstruction_loss: 66.9479 - kl_loss: 18.1023\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.3090 - reconstruction_loss: 97.1171 - kl_loss: 18.1919\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9486\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.1416\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 200.3663 - reconstruction_loss: 181.8220 - kl_loss: 18.5442\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 148.1039 - reconstruction_loss: 129.3860 - kl_loss: 18.7178\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.1082\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 21.1626\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.0682 - reconstruction_loss: 32.2122 - kl_loss: 13.8560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.2118 - reconstruction_loss: 30.2452 - kl_loss: 13.9667\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5265\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4554\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.0148 - reconstruction_loss: 20.7045 - kl_loss: 15.3102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.8382 - reconstruction_loss: 26.4948 - kl_loss: 15.3434\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4826\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.4402\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 191.8300 - reconstruction_loss: 175.8242 - kl_loss: 16.0058\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 183.9384 - reconstruction_loss: 167.8744 - kl_loss: 16.0641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5739\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5636\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.2625 - reconstruction_loss: 64.2144 - kl_loss: 25.0481\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 76.5577 - reconstruction_loss: 51.4509 - kl_loss: 25.1068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.7325\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.1565\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.4851 - reconstruction_loss: 24.9173 - kl_loss: 17.5678\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.3317 - reconstruction_loss: 15.7468 - kl_loss: 17.5850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.1374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.7711 - reconstruction_loss: 59.5101 - kl_loss: 14.2610\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.5659 - reconstruction_loss: 59.2975 - kl_loss: 14.2684\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.8705\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 14.8446\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.3100 - reconstruction_loss: 88.7382 - kl_loss: 19.5718\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.3253 - reconstruction_loss: 84.7386 - kl_loss: 19.5867\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3713\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.3773\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 183.7830 - reconstruction_loss: 167.3510 - kl_loss: 16.4320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.9330 - reconstruction_loss: 149.4774 - kl_loss: 16.4556\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6376\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6008\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 277.8123 - reconstruction_loss: 258.9158 - kl_loss: 18.8966\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 323.1528 - reconstruction_loss: 304.2151 - kl_loss: 18.9378\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.5180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.4127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 172.5121 - reconstruction_loss: 154.3914 - kl_loss: 18.1208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 201.7900 - reconstruction_loss: 183.6449 - kl_loss: 18.1451\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8968\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8430\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 237.3681 - reconstruction_loss: 217.0211 - kl_loss: 20.3470\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 209.8177 - reconstruction_loss: 189.4146 - kl_loss: 20.4031\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3113\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 13.2512\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 219.7158 - reconstruction_loss: 201.2836 - kl_loss: 18.4322\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 203.9860 - reconstruction_loss: 185.4893 - kl_loss: 18.4967\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3638\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.2305\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 234.9999 - reconstruction_loss: 214.5668 - kl_loss: 20.4331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 187.4538 - reconstruction_loss: 166.9922 - kl_loss: 20.4616\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2760\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 136.0178 - reconstruction_loss: 117.8504 - kl_loss: 18.1674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.7522 - reconstruction_loss: 106.5473 - kl_loss: 18.2050\n",
      "Success in episode 35 at time step 200 with reward -197.76366212745228\n",
      "Episode 36\n",
      "[ 0.99917614  0.04058384 -0.5604523 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.4768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 25.3978\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68.3793 - reconstruction_loss: 52.5700 - kl_loss: 15.8093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.4613 - reconstruction_loss: 67.6512 - kl_loss: 15.8100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7620\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 195.7470 - reconstruction_loss: 174.2008 - kl_loss: 21.5462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 164.9044 - reconstruction_loss: 143.2368 - kl_loss: 21.6676\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5419\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.7322 - reconstruction_loss: 116.4418 - kl_loss: 18.2905\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.5389 - reconstruction_loss: 100.2429 - kl_loss: 18.2960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.6403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 14.5910\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 233.6400 - reconstruction_loss: 212.9580 - kl_loss: 20.6820\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.1009 - reconstruction_loss: 152.3289 - kl_loss: 20.7720\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.9452\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.6285\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.6479 - reconstruction_loss: 82.5060 - kl_loss: 19.1419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 95.5136 - reconstruction_loss: 76.3389 - kl_loss: 19.1747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 193.0143 - reconstruction_loss: 171.5389 - kl_loss: 21.4754\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 190.5662 - reconstruction_loss: 169.0151 - kl_loss: 21.5511\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.0168\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.6897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.3435 - reconstruction_loss: 100.0007 - kl_loss: 18.3428\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 185.5549 - reconstruction_loss: 167.1786 - kl_loss: 18.3764\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.3884\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2743\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 192.2650 - reconstruction_loss: 172.4593 - kl_loss: 19.8057\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 156.5872 - reconstruction_loss: 136.7178 - kl_loss: 19.8693\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.1204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 23.7312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.4012 - reconstruction_loss: 105.6019 - kl_loss: 19.7993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 135.3121 - reconstruction_loss: 115.4836 - kl_loss: 19.8285\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5967\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 181.9135 - reconstruction_loss: 157.1338 - kl_loss: 24.7798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 172.5789 - reconstruction_loss: 147.7027 - kl_loss: 24.8762\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0272\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8475\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.8721 - reconstruction_loss: 138.6832 - kl_loss: 17.1889\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.5856 - reconstruction_loss: 156.3528 - kl_loss: 17.2329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2907\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0949\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 141.7861 - reconstruction_loss: 119.7254 - kl_loss: 22.0607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 142.9695 - reconstruction_loss: 120.8161 - kl_loss: 22.1534\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.9874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.7617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 133.8985 - reconstruction_loss: 115.0458 - kl_loss: 18.8528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 113.0966 - reconstruction_loss: 94.1777 - kl_loss: 18.9190\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9875\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.8813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 162.3587 - reconstruction_loss: 141.3177 - kl_loss: 21.0410\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 950us/step - loss: 143.4352 - reconstruction_loss: 122.2925 - kl_loss: 21.1428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.2880\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.4183\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.4373 - reconstruction_loss: 42.8562 - kl_loss: 23.5812\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.9184 - reconstruction_loss: 29.3249 - kl_loss: 23.5935\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4139\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3370\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 140.8565 - reconstruction_loss: 126.4035 - kl_loss: 14.4530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.6470 - reconstruction_loss: 117.1737 - kl_loss: 14.4733\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.6287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.3759\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 115.3564 - reconstruction_loss: 94.7640 - kl_loss: 20.5925\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.9179 - reconstruction_loss: 95.2755 - kl_loss: 20.6424\n",
      "Success in episode 36 at time step 200 with reward -180.43626990392994\n",
      "Episode 37\n",
      "[-0.4666166   0.8844597   0.10845624]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 12.0844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.0943\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 87.3402 - reconstruction_loss: 66.0923 - kl_loss: 21.2479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.9554 - reconstruction_loss: 49.4900 - kl_loss: 21.4654\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.5929\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.1309\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.5142 - reconstruction_loss: 45.6488 - kl_loss: 14.8655\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 947us/step - loss: 60.5104 - reconstruction_loss: 45.5791 - kl_loss: 14.9313\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.5768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.6036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63.0378 - reconstruction_loss: 46.4646 - kl_loss: 16.5732\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.2998 - reconstruction_loss: 52.7295 - kl_loss: 16.5703\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.3348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 21.3449\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.4357 - reconstruction_loss: 58.6599 - kl_loss: 21.7759\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.3670 - reconstruction_loss: 78.5600 - kl_loss: 21.8070\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5339\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6023\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 109.2912 - reconstruction_loss: 89.1300 - kl_loss: 20.1613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.7882 - reconstruction_loss: 79.5766 - kl_loss: 20.2116\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.5376\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.4209\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 154.8337 - reconstruction_loss: 131.4265 - kl_loss: 23.4072\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 834us/step - loss: 176.6307 - reconstruction_loss: 153.1551 - kl_loss: 23.4756\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5552\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 119.9693 - reconstruction_loss: 98.9296 - kl_loss: 21.0397\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.6584 - reconstruction_loss: 49.5474 - kl_loss: 21.1111\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.5556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.4030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.8471 - reconstruction_loss: 97.5182 - kl_loss: 20.3288\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 833us/step - loss: 88.2802 - reconstruction_loss: 67.8767 - kl_loss: 20.4035\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6727\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4966\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.5133 - reconstruction_loss: 95.7777 - kl_loss: 22.7356\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.8020 - reconstruction_loss: 92.9719 - kl_loss: 22.8301\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.7566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.5761\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.9533 - reconstruction_loss: 79.2355 - kl_loss: 19.7178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.4881 - reconstruction_loss: 98.7071 - kl_loss: 19.7810\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4968\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 141.5107 - reconstruction_loss: 115.9249 - kl_loss: 25.5858\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 110.6834 - reconstruction_loss: 84.9998 - kl_loss: 25.6836\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.4629\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 30.0597\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.8311 - reconstruction_loss: 75.8262 - kl_loss: 24.0049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.7663 - reconstruction_loss: 80.6737 - kl_loss: 24.0927\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.5609\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 95.2930 - reconstruction_loss: 73.6612 - kl_loss: 21.6318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.9432 - reconstruction_loss: 69.2322 - kl_loss: 21.7110\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2397\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.0174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 125.3512 - reconstruction_loss: 100.6838 - kl_loss: 24.6674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 126.5528 - reconstruction_loss: 101.7998 - kl_loss: 24.7530\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0647\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 104.3117 - reconstruction_loss: 83.9104 - kl_loss: 20.4014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 113.2989 - reconstruction_loss: 92.8344 - kl_loss: 20.4645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8667\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7203\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.3469 - reconstruction_loss: 94.0879 - kl_loss: 23.2590\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 139.2519 - reconstruction_loss: 115.9246 - kl_loss: 23.3273\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.5707\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.9523 - reconstruction_loss: 65.3948 - kl_loss: 22.5575\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86.3213 - reconstruction_loss: 63.6364 - kl_loss: 22.6849\n",
      "Success in episode 37 at time step 200 with reward -187.67914408712477\n",
      "Episode 38\n",
      "[ 0.21197408  0.9772753  -0.17033568]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9328\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.6789\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.2107 - reconstruction_loss: 52.6016 - kl_loss: 27.6091\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.4026 - reconstruction_loss: 60.6030 - kl_loss: 27.7997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 59.5591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 58.9394\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.0456 - reconstruction_loss: 23.4848 - kl_loss: 29.5608\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.7383 - reconstruction_loss: 45.1484 - kl_loss: 29.5899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.0684\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.9641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 200.9756 - reconstruction_loss: 183.0424 - kl_loss: 17.9332\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 199.8603 - reconstruction_loss: 181.9547 - kl_loss: 17.9056\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.3335\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.3823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 114.7696 - reconstruction_loss: 91.4688 - kl_loss: 23.3007\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.5417 - reconstruction_loss: 73.3151 - kl_loss: 23.2266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.0451\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 33.7866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 171.2265 - reconstruction_loss: 153.9143 - kl_loss: 17.3122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 194.6291 - reconstruction_loss: 177.3759 - kl_loss: 17.2532\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.3272\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 53.7574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.8455 - reconstruction_loss: 48.2411 - kl_loss: 20.6043\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.7266 - reconstruction_loss: 58.2359 - kl_loss: 20.4907\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.3741\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.3853\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 181.5285 - reconstruction_loss: 167.8432 - kl_loss: 13.6854\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.4365 - reconstruction_loss: 151.7955 - kl_loss: 13.6410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 74.2037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 70.6264\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 109.0134 - reconstruction_loss: 86.0048 - kl_loss: 23.0086\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 996us/step - loss: 98.7886 - reconstruction_loss: 75.9387 - kl_loss: 22.8499\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.7123\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 48.2897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 204.5495 - reconstruction_loss: 190.4466 - kl_loss: 14.1029\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 197.7108 - reconstruction_loss: 183.6491 - kl_loss: 14.0617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.8056\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 13.5111\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 156.9650 - reconstruction_loss: 136.8974 - kl_loss: 20.0677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 169.5961 - reconstruction_loss: 149.6789 - kl_loss: 19.9173\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7884\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.7910 - reconstruction_loss: 140.6807 - kl_loss: 15.1103\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 940us/step - loss: 159.1909 - reconstruction_loss: 144.1661 - kl_loss: 15.0248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.3158\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 40.5346\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 123.0830 - reconstruction_loss: 105.1424 - kl_loss: 17.9406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.1151 - reconstruction_loss: 114.2961 - kl_loss: 17.8190\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.0627\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.2694\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.9425 - reconstruction_loss: 99.8904 - kl_loss: 16.0521\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.4965 - reconstruction_loss: 92.5460 - kl_loss: 15.9505\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0818\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 202.7011 - reconstruction_loss: 185.4133 - kl_loss: 17.2878\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 222.4947 - reconstruction_loss: 205.3133 - kl_loss: 17.1813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.4737\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 36.1558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.6628 - reconstruction_loss: 90.9134 - kl_loss: 21.7494\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.6282 - reconstruction_loss: 83.0279 - kl_loss: 21.6002\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.0278\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.1528\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 199.7656 - reconstruction_loss: 185.1396 - kl_loss: 14.6260\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 192.8528 - reconstruction_loss: 178.2871 - kl_loss: 14.5657\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.2806\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4295\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.2015 - reconstruction_loss: 104.3514 - kl_loss: 17.8500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.1317 - reconstruction_loss: 104.4671 - kl_loss: 17.6646\n",
      "Success in episode 38 at time step 200 with reward -200.06682838888787\n",
      "Episode 39\n",
      "[-0.62102234 -0.78379285 -0.918652  ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.3231\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 26.6753\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 109.0520 - reconstruction_loss: 91.6950 - kl_loss: 17.3570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 81.8790 - reconstruction_loss: 64.8138 - kl_loss: 17.0652\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.3274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.0931\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.4401 - reconstruction_loss: 17.5610 - kl_loss: 20.8791\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.7632 - reconstruction_loss: 8.0754 - kl_loss: 20.6878\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.3274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.5187\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.7997 - reconstruction_loss: 6.8716 - kl_loss: 21.9281\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.3935 - reconstruction_loss: 4.6524 - kl_loss: 21.7411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.0711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.0184\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 76.6615 - reconstruction_loss: 63.4186 - kl_loss: 13.2428\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.7256 - reconstruction_loss: 54.6491 - kl_loss: 13.0765\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.1905\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.2922\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.7622 - reconstruction_loss: 8.3876 - kl_loss: 18.3746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.1986 - reconstruction_loss: 14.9087 - kl_loss: 18.2899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8485\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0026\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.6348 - reconstruction_loss: 27.7742 - kl_loss: 20.8605\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.9216 - reconstruction_loss: 17.1687 - kl_loss: 20.7529\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.1560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.1728\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103.1182 - reconstruction_loss: 88.5766 - kl_loss: 14.5416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.5459 - reconstruction_loss: 78.1097 - kl_loss: 14.4362\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.7060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 48.1574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.6594 - reconstruction_loss: 32.0820 - kl_loss: 12.5774\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.5214 - reconstruction_loss: 30.9550 - kl_loss: 12.5663\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.4699\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2240\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 95.4973 - reconstruction_loss: 76.7450 - kl_loss: 18.7522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.1839 - reconstruction_loss: 46.4388 - kl_loss: 18.7451\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.1886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.5756\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.3632 - reconstruction_loss: 99.0350 - kl_loss: 22.3281\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 109.4021 - reconstruction_loss: 87.0971 - kl_loss: 22.3050\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.0884\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.1320\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 146.4372 - reconstruction_loss: 123.5641 - kl_loss: 22.8731\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 880us/step - loss: 162.3477 - reconstruction_loss: 139.4542 - kl_loss: 22.8935\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.2484\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.0743\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.4440 - reconstruction_loss: 104.9667 - kl_loss: 16.4773\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 136.3390 - reconstruction_loss: 119.8459 - kl_loss: 16.4931\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.8277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.7237\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 147.1941 - reconstruction_loss: 121.9434 - kl_loss: 25.2507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 148.6339 - reconstruction_loss: 123.3291 - kl_loss: 25.3048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.6407\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 26.1692\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103.6476 - reconstruction_loss: 86.0281 - kl_loss: 17.6195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 105.9263 - reconstruction_loss: 88.2487 - kl_loss: 17.6776\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.6859\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.5131\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 138.4327 - reconstruction_loss: 112.9279 - kl_loss: 25.5049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 129.2339 - reconstruction_loss: 103.6462 - kl_loss: 25.5876\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.5985\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.2870\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.0989 - reconstruction_loss: 72.5942 - kl_loss: 17.5047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.1333 - reconstruction_loss: 61.5517 - kl_loss: 17.5816\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.7803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.2213\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 86.5818 - reconstruction_loss: 67.8531 - kl_loss: 18.7287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.9517 - reconstruction_loss: 59.1130 - kl_loss: 18.8387\n",
      "Success in episode 39 at time step 200 with reward -189.6547979244889\n",
      "Episode 40\n",
      "[ 0.786189   0.6179861 -0.9170462]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1421\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2000\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 95.0024 - reconstruction_loss: 69.4447 - kl_loss: 25.5577\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.6403 - reconstruction_loss: 59.8858 - kl_loss: 25.7545\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.3462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.6764\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 140.4962 - reconstruction_loss: 122.4846 - kl_loss: 18.0116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 127.0145 - reconstruction_loss: 108.8246 - kl_loss: 18.1898\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.9672\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.6798\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103.5968 - reconstruction_loss: 77.1489 - kl_loss: 26.4480\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.8873 - reconstruction_loss: 72.2833 - kl_loss: 26.6040\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.9313\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.9832\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.3658 - reconstruction_loss: 60.6342 - kl_loss: 17.7316\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 123.6792 - reconstruction_loss: 105.8048 - kl_loss: 17.8745\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.7031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3357\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.1160 - reconstruction_loss: 53.6750 - kl_loss: 27.4409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 992us/step - loss: 85.5320 - reconstruction_loss: 57.9735 - kl_loss: 27.5585\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.8526\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 34.4194\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.9988 - reconstruction_loss: 46.1002 - kl_loss: 18.8986\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.9716 - reconstruction_loss: 51.9239 - kl_loss: 19.0477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 63.7500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 62.6323\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.4258 - reconstruction_loss: 80.1150 - kl_loss: 16.3108\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.7344 - reconstruction_loss: 100.3456 - kl_loss: 16.3887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 129.6001 - reconstruction_loss: 114.2821 - kl_loss: 15.3180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 129.7885 - reconstruction_loss: 114.4534 - kl_loss: 15.3350\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 134.9782 - reconstruction_loss: 121.6068 - kl_loss: 13.3714\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 145.6220 - reconstruction_loss: 132.2374 - kl_loss: 13.3846\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5594\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5429\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 85.3119 - reconstruction_loss: 64.1506 - kl_loss: 21.1613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 917us/step - loss: 92.9044 - reconstruction_loss: 71.7964 - kl_loss: 21.1079\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.1845\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.3264\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 152.4824 - reconstruction_loss: 140.8228 - kl_loss: 11.6595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 142.8420 - reconstruction_loss: 131.2044 - kl_loss: 11.6377\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5678\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 76.2542 - reconstruction_loss: 55.9379 - kl_loss: 20.3163\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 87.9906 - reconstruction_loss: 67.7491 - kl_loss: 20.2415\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.5127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 31.4972\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.9433 - reconstruction_loss: 152.5731 - kl_loss: 13.3702\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 179.1057 - reconstruction_loss: 165.7849 - kl_loss: 13.3208\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4956\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 134.4013 - reconstruction_loss: 119.6813 - kl_loss: 14.7200\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 995us/step - loss: 131.3799 - reconstruction_loss: 116.7314 - kl_loss: 14.6486\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9497\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.4932 - reconstruction_loss: 100.3739 - kl_loss: 16.1193\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 794us/step - loss: 118.6960 - reconstruction_loss: 102.6585 - kl_loss: 16.0375\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 202.3244 - reconstruction_loss: 186.8992 - kl_loss: 15.4251\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 880us/step - loss: 168.5544 - reconstruction_loss: 153.1992 - kl_loss: 15.3552\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.4459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.1849\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 107.4897 - reconstruction_loss: 89.4437 - kl_loss: 18.0460\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.1554 - reconstruction_loss: 86.2194 - kl_loss: 17.9360\n",
      "Success in episode 40 at time step 200 with reward -197.22609032097216\n",
      "Episode 41\n",
      "[-0.983204    0.18251003 -0.94364715]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.4240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 26.6562\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.0582 - reconstruction_loss: 9.6803 - kl_loss: 21.3778\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.3323 - reconstruction_loss: 20.0391 - kl_loss: 21.2932\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.3926\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 16.4281\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.4520 - reconstruction_loss: 24.7812 - kl_loss: 19.6708\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.5616 - reconstruction_loss: 26.0022 - kl_loss: 19.5594\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.7876\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.6043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51.6766 - reconstruction_loss: 35.8519 - kl_loss: 15.8247\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.1051 - reconstruction_loss: 29.3393 - kl_loss: 15.7658\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6896\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 13.4676\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.8296 - reconstruction_loss: 8.0764 - kl_loss: 24.7532\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.0474 - reconstruction_loss: 11.4131 - kl_loss: 24.6343\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 40.6966\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.7744\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.0665 - reconstruction_loss: 56.3298 - kl_loss: 13.7367\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.9262 - reconstruction_loss: 60.2386 - kl_loss: 13.6876\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.7235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 28.5426\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 88.5320 - reconstruction_loss: 72.7492 - kl_loss: 15.7828\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.2897 - reconstruction_loss: 76.5631 - kl_loss: 15.7266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.0922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.0415\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.8613 - reconstruction_loss: 49.4486 - kl_loss: 21.4127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.3191 - reconstruction_loss: 58.9753 - kl_loss: 21.3438\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7211\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8793\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 150.3702 - reconstruction_loss: 130.0791 - kl_loss: 20.2911\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.4255 - reconstruction_loss: 99.1173 - kl_loss: 20.3082\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.2695\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 35.0500\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.0725 - reconstruction_loss: 72.5012 - kl_loss: 21.5713\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.9153 - reconstruction_loss: 78.3766 - kl_loss: 21.5386\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9966\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.9378\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.0574 - reconstruction_loss: 145.0344 - kl_loss: 20.0230\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.7070 - reconstruction_loss: 101.6175 - kl_loss: 20.0895\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.7045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.2679\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.0071 - reconstruction_loss: 95.0312 - kl_loss: 19.9758\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.3958 - reconstruction_loss: 92.4138 - kl_loss: 19.9820\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.2944\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.9757 - reconstruction_loss: 94.9231 - kl_loss: 23.0527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 133.2654 - reconstruction_loss: 110.1207 - kl_loss: 23.1447\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.4808\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.8819\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.0550 - reconstruction_loss: 101.6251 - kl_loss: 19.4299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 109.7533 - reconstruction_loss: 90.2956 - kl_loss: 19.4577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5512\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.4199\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 89.6412 - reconstruction_loss: 68.4322 - kl_loss: 21.2090\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.1909 - reconstruction_loss: 49.8854 - kl_loss: 21.3056\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.4296\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.0287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 84.2411 - reconstruction_loss: 67.8339 - kl_loss: 16.4072\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 940us/step - loss: 79.9281 - reconstruction_loss: 63.4725 - kl_loss: 16.4556\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 116.0358 - reconstruction_loss: 87.5117 - kl_loss: 28.5241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.1070 - reconstruction_loss: 56.4908 - kl_loss: 28.6162\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.3711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.9572\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.2517 - reconstruction_loss: 63.2691 - kl_loss: 19.9826\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.5869 - reconstruction_loss: 58.5048 - kl_loss: 20.0821\n",
      "Success in episode 41 at time step 200 with reward -188.9584943619221\n",
      "Episode 42\n",
      "[-0.99993503 -0.01139896  0.7773298 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.7698\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 15.2548\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.1952 - reconstruction_loss: 16.7979 - kl_loss: 17.3973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.5724 - reconstruction_loss: 5.1914 - kl_loss: 17.3810\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4888\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.4750 - reconstruction_loss: 36.0894 - kl_loss: 13.3855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.6743 - reconstruction_loss: 35.3094 - kl_loss: 13.3648\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 14.8942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.4970 - reconstruction_loss: 2.4520 - kl_loss: 22.0451\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.4574 - reconstruction_loss: 2.4397 - kl_loss: 22.0177\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.1203\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 20.4731\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.9638 - reconstruction_loss: 10.7795 - kl_loss: 18.1843\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.4139 - reconstruction_loss: 9.2159 - kl_loss: 18.1981\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5355\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5644\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.2051 - reconstruction_loss: 77.5915 - kl_loss: 12.6136\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.4969 - reconstruction_loss: 78.9011 - kl_loss: 12.5958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3808\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.0593\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62.7736 - reconstruction_loss: 41.4225 - kl_loss: 21.3511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.2048 - reconstruction_loss: 31.8325 - kl_loss: 21.3722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.5188\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.1157\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63.1505 - reconstruction_loss: 36.3207 - kl_loss: 26.8298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 956us/step - loss: 63.0769 - reconstruction_loss: 36.2182 - kl_loss: 26.8588\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.1272\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.9635\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86.1768 - reconstruction_loss: 69.6622 - kl_loss: 16.5147\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.7866 - reconstruction_loss: 76.2730 - kl_loss: 16.5136\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5490\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.4791 - reconstruction_loss: 106.0313 - kl_loss: 14.4478\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 131.8137 - reconstruction_loss: 117.3693 - kl_loss: 14.4444\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5782\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.5075\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 147.2879 - reconstruction_loss: 132.7840 - kl_loss: 14.5039\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 140.0563 - reconstruction_loss: 125.5861 - kl_loss: 14.4703\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.9229\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.3828\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 119.0330 - reconstruction_loss: 97.9500 - kl_loss: 21.0830\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.0941 - reconstruction_loss: 107.1085 - kl_loss: 20.9856\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.9450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.6271\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.8877 - reconstruction_loss: 161.9633 - kl_loss: 11.9243\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 171.2931 - reconstruction_loss: 159.4060 - kl_loss: 11.8871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2516\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 116.2171 - reconstruction_loss: 97.2075 - kl_loss: 19.0096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 115.8985 - reconstruction_loss: 97.0444 - kl_loss: 18.8541\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3145\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 126.0674 - reconstruction_loss: 113.4773 - kl_loss: 12.5901\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.3218 - reconstruction_loss: 108.7823 - kl_loss: 12.5395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0943\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.3496 - reconstruction_loss: 97.5118 - kl_loss: 14.8378\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.8977 - reconstruction_loss: 108.1651 - kl_loss: 14.7326\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8116\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.5268 - reconstruction_loss: 102.6446 - kl_loss: 13.8822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.3566 - reconstruction_loss: 100.5600 - kl_loss: 13.7965\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 86.8153 - reconstruction_loss: 70.5139 - kl_loss: 16.3015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.1170 - reconstruction_loss: 68.9280 - kl_loss: 16.1889\n",
      "Success in episode 42 at time step 200 with reward -211.3345716978157\n",
      "Episode 43\n",
      "[-0.06469366  0.9979052  -0.8461173 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5259\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.4207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 76.7237 - reconstruction_loss: 52.0583 - kl_loss: 24.6655\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 57.8910 - reconstruction_loss: 33.3499 - kl_loss: 24.5411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.0926\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.8898\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.9986 - reconstruction_loss: 16.6586 - kl_loss: 20.3400\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 951us/step - loss: 49.6868 - reconstruction_loss: 29.5945 - kl_loss: 20.0923\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0956\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 140.7448 - reconstruction_loss: 129.5639 - kl_loss: 11.1809\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 985us/step - loss: 142.2548 - reconstruction_loss: 131.1337 - kl_loss: 11.1211\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2579\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.9528 - reconstruction_loss: 43.0265 - kl_loss: 16.9263\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.4995 - reconstruction_loss: 31.7359 - kl_loss: 16.7635\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6270\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4872\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.5127 - reconstruction_loss: 119.5810 - kl_loss: 11.9317\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 914us/step - loss: 131.4175 - reconstruction_loss: 119.5318 - kl_loss: 11.8857\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5444\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6358\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.3105 - reconstruction_loss: 109.3945 - kl_loss: 11.9159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 995us/step - loss: 119.6391 - reconstruction_loss: 107.8086 - kl_loss: 11.8305\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5347\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86.9338 - reconstruction_loss: 74.2691 - kl_loss: 12.6647\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 914us/step - loss: 85.8476 - reconstruction_loss: 73.2277 - kl_loss: 12.6199\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0966\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.1039\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 136.8369 - reconstruction_loss: 126.5701 - kl_loss: 10.2668\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.2934 - reconstruction_loss: 124.0768 - kl_loss: 10.2165\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.9690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.6065\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.5875 - reconstruction_loss: 36.1490 - kl_loss: 15.4386\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 893us/step - loss: 54.0535 - reconstruction_loss: 38.6814 - kl_loss: 15.3721\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 139.3857 - reconstruction_loss: 128.4650 - kl_loss: 10.9208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 142.0214 - reconstruction_loss: 131.1214 - kl_loss: 10.9000\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8677\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.9683 - reconstruction_loss: 99.6038 - kl_loss: 12.3645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.4114 - reconstruction_loss: 98.0865 - kl_loss: 12.3248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5730\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5193\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 85.8355 - reconstruction_loss: 73.2034 - kl_loss: 12.6321\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 926us/step - loss: 83.8909 - reconstruction_loss: 71.2724 - kl_loss: 12.6185\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8707\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 144.2791 - reconstruction_loss: 134.4182 - kl_loss: 9.8610\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.8729 - reconstruction_loss: 125.0488 - kl_loss: 9.8241\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9862\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.8094\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 60.3827 - reconstruction_loss: 44.1791 - kl_loss: 16.2036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.0946 - reconstruction_loss: 38.9064 - kl_loss: 16.1881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0698\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9606\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 130.3679 - reconstruction_loss: 120.4222 - kl_loss: 9.9457\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 956us/step - loss: 133.9904 - reconstruction_loss: 124.0602 - kl_loss: 9.9302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2172\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8280\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 60.1902 - reconstruction_loss: 48.4418 - kl_loss: 11.7484\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.5399 - reconstruction_loss: 53.8077 - kl_loss: 11.7323\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2978\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 94.5389 - reconstruction_loss: 81.5219 - kl_loss: 13.0170\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.6266 - reconstruction_loss: 79.6152 - kl_loss: 13.0114\n",
      "Success in episode 43 at time step 200 with reward -226.12375627956072\n",
      "Episode 44\n",
      "[ 0.7623941   0.6471131  -0.27129003]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.3714\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 14.3826\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.5113 - reconstruction_loss: 97.8416 - kl_loss: 23.6697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 133.9456 - reconstruction_loss: 110.2838 - kl_loss: 23.6618\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.2256\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.5095\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 171.2708 - reconstruction_loss: 153.9095 - kl_loss: 17.3613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.9947 - reconstruction_loss: 142.6558 - kl_loss: 17.3389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.7704\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.6490\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 95.1152 - reconstruction_loss: 72.8910 - kl_loss: 22.2242\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.6429 - reconstruction_loss: 86.4044 - kl_loss: 22.2385\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.5012\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.7589\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.7032 - reconstruction_loss: 146.7150 - kl_loss: 12.9882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 149.7996 - reconstruction_loss: 136.7966 - kl_loss: 13.0030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.5522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 18.3787\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.6801 - reconstruction_loss: 86.6694 - kl_loss: 26.0106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 109.8544 - reconstruction_loss: 83.7800 - kl_loss: 26.0744\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.4950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.7627\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.0843 - reconstruction_loss: 39.8809 - kl_loss: 16.2034\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.7711 - reconstruction_loss: 20.4940 - kl_loss: 16.2771\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.5970\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.3973\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 105.5473 - reconstruction_loss: 95.2757 - kl_loss: 10.2717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 103.1140 - reconstruction_loss: 92.8346 - kl_loss: 10.2795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.6134\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 48.6603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.4388 - reconstruction_loss: 11.5365 - kl_loss: 15.9023\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.2790 - reconstruction_loss: 11.3617 - kl_loss: 15.9172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.7230\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 45.0769\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.5786 - reconstruction_loss: 28.4548 - kl_loss: 13.1239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.6043 - reconstruction_loss: 24.4463 - kl_loss: 13.1581\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.1270\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.3917\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 107.1301 - reconstruction_loss: 91.2161 - kl_loss: 15.9140\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.1253 - reconstruction_loss: 74.1979 - kl_loss: 15.9274\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8365\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 15.3335\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 117.3688 - reconstruction_loss: 91.5378 - kl_loss: 25.8309\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.1617 - reconstruction_loss: 84.2672 - kl_loss: 25.8946\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.0338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.1472\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 148.7950 - reconstruction_loss: 130.0182 - kl_loss: 18.7768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 164.8119 - reconstruction_loss: 146.0204 - kl_loss: 18.7915\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0117\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.6687\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 96.4403 - reconstruction_loss: 73.4936 - kl_loss: 22.9468\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.5787 - reconstruction_loss: 88.5486 - kl_loss: 23.0301\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.1930\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.8351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 158.5298 - reconstruction_loss: 140.8864 - kl_loss: 17.6435\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 179.4880 - reconstruction_loss: 161.8197 - kl_loss: 17.6683\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.8540\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.2344\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 156.6852 - reconstruction_loss: 136.7325 - kl_loss: 19.9528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 159.6823 - reconstruction_loss: 139.6627 - kl_loss: 20.0196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.7328\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 38.4929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 143.3459 - reconstruction_loss: 121.1011 - kl_loss: 22.2448\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 890us/step - loss: 148.2912 - reconstruction_loss: 125.9869 - kl_loss: 22.3043\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 24.9272\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.4053\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 110.9894 - reconstruction_loss: 91.3138 - kl_loss: 19.6756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 109.9447 - reconstruction_loss: 90.1153 - kl_loss: 19.8295\n",
      "Success in episode 44 at time step 200 with reward -175.84632492143453\n",
      "Episode 45\n",
      "[-0.22042279  0.97540444 -0.60642666]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1554\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.0291\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 82.9003 - reconstruction_loss: 60.1868 - kl_loss: 22.7134\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.2156 - reconstruction_loss: 24.2626 - kl_loss: 22.9530\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.7137\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3398\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.0397 - reconstruction_loss: 8.2235 - kl_loss: 18.8162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.3569 - reconstruction_loss: 27.3441 - kl_loss: 19.0128\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8171\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7914\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.1526 - reconstruction_loss: 100.4644 - kl_loss: 11.6881\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.7283 - reconstruction_loss: 98.9988 - kl_loss: 11.7295\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.5431\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.1010\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.6456 - reconstruction_loss: 13.9499 - kl_loss: 18.6958\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.1899 - reconstruction_loss: 12.4134 - kl_loss: 18.7765\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8788\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9211\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 129.8995 - reconstruction_loss: 118.6030 - kl_loss: 11.2965\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 133.1399 - reconstruction_loss: 121.8291 - kl_loss: 11.3108\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5234\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.4086\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.3960 - reconstruction_loss: 59.3870 - kl_loss: 18.0090\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 105.2131 - reconstruction_loss: 87.2033 - kl_loss: 18.0097\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5863\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.4599 - reconstruction_loss: 111.6256 - kl_loss: 10.8343\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.9908 - reconstruction_loss: 118.1648 - kl_loss: 10.8259\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0610\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.9476 - reconstruction_loss: 108.9089 - kl_loss: 11.0387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.1952 - reconstruction_loss: 105.1762 - kl_loss: 11.0190\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.5559\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.7818\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.1422 - reconstruction_loss: 47.2254 - kl_loss: 17.9167\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.8961 - reconstruction_loss: 49.0426 - kl_loss: 17.8535\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.5919\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 134.0313 - reconstruction_loss: 125.1783 - kl_loss: 8.8530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 138.0155 - reconstruction_loss: 129.1665 - kl_loss: 8.8490\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.3522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.4676\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.4095 - reconstruction_loss: 19.1217 - kl_loss: 16.2879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.3140 - reconstruction_loss: 29.1037 - kl_loss: 16.2102\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.2466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 22.9331\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 146.6993 - reconstruction_loss: 136.6846 - kl_loss: 10.0147\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 148.1379 - reconstruction_loss: 138.1328 - kl_loss: 10.0051\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9622\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.6495\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68.7272 - reconstruction_loss: 50.5113 - kl_loss: 18.2159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 854us/step - loss: 75.5455 - reconstruction_loss: 57.4335 - kl_loss: 18.1120\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6663\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5462\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.2276 - reconstruction_loss: 101.6045 - kl_loss: 10.6231\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.1285 - reconstruction_loss: 106.5173 - kl_loss: 10.6112\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.9020\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.8121 - reconstruction_loss: 94.6248 - kl_loss: 12.1874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 105.6691 - reconstruction_loss: 93.5375 - kl_loss: 12.1316\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8113\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.7589\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.4010 - reconstruction_loss: 56.5090 - kl_loss: 13.8920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.5934 - reconstruction_loss: 50.7566 - kl_loss: 13.8368\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4327\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0662\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 84.5280 - reconstruction_loss: 70.2031 - kl_loss: 14.3249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.0369 - reconstruction_loss: 67.8208 - kl_loss: 14.2161\n",
      "Success in episode 45 at time step 200 with reward -193.8393740893501\n",
      "Episode 46\n",
      "[-0.7508541  -0.6604681  -0.88978124]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7383\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6625\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.4050 - reconstruction_loss: 50.0034 - kl_loss: 14.4016\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.6291 - reconstruction_loss: 27.3472 - kl_loss: 14.2818\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.5891\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.3844\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.4325 - reconstruction_loss: 11.3020 - kl_loss: 17.1305\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.5285 - reconstruction_loss: 16.5189 - kl_loss: 17.0096\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.5530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.6700\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.8767 - reconstruction_loss: 9.5567 - kl_loss: 16.3200\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.3628 - reconstruction_loss: 7.0706 - kl_loss: 16.2922\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1559\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.0656\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.6788 - reconstruction_loss: 57.1222 - kl_loss: 9.5566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 61.2981 - reconstruction_loss: 51.7940 - kl_loss: 9.5041\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.1865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.7364\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.6561 - reconstruction_loss: 19.6881 - kl_loss: 17.9680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.1071 - reconstruction_loss: 22.2227 - kl_loss: 17.8844\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1135\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1470\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.7871 - reconstruction_loss: 40.9257 - kl_loss: 23.8614\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.9530 - reconstruction_loss: 44.1445 - kl_loss: 23.8085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.7063\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.1641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 60.0515 - reconstruction_loss: 45.5793 - kl_loss: 14.4722\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 954us/step - loss: 64.6538 - reconstruction_loss: 50.2090 - kl_loss: 14.4447\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 61.6805\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 58.8443\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 78.1884 - reconstruction_loss: 66.4585 - kl_loss: 11.7299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 927us/step - loss: 79.1219 - reconstruction_loss: 67.3888 - kl_loss: 11.7332\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.6897\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 41.7887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 99.5900 - reconstruction_loss: 77.4082 - kl_loss: 22.1818\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 860us/step - loss: 112.9189 - reconstruction_loss: 90.7550 - kl_loss: 22.1640\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.8703\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 20.0022\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 149.9406 - reconstruction_loss: 131.3523 - kl_loss: 18.5883\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 773us/step - loss: 148.6173 - reconstruction_loss: 130.0178 - kl_loss: 18.5995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.8691\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.0771\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 133.3353 - reconstruction_loss: 110.3530 - kl_loss: 22.9822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.0719 - reconstruction_loss: 85.0828 - kl_loss: 22.9891\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.3394\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.1747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 148.2936 - reconstruction_loss: 131.3150 - kl_loss: 16.9785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 150.8333 - reconstruction_loss: 133.8096 - kl_loss: 17.0237\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.7105\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 22.4941\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 105.3864 - reconstruction_loss: 77.5787 - kl_loss: 27.8078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 105.7136 - reconstruction_loss: 77.8807 - kl_loss: 27.8329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.1099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.5085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 247.3630 - reconstruction_loss: 230.3225 - kl_loss: 17.0405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 199.2595 - reconstruction_loss: 182.1437 - kl_loss: 17.1158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.6357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 21.0495\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 87.6332 - reconstruction_loss: 61.1133 - kl_loss: 26.5198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.1078 - reconstruction_loss: 57.5377 - kl_loss: 26.5701\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.1936\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.8443\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 138.2476 - reconstruction_loss: 120.3853 - kl_loss: 17.8623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 192.0127 - reconstruction_loss: 174.0802 - kl_loss: 17.9324\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.8046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.3519\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 91.6378 - reconstruction_loss: 72.7923 - kl_loss: 18.8455\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.4694 - reconstruction_loss: 70.5035 - kl_loss: 18.9659\n",
      "Success in episode 46 at time step 200 with reward -194.05304649602252\n",
      "Episode 47\n",
      "[-0.9810523  -0.19374324  0.5001216 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7187\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.5358\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.3794 - reconstruction_loss: 8.2929 - kl_loss: 14.0865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.7106 - reconstruction_loss: 8.5772 - kl_loss: 14.1334\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4113\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.7163 - reconstruction_loss: 19.0207 - kl_loss: 15.6955\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.1530 - reconstruction_loss: 17.4147 - kl_loss: 15.7383\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.8544\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.0056 - reconstruction_loss: 9.3826 - kl_loss: 23.6230\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.1118 - reconstruction_loss: 7.4696 - kl_loss: 23.6423\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0345\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9560\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.0659 - reconstruction_loss: 36.1917 - kl_loss: 12.8743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.7929 - reconstruction_loss: 18.9241 - kl_loss: 12.8689\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.8795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.5666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.7265 - reconstruction_loss: 29.3482 - kl_loss: 13.3783\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.3553 - reconstruction_loss: 30.9790 - kl_loss: 13.3763\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.6557\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5193\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.0981 - reconstruction_loss: 25.9641 - kl_loss: 28.1340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.1661 - reconstruction_loss: 30.0302 - kl_loss: 28.1359\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.7086\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.6581\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.4991 - reconstruction_loss: 17.5027 - kl_loss: 17.9964\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.3483 - reconstruction_loss: 16.3043 - kl_loss: 18.0440\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9658\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 107.7615 - reconstruction_loss: 98.0538 - kl_loss: 9.7076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.6233 - reconstruction_loss: 94.9207 - kl_loss: 9.7027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.4583\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.8351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 72.6529 - reconstruction_loss: 53.4310 - kl_loss: 19.2219\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.9312 - reconstruction_loss: 34.7106 - kl_loss: 19.2207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.5171\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 144.0198 - reconstruction_loss: 133.9410 - kl_loss: 10.0788\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 142.5885 - reconstruction_loss: 132.5177 - kl_loss: 10.0708\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4226\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4883\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.4244 - reconstruction_loss: 56.5197 - kl_loss: 13.9047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 968us/step - loss: 75.9758 - reconstruction_loss: 62.0911 - kl_loss: 13.8846\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8041\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7353\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 103.9602 - reconstruction_loss: 91.7133 - kl_loss: 12.2469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.9065 - reconstruction_loss: 94.6983 - kl_loss: 12.2081\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6601\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.4399\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.8659 - reconstruction_loss: 103.0767 - kl_loss: 11.7892\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.6873 - reconstruction_loss: 112.9161 - kl_loss: 11.7712\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2710\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9589\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.2952 - reconstruction_loss: 66.1576 - kl_loss: 16.1376\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.8721 - reconstruction_loss: 63.8032 - kl_loss: 16.0689\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1040\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 135.9026 - reconstruction_loss: 127.6521 - kl_loss: 8.2505\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 898us/step - loss: 136.7791 - reconstruction_loss: 128.5408 - kl_loss: 8.2383\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.7607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.4694\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.2282 - reconstruction_loss: 40.4048 - kl_loss: 16.8234\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.9420 - reconstruction_loss: 41.2073 - kl_loss: 16.7347\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7812\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5355\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.6497 - reconstruction_loss: 55.8516 - kl_loss: 14.7981\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.7275 - reconstruction_loss: 57.0145 - kl_loss: 14.7131\n",
      "Success in episode 47 at time step 200 with reward -226.41344909463675\n",
      "Episode 48\n",
      "[-0.9986903   0.05116276 -0.33383274]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3598\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.0375\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.3669 - reconstruction_loss: 5.7990 - kl_loss: 17.5679\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.8456 - reconstruction_loss: 10.4319 - kl_loss: 17.4137\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.7579 - reconstruction_loss: 8.7646 - kl_loss: 14.9933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.6556 - reconstruction_loss: 7.7937 - kl_loss: 14.8619\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1719\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.5168 - reconstruction_loss: 16.6179 - kl_loss: 13.8989\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.2611 - reconstruction_loss: 16.4465 - kl_loss: 13.8146\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6470\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.7229\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.9582 - reconstruction_loss: 9.4517 - kl_loss: 20.5065\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.4526 - reconstruction_loss: 20.0720 - kl_loss: 20.3806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4132\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3564\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.7791 - reconstruction_loss: 31.9305 - kl_loss: 10.8486\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 45.5942 - reconstruction_loss: 34.7849 - kl_loss: 10.8093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 40.0422\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.5529\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.5037 - reconstruction_loss: 15.9740 - kl_loss: 14.5297\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.9047 - reconstruction_loss: 11.4052 - kl_loss: 14.4995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7463\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8497\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.5250 - reconstruction_loss: 55.7522 - kl_loss: 24.7728\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 831us/step - loss: 67.2223 - reconstruction_loss: 42.4887 - kl_loss: 24.7336\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.5249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 47.2247\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.9817 - reconstruction_loss: 22.1046 - kl_loss: 20.8771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.9800 - reconstruction_loss: 22.1359 - kl_loss: 20.8441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5107\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 108.7863 - reconstruction_loss: 100.6688 - kl_loss: 8.1175\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.4890 - reconstruction_loss: 100.3832 - kl_loss: 8.1058\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.7108\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.1918\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.8562 - reconstruction_loss: 17.2579 - kl_loss: 15.5984\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.0627 - reconstruction_loss: 23.4911 - kl_loss: 15.5716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6642\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.6970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 113.3749 - reconstruction_loss: 105.9661 - kl_loss: 7.4088\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 956us/step - loss: 112.0122 - reconstruction_loss: 104.6067 - kl_loss: 7.4055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.0255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.0915\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.1392 - reconstruction_loss: 50.8983 - kl_loss: 13.2410\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63.2466 - reconstruction_loss: 50.0124 - kl_loss: 13.2342\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.7637\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 89.9332 - reconstruction_loss: 79.3794 - kl_loss: 10.5538\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.7375 - reconstruction_loss: 73.1850 - kl_loss: 10.5524\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.1014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 55.0515\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.6225 - reconstruction_loss: 26.9044 - kl_loss: 15.7181\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.7838 - reconstruction_loss: 24.0560 - kl_loss: 15.7278\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.2914\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.1275 - reconstruction_loss: 105.7866 - kl_loss: 9.3409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.2852 - reconstruction_loss: 102.9474 - kl_loss: 9.3378\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4555\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1088\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.6043 - reconstruction_loss: 48.0602 - kl_loss: 13.5441\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 924us/step - loss: 59.0075 - reconstruction_loss: 45.4730 - kl_loss: 13.5345\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0574\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7943\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58.0385 - reconstruction_loss: 44.1829 - kl_loss: 13.8556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.7202 - reconstruction_loss: 43.8886 - kl_loss: 13.8316\n",
      "Success in episode 48 at time step 200 with reward -225.54254720443973\n",
      "Episode 49\n",
      "[ 0.7813519  -0.62409073  0.16897675]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.0429\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.6825\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.8555 - reconstruction_loss: 56.7427 - kl_loss: 10.1127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.1209 - reconstruction_loss: 54.0756 - kl_loss: 10.0453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9175\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.8039\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 47.2229 - reconstruction_loss: 33.9460 - kl_loss: 13.2768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.8098 - reconstruction_loss: 33.5191 - kl_loss: 13.2907\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7131\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.8393 - reconstruction_loss: 81.9506 - kl_loss: 10.8887\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 931us/step - loss: 89.9228 - reconstruction_loss: 79.0805 - kl_loss: 10.8423\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 44.6722 - reconstruction_loss: 30.4732 - kl_loss: 14.1990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.1136 - reconstruction_loss: 25.9100 - kl_loss: 14.2036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.7413\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.2374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.3377 - reconstruction_loss: 97.4020 - kl_loss: 9.9358\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.7530 - reconstruction_loss: 83.8607 - kl_loss: 9.8923\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0658\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1060\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.4891 - reconstruction_loss: 25.8043 - kl_loss: 13.6849\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.2612 - reconstruction_loss: 24.5800 - kl_loss: 13.6812\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6293\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.4771\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.6320 - reconstruction_loss: 89.9240 - kl_loss: 8.7080\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.6493 - reconstruction_loss: 92.9663 - kl_loss: 8.6829\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0523\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9272\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.2362 - reconstruction_loss: 20.8304 - kl_loss: 15.4058\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.6587 - reconstruction_loss: 13.2551 - kl_loss: 15.4036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.3737\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 123.8585 - reconstruction_loss: 114.4208 - kl_loss: 9.4377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.9872 - reconstruction_loss: 113.5612 - kl_loss: 9.4260\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9460\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.7147\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.1138 - reconstruction_loss: 45.3586 - kl_loss: 13.7552\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.7298 - reconstruction_loss: 40.9942 - kl_loss: 13.7356\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.4011\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.6288\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.7457 - reconstruction_loss: 80.9739 - kl_loss: 9.7717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.5461 - reconstruction_loss: 84.7703 - kl_loss: 9.7758\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0147\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 97.0605 - reconstruction_loss: 86.2252 - kl_loss: 10.8352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.4424 - reconstruction_loss: 82.6389 - kl_loss: 10.8036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9870\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.2698 - reconstruction_loss: 36.3470 - kl_loss: 12.9228\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.8905 - reconstruction_loss: 33.9693 - kl_loss: 12.9211\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.8327\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6098\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.8027 - reconstruction_loss: 95.5463 - kl_loss: 9.2564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.9156 - reconstruction_loss: 90.6834 - kl_loss: 9.2322\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5519\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.1559\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.6694 - reconstruction_loss: 28.2222 - kl_loss: 14.4472\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.1321 - reconstruction_loss: 20.7031 - kl_loss: 14.4290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8871\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7924\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.0910 - reconstruction_loss: 108.2517 - kl_loss: 7.8394\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.8112 - reconstruction_loss: 107.9767 - kl_loss: 7.8344\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2536\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1234\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.6914 - reconstruction_loss: 54.2728 - kl_loss: 11.4186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.5170 - reconstruction_loss: 54.1289 - kl_loss: 11.3881\n",
      "Success in episode 49 at time step 200 with reward -199.70819896045282\n",
      "Episode 50\n",
      "[-0.95170844 -0.30700332  0.18684821]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8731\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.9605\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.4335 - reconstruction_loss: 5.7098 - kl_loss: 11.7237\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.9455 - reconstruction_loss: 6.2657 - kl_loss: 11.6798\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9690\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20.1203 - reconstruction_loss: 5.4351 - kl_loss: 14.6852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.0717 - reconstruction_loss: 2.4122 - kl_loss: 14.6596\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1229\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.9676 - reconstruction_loss: 10.7988 - kl_loss: 18.1689\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.3252 - reconstruction_loss: 25.2024 - kl_loss: 18.1228\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1270\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.1490\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.6832 - reconstruction_loss: 32.4868 - kl_loss: 8.1963\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.6252 - reconstruction_loss: 21.4457 - kl_loss: 8.1795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.5921\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 36.0972\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.0779 - reconstruction_loss: 21.9709 - kl_loss: 16.1070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.8675 - reconstruction_loss: 12.7541 - kl_loss: 16.1133\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2463\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.2685 - reconstruction_loss: 80.7910 - kl_loss: 21.4775\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 936us/step - loss: 113.0584 - reconstruction_loss: 91.5803 - kl_loss: 21.4781\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.8199\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 20.2434\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.4938 - reconstruction_loss: 38.5465 - kl_loss: 16.9473\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.0032 - reconstruction_loss: 29.0229 - kl_loss: 16.9803\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6209\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.2134 - reconstruction_loss: 57.4632 - kl_loss: 7.7502\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.5295 - reconstruction_loss: 57.7735 - kl_loss: 7.7560\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.9743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 29.3638\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.0510 - reconstruction_loss: 16.2795 - kl_loss: 13.7715\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 977us/step - loss: 33.5225 - reconstruction_loss: 19.7648 - kl_loss: 13.7577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.7829\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.9896\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.5194 - reconstruction_loss: 10.4887 - kl_loss: 15.0307\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.7589 - reconstruction_loss: 14.7421 - kl_loss: 15.0169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8482\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9035\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 82.0078 - reconstruction_loss: 74.5047 - kl_loss: 7.5031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.4036 - reconstruction_loss: 72.8962 - kl_loss: 7.5074\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.5880\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.4375 - reconstruction_loss: 16.8091 - kl_loss: 12.6284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 871us/step - loss: 23.8800 - reconstruction_loss: 11.2382 - kl_loss: 12.6418\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.8848\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.7300\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.5293 - reconstruction_loss: 71.2018 - kl_loss: 9.3275\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.3016 - reconstruction_loss: 73.9766 - kl_loss: 9.3250\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.1246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5249\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23.4500 - reconstruction_loss: 8.0055 - kl_loss: 15.4445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.1369 - reconstruction_loss: 4.6932 - kl_loss: 15.4437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.9203\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 53.2136\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.0462 - reconstruction_loss: 47.1939 - kl_loss: 8.8523\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.0274 - reconstruction_loss: 45.1769 - kl_loss: 8.8505\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5519\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2310\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.4061 - reconstruction_loss: 22.9362 - kl_loss: 12.4699\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.8341 - reconstruction_loss: 26.3631 - kl_loss: 12.4709\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.1629\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7081\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.7586 - reconstruction_loss: 29.5877 - kl_loss: 13.1709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.3884 - reconstruction_loss: 31.2141 - kl_loss: 13.1742\n",
      "Success in episode 50 at time step 200 with reward -173.42126663971817\n",
      "Episode 51\n",
      "[ 0.45262766 -0.8916996   0.26535043]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.4850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73.5838 - reconstruction_loss: 66.2821 - kl_loss: 7.3017\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.8571 - reconstruction_loss: 59.5605 - kl_loss: 7.2966\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.6713\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.8222\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.8561 - reconstruction_loss: 34.7508 - kl_loss: 13.1053\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.1798 - reconstruction_loss: 15.0461 - kl_loss: 13.1337\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 66.2067\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 66.1460\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 146.2971 - reconstruction_loss: 130.4978 - kl_loss: 15.7993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 166.3904 - reconstruction_loss: 150.5565 - kl_loss: 15.8338\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.8851\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 144.7937 - reconstruction_loss: 119.6095 - kl_loss: 25.1842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.0805 - reconstruction_loss: 95.8688 - kl_loss: 25.2117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 58.6308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 58.1180\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 158.5543 - reconstruction_loss: 145.2477 - kl_loss: 13.3066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 167.1367 - reconstruction_loss: 153.7869 - kl_loss: 13.3497\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.4698\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 16.3740\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 136.1280 - reconstruction_loss: 110.4740 - kl_loss: 25.6540\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 140.3780 - reconstruction_loss: 114.6791 - kl_loss: 25.6989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.3146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.4507\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 247.2887 - reconstruction_loss: 233.0274 - kl_loss: 14.2613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 235.1170 - reconstruction_loss: 220.7903 - kl_loss: 14.3267\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 40.4320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 39.7520\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 140.3919 - reconstruction_loss: 113.8951 - kl_loss: 26.4968\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 152.5511 - reconstruction_loss: 125.9609 - kl_loss: 26.5901\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.7793\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.9219\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 237.9702 - reconstruction_loss: 218.7825 - kl_loss: 19.1877\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 201.0915 - reconstruction_loss: 181.8284 - kl_loss: 19.2631\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9694\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.5707\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.9021 - reconstruction_loss: 99.5028 - kl_loss: 21.3993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.2649 - reconstruction_loss: 103.7794 - kl_loss: 21.4854\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.7910\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 55.8160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 144.5188 - reconstruction_loss: 130.1528 - kl_loss: 14.3660\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.0890 - reconstruction_loss: 117.6526 - kl_loss: 14.4364\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.9534\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.6620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.3628 - reconstruction_loss: 91.2731 - kl_loss: 26.0897\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.8064 - reconstruction_loss: 92.6727 - kl_loss: 26.1338\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.1053\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.6276\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 195.2990 - reconstruction_loss: 177.5452 - kl_loss: 17.7537\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 195.2716 - reconstruction_loss: 177.4500 - kl_loss: 17.8216\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.6601\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 40.4649\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.6471 - reconstruction_loss: 88.9209 - kl_loss: 25.7262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.1495 - reconstruction_loss: 81.3660 - kl_loss: 25.7835\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 40.0933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 39.4589\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.6466 - reconstruction_loss: 154.0159 - kl_loss: 19.6307\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 201.5600 - reconstruction_loss: 181.8676 - kl_loss: 19.6924\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9845\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.4546 - reconstruction_loss: 87.3402 - kl_loss: 20.1144\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.9650 - reconstruction_loss: 78.8179 - kl_loss: 20.1471\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.5257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.5169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.8163 - reconstruction_loss: 103.1259 - kl_loss: 19.6905\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.3800 - reconstruction_loss: 102.5850 - kl_loss: 19.7950\n",
      "Success in episode 51 at time step 200 with reward -174.19783537850012\n",
      "Episode 52\n",
      "[0.9813057  0.1924555  0.96424913]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.7767\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 22.4911\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.7704 - reconstruction_loss: 95.6326 - kl_loss: 22.1378\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.1841 - reconstruction_loss: 97.7983 - kl_loss: 22.3858\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3413\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1672\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 138.2566 - reconstruction_loss: 116.5481 - kl_loss: 21.7085\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 154.5299 - reconstruction_loss: 132.7260 - kl_loss: 21.8039\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.1235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.4006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 144.5750 - reconstruction_loss: 118.0821 - kl_loss: 26.4928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 138.8754 - reconstruction_loss: 112.1575 - kl_loss: 26.7179\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.9655\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 18.3613\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 139.9932 - reconstruction_loss: 123.3771 - kl_loss: 16.6161\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 139.0127 - reconstruction_loss: 122.3198 - kl_loss: 16.6929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.5081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 19.7929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.8451 - reconstruction_loss: 133.4381 - kl_loss: 26.4069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 137.0654 - reconstruction_loss: 110.4928 - kl_loss: 26.5726\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.7607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.9774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.7610 - reconstruction_loss: 44.7472 - kl_loss: 17.0138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.4510 - reconstruction_loss: 42.3416 - kl_loss: 17.1094\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 75.3642\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 74.2098\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63.2493 - reconstruction_loss: 48.5004 - kl_loss: 14.7489\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.8095 - reconstruction_loss: 50.0008 - kl_loss: 14.8087\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.2435\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.1706\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 123.4391 - reconstruction_loss: 96.8731 - kl_loss: 26.5660\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.3578 - reconstruction_loss: 95.6805 - kl_loss: 26.6773\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.6350\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.1214\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 108.6552 - reconstruction_loss: 90.0771 - kl_loss: 18.5782\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.9876 - reconstruction_loss: 89.3586 - kl_loss: 18.6290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8526\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.6045\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 104.0273 - reconstruction_loss: 75.7659 - kl_loss: 28.2613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.5524 - reconstruction_loss: 83.1821 - kl_loss: 28.3702\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.0759\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.9586\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.2412 - reconstruction_loss: 104.6379 - kl_loss: 16.6033\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.4334 - reconstruction_loss: 99.7831 - kl_loss: 16.6503\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.2911\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.9375\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.2864 - reconstruction_loss: 46.2389 - kl_loss: 34.0475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.4412 - reconstruction_loss: 38.2748 - kl_loss: 34.1664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.2944\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.6036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 132.9320 - reconstruction_loss: 114.0955 - kl_loss: 18.8365\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 148.8565 - reconstruction_loss: 129.9738 - kl_loss: 18.8827\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.4529\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 74.1300 - reconstruction_loss: 46.0674 - kl_loss: 28.0626\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.4716 - reconstruction_loss: 44.3082 - kl_loss: 28.1634\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.0205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 26.0341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 124.6373 - reconstruction_loss: 98.6285 - kl_loss: 26.0088\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.8998 - reconstruction_loss: 96.8004 - kl_loss: 26.0994\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.0547\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 31.2171\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.2207 - reconstruction_loss: 64.9867 - kl_loss: 22.2340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 97.0852 - reconstruction_loss: 74.7904 - kl_loss: 22.2948\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.7399\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.9002\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 91.3936 - reconstruction_loss: 67.2291 - kl_loss: 24.1645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.4088 - reconstruction_loss: 66.0942 - kl_loss: 24.3146\n",
      "Success in episode 52 at time step 200 with reward -195.6444417155441\n",
      "Episode 53\n",
      "[ 0.7125998  -0.70157075 -0.9321873 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.4263\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.6709 - reconstruction_loss: 103.3841 - kl_loss: 9.2869\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.7095 - reconstruction_loss: 103.3555 - kl_loss: 9.3539\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 94.8720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 91.8681\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.5150 - reconstruction_loss: 75.7528 - kl_loss: 17.7622\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.6401 - reconstruction_loss: 75.8287 - kl_loss: 17.8114\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.8987\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.9220\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 158.5834 - reconstruction_loss: 148.9536 - kl_loss: 9.6298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 856us/step - loss: 155.8633 - reconstruction_loss: 146.2198 - kl_loss: 9.6435\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 60.5972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 57.6857\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.3819 - reconstruction_loss: 88.1689 - kl_loss: 16.2130\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.1433 - reconstruction_loss: 74.9808 - kl_loss: 16.1626\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 69.4704\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 65.5559\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 136.2824 - reconstruction_loss: 124.3444 - kl_loss: 11.9380\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 138.2796 - reconstruction_loss: 126.3625 - kl_loss: 11.9171\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.3218\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 49.0840\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 138.4491 - reconstruction_loss: 125.3398 - kl_loss: 13.1093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 123.0517 - reconstruction_loss: 109.9921 - kl_loss: 13.0597\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.4465\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.8030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 100.0377 - reconstruction_loss: 84.3660 - kl_loss: 15.6717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.9038 - reconstruction_loss: 81.3417 - kl_loss: 15.5620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.5299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 31.7396\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.2023 - reconstruction_loss: 147.6832 - kl_loss: 11.5191\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 981us/step - loss: 161.1225 - reconstruction_loss: 149.6324 - kl_loss: 11.4901\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.9497\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 44.3967\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.8492 - reconstruction_loss: 100.4838 - kl_loss: 17.3654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.6826 - reconstruction_loss: 107.4948 - kl_loss: 17.1878\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5643\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.1195\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.1617 - reconstruction_loss: 118.4630 - kl_loss: 9.6988\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 123.5650 - reconstruction_loss: 113.8946 - kl_loss: 9.6704\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.0724\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.6947\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.3055 - reconstruction_loss: 70.6994 - kl_loss: 14.6061\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.5373 - reconstruction_loss: 57.0703 - kl_loss: 14.4670\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.5709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.8436\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 158.2226 - reconstruction_loss: 145.2117 - kl_loss: 13.0109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 149.4930 - reconstruction_loss: 136.5571 - kl_loss: 12.9359\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.2567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.8275\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.9923 - reconstruction_loss: 69.2505 - kl_loss: 14.7418\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 947us/step - loss: 72.1395 - reconstruction_loss: 57.5316 - kl_loss: 14.6079\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2901\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 15.7615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.7047 - reconstruction_loss: 102.0444 - kl_loss: 10.6603\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 916us/step - loss: 107.4531 - reconstruction_loss: 96.8433 - kl_loss: 10.6098\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.6830\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.1946\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.2383 - reconstruction_loss: 39.9306 - kl_loss: 16.3077\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 978us/step - loss: 54.9210 - reconstruction_loss: 38.7342 - kl_loss: 16.1868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5559\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.5722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 97.3980 - reconstruction_loss: 87.7324 - kl_loss: 9.6656\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.6371 - reconstruction_loss: 89.9971 - kl_loss: 9.6400\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.1818\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.1903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 97.1628 - reconstruction_loss: 84.9601 - kl_loss: 12.2027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.7077 - reconstruction_loss: 84.6124 - kl_loss: 12.0953\n",
      "Success in episode 53 at time step 200 with reward -208.37666999926824\n",
      "Episode 54\n",
      "[-0.76095456  0.6488052  -0.56090105]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.6139\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 22.5923\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.0229 - reconstruction_loss: 3.3162 - kl_loss: 26.7067\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.9330 - reconstruction_loss: 2.4109 - kl_loss: 26.5221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.2028\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.8359\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.0496 - reconstruction_loss: 54.1307 - kl_loss: 10.9189\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.3145 - reconstruction_loss: 44.4988 - kl_loss: 10.8157\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 58.4822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 58.8995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38.6523 - reconstruction_loss: 21.1022 - kl_loss: 17.5502\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.5400 - reconstruction_loss: 23.1024 - kl_loss: 17.4376\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7611\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.9443\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.1985 - reconstruction_loss: 3.4675 - kl_loss: 27.7310\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.8619 - reconstruction_loss: 4.2671 - kl_loss: 27.5948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 65.8186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 63.1046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.0156 - reconstruction_loss: 36.9687 - kl_loss: 12.0469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 882us/step - loss: 37.2172 - reconstruction_loss: 25.2155 - kl_loss: 12.0017\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 68.1912\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 66.9824\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.0195 - reconstruction_loss: 77.0510 - kl_loss: 12.9685\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.4662 - reconstruction_loss: 67.5657 - kl_loss: 12.9005\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 122.7553\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 120.4875\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.0063 - reconstruction_loss: 32.4062 - kl_loss: 17.6001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.7850 - reconstruction_loss: 49.2274 - kl_loss: 17.5575\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9695\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 13.6655\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.1530 - reconstruction_loss: 28.4149 - kl_loss: 30.7381\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.8911 - reconstruction_loss: 26.2337 - kl_loss: 30.6574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 82.0617\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 79.2477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.0931 - reconstruction_loss: 15.0879 - kl_loss: 19.0052\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.9683 - reconstruction_loss: 13.9643 - kl_loss: 19.0040\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.0646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.3866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 104.8644 - reconstruction_loss: 96.4445 - kl_loss: 8.4199\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 951us/step - loss: 111.5948 - reconstruction_loss: 103.1883 - kl_loss: 8.4065\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.1370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 41.4497\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.0880 - reconstruction_loss: 13.9478 - kl_loss: 14.1403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.7247 - reconstruction_loss: 10.5750 - kl_loss: 14.1497\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 60.2386\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 58.3207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.6741 - reconstruction_loss: 33.2585 - kl_loss: 14.4157\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.0237 - reconstruction_loss: 13.5892 - kl_loss: 14.4345\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1232\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8105\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103.5834 - reconstruction_loss: 94.4558 - kl_loss: 9.1276\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 109.1952 - reconstruction_loss: 100.0833 - kl_loss: 9.1119\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 142.0889\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 136.7544\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.0914 - reconstruction_loss: 20.1763 - kl_loss: 13.9151\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.7165 - reconstruction_loss: 13.7522 - kl_loss: 13.9643\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 71.2124\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 71.5073\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 105.4207 - reconstruction_loss: 87.0406 - kl_loss: 18.3801\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 103.0242 - reconstruction_loss: 84.6119 - kl_loss: 18.4123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.9014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.9098\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.3006 - reconstruction_loss: 20.9347 - kl_loss: 25.3659\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.7261 - reconstruction_loss: 20.3748 - kl_loss: 25.3513\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.4588\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.6960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 52.0632 - reconstruction_loss: 35.1393 - kl_loss: 16.9239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.0775 - reconstruction_loss: 35.1282 - kl_loss: 16.9493\n",
      "Success in episode 54 at time step 200 with reward -161.51077955972843\n",
      "Episode 55\n",
      "[ 0.15228952  0.9883359  -0.75465304]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.7882\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.3706 - reconstruction_loss: 42.5455 - kl_loss: 27.8251\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.1363 - reconstruction_loss: 29.3090 - kl_loss: 27.8273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 64.5564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 61.0728\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.6956 - reconstruction_loss: 25.0068 - kl_loss: 18.6887\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 962us/step - loss: 51.4403 - reconstruction_loss: 32.7262 - kl_loss: 18.7141\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 117.3364\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 111.8704\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68.7057 - reconstruction_loss: 54.9643 - kl_loss: 13.7414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.6715 - reconstruction_loss: 40.8273 - kl_loss: 13.8442\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.9507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.4926\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 98.7020 - reconstruction_loss: 74.1721 - kl_loss: 24.5298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.3644 - reconstruction_loss: 64.7759 - kl_loss: 24.5885\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.6974\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 23.8665\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 46.3073 - reconstruction_loss: 28.6966 - kl_loss: 17.6107\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 927us/step - loss: 41.1758 - reconstruction_loss: 23.5220 - kl_loss: 17.6538\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.1106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.5216\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.8820 - reconstruction_loss: 28.2829 - kl_loss: 14.5991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.8241 - reconstruction_loss: 26.1813 - kl_loss: 14.6428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 93.5552\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 91.8463\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.1263 - reconstruction_loss: 87.0462 - kl_loss: 9.0802\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 98.5573 - reconstruction_loss: 89.4655 - kl_loss: 9.0918\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.7531\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.6034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.6461 - reconstruction_loss: 103.9391 - kl_loss: 11.7070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.4559 - reconstruction_loss: 99.7458 - kl_loss: 11.7102\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 80.4626\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 77.1114\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.6682 - reconstruction_loss: 56.7846 - kl_loss: 15.8836\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.6724 - reconstruction_loss: 55.8117 - kl_loss: 15.8606\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.2742\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.6869\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.4096 - reconstruction_loss: 113.7557 - kl_loss: 8.6539\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 137.5313 - reconstruction_loss: 128.8828 - kl_loss: 8.6485\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.6851\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.8029\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.6257 - reconstruction_loss: 42.0952 - kl_loss: 13.5305\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.0649 - reconstruction_loss: 38.5987 - kl_loss: 13.4662\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6190\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.4299\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.9281 - reconstruction_loss: 74.6031 - kl_loss: 9.3250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 952us/step - loss: 81.6406 - reconstruction_loss: 72.3298 - kl_loss: 9.3109\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.3354\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 20.4117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.5524 - reconstruction_loss: 43.0975 - kl_loss: 11.4549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.7060 - reconstruction_loss: 41.3045 - kl_loss: 11.4016\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.0833\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6065\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 96.7441 - reconstruction_loss: 87.1105 - kl_loss: 9.6336\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.6816 - reconstruction_loss: 79.0772 - kl_loss: 9.6044\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9759\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.2351 - reconstruction_loss: 76.5958 - kl_loss: 11.6392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 835us/step - loss: 90.8146 - reconstruction_loss: 79.2173 - kl_loss: 11.5972\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.4896\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.5020\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.3968 - reconstruction_loss: 67.3866 - kl_loss: 12.0102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.8100 - reconstruction_loss: 73.8497 - kl_loss: 11.9603\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.5726\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.5014\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 71.2854 - reconstruction_loss: 57.1049 - kl_loss: 14.1805\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.4728 - reconstruction_loss: 55.3476 - kl_loss: 14.1252\n",
      "Success in episode 55 at time step 200 with reward -183.9979666705277\n",
      "Episode 56\n",
      "[ 0.9671178   0.25432888 -0.04887919]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.3441\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 41.1318\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.3121 - reconstruction_loss: 69.7794 - kl_loss: 18.5327\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.2416 - reconstruction_loss: 83.7356 - kl_loss: 18.5061\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5629\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4502\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.3710 - reconstruction_loss: 28.6630 - kl_loss: 25.7080\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.4117 - reconstruction_loss: 20.7323 - kl_loss: 25.6793\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.0529\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 49.8387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.4245 - reconstruction_loss: 13.5373 - kl_loss: 15.8871\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.5824 - reconstruction_loss: 7.7455 - kl_loss: 15.8368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.3286\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.4192\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 67.6661 - reconstruction_loss: 57.3629 - kl_loss: 10.3032\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.4560 - reconstruction_loss: 55.1790 - kl_loss: 10.2770\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6237\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.7284 - reconstruction_loss: 48.0603 - kl_loss: 11.6681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 870us/step - loss: 60.6680 - reconstruction_loss: 49.0160 - kl_loss: 11.6520\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.5923\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.1431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.3364 - reconstruction_loss: 83.1186 - kl_loss: 9.2178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.9767 - reconstruction_loss: 81.7736 - kl_loss: 9.2031\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 82.2640\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 82.8741\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.9190 - reconstruction_loss: 11.9989 - kl_loss: 14.9201\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.2209 - reconstruction_loss: 18.3202 - kl_loss: 14.9008\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.9481\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.1142\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.9991 - reconstruction_loss: 15.6335 - kl_loss: 11.3656\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.1873 - reconstruction_loss: 17.8274 - kl_loss: 11.3599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.3729\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 43.0858\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 47.3424 - reconstruction_loss: 35.5584 - kl_loss: 11.7840\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.6435 - reconstruction_loss: 44.8410 - kl_loss: 11.8025\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.3425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.5423\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.1101 - reconstruction_loss: 75.9873 - kl_loss: 24.1228\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 964us/step - loss: 92.5460 - reconstruction_loss: 68.4016 - kl_loss: 24.1444\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6156\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5597\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.0177 - reconstruction_loss: 43.3691 - kl_loss: 20.6487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.7039 - reconstruction_loss: 68.0362 - kl_loss: 20.6678\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 105.2959\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 103.8132\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 96.0088 - reconstruction_loss: 79.3323 - kl_loss: 16.6765\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 970us/step - loss: 95.3578 - reconstruction_loss: 78.6626 - kl_loss: 16.6951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9826\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 72.7262 - reconstruction_loss: 42.7574 - kl_loss: 29.9688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.6059 - reconstruction_loss: 37.5941 - kl_loss: 30.0118\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 64.7301\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 63.6577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.4594 - reconstruction_loss: 144.1136 - kl_loss: 15.3458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 961us/step - loss: 121.7709 - reconstruction_loss: 106.3721 - kl_loss: 15.3989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.7757\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.0418\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.5792 - reconstruction_loss: 79.6205 - kl_loss: 31.9588\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.8218 - reconstruction_loss: 75.7953 - kl_loss: 32.0265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 87.2619\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 86.5866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 157.3561 - reconstruction_loss: 135.6446 - kl_loss: 21.7115\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 158.8429 - reconstruction_loss: 137.0666 - kl_loss: 21.7763\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.1948\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.6678\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 74.0455 - reconstruction_loss: 55.4384 - kl_loss: 18.6071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.7844 - reconstruction_loss: 53.0606 - kl_loss: 18.7238\n",
      "Success in episode 56 at time step 200 with reward -152.8721331200596\n",
      "Episode 57\n",
      "[-0.69292337  0.7210112   0.90709007]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.6341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 19.6433\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 32.4649 - reconstruction_loss: 8.7625 - kl_loss: 23.7024\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.8327 - reconstruction_loss: 6.0267 - kl_loss: 23.8060\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.8886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 30.2330\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.4813 - reconstruction_loss: 32.3072 - kl_loss: 11.1741\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.0440 - reconstruction_loss: 28.8520 - kl_loss: 11.1919\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 63.8874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 62.5191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.2264 - reconstruction_loss: 15.3985 - kl_loss: 15.8279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.1573 - reconstruction_loss: 21.3201 - kl_loss: 15.8372\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.1122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.2152\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.4252 - reconstruction_loss: 26.7228 - kl_loss: 27.7024\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.1485 - reconstruction_loss: 34.4063 - kl_loss: 27.7422\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.5212\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 19.0181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.9118 - reconstruction_loss: 49.7451 - kl_loss: 20.1667\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.5358 - reconstruction_loss: 35.2438 - kl_loss: 20.2919\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.7190\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 21.4108\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.4154 - reconstruction_loss: 74.1497 - kl_loss: 25.2658\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.4061 - reconstruction_loss: 71.0616 - kl_loss: 25.3445\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6587\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.6363\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.0974 - reconstruction_loss: 34.9101 - kl_loss: 24.1874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.8205 - reconstruction_loss: 33.5500 - kl_loss: 24.2705\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 81.5163\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 80.0625\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.3414 - reconstruction_loss: 78.0629 - kl_loss: 18.2785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 127.4037 - reconstruction_loss: 109.1108 - kl_loss: 18.2929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8691\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 15.3931\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.8796 - reconstruction_loss: 32.1743 - kl_loss: 33.7053\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.1788 - reconstruction_loss: 35.4005 - kl_loss: 33.7784\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.1172\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 28.2666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 86.9482 - reconstruction_loss: 64.8988 - kl_loss: 22.0494\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.9843 - reconstruction_loss: 69.8932 - kl_loss: 22.0911\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8409\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 57.8515 - reconstruction_loss: 28.6952 - kl_loss: 29.1563\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.8059 - reconstruction_loss: 39.5865 - kl_loss: 29.2194\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.0756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 45.5897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 86.1900 - reconstruction_loss: 58.4645 - kl_loss: 27.7256\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.5696 - reconstruction_loss: 57.8019 - kl_loss: 27.7677\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6976\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1128\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 50.5571 - reconstruction_loss: 24.3311 - kl_loss: 26.2259\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 912us/step - loss: 46.6677 - reconstruction_loss: 20.3942 - kl_loss: 26.2735\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 106.5419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 104.8673\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 91.7461 - reconstruction_loss: 73.2223 - kl_loss: 18.5238\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 914us/step - loss: 95.1632 - reconstruction_loss: 76.6222 - kl_loss: 18.5410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.9502\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 29.0148\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.8153 - reconstruction_loss: 25.0902 - kl_loss: 35.7251\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.0560 - reconstruction_loss: 19.3041 - kl_loss: 35.7519\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 68.8449\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 68.0821\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.4744 - reconstruction_loss: 58.2037 - kl_loss: 29.2707\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.2523 - reconstruction_loss: 61.9840 - kl_loss: 29.2682\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.3937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.6705\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.9672 - reconstruction_loss: 40.1925 - kl_loss: 24.7746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.4276 - reconstruction_loss: 37.6178 - kl_loss: 24.8098\n",
      "Success in episode 57 at time step 200 with reward -192.42969315885136\n",
      "Episode 58\n",
      "[-0.5424117  -0.84011286  0.44080484]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.0496\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 15.9460\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58.6399 - reconstruction_loss: 48.3091 - kl_loss: 10.3307\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.0632 - reconstruction_loss: 38.7496 - kl_loss: 10.3135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.7960\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.9588\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.0593 - reconstruction_loss: 9.2428 - kl_loss: 20.8165\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.4860 - reconstruction_loss: 12.7161 - kl_loss: 20.7700\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.6778\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.1187\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.5823 - reconstruction_loss: 5.3296 - kl_loss: 32.2528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.0657 - reconstruction_loss: 3.7767 - kl_loss: 32.2890\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4693\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.7553\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.3663 - reconstruction_loss: 74.5300 - kl_loss: 8.8363\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.1803 - reconstruction_loss: 74.3375 - kl_loss: 8.8428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.5416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.5647\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.2277 - reconstruction_loss: 24.9649 - kl_loss: 14.2628\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.6185 - reconstruction_loss: 23.3690 - kl_loss: 14.2495\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 129.0574\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 126.1080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 44.8264 - reconstruction_loss: 26.4715 - kl_loss: 18.3549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.8503 - reconstruction_loss: 32.5280 - kl_loss: 18.3224\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7660\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2407\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.6840 - reconstruction_loss: 125.6987 - kl_loss: 8.9853\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.7235 - reconstruction_loss: 115.7506 - kl_loss: 8.9729\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.6418\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.3602\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.1255 - reconstruction_loss: 100.2699 - kl_loss: 16.8556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.6081 - reconstruction_loss: 90.8650 - kl_loss: 16.7431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.7378\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.0814\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.7682 - reconstruction_loss: 97.7528 - kl_loss: 9.0154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.3341 - reconstruction_loss: 97.3356 - kl_loss: 8.9985\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.6325\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.3985\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.3736 - reconstruction_loss: 85.2498 - kl_loss: 13.1238\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.1818 - reconstruction_loss: 83.1578 - kl_loss: 13.0241\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.6191\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.2876\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.1718 - reconstruction_loss: 81.9520 - kl_loss: 11.2198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.8166 - reconstruction_loss: 82.6506 - kl_loss: 11.1660\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.2455\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.2655\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 141.7285 - reconstruction_loss: 131.8339 - kl_loss: 9.8946\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 139.7419 - reconstruction_loss: 129.8990 - kl_loss: 9.8430\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.4519\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2016\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.7782 - reconstruction_loss: 74.4244 - kl_loss: 15.3538\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.5402 - reconstruction_loss: 77.3038 - kl_loss: 15.2364\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8649\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.4081 - reconstruction_loss: 104.7363 - kl_loss: 7.6718\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.2480 - reconstruction_loss: 103.5804 - kl_loss: 7.6676\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.3239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.4067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.7072 - reconstruction_loss: 52.2102 - kl_loss: 12.4970\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.3531 - reconstruction_loss: 56.9470 - kl_loss: 12.4061\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.2047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.2407\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 107.9426 - reconstruction_loss: 98.0810 - kl_loss: 9.8617\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.8360 - reconstruction_loss: 102.9974 - kl_loss: 9.8386\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.9324\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.3947\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 78.2008 - reconstruction_loss: 64.9762 - kl_loss: 13.2246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.7700 - reconstruction_loss: 64.6293 - kl_loss: 13.1407\n",
      "Success in episode 58 at time step 200 with reward -200.00213066945545\n",
      "Episode 59\n",
      "[ 0.29171696  0.9565047  -0.7257203 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.4308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 14.2822\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54.3798 - reconstruction_loss: 19.5106 - kl_loss: 34.8692\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.0364 - reconstruction_loss: 13.3129 - kl_loss: 34.7235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.4682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.7123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.1950 - reconstruction_loss: 6.3854 - kl_loss: 21.8096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.2481 - reconstruction_loss: 15.6791 - kl_loss: 21.5691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 94.6487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 92.6000\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.5718 - reconstruction_loss: 37.3108 - kl_loss: 15.2610\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.1987 - reconstruction_loss: 27.1014 - kl_loss: 15.0974\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6280\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8412\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.6037 - reconstruction_loss: 83.0541 - kl_loss: 9.5496\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.4557 - reconstruction_loss: 89.9272 - kl_loss: 9.5285\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 81.5672\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 79.4819\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.5326 - reconstruction_loss: 5.1817 - kl_loss: 12.3509\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.9133 - reconstruction_loss: 7.6437 - kl_loss: 12.2696\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.4683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 21.0658\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.0557 - reconstruction_loss: 65.4167 - kl_loss: 8.6390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.9561 - reconstruction_loss: 71.3595 - kl_loss: 8.5966\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0270\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.3670 - reconstruction_loss: 28.6971 - kl_loss: 12.6699\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 898us/step - loss: 40.3926 - reconstruction_loss: 27.7436 - kl_loss: 12.6490\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.2515\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.4393\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 114.1019 - reconstruction_loss: 107.1164 - kl_loss: 6.9855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.8341 - reconstruction_loss: 110.8647 - kl_loss: 6.9694\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0801\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.0467\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.4996 - reconstruction_loss: 35.4279 - kl_loss: 14.0717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.7085 - reconstruction_loss: 42.6522 - kl_loss: 14.0563\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.1814\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.0123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 116.5522 - reconstruction_loss: 108.7450 - kl_loss: 7.8072\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.6239 - reconstruction_loss: 110.8233 - kl_loss: 7.8006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.2582\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.8340\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21.9988 - reconstruction_loss: 8.5556 - kl_loss: 13.4432\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.6643 - reconstruction_loss: 11.2319 - kl_loss: 13.4325\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.1292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 55.2249\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.0446 - reconstruction_loss: 47.6054 - kl_loss: 8.4391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.7763 - reconstruction_loss: 51.3495 - kl_loss: 8.4268\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.1093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 13.0211\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51.7186 - reconstruction_loss: 39.7156 - kl_loss: 12.0030\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.3108 - reconstruction_loss: 36.2987 - kl_loss: 12.0122\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.7967\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.6455\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.6851 - reconstruction_loss: 64.7460 - kl_loss: 9.9391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.0618 - reconstruction_loss: 60.1257 - kl_loss: 9.9361\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.4975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.2788\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63.4426 - reconstruction_loss: 52.2283 - kl_loss: 11.2143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.2817 - reconstruction_loss: 53.0640 - kl_loss: 11.2177\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.3293\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.1344\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.1922 - reconstruction_loss: 35.4525 - kl_loss: 12.7397\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.5120 - reconstruction_loss: 30.7552 - kl_loss: 12.7568\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.7764\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.7755\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 57.0004 - reconstruction_loss: 44.2648 - kl_loss: 12.7356\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.7774 - reconstruction_loss: 44.0225 - kl_loss: 12.7548\n",
      "Success in episode 59 at time step 200 with reward -178.17615527525095\n",
      "Episode 60\n",
      "[ 0.07801007  0.9969526  -0.57059747]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4041\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.5944\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 43.3723 - reconstruction_loss: 14.0117 - kl_loss: 29.3606\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.1971 - reconstruction_loss: 22.8839 - kl_loss: 29.3132\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.0962\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.7380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.3072 - reconstruction_loss: 11.8338 - kl_loss: 18.4735\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.4708 - reconstruction_loss: 8.9624 - kl_loss: 18.5084\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3949\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.6918\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 74.7672 - reconstruction_loss: 67.5416 - kl_loss: 7.2256\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 986us/step - loss: 76.6401 - reconstruction_loss: 69.4262 - kl_loss: 7.2139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.9511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 53.6823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.9761 - reconstruction_loss: 22.9811 - kl_loss: 11.9951\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.2462 - reconstruction_loss: 25.2162 - kl_loss: 12.0300\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.2623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.0990\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 100.3314 - reconstruction_loss: 92.1831 - kl_loss: 8.1483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.5261 - reconstruction_loss: 93.3898 - kl_loss: 8.1363\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.8222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.7608\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.1344 - reconstruction_loss: 52.0461 - kl_loss: 9.0882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 61.9213 - reconstruction_loss: 52.8239 - kl_loss: 9.0974\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.3997\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.3254\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.5936 - reconstruction_loss: 77.2337 - kl_loss: 10.3598\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 87.0383 - reconstruction_loss: 76.6776 - kl_loss: 10.3607\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.3614\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 23.6599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.8018 - reconstruction_loss: 80.4812 - kl_loss: 7.3205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.5549 - reconstruction_loss: 82.2358 - kl_loss: 7.3191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.1804\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 47.6158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.0318 - reconstruction_loss: 46.1015 - kl_loss: 13.9303\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 61.5631 - reconstruction_loss: 47.6439 - kl_loss: 13.9192\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.8505\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 21.2165\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.0126 - reconstruction_loss: 82.6060 - kl_loss: 7.4067\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.1226 - reconstruction_loss: 83.7279 - kl_loss: 7.3948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.9472\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.8725\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.9211 - reconstruction_loss: 57.8367 - kl_loss: 8.0844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.5263 - reconstruction_loss: 58.4505 - kl_loss: 8.0757\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.9162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.4600\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 46.0970 - reconstruction_loss: 36.0647 - kl_loss: 10.0324\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.2140 - reconstruction_loss: 37.2017 - kl_loss: 10.0122\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 72.7162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 72.4808\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.1100 - reconstruction_loss: 12.9539 - kl_loss: 14.1561\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.5945 - reconstruction_loss: 11.4661 - kl_loss: 14.1284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6916\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.2878 - reconstruction_loss: 52.8627 - kl_loss: 6.4251\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.4358 - reconstruction_loss: 60.0111 - kl_loss: 6.4248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 146.2699\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 143.2165\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.9666 - reconstruction_loss: 4.5708 - kl_loss: 13.3958\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.5856 - reconstruction_loss: 8.2042 - kl_loss: 13.3814\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 152.4174\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 150.0449\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.1143 - reconstruction_loss: 91.3989 - kl_loss: 16.7154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.7992 - reconstruction_loss: 80.0565 - kl_loss: 16.7428\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.0262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.0873\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.5043 - reconstruction_loss: 46.6156 - kl_loss: 12.8887\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 61.0289 - reconstruction_loss: 48.1402 - kl_loss: 12.8887\n",
      "Success in episode 60 at time step 200 with reward -203.8628271574294\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "daifa, results_one = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=60, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "        reward  timesteps  num_actions\n0  -240.178894        204           34\n1  -236.250809        204           34\n2  -177.518585        204           34\n3  -294.603455        204           34\n4  -225.212326        204           34\n5  -243.501526        204           34\n6  -241.293915        204           34\n7  -245.386963        204           34\n8  -272.843750        204           34\n9  -231.791687        204           34\n10 -295.409882        204           34\n11 -159.568558        204           34\n12 -233.674576        204           34\n13 -238.025818        204           34\n14 -237.496368        204           34\n15 -236.328537        204           34\n16 -247.163284        204           34\n17 -273.942078        204           34\n18 -232.563431        204           34\n19 -207.779099        204           34",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-240.178894</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-236.250809</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-177.518585</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-294.603455</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-225.212326</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-243.501526</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-241.293915</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-245.386963</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-272.843750</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-231.791687</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-295.409882</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-159.568558</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-233.674576</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-238.025818</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-237.496368</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-236.328537</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-247.163284</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-273.942078</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-232.563431</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-207.779099</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x29bcd7b50>]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results_one.VFE_post_run)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "daifa.train_vae = False\n",
    "daifa.model_vae.show_training = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[0.89164525 0.4527347  0.46346357]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2106\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.9535\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 21.1716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 61.3554\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 62.4517\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0953\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.7895\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.2925\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.0861\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.4001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.0964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5889\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 69.6212\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 67.9569\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 13.4316\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.0424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 43.5164\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8753\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6336\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.6125\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 48.5890\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.9314\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.8511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.5987\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3410\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.2139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 62.2755\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 60.1510\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.7945\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.9926\n",
      "Success in episode 1 at time step 200 with reward -177.10631085778076\n",
      "Episode 2\n",
      "[-0.6902832   0.7235393   0.94377536]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 55.1338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 55.3596\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.6598\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.9988\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9317\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8507\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.2622\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.8107\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.6086\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.2803\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0726\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4894\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8555\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.8991\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8054\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4053\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 14.6721\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.3291\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7848\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7104\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6579\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.7169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9648\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.8671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3535\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.8177\n",
      "training on full data\n",
      "2 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9607\n",
      "Success in episode 2 at time step 200 with reward -295.6842201397935\n",
      "Episode 3\n",
      "[-0.27815387 -0.96053654 -0.22486311]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2273\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.6173\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5862\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1682\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.9374\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.0059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2654\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6603\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7227\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9063\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.2492\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3322\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.2587\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5182\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2261\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6495\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.3664\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5355\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.1795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7156\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.0378\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2269\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6970\n",
      "training on full data\n",
      "6 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3777\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1355\n",
      "Success in episode 3 at time step 200 with reward -291.60104878708944\n",
      "Episode 4\n",
      "[-0.46820292  0.883621    0.5799883 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4648\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.9549\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.2907\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2582\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4487\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5669\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.4033\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3617\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2206\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5816\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.1302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6526\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0233\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1722\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6138\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6877\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8878\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6569\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7011\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9837\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8612\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8987\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2493\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2924\n",
      "fast thinking\n",
      "training on full data\n",
      "2 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5532\n",
      "Success in episode 4 at time step 200 with reward -303.70020058668183\n",
      "Episode 5\n",
      "[-0.82350445 -0.56730986  0.1957984 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6653\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2254\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3991\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3145\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2663\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0675\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1693\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6379\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0712\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0481\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9867\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9832\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8412\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0723\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6504\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6347\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6740\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6370\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1889\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1727\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2343\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2187\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1697\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4147\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3610\n",
      "Success in episode 5 at time step 200 with reward -320.2211283425629\n",
      "Episode 6\n",
      "[-0.14063767 -0.9900611   0.09389881]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8610\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.8593\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.0757\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.7839\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0024\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0228\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 124.7359\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 119.9790\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.7188\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 14.8747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0463\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5602\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4035\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0543\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0406\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0004\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6944\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.4270\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2883\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8279\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3061\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3550\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2998\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9155\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0348\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9619\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7980\n",
      "Success in episode 6 at time step 200 with reward -246.29350659912316\n",
      "Episode 7\n",
      "[-0.18083517  0.9835134  -0.86173844]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5418\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.4680\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4635\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9034\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7893\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7842\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9283\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2361\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2406\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6939\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0766\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1371\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5632\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4826\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1432\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.0389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8494\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3655\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3482\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7988\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3588\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3123\n",
      "fast thinking\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4206\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2212\n",
      "Success in episode 7 at time step 200 with reward -295.08342462674466\n",
      "Episode 8\n",
      "[-0.5313134   0.8471753   0.06660543]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6562\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5612\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.1693\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2895\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5190\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1384\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1392\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7256\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5177\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6622\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0871\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5018\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3464\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6002\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7384\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6848\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4490\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4650\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9973\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8325\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7405\n",
      "Success in episode 8 at time step 200 with reward -302.4139312821975\n",
      "Episode 9\n",
      "[-0.03224153  0.9994801   0.558613  ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5373\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.4461\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.2578\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.9995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.8965\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 52.1804\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4040\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.4382\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.5994\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 13.9488\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3455\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 19.7938\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9504\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8345\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8413\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.1400\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.8363\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8480\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7120\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4884\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2307\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9841\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3902\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9396\n",
      "fast thinking\n",
      "training on full data\n",
      "2 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0662\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9476\n",
      "Success in episode 9 at time step 200 with reward -238.0553614281409\n",
      "Episode 10\n",
      "[ 0.08381242 -0.99648154 -0.99922097]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7465\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7218\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9880\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0835\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5702\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5290\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5817\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.5600\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2837\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8945\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5085\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2401\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8589\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7908\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9457\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9219\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3825\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3536\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3238\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1421\n",
      "training on full data\n",
      "3 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5668\n",
      "Success in episode 10 at time step 200 with reward -289.5180154924622\n",
      "Episode 11\n",
      "[ 0.88377196  0.4679179  -0.91967106]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.0011\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 37.3715\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.5338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 38.0470\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.0703\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 28.0743\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.9372\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.7119\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2330\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.0848\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.5939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.8435\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7002\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4166\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.1376\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 35.0001\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8245\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4977\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.6917\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 32.9602\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7539\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.3684\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.2378\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.5351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.9899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.9064\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 16.7726\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4217\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.8711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.3824\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.2788\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.7465\n",
      "Success in episode 11 at time step 200 with reward -204.03054276303322\n",
      "Episode 12\n",
      "[-0.97970855  0.20042734  0.96173143]\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2025\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.4495\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8892\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8675\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7575\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1782\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8975\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1704\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.6997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7439\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5589\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7976\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.1453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0748\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0403\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2476\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9029\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8984\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1782\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1855\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1718\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1476\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7826\n",
      "training on full data\n",
      "3 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6449\n",
      "Success in episode 12 at time step 200 with reward -303.23222764499906\n",
      "Episode 13\n",
      "[ 0.28702193 -0.957924    0.34901375]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.5866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9636\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9426\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7777\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3315\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4528\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.6447\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7478\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6758\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1985\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.7897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1862\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0660\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9000\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4382\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4500\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5765\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4797\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4175\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4906\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6141\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4774\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.4832\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3936\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3990\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3424\n",
      "Success in episode 13 at time step 200 with reward -299.32152156449877\n",
      "Episode 14\n",
      "[-0.4206304  -0.9072321  -0.43874496]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.6943\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9675\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8708\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9698\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1886\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3541\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2509\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7054\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6511\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7636\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7035\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7026\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7021\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2921\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3027\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7310\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6515\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6204\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3354\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3154\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2689\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2919\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3207\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2213\n",
      "training on full data\n",
      "5 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0180\n",
      "Success in episode 14 at time step 200 with reward -297.14806411061306\n",
      "Episode 15\n",
      "[ 0.14929186 -0.9887932  -0.38921458]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5765\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5889\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.6757\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.5321\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8981\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.8428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 73.6992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 70.9637\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3222\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6811\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.6482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1094\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0812\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0573\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2984\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2753\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8838\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8109\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1510\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4602\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3031\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8961\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8347\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6457\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5426\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9130\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8256\n",
      "Success in episode 15 at time step 200 with reward -259.5628193443614\n",
      "Episode 16\n",
      "[0.93796945 0.3467179  0.5605539 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.4688\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.1425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 22.4816\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.9346\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.9252\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.9386\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 26.7975\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9268\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3969\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3301\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8472\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.7285\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.7572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.5111\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8630\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8174\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5061\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 13.4579\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9775\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9180\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.0479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 50.1257\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9253\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.2725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 19.7324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2112\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.9269\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.8118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5571\n",
      "Success in episode 16 at time step 200 with reward -179.884378473262\n",
      "Episode 17\n",
      "[ 0.54939455 -0.83556306  0.00896839]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.2104\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 23.0021\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 275.7720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 270.1480\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 124.1932\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 121.8786\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.7695\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.7067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.6464\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 26.4347\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.8471\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.4159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.1267\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 29.4153\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5686\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3580\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.0437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2772\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.2156\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1828\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.8472\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9551\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3021\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3060\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3892\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.3824\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8935\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9121\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9531\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9024\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.2069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.7872\n",
      "Success in episode 17 at time step 200 with reward -232.03397443836778\n",
      "Episode 18\n",
      "[-0.9244872 -0.3812131  0.6741809]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.8765\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9359\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0325\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0621\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3424\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6799\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.6849\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3776\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9581\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6715\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.5106\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4258\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2587\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4587\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3977\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9582\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9230\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4420\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9976\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0198\n",
      "fast thinking\n",
      "training on full data\n",
      "3 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8401\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7819\n",
      "Success in episode 18 at time step 200 with reward -282.40372542529434\n",
      "Episode 19\n",
      "[-0.87785685  0.47892308  0.268634  ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.1278\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9711\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9661\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.8977\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9984\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9388\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3969\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3513\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0086\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9400\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8844\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1476\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1400\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1847\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0936\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6468\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9961\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4175\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3854\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2863\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2396\n",
      "fast thinking\n",
      "training on full data\n",
      "7 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6328\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6112\n",
      "Success in episode 19 at time step 200 with reward -281.89047123910717\n",
      "Episode 20\n",
      "[ 0.9785394  -0.20605984  0.24496853]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 119.6567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 116.1589\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.9480\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 41.2473\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.6636\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8100\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7364\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.8583\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 26.0115\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.0723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 36.2456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2770\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.2273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6826\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.5455\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.3556\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9458\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2940\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5986\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2606\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2481\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4229\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2517\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1111\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7985\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7708\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2939\n",
      "Success in episode 20 at time step 200 with reward -218.95201163436744\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "daifa, results_two = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# # make the HABIT ACTION NET\n",
    "# habit_net = HabitualAction(latent_dim, 1, [16, 16], train_epochs=2, show_training=True)\n",
    "# habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "#\n",
    "# daifa.habit_action_model = habit_net\n",
    "#\n",
    "# actor_model = get_actor(latent_dim, 1)\n",
    "# critic_model = get_critic(latent_dim, 1)\n",
    "#\n",
    "# target_actor = get_actor(latent_dim, 1)\n",
    "# target_critic = get_critic(latent_dim, 1)\n",
    "#\n",
    "# # Making the weights equal initially\n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "#\n",
    "# critic_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "# actor_optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "#\n",
    "# habit_net = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau=0.005, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "        reward  timesteps  num_actions\n0  -215.116028        204           34\n1  -278.625488        204           34\n2  -277.194183        204           34\n3  -147.607010        204           34\n4  -245.454132        204           34\n5  -238.050583        204           34\n6  -198.725281        204           34\n7  -214.721695        204           34\n8  -295.793976        204           34\n9  -231.240402        204           34\n10 -224.348999        204           34\n11 -158.061401        204           34\n12 -197.407715        204           34\n13 -204.956879        204           34\n14 -240.626740        204           34\n15 -238.016098        204           34\n16 -296.053040        204           34\n17 -201.960495        204           34\n18 -261.613770        204           34\n19 -217.205887        204           34",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-215.116028</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-278.625488</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-277.194183</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-147.607010</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-245.454132</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-238.050583</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-198.725281</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-214.721695</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-295.793976</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-231.240402</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-224.348999</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-158.061401</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-197.407715</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-204.956879</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-240.626740</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-238.016098</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-296.053040</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-201.960495</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-261.613770</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-217.205887</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[ 0.01197588 -0.9999283   0.57208574]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7377\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9838\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9921\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4153\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5552\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0600\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0808\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5400\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0131\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9940\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5350\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4657\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3869\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0569\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9574\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0797\n",
      "training on full data\n",
      "3 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0994\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9793\n",
      "Success in episode 1 at time step 200 with reward -280.5254926977061\n",
      "Episode 2\n",
      "[-0.84511137  0.5345903  -0.4199457 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8128\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1813\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2721\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9876\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0263\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2110\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8613\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1115\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0341\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5134\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3362\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8389\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7310\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7345\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6621\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2228\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1104\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0998\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1339\n",
      "training on full data\n",
      "3 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5123\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4769\n",
      "Success in episode 2 at time step 200 with reward -290.832192444709\n",
      "Episode 3\n",
      "[0.712352  0.7018224 0.8321315]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1449\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1042\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.3021\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 16.1536\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.6955\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.0488\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.7200\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.3463\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.8070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.3286\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0441\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 16.9326\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0496\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9686\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0697\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7703\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8765\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2334\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4719\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0379\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8007\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5190\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5253\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9481\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9233\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5108\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3523\n",
      "Success in episode 3 at time step 200 with reward -238.6355261452773\n",
      "Episode 4\n",
      "[0.98424345 0.17681874 0.70145506]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.3302\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.0904\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8720\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.8459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.0047\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7838\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.7415\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.6688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.0032\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1961\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0140\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.8411\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.2395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.5933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1661\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.1090\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.7184\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5545\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4189\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.1587\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 21.0477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.8192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.6060\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5940\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4513\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.6134\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.7158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5790\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3457\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1739\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6224\n",
      "Success in episode 4 at time step 200 with reward -176.60038504829635\n",
      "Episode 5\n",
      "[0.04772982 0.9988603  0.8132008 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1076\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2480\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.3831\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2461\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.3623\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1342\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2243\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 72.7478\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 71.1319\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5952\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9350\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9464\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.8895\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6188\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5252\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.4581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.4069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5605\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.4225\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.3047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.8562\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.5889\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.6659\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6116\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1704\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8318\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6994\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3791\n",
      "Success in episode 5 at time step 200 with reward -202.11748923706105\n",
      "Episode 6\n",
      "[-0.4356525   0.90011495 -0.8924787 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9403\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.7644\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.6333\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1244\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3931\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9871\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3096\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8434\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5568\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2346\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2331\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9806\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9836\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8563\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8186\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2269\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1211\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2911\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2197\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0586\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7574\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7839\n",
      "training on full data\n",
      "2 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9157\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8218\n",
      "Success in episode 6 at time step 200 with reward -296.4680712073047\n",
      "Episode 7\n",
      "[ 0.7964246  -0.60473776 -0.09037834]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4845\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0945\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.1660\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 22.7048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.3828\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 18.1965\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7506\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 69.4148\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 67.0441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.6024\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8815\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0074\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5327\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.4811\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5471\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5276\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2969\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.1258\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3193\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0655\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3470\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3166\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6679\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1883\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0211\n",
      "fast thinking\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0274\n",
      "Success in episode 7 at time step 200 with reward -221.11129494203382\n",
      "Episode 8\n",
      "[ 0.9415665  -0.336827   -0.40236956]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.7761\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.0752\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9736\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9901\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.7028\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6373\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6019\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0135\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9896\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5535\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.5192\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6465\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2703\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1527\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5256\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1014\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6473\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3702\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4888\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4780\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2849\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3923\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5668\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5584\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5705\n",
      "Success in episode 8 at time step 200 with reward -273.07974349364383\n",
      "Episode 9\n",
      "[-0.9180452   0.39647567 -0.47025457]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7901\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4804\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5426\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7084\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1605\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2031\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1131\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1115\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2181\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2246\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0607\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8627\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6745\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5697\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2344\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9767\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4777\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5013\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4715\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4348\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5367\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6561\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7571\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7458\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8263\n",
      "Success in episode 9 at time step 200 with reward -324.55848584301623\n",
      "Episode 10\n",
      "[-0.5060007  -0.8625331  -0.79432577]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9286\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7834\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0927\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6145\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5877\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5463\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5945\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0953\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1905\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0724\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7860\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6950\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5163\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3498\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5251\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4319\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1076\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9534\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0891\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2011\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2325\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5512\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5862\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8383\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8087\n",
      "Success in episode 10 at time step 200 with reward -300.66035431428134\n",
      "Episode 11\n",
      "[ 0.6707477  -0.74168557  0.36920983]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.9441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8435\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6336\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6328\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5038\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.3646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7210\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6791\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6160\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.4661\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.0649\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1149\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0731\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2730\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3545\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1117\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4094\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7203\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7378\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3232\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7401\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6083\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9952\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9581\n",
      "Success in episode 11 at time step 200 with reward -268.1009704919865\n",
      "Episode 12\n",
      "[ 0.30325115 -0.95291066  0.08252774]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.9608\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.4624\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3300\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.5167\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 24.2666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.0820\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 13.9595\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.1569\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 15.6257\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5599\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.4901\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3714\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0702\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.8425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.2078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.7297\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 24.6892\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.2835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.2645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6117\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.3646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.1838\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.2470\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8054\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.4799\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.1408\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.7369\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8367\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.0894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 48.5938\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.2405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.6052\n",
      "Success in episode 12 at time step 200 with reward -179.27990607231638\n",
      "Episode 13\n",
      "[-0.9089826  -0.41683418 -0.04881703]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.6860\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.6671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4007\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7632\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9190\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3104\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4297\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3730\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5668\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1576\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8678\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5897\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3912\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2343\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2830\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9456\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9312\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3764\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3858\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7263\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1435\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1025\n",
      "training on full data\n",
      "4 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0193\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8008\n",
      "Success in episode 13 at time step 200 with reward -301.34560880475266\n",
      "Episode 14\n",
      "[-0.13879012  0.9903218  -0.9935464 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7510\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8071\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0966\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0643\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.4358\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5708\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9232\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9107\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5247\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0970\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6973\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8930\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7496\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9409\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5503\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5437\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1828\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1112\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5542\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5193\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6917\n",
      "Success in episode 14 at time step 200 with reward -290.1540956031001\n",
      "Episode 15\n",
      "[-0.11311609  0.9935818   0.34800577]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7751\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7519\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7262\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9910\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.9069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4904\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2099\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3713\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4145\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4350\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3504\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.1645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2526\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.8241\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4343\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2762\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4529\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2638\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8856\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9145\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9624\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3013\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2755\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2157\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1884\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8637\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8024\n",
      "Success in episode 15 at time step 200 with reward -293.93385943650776\n",
      "Episode 16\n",
      "[-0.13129993  0.99134266 -0.70653206]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4064\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.4162\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4321\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.1550\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3888\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9005\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0362\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6345\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6369\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1190\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0381\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9837\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0182\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0161\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.9838\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0233\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7489\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3254\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2910\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9935\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7669\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3429\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3092\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9991\n",
      "training on full data\n",
      "3 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4033\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3765\n",
      "Success in episode 16 at time step 200 with reward -283.7388718129546\n",
      "Episode 17\n",
      "[-0.8252994  -0.5646954   0.22242564]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6751\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.5360\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5969\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8295\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4618\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1621\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2788\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2898\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3532\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2963\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7503\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9586\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8152\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0585\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1604\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1738\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5867\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1173\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3699\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3661\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1620\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1720\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2528\n",
      "Success in episode 17 at time step 200 with reward -306.459035050168\n",
      "Episode 18\n",
      "[0.58390146 0.8118245  0.4280271 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.4878\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 14.4249\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4283\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.3589\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1958\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2249\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.2441\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.1472\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4778\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0354\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0261\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.2598\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 18.6932\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.3247\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1567\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.1734\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2434\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1025\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9303\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0638\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.9850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9379\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7979\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3867\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6580\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5348\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3690\n",
      "Success in episode 18 at time step 200 with reward -185.2560327011188\n",
      "Episode 19\n",
      "[0.03286463 0.9994598  0.92055446]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8854\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.0645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.1439\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6003\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5776\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.1733\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.5728\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 22.6960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.8542\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.6196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 57.7896\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 57.0061\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.7100\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 23.1313\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.3934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.3425\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 68.4210\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 66.3951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8354\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.9551\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 18.9300\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2910\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8880\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9923\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.3092\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.0971\n",
      "Success in episode 19 at time step 200 with reward -153.6447703183406\n",
      "Episode 20\n",
      "[-0.94537234 -0.32599252 -0.64262825]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8015\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4935\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5026\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9231\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8770\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0847\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0800\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8643\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3147\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2154\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6630\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5554\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9814\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9676\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5448\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5257\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9641\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6248\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6028\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3669\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4356\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4405\n",
      "training on full data\n",
      "4 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7913\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6120\n",
      "Success in episode 20 at time step 200 with reward -299.9720299705178\n"
     ]
    }
   ],
   "source": [
    "daifa.habit_action_model.show_training = False\n",
    "daifa.train_habit_net = True\n",
    "daifa.train_after_exploring = True\n",
    "daifa.use_kl_intrinsic = True\n",
    "daifa.use_kl_extrinsic = False\n",
    "daifa.use_fast_thinking = True\n",
    "daifa.uncertainty_tolerance = 0.1\n",
    "\n",
    "# daifa.tran.show_training = False\n",
    "# daifa.prior_model.show_training = False\n",
    "\n",
    "# train the agent on the env\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "daifa, results_three = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "        reward  timesteps  num_actions\n0  -292.885406        204           34\n1  -178.971710        204           34\n2  -216.117401        204           34\n3  -225.894226        204           34\n4  -239.553558        204           34\n5  -233.857758        204           34\n6  -273.116150        204           34\n7  -196.400711        204           34\n8  -274.928802        204           34\n9  -239.118088        204           34\n10 -217.762741        204           34\n11 -221.487839        204           34\n12 -246.811676        204           34\n13 -232.531311        204           34\n14 -230.806229        204           34\n15 -158.019882        204           34\n16 -241.984879        204           34\n17 -149.776886        204           34\n18 -242.157303        204           34\n19 -242.104965        204           34",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-292.885406</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-178.971710</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-216.117401</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-225.894226</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-239.553558</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-233.857758</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-273.116150</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-196.400711</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-274.928802</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-239.118088</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-217.762741</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-221.487839</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-246.811676</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-232.531311</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-230.806229</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-158.019882</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-241.984879</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-149.776886</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-242.157303</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-242.104965</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x17470aca0>,\n <matplotlib.lines.Line2D at 0x174af0d90>,\n <matplotlib.lines.Line2D at 0x175a4e4f0>]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100), np.zeros(100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(obs_pos)\n",
    "\n",
    "utils = daifa.prior_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(vel_pos)\n",
    "\n",
    "utils = daifa.prior_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(obs_pos)\n",
    "\n",
    "# utils = daifa.habit_action_model.actor_model(latent_mean)\n",
    "utils = daifa.select_fast_thinking_policy(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(vel_pos)\n",
    "\n",
    "# utils = daifa.habit_action_model.actor_model(latent_mean)\n",
    "utils = daifa.select_fast_thinking_policy(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99992335]]\n",
      "[[-0.88178855]]\n",
      "[[0.99789256]]\n",
      "[[-0.9998848]]\n",
      "[[-1.]]\n",
      "[[-0.9999769]]\n",
      "[[0.99991107]]\n",
      "[[-0.16730253]]\n",
      "[[-0.9999739]]\n",
      "[[-0.8034171]]\n",
      "[[0.9970662]]\n",
      "[[-0.9999894]]\n",
      "[[-1.]]\n",
      "[[-0.99990356]]\n",
      "[[0.999879]]\n",
      "[[-0.97488046]]\n",
      "[[-0.9999994]]\n",
      "[[-0.9999995]]\n",
      "[[-0.9999298]]\n",
      "[[0.99215883]]\n",
      "[[-0.9951003]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.9989349]]\n",
      "[[0.9983758]]\n",
      "[[-0.966355]]\n",
      "[[0.9603897]]\n",
      "[[0.9575787]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.999734]]\n",
      "[[0.9678278]]\n",
      "[[-0.999788]]\n",
      "[[0.99940175]]\n",
      "[[0.99864036]]\n",
      "[[0.9983579]]\n",
      "[[0.9979407]]\n",
      "[[0.9991464]]\n",
      "[[0.99915755]]\n",
      "[[0.99778235]]\n",
      "[[0.9984719]]\n",
      "[[0.99706304]]\n",
      "[[0.998763]]\n",
      "[[0.9988698]]\n",
      "[[0.9993559]]\n",
      "[[0.9990138]]\n",
      "[[0.99794537]]\n",
      "[[0.99839854]]\n",
      "[[0.99911153]]\n",
      "[[0.9989697]]\n",
      "[[0.9992252]]\n",
      "[[0.9982412]]\n",
      "[[0.9965633]]\n",
      "[[0.99803454]]\n",
      "[[0.9994383]]\n",
      "[[0.99928397]]\n",
      "[[0.99822474]]\n",
      "[[0.99837875]]\n",
      "[[0.998344]]\n",
      "[[0.9986446]]\n",
      "[[0.99930716]]\n",
      "[[0.9991316]]\n",
      "[[0.9982009]]\n",
      "[[0.9972399]]\n",
      "[[0.9988507]]\n",
      "[[0.9991977]]\n",
      "[[0.9992266]]\n",
      "[[0.7610545]]\n",
      "[[0.99936944]]\n",
      "[[0.9997359]]\n",
      "[[0.9990438]]\n",
      "[[0.99532485]]\n",
      "[[0.6572884]]\n",
      "[[0.9957741]]\n",
      "[[0.99969804]]\n",
      "[[0.9997258]]\n",
      "[[0.99891406]]\n",
      "[[0.94040376]]\n",
      "[[0.38200775]]\n",
      "[[0.9993763]]\n",
      "[[0.9997448]]\n",
      "[[0.9992805]]\n",
      "[[0.9952285]]\n",
      "[[-0.93943685]]\n",
      "[[0.9984129]]\n",
      "[[0.99984366]]\n",
      "[[0.99897325]]\n",
      "[[0.9981243]]\n",
      "[[-0.75387]]\n",
      "[[0.98440313]]\n",
      "[[0.9997273]]\n",
      "[[0.99954104]]\n",
      "[[0.9986005]]\n",
      "[[0.66133595]]\n",
      "[[-0.95227736]]\n",
      "[[0.9994654]]\n",
      "[[0.99972373]]\n",
      "[[0.9988063]]\n",
      "[[0.99198824]]\n",
      "[[-0.9997448]]\n",
      "[[-0.81360483]]\n",
      "[[0.98846155]]\n",
      "[[0.998729]]\n",
      "[[-0.9997293]]\n",
      "[[-0.9999994]]\n",
      "[[0.9901023]]\n",
      "[[0.9996195]]\n",
      "[[0.9842945]]\n",
      "[[0.99777156]]\n",
      "[[-0.9990607]]\n",
      "[[-0.9999959]]\n",
      "[[0.9973255]]\n",
      "[[0.9998672]]\n",
      "[[0.98625666]]\n",
      "[[0.998695]]\n",
      "[[-0.9993962]]\n",
      "[[-0.99999744]]\n",
      "[[0.98042065]]\n",
      "[[0.9998296]]\n",
      "[[0.9862935]]\n",
      "[[0.9992096]]\n",
      "[[-0.9998129]]\n",
      "[[-0.99999815]]\n",
      "[[0.994805]]\n",
      "[[0.99963593]]\n",
      "[[0.9870506]]\n",
      "[[0.99905175]]\n",
      "[[-0.9997644]]\n",
      "[[-0.99999756]]\n",
      "[[0.9931796]]\n",
      "[[0.99962646]]\n",
      "[[0.9890735]]\n",
      "[[0.99827486]]\n",
      "[[-0.99973375]]\n",
      "[[-0.9999985]]\n",
      "[[-0.34690377]]\n",
      "[[0.9970365]]\n",
      "[[-0.9937075]]\n",
      "[[-0.9999985]]\n",
      "[[-0.99661165]]\n",
      "[[0.99990404]]\n",
      "[[-0.3368223]]\n",
      "[[-0.99675363]]\n",
      "[[0.9937415]]\n",
      "[[-0.95446336]]\n",
      "[[-1.]]\n",
      "[[-0.9999811]]\n",
      "[[0.99991393]]\n",
      "[[0.774462]]\n",
      "[[-0.8210585]]\n",
      "[[0.99798965]]\n",
      "[[-0.99943346]]\n",
      "[[-1.]]\n",
      "[[-0.99916816]]\n",
      "[[0.9999176]]\n",
      "[[-0.9466849]]\n",
      "[[-0.9999215]]\n",
      "[[0.14903238]]\n",
      "[[0.9968899]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.9999995]]\n",
      "[[0.99963546]]\n",
      "[[0.9573536]]\n",
      "[[-0.9998998]]\n",
      "[[-0.24675551]]\n",
      "[[0.9967874]]\n",
      "[[-0.9999918]]\n",
      "[[-1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "       reward  timesteps  num_actions\n0 -163.081482        204           34\n1 -295.807770        204           34\n2 -253.660629        204           34\n3 -202.541702        204           34\n4 -180.507751        204           34",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-163.081482</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-295.807770</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-253.660629</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-202.541702</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-180.507751</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 5, daifa.agent_time_ratio, show_env=True)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.4574806  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "60 167\n",
      "No Success\n",
      "Episode 2\n",
      "[-0.47844926  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "64 167\n",
      "No Success\n",
      "Episode 3\n",
      "[-0.5311618  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "37 147\n",
      "Success in episode 3 at time step 877\n",
      "Episode 4\n",
      "[-0.46539444  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "60 167\n",
      "No Success\n",
      "Episode 5\n",
      "[-0.43320078  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "70 167\n",
      "No Success\n",
      "Episode 6\n",
      "[-0.56359833  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "54 167\n",
      "No Success\n",
      "Episode 7\n",
      "[-0.41705218  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "51 167\n",
      "No Success\n",
      "Episode 8\n",
      "[-0.49579993  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "90 167\n",
      "No Success\n",
      "Episode 9\n",
      "[-0.40739912  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "94 167\n",
      "No Success\n",
      "Episode 10\n",
      "[-0.57973146  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "54 167\n",
      "No Success\n",
      "Episode 11\n",
      "[-0.47232947  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "62 167\n",
      "No Success\n",
      "Episode 12\n",
      "[-0.49276304  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "72 167\n",
      "No Success\n",
      "Episode 13\n",
      "[-0.5930956  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "114 167\n",
      "No Success\n",
      "Episode 14\n",
      "[-0.5572101  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "56 167\n",
      "No Success\n",
      "Episode 15\n",
      "[-0.4525781  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "76 167\n",
      "No Success\n",
      "Episode 16\n",
      "[-0.49785632  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "64 167\n",
      "No Success\n",
      "Episode 17\n",
      "[-0.5918306  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "38 97\n",
      "Success in episode 17 at time step 582\n",
      "Episode 18\n",
      "[-0.50446576  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "46 119\n",
      "Success in episode 18 at time step 711\n",
      "Episode 19\n",
      "[-0.5515545  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "48 167\n",
      "No Success\n",
      "Episode 20\n",
      "[-0.4675508  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "46 167\n",
      "No Success\n",
      "Episode 21\n",
      "[-0.5987304  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "24 84\n",
      "Success in episode 21 at time step 500\n",
      "Episode 22\n",
      "[-0.46599886  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "14 44\n",
      "Success in episode 22 at time step 259\n",
      "Episode 23\n",
      "[-0.5442812  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 26\n",
      "Success in episode 23 at time step 154\n",
      "Episode 24\n",
      "[-0.56831306  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "65 167\n",
      "No Success\n",
      "Episode 25\n",
      "[-0.4211194  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "35 113\n",
      "Success in episode 25 at time step 676\n",
      "Episode 26\n",
      "[-0.48089644  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 44\n",
      "Success in episode 26 at time step 259\n",
      "Episode 27\n",
      "[-0.49131438  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "13 45\n",
      "Success in episode 27 at time step 265\n",
      "Episode 28\n",
      "[-0.4746755  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 27\n",
      "Success in episode 28 at time step 157\n",
      "Episode 29\n",
      "[-0.59761935  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 57\n",
      "Success in episode 29 at time step 342\n",
      "Episode 30\n",
      "[-0.4673799  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "10 27\n",
      "Success in episode 30 at time step 159\n",
      "Episode 31\n",
      "[-0.5692723  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 26\n",
      "Success in episode 31 at time step 153\n",
      "Episode 32\n",
      "[-0.4338846  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "45 134\n",
      "Success in episode 32 at time step 802\n",
      "Episode 33\n",
      "[-0.43634194  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "31 84\n",
      "Success in episode 33 at time step 503\n",
      "Episode 34\n",
      "[-0.57066995  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "41 107\n",
      "Success in episode 34 at time step 638\n",
      "Episode 35\n",
      "[-0.4396064  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "9 29\n",
      "Success in episode 35 at time step 170\n",
      "Episode 36\n",
      "[-0.4796282  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "9 27\n",
      "Success in episode 36 at time step 159\n",
      "Episode 37\n",
      "[-0.502011  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "13 29\n",
      "Success in episode 37 at time step 172\n",
      "Episode 38\n",
      "[-0.48284528  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "14 39\n",
      "Success in episode 38 at time step 234\n",
      "Episode 39\n",
      "[-0.41663125  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 40\n",
      "Success in episode 39 at time step 238\n",
      "Episode 40\n",
      "[-0.52127314  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 28\n",
      "Success in episode 40 at time step 165\n",
      "Episode 41\n",
      "[-0.5059667  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "10 25\n",
      "Success in episode 41 at time step 150\n",
      "Episode 42\n",
      "[-0.5644291  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "25 55\n",
      "Success in episode 42 at time step 326\n",
      "Episode 43\n",
      "[-0.56273377  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "73 159\n",
      "Success in episode 43 at time step 950\n",
      "Episode 44\n",
      "[-0.41874164  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 45\n",
      "Success in episode 44 at time step 269\n",
      "Episode 45\n",
      "[-0.49548292  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "28 44\n",
      "Success in episode 45 at time step 264\n",
      "Episode 46\n",
      "[-0.5138384  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "30 76\n",
      "Success in episode 46 at time step 452\n",
      "Episode 47\n",
      "[-0.4449977  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 27\n",
      "Success in episode 47 at time step 159\n",
      "Episode 48\n",
      "[-0.51948357  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "56 167\n",
      "No Success\n",
      "Episode 49\n",
      "[-0.58224535  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "66 137\n",
      "Success in episode 49 at time step 819\n",
      "Episode 50\n",
      "[-0.40817586  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 33\n",
      "Success in episode 50 at time step 195\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "daifa, results_four = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=50, render_env=False, flip_dynamics=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.43750945  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "67 167\n",
      "No Success\n",
      "Episode 2\n",
      "[-0.5206395  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "6 56\n",
      "Success in episode 2 at time step 334\n",
      "Episode 3\n",
      "[-0.53664637  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 53\n",
      "Success in episode 3 at time step 315\n",
      "Episode 4\n",
      "[-0.4584642  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 53\n",
      "Success in episode 4 at time step 314\n",
      "Episode 5\n",
      "[-0.44199425  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "24 62\n",
      "Success in episode 5 at time step 372\n",
      "Episode 6\n",
      "[-0.5820085  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "25 64\n",
      "Success in episode 6 at time step 381\n",
      "Episode 7\n",
      "[-0.43078417  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 24\n",
      "Success in episode 7 at time step 143\n",
      "Episode 8\n",
      "[-0.548233  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "5 25\n",
      "Success in episode 8 at time step 149\n",
      "Episode 9\n",
      "[-0.45224836  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 25\n",
      "Success in episode 9 at time step 149\n",
      "Episode 10\n",
      "[-0.50875205  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "29 62\n",
      "Success in episode 10 at time step 369\n",
      "Episode 11\n",
      "[-0.44876334  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "40 127\n",
      "Success in episode 11 at time step 761\n",
      "Episode 12\n",
      "[-0.5969452  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "56 129\n",
      "Success in episode 12 at time step 771\n",
      "Episode 13\n",
      "[-0.5480532  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "21 52\n",
      "Success in episode 13 at time step 310\n",
      "Episode 14\n",
      "[-0.505934  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 71\n",
      "Success in episode 14 at time step 424\n",
      "Episode 15\n",
      "[-0.43395558  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "75 167\n",
      "No Success\n",
      "Episode 16\n",
      "[-0.46172836  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "59 98\n",
      "Success in episode 16 at time step 586\n",
      "Episode 17\n",
      "[-0.58230996  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "60 102\n",
      "Success in episode 17 at time step 608\n",
      "Episode 18\n",
      "[-0.5118248  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "73 167\n",
      "No Success\n",
      "Episode 19\n",
      "[-0.50273025  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "39 103\n",
      "Success in episode 19 at time step 616\n",
      "Episode 20\n",
      "[-0.46269357  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "35 86\n",
      "Success in episode 20 at time step 515\n",
      "Episode 21\n",
      "[-0.57779664  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "20 46\n",
      "Success in episode 21 at time step 274\n",
      "Episode 22\n",
      "[-0.45123324  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "9 38\n",
      "Success in episode 22 at time step 227\n",
      "Episode 23\n",
      "[-0.49308443  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "82 122\n",
      "Success in episode 23 at time step 728\n",
      "Episode 24\n",
      "[-0.551804  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "55 99\n",
      "Success in episode 24 at time step 593\n",
      "Episode 25\n",
      "[-0.47908628  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "21 50\n",
      "Success in episode 25 at time step 296\n",
      "Episode 26\n",
      "[-0.46834362  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "30 88\n",
      "Success in episode 26 at time step 523\n",
      "Episode 27\n",
      "[-0.5952358  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "100 167\n",
      "No Success\n",
      "Episode 28\n",
      "[-0.43523842  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 25\n",
      "Success in episode 28 at time step 146\n",
      "Episode 29\n",
      "[-0.44274956  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "1 26\n",
      "Success in episode 29 at time step 153\n",
      "Episode 30\n",
      "[-0.4964754  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 66\n",
      "Success in episode 30 at time step 394\n",
      "Episode 31\n",
      "[-0.56049025  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 35\n",
      "Success in episode 31 at time step 209\n",
      "Episode 32\n",
      "[-0.49966016  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "110 167\n",
      "No Success\n",
      "Episode 33\n",
      "[-0.49757892  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 25\n",
      "Success in episode 33 at time step 149\n",
      "Episode 34\n",
      "[-0.51177806  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 22\n",
      "Success in episode 34 at time step 132\n",
      "Episode 35\n",
      "[-0.47141552  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "6 34\n",
      "Success in episode 35 at time step 203\n",
      "Episode 36\n",
      "[-0.5119726  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "36 81\n",
      "Success in episode 36 at time step 485\n",
      "Episode 37\n",
      "[-0.41683808  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "61 100\n",
      "Success in episode 37 at time step 599\n",
      "Episode 38\n",
      "[-0.40439466  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 23\n",
      "Success in episode 38 at time step 138\n",
      "Episode 39\n",
      "[-0.50185466  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "72 143\n",
      "Success in episode 39 at time step 856\n",
      "Episode 40\n",
      "[-0.40703216  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 25\n",
      "Success in episode 40 at time step 149\n",
      "Episode 41\n",
      "[-0.58549064  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "88 167\n",
      "No Success\n",
      "Episode 42\n",
      "[-0.56644875  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 46\n",
      "Success in episode 42 at time step 274\n",
      "Episode 43\n",
      "[-0.49367848  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "19 47\n",
      "Success in episode 43 at time step 280\n",
      "Episode 44\n",
      "[-0.4626446  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "5 25\n",
      "Success in episode 44 at time step 147\n",
      "Episode 45\n",
      "[-0.5570115  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "100 167\n",
      "No Success\n",
      "Episode 46\n",
      "[-0.58739907  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 36\n",
      "Success in episode 46 at time step 215\n",
      "Episode 47\n",
      "[-0.5529454  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 26\n",
      "Success in episode 47 at time step 154\n",
      "Episode 48\n",
      "[-0.5576318  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "31 63\n",
      "Success in episode 48 at time step 378\n",
      "Episode 49\n",
      "[-0.52270186  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "47 106\n",
      "Success in episode 49 at time step 634\n",
      "Episode 50\n",
      "[-0.49432787  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "96 167\n",
      "No Success\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results_four = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=50, render_env=False, flip_dynamics=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "      reward  timesteps  num_actions\n0  -6.068081       1002          167\n1  -6.230983       1002          167\n2  -6.431071       1002          167\n3  -6.138415       1002          167\n4  -6.204773       1002          167\n5  -6.439107       1002          167\n6  -6.316643       1002          167\n7  -6.352924       1002          167\n8  -6.375400       1002          167\n9  -6.346141       1002          167\n10 -6.284814       1002          167\n11 -6.313875       1002          167\n12 -6.224095       1002          167\n13 -6.106530       1002          167\n14 -6.240360       1002          167\n15 -6.177990       1002          167\n16 -6.252221       1002          167\n17 -6.372750       1002          167\n18 -6.166451       1002          167\n19 -6.136749       1002          167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-6.068081</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-6.230983</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-6.431071</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-6.138415</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-6.204773</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-6.439107</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-6.316643</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-6.352924</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-6.375400</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-6.346141</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-6.284814</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-6.313875</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-6.224095</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-6.106530</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-6.240360</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-6.177990</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-6.252221</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-6.372750</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-6.166451</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-6.136749</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "     reward  timesteps  num_actions\n0 -6.402275       1002          167\n1 -6.180096       1002          167\n2 -6.352802       1002          167\n3 -6.167919       1002          167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-6.402275</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-6.180096</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-6.352802</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-6.167919</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 4, daifa.agent_time_ratio, show_env=False)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_results = pd.concat([results_one, results_two, results_three, results_four])\n",
    "full_results.reset_index(drop=True)\n",
    "full_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "T = np.arange(len(full_results))\n",
    "plt.plot(T, full_results.percent_use_fast_thinking)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(T, full_results.success)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(T, full_results.total_reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(T, full_results.sim_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}