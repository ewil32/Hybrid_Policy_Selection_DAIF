{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run The Agent on Mountain Car"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from vae_recurrent import VAE, create_decoder, create_encoder\n",
    "from transition_gru import TransitionGRU\n",
    "from recurrent_agent import DAIFAgentRecurrent\n",
    "from prior_model import PriorModelBellman\n",
    "from habitual_action_network import HabitualAction, compute_discounted_cumulative_reward\n",
    "from ddpg import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations, test_policy, habit_policy\n",
    "from train_agent import train_single_agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# from identity_vae import IdentityVAE, identity_encoder, identity_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with no prior model FEEF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def test_policy(env, policy_func, observation_max, observation_min, obs_stddev, num_episodes, num_action_repeats, show_env=False):\n",
    "\n",
    "    all_rewards = []\n",
    "    all_times = []\n",
    "    all_num_actions = []\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "        if show_env:\n",
    "            env.render()\n",
    "\n",
    "        done = False\n",
    "        rewards = []\n",
    "        t = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            obs = obs.reshape(1, obs.shape[0])\n",
    "            obs = transform_observations(obs, observation_max, observation_min, obs_stddev)\n",
    "\n",
    "            action = policy_func(obs)\n",
    "            action = action.numpy()\n",
    "            print(action)\n",
    "\n",
    "            for k in range(num_action_repeats):\n",
    "                obs, reward, done, info = env.step(action)\n",
    "\n",
    "                t += 1\n",
    "\n",
    "                if show_env:\n",
    "                    env.render()\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "        rows.append([np.sum(rewards), t, t//num_action_repeats])\n",
    "        # all_rewards.append(np.sum(rewards))\n",
    "        # all_times.append(t)\n",
    "        # all_num_actions.append(t//num_action_repeats)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    results = pd.DataFrame(rows, columns=[\"reward\", \"timesteps\", \"num_actions\"])\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "pln_hrzn = 5\n",
    "latent_dim = 2\n",
    "obs_dim = 3\n",
    "\n",
    "# make the VAE\n",
    "enc = create_encoder(obs_dim, latent_dim, [20])\n",
    "dec = create_decoder(latent_dim, obs_dim, [20])\n",
    "vae = VAE(enc, dec, latent_dim,  [0]*latent_dim, [0.3]*latent_dim, train_epochs=2, show_training=True)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the TRANSITION\n",
    "tran = TransitionGRU(latent_dim, 1, 2*pln_hrzn*latent_dim, 2, train_epochs=2, show_training=True)\n",
    "tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the HABIT ACTION NET\n",
    "habit_net = HabitualAction(latent_dim, 1, [16, 16], train_epochs=2, show_training=False)\n",
    "habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# # # make the HABIT ACTION NET\n",
    "# actor_model = get_actor(2, 1)\n",
    "# critic_model = get_critic(2, 1)\n",
    "# target_actor = get_actor(2, 1)\n",
    "# target_critic = get_critic(2, 1)\n",
    "#\n",
    "# # Making the weights equal initially\n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "# critic_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "# actor_optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "# habit_net = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau=0.005, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)\n",
    "\n",
    "\n",
    "# make the PRIOR NET\n",
    "prior_model = PriorModelBellman(latent_dim, output_dim=1, scaling_factor=0.01, show_training=False, use_tanh_on_output=False)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0, 0, 0]\n",
    "prior_stddev = [1, 1, 1]\n",
    "\n",
    "observation_max = np.array([1, 1, 8])\n",
    "observation_min = np.array([-1, -1, -8])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0,0])  # no noise on prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           habit_net,\n",
    "                           planning_horizon=pln_hrzn,\n",
    "                           use_kl_extrinsic=True,  # maybe this works\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           train_habit_net=True,\n",
    "                           train_prior_model=True,\n",
    "                           train_tran=True,\n",
    "                           train_after_exploring=True,\n",
    "                           train_with_replay=True,\n",
    "                           use_fast_thinking=True,\n",
    "                           habit_model_type=\"PG\",\n",
    "                           uncertainty_tolerance=0.1,\n",
    "                           min_rewards_needed_to_train_prior=-30)\n",
    "\n",
    "\n",
    "daifa.train_prior = True\n",
    "daifa.prior_model.show_training = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.8265499], dtype=float32)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "env.action_space.sample()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[ 0.4638155  -0.8859318  -0.06816371]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 1s 539ms/step - kl_loss: 0.2754\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2663\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 324.1006 - reconstruction_loss: 315.5265 - kl_loss: 8.5741\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 387.2435 - reconstruction_loss: 378.7694 - kl_loss: 8.4741\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2898\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2828\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 245.9514 - reconstruction_loss: 232.8542 - kl_loss: 13.0972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.9973 - reconstruction_loss: 161.0199 - kl_loss: 12.9774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4601\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.9170 - reconstruction_loss: 100.7787 - kl_loss: 14.1384\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1094.2727 - reconstruction_loss: 1080.1531 - kl_loss: 14.1196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2135\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2116\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 493.8768 - reconstruction_loss: 487.4369 - kl_loss: 6.4399\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 407.0433 - reconstruction_loss: 400.6357 - kl_loss: 6.4075\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9326\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9155\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 499.7076 - reconstruction_loss: 485.7896 - kl_loss: 13.9181\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 314.3023 - reconstruction_loss: 300.4974 - kl_loss: 13.8049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2989\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2912\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 598.5396 - reconstruction_loss: 590.0322 - kl_loss: 8.5075\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 363.0604 - reconstruction_loss: 354.6196 - kl_loss: 8.4407\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9427\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 402.1443 - reconstruction_loss: 392.0053 - kl_loss: 10.1390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 362.2626 - reconstruction_loss: 352.2020 - kl_loss: 10.0607\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 285.2629 - reconstruction_loss: 273.3667 - kl_loss: 11.8962\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 430.4625 - reconstruction_loss: 418.6394 - kl_loss: 11.8231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5251\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5045\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 515.6823 - reconstruction_loss: 509.9863 - kl_loss: 5.6960\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 400.0526 - reconstruction_loss: 394.3983 - kl_loss: 5.6542\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7317\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6994\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 495.8690 - reconstruction_loss: 483.3972 - kl_loss: 12.4718\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 307.3298 - reconstruction_loss: 294.9460 - kl_loss: 12.3838\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1676\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 523.1639 - reconstruction_loss: 514.8994 - kl_loss: 8.2645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 393.7214 - reconstruction_loss: 385.5204 - kl_loss: 8.2009\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5945\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5713\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 367.1373 - reconstruction_loss: 358.9858 - kl_loss: 8.1514\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 378.8415 - reconstruction_loss: 370.7451 - kl_loss: 8.0964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3084\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 319.5423 - reconstruction_loss: 307.1559 - kl_loss: 12.3864\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 328.5045 - reconstruction_loss: 316.1813 - kl_loss: 12.3231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1636\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1601\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 376.9866 - reconstruction_loss: 371.4853 - kl_loss: 5.5014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 400.5951 - reconstruction_loss: 395.1261 - kl_loss: 5.4690\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2028\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2026\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 161.1248 - reconstruction_loss: 147.0074 - kl_loss: 14.1174\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 234.0878 - reconstruction_loss: 220.0101 - kl_loss: 14.0776\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 497.4788 - reconstruction_loss: 483.8135 - kl_loss: 13.6653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 284.6401 - reconstruction_loss: 271.0828 - kl_loss: 13.5574\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 293ms/step - kl_loss: 0.2128\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2082\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 388.8336 - reconstruction_loss: 379.2283 - kl_loss: 9.6054\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 350.6684 - reconstruction_loss: 341.1397 - kl_loss: 9.5287\n",
      "Success in episode 1 at time step 200 with reward -204.35860918945718\n",
      "Episode 2\n",
      "[-0.8032636  -0.59562373 -0.4775492 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2830\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2800\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 231.6854 - reconstruction_loss: 227.0778 - kl_loss: 4.6076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 158.3567 - reconstruction_loss: 153.7497 - kl_loss: 4.6070\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3687\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 219.0979 - reconstruction_loss: 216.0315 - kl_loss: 3.0663\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 229.6289 - reconstruction_loss: 226.5675 - kl_loss: 3.0614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3983\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3913\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 220.7384 - reconstruction_loss: 216.1534 - kl_loss: 4.5850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 210.0394 - reconstruction_loss: 205.4727 - kl_loss: 4.5666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2573\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2546\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 267.0148 - reconstruction_loss: 262.0732 - kl_loss: 4.9416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 280.1408 - reconstruction_loss: 275.1997 - kl_loss: 4.9411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1635\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1612\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 200.7030 - reconstruction_loss: 197.6048 - kl_loss: 3.0982\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 234.6658 - reconstruction_loss: 231.5742 - kl_loss: 3.0916\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2564\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 218.7313 - reconstruction_loss: 214.1773 - kl_loss: 4.5540\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 167.3526 - reconstruction_loss: 162.8097 - kl_loss: 4.5429\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3847\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3771\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 263.4841 - reconstruction_loss: 257.5647 - kl_loss: 5.9194\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 260.5184 - reconstruction_loss: 254.6040 - kl_loss: 5.9144\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1326\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1298\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 282.5313 - reconstruction_loss: 279.2922 - kl_loss: 3.2392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 230.0796 - reconstruction_loss: 226.8506 - kl_loss: 3.2290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2398\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 218.0199 - reconstruction_loss: 212.8340 - kl_loss: 5.1859\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 179.0944 - reconstruction_loss: 173.9184 - kl_loss: 5.1760\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2055\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2009\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 222.6418 - reconstruction_loss: 217.7453 - kl_loss: 4.8965\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 232.0494 - reconstruction_loss: 227.1582 - kl_loss: 4.8912\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1474\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1446\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 283.4513 - reconstruction_loss: 279.7164 - kl_loss: 3.7349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 312.2389 - reconstruction_loss: 308.5155 - kl_loss: 3.7233\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4643\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4544\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 242.9643 - reconstruction_loss: 236.2319 - kl_loss: 6.7324\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 237.5183 - reconstruction_loss: 230.8051 - kl_loss: 6.7132\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1611\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 287.7873 - reconstruction_loss: 282.7113 - kl_loss: 5.0760\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 248.2322 - reconstruction_loss: 243.1608 - kl_loss: 5.0713\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0736\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0713\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 257.1626 - reconstruction_loss: 253.9371 - kl_loss: 3.2255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 195.2426 - reconstruction_loss: 192.0264 - kl_loss: 3.2161\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2282\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 249.2897 - reconstruction_loss: 243.2405 - kl_loss: 6.0492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 329.5412 - reconstruction_loss: 323.5008 - kl_loss: 6.0404\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1302\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1286\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 281.9938 - reconstruction_loss: 275.0733 - kl_loss: 6.9205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 287.1325 - reconstruction_loss: 280.2144 - kl_loss: 6.9182\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.1700\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1655\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 230.7586 - reconstruction_loss: 226.2225 - kl_loss: 4.5361\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 221.7375 - reconstruction_loss: 217.2143 - kl_loss: 4.5232\n",
      "Success in episode 2 at time step 200 with reward -236.86545826262198\n",
      "Episode 3\n",
      "[ 0.9792771  -0.20252503  0.2821334 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2496\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2490\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 457.8551 - reconstruction_loss: 444.2084 - kl_loss: 13.6467\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 258.4215 - reconstruction_loss: 244.8478 - kl_loss: 13.5737\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1436\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 410.8792 - reconstruction_loss: 402.4026 - kl_loss: 8.4766\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 359.8191 - reconstruction_loss: 351.3869 - kl_loss: 8.4321\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1963\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1983\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 454.7323 - reconstruction_loss: 443.4208 - kl_loss: 11.3115\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 288.2802 - reconstruction_loss: 276.9995 - kl_loss: 11.2807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2064\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 188.1070 - reconstruction_loss: 180.3068 - kl_loss: 7.8002\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 293.1533 - reconstruction_loss: 285.3666 - kl_loss: 7.7866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2233\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2250\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 385.9447 - reconstruction_loss: 378.5478 - kl_loss: 7.3969\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 473.7740 - reconstruction_loss: 466.3968 - kl_loss: 7.3772\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2247\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 366.8297 - reconstruction_loss: 360.8361 - kl_loss: 5.9935\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 322.1545 - reconstruction_loss: 316.1746 - kl_loss: 5.9799\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 273.6300 - reconstruction_loss: 263.3659 - kl_loss: 10.2641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 324.0415 - reconstruction_loss: 313.8038 - kl_loss: 10.2377\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1913\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 362.3162 - reconstruction_loss: 357.9346 - kl_loss: 4.3816\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 369.8048 - reconstruction_loss: 365.4334 - kl_loss: 4.3715\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2144\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 237.8735 - reconstruction_loss: 226.0255 - kl_loss: 11.8479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 372.0137 - reconstruction_loss: 360.1970 - kl_loss: 11.8168\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3056\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 419.9092 - reconstruction_loss: 408.8298 - kl_loss: 11.0794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 233.2671 - reconstruction_loss: 222.2620 - kl_loss: 11.0050\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 300.0087 - reconstruction_loss: 292.4817 - kl_loss: 7.5270\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 362.5273 - reconstruction_loss: 355.0070 - kl_loss: 7.5202\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0912\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0908\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 277.3071 - reconstruction_loss: 272.6336 - kl_loss: 4.6735\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 263.2288 - reconstruction_loss: 258.5566 - kl_loss: 4.6722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2171\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 267.0281 - reconstruction_loss: 257.9605 - kl_loss: 9.0677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 300.9291 - reconstruction_loss: 291.8817 - kl_loss: 9.0474\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1535\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 326.5751 - reconstruction_loss: 321.3236 - kl_loss: 5.2516\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 297.7809 - reconstruction_loss: 292.5341 - kl_loss: 5.2468\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1465\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1458\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 248.9139 - reconstruction_loss: 239.3906 - kl_loss: 9.5234\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 260.4641 - reconstruction_loss: 250.9652 - kl_loss: 9.4989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1885\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1882\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 338.5694 - reconstruction_loss: 331.0641 - kl_loss: 7.5053\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 369.7334 - reconstruction_loss: 362.2444 - kl_loss: 7.4890\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1673\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 290.0642 - reconstruction_loss: 281.8661 - kl_loss: 8.1981\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 313.1434 - reconstruction_loss: 304.9730 - kl_loss: 8.1705\n",
      "Success in episode 3 at time step 200 with reward -174.45552863957278\n",
      "Episode 4\n",
      "[-0.29598173  0.9551936  -0.50697494]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1022\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 182.8999 - reconstruction_loss: 179.8167 - kl_loss: 3.0832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 256.3328 - reconstruction_loss: 253.2523 - kl_loss: 3.0805\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2951\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2959\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 191.3150 - reconstruction_loss: 185.0509 - kl_loss: 6.2640\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 196.4311 - reconstruction_loss: 190.1999 - kl_loss: 6.2312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0674\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 237.1962 - reconstruction_loss: 231.9366 - kl_loss: 5.2596\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 247.9193 - reconstruction_loss: 242.6663 - kl_loss: 5.2529\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0911\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0910\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 215.1176 - reconstruction_loss: 211.9864 - kl_loss: 3.1312\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 214.5525 - reconstruction_loss: 211.4220 - kl_loss: 3.1305\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2625\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 225.8001 - reconstruction_loss: 219.4722 - kl_loss: 6.3279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 232.5525 - reconstruction_loss: 226.2473 - kl_loss: 6.3052\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0945\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 330.7728 - reconstruction_loss: 325.0914 - kl_loss: 5.6813\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 280.5421 - reconstruction_loss: 274.8699 - kl_loss: 5.6721\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0745\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 228.6328 - reconstruction_loss: 223.2171 - kl_loss: 5.4158\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 976us/step - loss: 229.4826 - reconstruction_loss: 224.0682 - kl_loss: 5.4144\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1301\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1297\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 222.0890 - reconstruction_loss: 216.6802 - kl_loss: 5.4088\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 220.3537 - reconstruction_loss: 214.9563 - kl_loss: 5.3974\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3718\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3687\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 229.1854 - reconstruction_loss: 220.4494 - kl_loss: 8.7360\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 224.0131 - reconstruction_loss: 215.3042 - kl_loss: 8.7089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0906\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 390.8427 - reconstruction_loss: 384.0134 - kl_loss: 6.8293\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 321.2191 - reconstruction_loss: 314.3957 - kl_loss: 6.8234\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1295\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1300\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 226.9211 - reconstruction_loss: 218.2841 - kl_loss: 8.6370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 242.9076 - reconstruction_loss: 234.2762 - kl_loss: 8.6314\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0621\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 289.1005 - reconstruction_loss: 285.3340 - kl_loss: 3.7664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 309.2567 - reconstruction_loss: 305.4927 - kl_loss: 3.7640\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1815\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1811\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 271.0368 - reconstruction_loss: 263.2486 - kl_loss: 7.7882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 255.8104 - reconstruction_loss: 248.0351 - kl_loss: 7.7753\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0997\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0996\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 480.2438 - reconstruction_loss: 471.9255 - kl_loss: 8.3183\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 341.2338 - reconstruction_loss: 332.9328 - kl_loss: 8.3010\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0816\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0814\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 210.8141 - reconstruction_loss: 203.2724 - kl_loss: 7.5417\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 327.9817 - reconstruction_loss: 320.4441 - kl_loss: 7.5376\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1648\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1642\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 325.5140 - reconstruction_loss: 318.7166 - kl_loss: 6.7975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 357.7838 - reconstruction_loss: 350.9938 - kl_loss: 6.7900\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1346\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 255.3777 - reconstruction_loss: 249.2542 - kl_loss: 6.1235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 261.9218 - reconstruction_loss: 255.8091 - kl_loss: 6.1127\n",
      "Success in episode 4 at time step 200 with reward -186.38332365255368\n",
      "Episode 5\n",
      "[-0.5617482  0.8273083 -0.5997765]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2897\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 196.8526 - reconstruction_loss: 193.8708 - kl_loss: 2.9819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 195.3369 - reconstruction_loss: 192.3553 - kl_loss: 2.9816\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1806\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1781\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 203.4296 - reconstruction_loss: 199.5434 - kl_loss: 3.8862\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 212.9734 - reconstruction_loss: 209.0870 - kl_loss: 3.8864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1023\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1011\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 258.5597 - reconstruction_loss: 255.1720 - kl_loss: 3.3876\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 191.7710 - reconstruction_loss: 188.3843 - kl_loss: 3.3867\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1895\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 149.7611 - reconstruction_loss: 146.2261 - kl_loss: 3.5350\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 222.3911 - reconstruction_loss: 218.8609 - kl_loss: 3.5302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 233.8971 - reconstruction_loss: 229.1491 - kl_loss: 4.7480\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 213.4135 - reconstruction_loss: 208.6653 - kl_loss: 4.7481\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1072\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 250.1468 - reconstruction_loss: 245.2378 - kl_loss: 4.9091\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 223.6181 - reconstruction_loss: 218.7099 - kl_loss: 4.9082\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0769\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0768\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 248.2244 - reconstruction_loss: 243.6698 - kl_loss: 4.5547\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 232.0598 - reconstruction_loss: 227.5151 - kl_loss: 4.5447\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2642\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 292.5355 - reconstruction_loss: 285.8114 - kl_loss: 6.7241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 247.4486 - reconstruction_loss: 240.7456 - kl_loss: 6.7030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0526\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0532\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 268.6136 - reconstruction_loss: 262.8585 - kl_loss: 5.7550\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 254.1986 - reconstruction_loss: 248.4418 - kl_loss: 5.7568\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1671\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1652\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 216.4515 - reconstruction_loss: 213.0578 - kl_loss: 3.3938\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 201.0348 - reconstruction_loss: 197.6405 - kl_loss: 3.3943\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2329\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2307\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 173.0640 - reconstruction_loss: 168.9142 - kl_loss: 4.1498\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 198.7400 - reconstruction_loss: 194.5928 - kl_loss: 4.1472\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0559\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 231.0731 - reconstruction_loss: 226.6981 - kl_loss: 4.3750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 992us/step - loss: 230.5276 - reconstruction_loss: 226.1497 - kl_loss: 4.3778\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0516\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 191.0140 - reconstruction_loss: 188.1006 - kl_loss: 2.9135\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 200.6041 - reconstruction_loss: 197.6912 - kl_loss: 2.9129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2064\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2041\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 175.8023 - reconstruction_loss: 170.8987 - kl_loss: 4.9036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 203.7763 - reconstruction_loss: 198.8736 - kl_loss: 4.9027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 257.8474 - reconstruction_loss: 251.2163 - kl_loss: 6.6310\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 201.7104 - reconstruction_loss: 195.0780 - kl_loss: 6.6324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1215\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1215\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 282.0002 - reconstruction_loss: 278.4905 - kl_loss: 3.5097\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 275.0896 - reconstruction_loss: 271.5803 - kl_loss: 3.5093\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1278\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1259\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 222.4462 - reconstruction_loss: 217.9349 - kl_loss: 4.5113\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 208.6003 - reconstruction_loss: 204.0899 - kl_loss: 4.5104\n",
      "Success in episode 5 at time step 200 with reward -217.27915454230111\n",
      "Episode 6\n",
      "[ 0.8444329   0.53566134 -0.6131566 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0745\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 219.2846 - reconstruction_loss: 212.5078 - kl_loss: 6.7768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 318.9432 - reconstruction_loss: 312.1604 - kl_loss: 6.7827\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2342\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 268.4486 - reconstruction_loss: 260.8307 - kl_loss: 7.6179\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 245.1979 - reconstruction_loss: 237.5769 - kl_loss: 7.6210\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1880\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 283.5648 - reconstruction_loss: 279.0198 - kl_loss: 4.5449\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 326.0819 - reconstruction_loss: 321.5320 - kl_loss: 4.5499\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2621\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 388.8329 - reconstruction_loss: 378.8268 - kl_loss: 10.0060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 412.8247 - reconstruction_loss: 402.8102 - kl_loss: 10.0145\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0574\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 255.2521 - reconstruction_loss: 251.2699 - kl_loss: 3.9822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 282.5981 - reconstruction_loss: 278.6230 - kl_loss: 3.9750\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2727\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2724\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 294.4989 - reconstruction_loss: 287.2107 - kl_loss: 7.2882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 260.8746 - reconstruction_loss: 253.5980 - kl_loss: 7.2766\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1558\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1562\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 296.2582 - reconstruction_loss: 289.2485 - kl_loss: 7.0097\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 259.8311 - reconstruction_loss: 252.8170 - kl_loss: 7.0142\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2058\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 223.0417 - reconstruction_loss: 212.9662 - kl_loss: 10.0755\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 234.4716 - reconstruction_loss: 224.3999 - kl_loss: 10.0718\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1141\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 287.4071 - reconstruction_loss: 281.9205 - kl_loss: 5.4866\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 544.9043 - reconstruction_loss: 539.4187 - kl_loss: 5.4856\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 274.3561 - reconstruction_loss: 265.4052 - kl_loss: 8.9510\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 323.5028 - reconstruction_loss: 314.5717 - kl_loss: 8.9311\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1651\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1640\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 304.6886 - reconstruction_loss: 298.2388 - kl_loss: 6.4497\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 218.3403 - reconstruction_loss: 211.9087 - kl_loss: 6.4316\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1079\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 239.5509 - reconstruction_loss: 234.5169 - kl_loss: 5.0340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 249.2622 - reconstruction_loss: 244.2348 - kl_loss: 5.0274\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3174\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3164\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 228.3651 - reconstruction_loss: 221.1946 - kl_loss: 7.1704\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 229.3089 - reconstruction_loss: 222.1468 - kl_loss: 7.1621\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1229\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 303.2651 - reconstruction_loss: 296.6105 - kl_loss: 6.6546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 307.1302 - reconstruction_loss: 300.4737 - kl_loss: 6.6565\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1383\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 216.2886 - reconstruction_loss: 207.9858 - kl_loss: 8.3028\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 281.4307 - reconstruction_loss: 273.1360 - kl_loss: 8.2947\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0494\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0492\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 208.3449 - reconstruction_loss: 204.9626 - kl_loss: 3.3824\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 270.1531 - reconstruction_loss: 266.7693 - kl_loss: 3.3837\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.1593\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 261.0307 - reconstruction_loss: 254.2616 - kl_loss: 6.7691\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 274.8013 - reconstruction_loss: 268.0426 - kl_loss: 6.7586\n",
      "Success in episode 6 at time step 200 with reward -170.05472076627035\n",
      "Episode 7\n",
      "[-0.11588862  0.99326223 -0.14632575]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1342\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1352\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 208.8295 - reconstruction_loss: 205.8398 - kl_loss: 2.9897\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 233.0252 - reconstruction_loss: 230.0304 - kl_loss: 2.9948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2529\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2536\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 198.4821 - reconstruction_loss: 193.3702 - kl_loss: 5.1119\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 171.8064 - reconstruction_loss: 166.7038 - kl_loss: 5.1026\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0996\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0987\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 258.1573 - reconstruction_loss: 251.6452 - kl_loss: 6.5122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 324.6601 - reconstruction_loss: 318.1413 - kl_loss: 6.5188\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1012\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 245.7808 - reconstruction_loss: 239.7659 - kl_loss: 6.0149\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 270.0714 - reconstruction_loss: 264.0516 - kl_loss: 6.0198\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2891\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 275.0938 - reconstruction_loss: 269.1427 - kl_loss: 5.9511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 271.2640 - reconstruction_loss: 265.3242 - kl_loss: 5.9398\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1134\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 257.5894 - reconstruction_loss: 251.0913 - kl_loss: 6.4981\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 312.6288 - reconstruction_loss: 306.1290 - kl_loss: 6.4998\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0980\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 227.4774 - reconstruction_loss: 221.7563 - kl_loss: 5.7211\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 256.6219 - reconstruction_loss: 250.9135 - kl_loss: 5.7083\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3128\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3121\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 299.4191 - reconstruction_loss: 292.6479 - kl_loss: 6.7712\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 321.4130 - reconstruction_loss: 314.6415 - kl_loss: 6.7715\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1899\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1889\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 285.2995 - reconstruction_loss: 276.7196 - kl_loss: 8.5799\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 221.3833 - reconstruction_loss: 212.8110 - kl_loss: 8.5723\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1057\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 247.2646 - reconstruction_loss: 242.5795 - kl_loss: 4.6852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 288.4070 - reconstruction_loss: 283.7195 - kl_loss: 4.6874\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2293\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2274\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 347.0081 - reconstruction_loss: 338.5314 - kl_loss: 8.4768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 328.5840 - reconstruction_loss: 320.1132 - kl_loss: 8.4709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1305\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 296.4216 - reconstruction_loss: 292.8438 - kl_loss: 3.5778\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 864us/step - loss: 297.4264 - reconstruction_loss: 293.8524 - kl_loss: 3.5739\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1478\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1467\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 249.7573 - reconstruction_loss: 241.7417 - kl_loss: 8.0157\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 246.0226 - reconstruction_loss: 238.0071 - kl_loss: 8.0155\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2079\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2042\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 254.5576 - reconstruction_loss: 248.2593 - kl_loss: 6.2983\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 288.5701 - reconstruction_loss: 282.2720 - kl_loss: 6.2980\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1844\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 284.3715 - reconstruction_loss: 279.4035 - kl_loss: 4.9680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 293.9361 - reconstruction_loss: 288.9626 - kl_loss: 4.9735\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 290.7582 - reconstruction_loss: 281.3275 - kl_loss: 9.4308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 502.5529 - reconstruction_loss: 493.1201 - kl_loss: 9.4329\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1777\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1766\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 268.8717 - reconstruction_loss: 262.7248 - kl_loss: 6.1469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 266.9608 - reconstruction_loss: 260.8242 - kl_loss: 6.1366\n",
      "Success in episode 7 at time step 200 with reward -208.04595597641628\n",
      "Episode 8\n",
      "[-0.3433473  0.9392085 -0.5332817]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2059\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2064\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 278.0588 - reconstruction_loss: 275.0268 - kl_loss: 3.0321\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 240.8711 - reconstruction_loss: 237.8456 - kl_loss: 3.0255\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3669\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3684\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 214.7912 - reconstruction_loss: 209.9826 - kl_loss: 4.8086\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 163.8487 - reconstruction_loss: 159.0558 - kl_loss: 4.7929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0930\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0931\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 209.8073 - reconstruction_loss: 203.8370 - kl_loss: 5.9703\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 296.8316 - reconstruction_loss: 290.8450 - kl_loss: 5.9865\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1619\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1606\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 211.6218 - reconstruction_loss: 205.8240 - kl_loss: 5.7979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 169.6435 - reconstruction_loss: 163.8500 - kl_loss: 5.7936\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0943\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 273.8886 - reconstruction_loss: 269.8899 - kl_loss: 3.9987\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 263.3102 - reconstruction_loss: 259.3246 - kl_loss: 3.9855\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3902\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3893\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 265.4767 - reconstruction_loss: 258.6818 - kl_loss: 6.7949\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 244.8113 - reconstruction_loss: 238.0109 - kl_loss: 6.8004\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1548\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 250.4369 - reconstruction_loss: 242.6443 - kl_loss: 7.7926\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 228.0356 - reconstruction_loss: 220.2489 - kl_loss: 7.7868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5837\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 344.5607 - reconstruction_loss: 336.8293 - kl_loss: 7.7314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 316.0689 - reconstruction_loss: 308.3395 - kl_loss: 7.7294\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2357\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 361.8235 - reconstruction_loss: 353.8989 - kl_loss: 7.9246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 309.3080 - reconstruction_loss: 301.3905 - kl_loss: 7.9176\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 278.2713 - reconstruction_loss: 270.7969 - kl_loss: 7.4744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 130.2344 - reconstruction_loss: 122.7638 - kl_loss: 7.4706\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1043\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 243.5886 - reconstruction_loss: 240.4257 - kl_loss: 3.1629\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 239.9124 - reconstruction_loss: 236.7521 - kl_loss: 3.1604\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1704\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 339.6914 - reconstruction_loss: 331.8735 - kl_loss: 7.8180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 240.3725 - reconstruction_loss: 232.5737 - kl_loss: 7.7988\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1712\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1688\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 183.7034 - reconstruction_loss: 174.4003 - kl_loss: 9.3031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 994us/step - loss: 237.1725 - reconstruction_loss: 227.8822 - kl_loss: 9.2903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1459\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 254.4268 - reconstruction_loss: 249.7016 - kl_loss: 4.7252\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 245.3213 - reconstruction_loss: 240.5939 - kl_loss: 4.7274\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4375\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4340\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 219.5923 - reconstruction_loss: 214.3282 - kl_loss: 5.2641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 977us/step - loss: 232.0702 - reconstruction_loss: 226.8185 - kl_loss: 5.2517\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1668\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 339.0635 - reconstruction_loss: 331.4713 - kl_loss: 7.5922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 894us/step - loss: 334.0098 - reconstruction_loss: 326.4128 - kl_loss: 7.5970\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2278\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2267\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 241.4339 - reconstruction_loss: 235.2413 - kl_loss: 6.1925\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 243.3484 - reconstruction_loss: 237.1629 - kl_loss: 6.1855\n",
      "Success in episode 8 at time step 200 with reward -162.43131317823185\n",
      "Episode 9\n",
      "[ 0.9025003  -0.43068925 -0.19555306]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.6277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6218\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 322.2811 - reconstruction_loss: 314.8424 - kl_loss: 7.4387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 315.3931 - reconstruction_loss: 307.9485 - kl_loss: 7.4446\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4719\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4655\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 329.2440 - reconstruction_loss: 321.2533 - kl_loss: 7.9906\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 965us/step - loss: 381.8038 - reconstruction_loss: 373.8606 - kl_loss: 7.9432\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1968\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1973\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 397.6859 - reconstruction_loss: 389.6661 - kl_loss: 8.0198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 369.8482 - reconstruction_loss: 361.8126 - kl_loss: 8.0356\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3163\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3111\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 373.4226 - reconstruction_loss: 365.9246 - kl_loss: 7.4980\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 378.8574 - reconstruction_loss: 371.4108 - kl_loss: 7.4465\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1845\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1838\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 464.2933 - reconstruction_loss: 456.0674 - kl_loss: 8.2260\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 349.3025 - reconstruction_loss: 341.0987 - kl_loss: 8.2038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5256\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5214\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 355.3181 - reconstruction_loss: 347.6968 - kl_loss: 7.6213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 318.2271 - reconstruction_loss: 310.6103 - kl_loss: 7.6168\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 338.3772 - reconstruction_loss: 331.0422 - kl_loss: 7.3350\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 470.1382 - reconstruction_loss: 462.8571 - kl_loss: 7.2811\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 325.8494 - reconstruction_loss: 317.6425 - kl_loss: 8.2068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 349.5760 - reconstruction_loss: 341.3691 - kl_loss: 8.2069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3486\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3422\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 342.6638 - reconstruction_loss: 335.1427 - kl_loss: 7.5212\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 342.9717 - reconstruction_loss: 335.4791 - kl_loss: 7.4926\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2412\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2396\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 267.0021 - reconstruction_loss: 259.1228 - kl_loss: 7.8794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 972us/step - loss: 405.3235 - reconstruction_loss: 397.4725 - kl_loss: 7.8510\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4435\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4402\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 307.8913 - reconstruction_loss: 300.6936 - kl_loss: 7.1976\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 778us/step - loss: 317.0761 - reconstruction_loss: 309.8804 - kl_loss: 7.1958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4767\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 370.2753 - reconstruction_loss: 363.6837 - kl_loss: 6.5916\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 332.6431 - reconstruction_loss: 326.1052 - kl_loss: 6.5379\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1980\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1973\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 363.2870 - reconstruction_loss: 355.4412 - kl_loss: 7.8458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 266.0488 - reconstruction_loss: 258.2029 - kl_loss: 7.8459\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4628\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 302.7434 - reconstruction_loss: 294.9546 - kl_loss: 7.7888\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 969us/step - loss: 273.9374 - reconstruction_loss: 266.1524 - kl_loss: 7.7850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2807\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2775\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 413.2363 - reconstruction_loss: 406.3201 - kl_loss: 6.9162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 988us/step - loss: 316.5707 - reconstruction_loss: 309.6917 - kl_loss: 6.8791\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3786\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3755\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 323.9074 - reconstruction_loss: 316.3607 - kl_loss: 7.5467\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 344.3951 - reconstruction_loss: 336.8370 - kl_loss: 7.5581\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3963\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 317.2438 - reconstruction_loss: 309.9796 - kl_loss: 7.2642\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 317.8704 - reconstruction_loss: 310.6292 - kl_loss: 7.2411\n",
      "Success in episode 9 at time step 200 with reward -230.36654093891715\n",
      "Episode 10\n",
      "[ 0.581252    0.8137236  -0.75453764]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1326\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1304\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 167.8824 - reconstruction_loss: 162.9508 - kl_loss: 4.9316\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 265.4095 - reconstruction_loss: 260.4627 - kl_loss: 4.9468\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3815\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3678\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 166.0277 - reconstruction_loss: 162.1280 - kl_loss: 3.8997\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 229.0219 - reconstruction_loss: 225.1176 - kl_loss: 3.9042\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3489\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3487\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 231.2893 - reconstruction_loss: 223.8415 - kl_loss: 7.4478\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 214.4488 - reconstruction_loss: 206.9703 - kl_loss: 7.4785\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6051\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 181.4195 - reconstruction_loss: 174.4139 - kl_loss: 7.0056\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 849us/step - loss: 195.9680 - reconstruction_loss: 188.9771 - kl_loss: 6.9909\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0498\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0489\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 274.4182 - reconstruction_loss: 270.5560 - kl_loss: 3.8623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 967us/step - loss: 252.9008 - reconstruction_loss: 249.0254 - kl_loss: 3.8754\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2512\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 163.2738 - reconstruction_loss: 157.1168 - kl_loss: 6.1570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 183.9289 - reconstruction_loss: 177.7473 - kl_loss: 6.1816\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6168\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6103\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 285.2050 - reconstruction_loss: 278.5017 - kl_loss: 6.7033\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 198.3698 - reconstruction_loss: 191.6704 - kl_loss: 6.6993\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3727\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 109.0732 - reconstruction_loss: 101.9992 - kl_loss: 7.0740\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 230.6300 - reconstruction_loss: 223.5470 - kl_loss: 7.0830\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1136\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1166\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 262.1772 - reconstruction_loss: 259.0008 - kl_loss: 3.1764\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 255.8914 - reconstruction_loss: 252.7126 - kl_loss: 3.1788\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2652\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 168.3306 - reconstruction_loss: 161.0382 - kl_loss: 7.2924\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 200.1648 - reconstruction_loss: 192.8507 - kl_loss: 7.3141\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 203.9190 - reconstruction_loss: 197.2999 - kl_loss: 6.6191\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 944us/step - loss: 349.6921 - reconstruction_loss: 343.0727 - kl_loss: 6.6194\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1343\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 269.1064 - reconstruction_loss: 264.0867 - kl_loss: 5.0197\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 273.3820 - reconstruction_loss: 268.3578 - kl_loss: 5.0242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3703\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3730\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 189.5156 - reconstruction_loss: 184.9050 - kl_loss: 4.6106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 211.6684 - reconstruction_loss: 207.0504 - kl_loss: 4.6180\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2828\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2790\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 252.8638 - reconstruction_loss: 245.2826 - kl_loss: 7.5812\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 233.1378 - reconstruction_loss: 225.5436 - kl_loss: 7.5942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4912\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 251.2779 - reconstruction_loss: 243.7115 - kl_loss: 7.5664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 194.9325 - reconstruction_loss: 187.3627 - kl_loss: 7.5698\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0869\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0901\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 283.3886 - reconstruction_loss: 279.6728 - kl_loss: 3.7158\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 262.7375 - reconstruction_loss: 259.0201 - kl_loss: 3.7174\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3042\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 227.3670 - reconstruction_loss: 221.4407 - kl_loss: 5.9263\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 232.7081 - reconstruction_loss: 226.7712 - kl_loss: 5.9369\n",
      "Success in episode 10 at time step 200 with reward -178.3236986567963\n",
      "Episode 11\n",
      "[-0.12320994  0.9923806   0.7329694 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2640\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2631\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 283.5260 - reconstruction_loss: 280.1641 - kl_loss: 3.3619\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 255.3754 - reconstruction_loss: 252.0126 - kl_loss: 3.3628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8365\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 183.2215 - reconstruction_loss: 178.6145 - kl_loss: 4.6070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 244.8016 - reconstruction_loss: 240.1767 - kl_loss: 4.6249\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2317\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 210.3661 - reconstruction_loss: 202.2270 - kl_loss: 8.1391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 210.4863 - reconstruction_loss: 202.3251 - kl_loss: 8.1612\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0684\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0593\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 236.5639 - reconstruction_loss: 230.5794 - kl_loss: 5.9846\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 241.8249 - reconstruction_loss: 235.8440 - kl_loss: 5.9809\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2480\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2473\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 264.7481 - reconstruction_loss: 255.3614 - kl_loss: 9.3866\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 285.1550 - reconstruction_loss: 275.7305 - kl_loss: 9.4244\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1514\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 245.8660 - reconstruction_loss: 241.0698 - kl_loss: 4.7962\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 244.9713 - reconstruction_loss: 240.1822 - kl_loss: 4.7891\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2189\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 264.8053 - reconstruction_loss: 255.7639 - kl_loss: 9.0414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 205.3701 - reconstruction_loss: 196.3038 - kl_loss: 9.0662\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2158\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 233.4008 - reconstruction_loss: 224.3750 - kl_loss: 9.0258\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 306.9650 - reconstruction_loss: 297.9070 - kl_loss: 9.0580\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3824\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3772\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 291.9464 - reconstruction_loss: 286.2266 - kl_loss: 5.7198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 278.7937 - reconstruction_loss: 273.0869 - kl_loss: 5.7068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7208\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 252.1637 - reconstruction_loss: 243.5588 - kl_loss: 8.6049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 293.9521 - reconstruction_loss: 285.3100 - kl_loss: 8.6421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1263\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 268.6813 - reconstruction_loss: 264.0797 - kl_loss: 4.6016\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 868us/step - loss: 307.7444 - reconstruction_loss: 303.1591 - kl_loss: 4.5854\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3569\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 222.9357 - reconstruction_loss: 213.0296 - kl_loss: 9.9061\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 909us/step - loss: 220.1772 - reconstruction_loss: 210.2343 - kl_loss: 9.9430\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9092\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 315.3536 - reconstruction_loss: 310.1182 - kl_loss: 5.2354\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 291.6852 - reconstruction_loss: 286.4694 - kl_loss: 5.2158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3521\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3547\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 179.6409 - reconstruction_loss: 170.4210 - kl_loss: 9.2199\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 848us/step - loss: 197.9973 - reconstruction_loss: 188.7503 - kl_loss: 9.2470\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 232.7074 - reconstruction_loss: 224.0619 - kl_loss: 8.6455\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 314.1967 - reconstruction_loss: 305.5349 - kl_loss: 8.6618\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4434\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4367\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 253.9828 - reconstruction_loss: 247.6090 - kl_loss: 6.3737\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 325.1382 - reconstruction_loss: 318.7702 - kl_loss: 6.3681\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6330\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 247.9403 - reconstruction_loss: 240.7899 - kl_loss: 7.1504\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 252.6856 - reconstruction_loss: 245.5126 - kl_loss: 7.1730\n",
      "Success in episode 11 at time step 200 with reward -207.28729519661982\n",
      "Episode 12\n",
      "[-0.85929173  0.51148576  0.88303643]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.5865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5584\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 160.2884 - reconstruction_loss: 155.0602 - kl_loss: 5.2282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 147.1997 - reconstruction_loss: 141.9091 - kl_loss: 5.2907\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1589\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1586\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 153.7039 - reconstruction_loss: 147.3374 - kl_loss: 6.3665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.8750 - reconstruction_loss: 50.4470 - kl_loss: 6.4280\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1966\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1869\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 209.2115 - reconstruction_loss: 205.0026 - kl_loss: 4.2089\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 176.6756 - reconstruction_loss: 172.4341 - kl_loss: 4.2415\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1757\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 158.4455 - reconstruction_loss: 152.0434 - kl_loss: 6.4021\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 143.2709 - reconstruction_loss: 136.8097 - kl_loss: 6.4612\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2861\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 150.6463 - reconstruction_loss: 144.4877 - kl_loss: 6.1585\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 937us/step - loss: 130.3634 - reconstruction_loss: 124.1552 - kl_loss: 6.2082\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1200\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 239.1012 - reconstruction_loss: 235.0763 - kl_loss: 4.0249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 945us/step - loss: 213.7854 - reconstruction_loss: 209.7326 - kl_loss: 4.0528\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0953\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0851\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.0818 - reconstruction_loss: 126.8350 - kl_loss: 7.2468\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 930us/step - loss: 87.4346 - reconstruction_loss: 80.1300 - kl_loss: 7.3046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1957\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 174.4652 - reconstruction_loss: 168.8574 - kl_loss: 5.6078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 140.2621 - reconstruction_loss: 134.6171 - kl_loss: 5.6450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0509\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 231.1286 - reconstruction_loss: 226.0369 - kl_loss: 5.0917\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 181.5771 - reconstruction_loss: 176.4432 - kl_loss: 5.1339\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0632\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 195.2049 - reconstruction_loss: 187.3689 - kl_loss: 7.8360\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.9102 - reconstruction_loss: 107.0027 - kl_loss: 7.9075\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1087\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 153.5815 - reconstruction_loss: 148.0729 - kl_loss: 5.5087\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 257.4241 - reconstruction_loss: 251.8740 - kl_loss: 5.5501\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0731\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 174.8865 - reconstruction_loss: 169.6969 - kl_loss: 5.1896\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 895us/step - loss: 164.8201 - reconstruction_loss: 159.5900 - kl_loss: 5.2301\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0676\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0649\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 87.5600 - reconstruction_loss: 78.8064 - kl_loss: 8.7536\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.9474 - reconstruction_loss: 165.1264 - kl_loss: 8.8209\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0715\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0692\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 178.3161 - reconstruction_loss: 173.0064 - kl_loss: 5.3097\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 158.1517 - reconstruction_loss: 152.8027 - kl_loss: 5.3489\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1266\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1260\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.3368 - reconstruction_loss: 115.5569 - kl_loss: 6.7799\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.4014 - reconstruction_loss: 113.5751 - kl_loss: 6.8264\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0926\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.8810 - reconstruction_loss: 62.6569 - kl_loss: 8.2241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: 126.3221 - reconstruction_loss: 118.0461 - kl_loss: 8.2759\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1443\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 143.6720 - reconstruction_loss: 136.8409 - kl_loss: 6.8310\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 142.9929 - reconstruction_loss: 136.0647 - kl_loss: 6.9282\n",
      "Success in episode 12 at time step 200 with reward -264.61059117332553\n",
      "Episode 13\n",
      "[-0.4769283 -0.8789422  0.5008489]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2915\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.8539 - reconstruction_loss: 108.6384 - kl_loss: 11.2155\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 952us/step - loss: 135.0275 - reconstruction_loss: 123.6197 - kl_loss: 11.4078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2181\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2234\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 228.4235 - reconstruction_loss: 222.5615 - kl_loss: 5.8620\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 981us/step - loss: 303.1334 - reconstruction_loss: 297.2435 - kl_loss: 5.8899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2200\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 225.8977 - reconstruction_loss: 220.7948 - kl_loss: 5.1029\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 251.3834 - reconstruction_loss: 246.2333 - kl_loss: 5.1501\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3255\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.2874 - reconstruction_loss: 65.6725 - kl_loss: 11.6149\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.4558 - reconstruction_loss: 62.7316 - kl_loss: 11.7242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4499\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4525\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 147.3777 - reconstruction_loss: 141.4377 - kl_loss: 5.9399\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 192.5082 - reconstruction_loss: 186.5486 - kl_loss: 5.9596\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1995\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 295.6414 - reconstruction_loss: 290.7633 - kl_loss: 4.8780\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 210.8952 - reconstruction_loss: 205.9903 - kl_loss: 4.9048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4023\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.4827 - reconstruction_loss: 70.8772 - kl_loss: 12.6056\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 946us/step - loss: 123.1602 - reconstruction_loss: 110.4898 - kl_loss: 12.6704\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5936\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5750\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 203.0876 - reconstruction_loss: 196.9562 - kl_loss: 6.1314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 213.7725 - reconstruction_loss: 207.6281 - kl_loss: 6.1444\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1575\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1546\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 197.5667 - reconstruction_loss: 192.0934 - kl_loss: 5.4733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 194.2868 - reconstruction_loss: 188.7969 - kl_loss: 5.4899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.0990 - reconstruction_loss: 69.0680 - kl_loss: 13.0310\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 87.7843 - reconstruction_loss: 74.7220 - kl_loss: 13.0623\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6862\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6775\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 240.7493 - reconstruction_loss: 234.3361 - kl_loss: 6.4132\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 191.9707 - reconstruction_loss: 185.5477 - kl_loss: 6.4231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6600\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6575\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 237.9131 - reconstruction_loss: 233.7307 - kl_loss: 4.1824\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 261.0729 - reconstruction_loss: 256.8849 - kl_loss: 4.1880\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2288\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 357.6765 - reconstruction_loss: 352.2198 - kl_loss: 5.4566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 289.4011 - reconstruction_loss: 283.9413 - kl_loss: 5.4597\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5935\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.8205 - reconstruction_loss: 109.8358 - kl_loss: 12.9847\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 138.2365 - reconstruction_loss: 125.2408 - kl_loss: 12.9956\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8218\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8173\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 228.9488 - reconstruction_loss: 220.6031 - kl_loss: 8.3457\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.7762 - reconstruction_loss: 62.4185 - kl_loss: 8.3577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2639\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2567\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 354.7675 - reconstruction_loss: 350.5984 - kl_loss: 4.1692\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 349.8135 - reconstruction_loss: 345.6429 - kl_loss: 4.1706\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6130\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 182.7316 - reconstruction_loss: 174.4812 - kl_loss: 8.2504\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 162.6390 - reconstruction_loss: 154.3719 - kl_loss: 8.2670\n",
      "Success in episode 13 at time step 200 with reward -203.05143595181482\n",
      "Episode 14\n",
      "[ 0.9999967  -0.00255674  0.39211148]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0296\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0342\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 129.6857 - reconstruction_loss: 122.7199 - kl_loss: 6.9658\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 305.8751 - reconstruction_loss: 298.8573 - kl_loss: 7.0178\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7554\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7440\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 239.3284 - reconstruction_loss: 226.1192 - kl_loss: 13.2093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 164.2739 - reconstruction_loss: 151.0330 - kl_loss: 13.2409\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9451\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9386\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 211.8123 - reconstruction_loss: 206.5298 - kl_loss: 5.2824\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 307.0215 - reconstruction_loss: 301.7309 - kl_loss: 5.2906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5329\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 234.2038 - reconstruction_loss: 225.3991 - kl_loss: 8.8047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 221.6144 - reconstruction_loss: 212.7899 - kl_loss: 8.8245\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1951\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.2181 - reconstruction_loss: 144.0242 - kl_loss: 11.1940\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 100.4705 - reconstruction_loss: 89.2711 - kl_loss: 11.1994\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0355\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 209.0745 - reconstruction_loss: 203.8329 - kl_loss: 5.2416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 234.5926 - reconstruction_loss: 229.3369 - kl_loss: 5.2557\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8160\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8182\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 264.1046 - reconstruction_loss: 258.6085 - kl_loss: 5.4961\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 231.4549 - reconstruction_loss: 225.9419 - kl_loss: 5.5130\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4461\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4465\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 181.7346 - reconstruction_loss: 172.1338 - kl_loss: 9.6008\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 176.6313 - reconstruction_loss: 167.0211 - kl_loss: 9.6101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1137\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.4922 - reconstruction_loss: 165.7862 - kl_loss: 7.7060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 204.5110 - reconstruction_loss: 196.7966 - kl_loss: 7.7145\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1467\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1402\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.2747 - reconstruction_loss: 43.7047 - kl_loss: 10.5700\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 199.8874 - reconstruction_loss: 189.2834 - kl_loss: 10.6040\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1831\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1802\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 333.3627 - reconstruction_loss: 328.8593 - kl_loss: 4.5035\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 322.2844 - reconstruction_loss: 317.7752 - kl_loss: 4.5091\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3811\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 295.3752 - reconstruction_loss: 283.8750 - kl_loss: 11.5002\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 210.1153 - reconstruction_loss: 198.5658 - kl_loss: 11.5495\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9145\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8974\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 269.3564 - reconstruction_loss: 262.2958 - kl_loss: 7.0607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 985us/step - loss: 289.2595 - reconstruction_loss: 282.1776 - kl_loss: 7.0819\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8107\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 262.2965 - reconstruction_loss: 253.8879 - kl_loss: 8.4086\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 989us/step - loss: 234.1180 - reconstruction_loss: 225.6783 - kl_loss: 8.4397\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7576\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7297\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 264.1847 - reconstruction_loss: 254.8825 - kl_loss: 9.3022\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 985us/step - loss: 270.9552 - reconstruction_loss: 261.6438 - kl_loss: 9.3115\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1550\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1430\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 258.4460 - reconstruction_loss: 252.3928 - kl_loss: 6.0532\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 298.9137 - reconstruction_loss: 292.8651 - kl_loss: 6.0486\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4626\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4484\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 206.0904 - reconstruction_loss: 197.7707 - kl_loss: 8.3197\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 217.9853 - reconstruction_loss: 209.6426 - kl_loss: 8.3427\n",
      "Success in episode 14 at time step 200 with reward -174.3935102517413\n",
      "Episode 15\n",
      "[ 0.47835627  0.87816584 -0.9767489 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7941\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 182.4917 - reconstruction_loss: 176.9884 - kl_loss: 5.5033\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 986us/step - loss: 172.7107 - reconstruction_loss: 167.1989 - kl_loss: 5.5118\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8080\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7771\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 298.7283 - reconstruction_loss: 290.7278 - kl_loss: 8.0004\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 253.5069 - reconstruction_loss: 245.4612 - kl_loss: 8.0456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2964\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2840\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 299.4606 - reconstruction_loss: 292.1298 - kl_loss: 7.3308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 258.9579 - reconstruction_loss: 251.6030 - kl_loss: 7.3549\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6797\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6510\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 221.5531 - reconstruction_loss: 210.1049 - kl_loss: 11.4481\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 141.4050 - reconstruction_loss: 129.9045 - kl_loss: 11.5005\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2112\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1973\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 311.6586 - reconstruction_loss: 307.7704 - kl_loss: 3.8881\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 292.5665 - reconstruction_loss: 288.6740 - kl_loss: 3.8925\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8989\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8873\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 237.2197 - reconstruction_loss: 227.6715 - kl_loss: 9.5482\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 194.6577 - reconstruction_loss: 185.0727 - kl_loss: 9.5850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7949\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7593\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 252.8843 - reconstruction_loss: 243.9370 - kl_loss: 8.9473\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 983us/step - loss: 241.2760 - reconstruction_loss: 232.2845 - kl_loss: 8.9915\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5146\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 367.9584 - reconstruction_loss: 360.9799 - kl_loss: 6.9785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 260.6808 - reconstruction_loss: 253.6862 - kl_loss: 6.9947\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6632\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6484\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 142.0650 - reconstruction_loss: 129.7252 - kl_loss: 12.3398\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 983us/step - loss: 128.4045 - reconstruction_loss: 116.0022 - kl_loss: 12.4023\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5818\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 261.4207 - reconstruction_loss: 257.1752 - kl_loss: 4.2455\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 311.2498 - reconstruction_loss: 306.9909 - kl_loss: 4.2589\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8429\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8330\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 183.5449 - reconstruction_loss: 173.1788 - kl_loss: 10.3661\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 221.3891 - reconstruction_loss: 210.9835 - kl_loss: 10.4056\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6074\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5731\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 223.2336 - reconstruction_loss: 213.2348 - kl_loss: 9.9988\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 183.3738 - reconstruction_loss: 173.3386 - kl_loss: 10.0353\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7560\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 318.5685 - reconstruction_loss: 311.7551 - kl_loss: 6.8134\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 312.3320 - reconstruction_loss: 305.5180 - kl_loss: 6.8140\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8943\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8789\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 160.8214 - reconstruction_loss: 147.1560 - kl_loss: 13.6655\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 169.8634 - reconstruction_loss: 156.1613 - kl_loss: 13.7022\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6627\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6321\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 265.3774 - reconstruction_loss: 258.6724 - kl_loss: 6.7050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 275.3177 - reconstruction_loss: 268.5909 - kl_loss: 6.7268\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7500\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 174.1065 - reconstruction_loss: 164.4886 - kl_loss: 9.6179\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: 145.8680 - reconstruction_loss: 136.2338 - kl_loss: 9.6342\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3860\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3545\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 217.7837 - reconstruction_loss: 209.0164 - kl_loss: 8.7673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 208.8491 - reconstruction_loss: 200.0484 - kl_loss: 8.8007\n",
      "Success in episode 15 at time step 200 with reward -231.53622877691643\n",
      "Episode 16\n",
      "[ 0.94007635 -0.34096405  0.9759308 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7053\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7896\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 158.3164 - reconstruction_loss: 152.3698 - kl_loss: 5.9466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 203.5480 - reconstruction_loss: 197.5389 - kl_loss: 6.0091\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5896\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.5070 - reconstruction_loss: 102.4422 - kl_loss: 14.0647\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 178.7892 - reconstruction_loss: 164.6916 - kl_loss: 14.0976\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9962\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9765\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 81.3332 - reconstruction_loss: 72.6612 - kl_loss: 8.6720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 160.7309 - reconstruction_loss: 152.0268 - kl_loss: 8.7041\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6054\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5893\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 270.3458 - reconstruction_loss: 263.5260 - kl_loss: 6.8198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 218.5421 - reconstruction_loss: 211.6889 - kl_loss: 6.8532\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9987\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 180.0936 - reconstruction_loss: 169.5686 - kl_loss: 10.5249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 177.6547 - reconstruction_loss: 167.0770 - kl_loss: 10.5777\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8625\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 127.2899 - reconstruction_loss: 115.8829 - kl_loss: 11.4071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 187.9193 - reconstruction_loss: 176.4594 - kl_loss: 11.4599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8964\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8757\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 272.0001 - reconstruction_loss: 265.5797 - kl_loss: 6.4205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 203.6326 - reconstruction_loss: 197.1821 - kl_loss: 6.4505\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7637\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 101.8736 - reconstruction_loss: 86.9095 - kl_loss: 14.9640\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 139.9858 - reconstruction_loss: 124.9781 - kl_loss: 15.0077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7510\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7286\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 239.6314 - reconstruction_loss: 234.3118 - kl_loss: 5.3196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 229.2930 - reconstruction_loss: 223.9395 - kl_loss: 5.3536\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2569\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 141.9850 - reconstruction_loss: 128.4860 - kl_loss: 13.4990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.6263 - reconstruction_loss: 64.1007 - kl_loss: 13.5256\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1306\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0811\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 219.3094 - reconstruction_loss: 209.7324 - kl_loss: 9.5770\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 247.1917 - reconstruction_loss: 237.5858 - kl_loss: 9.6059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 216.4547 - reconstruction_loss: 206.9672 - kl_loss: 9.4875\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 189.7155 - reconstruction_loss: 180.2076 - kl_loss: 9.5080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1393\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1019\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 81.5168 - reconstruction_loss: 66.8704 - kl_loss: 14.6465\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.3279 - reconstruction_loss: 45.6311 - kl_loss: 14.6968\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6145\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 266.0698 - reconstruction_loss: 258.8802 - kl_loss: 7.1896\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 809us/step - loss: 214.8362 - reconstruction_loss: 207.6248 - kl_loss: 7.2114\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0640\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0087\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.7179 - reconstruction_loss: 121.0851 - kl_loss: 11.6328\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 151.3296 - reconstruction_loss: 139.6391 - kl_loss: 11.6905\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0823\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0166\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.7110 - reconstruction_loss: 53.2876 - kl_loss: 15.4233\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 151.8246 - reconstruction_loss: 136.3525 - kl_loss: 15.4721\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0295\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0124\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 179.6767 - reconstruction_loss: 168.8375 - kl_loss: 10.8391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.9812 - reconstruction_loss: 145.0678 - kl_loss: 10.9134\n",
      "Success in episode 16 at time step 200 with reward -191.19649179232553\n",
      "Episode 17\n",
      "[ 0.11533478  0.99332666 -0.20882168]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9943\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9820\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.8120 - reconstruction_loss: 88.5404 - kl_loss: 5.2716\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 138.1704 - reconstruction_loss: 132.8615 - kl_loss: 5.3089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4243\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.1544 - reconstruction_loss: 94.1483 - kl_loss: 13.0061\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.2807 - reconstruction_loss: 99.2550 - kl_loss: 13.0257\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9578\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9592\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 155.0296 - reconstruction_loss: 144.6689 - kl_loss: 10.3607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.8422 - reconstruction_loss: 124.4443 - kl_loss: 10.3979\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0797\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0508\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103.8135 - reconstruction_loss: 97.4421 - kl_loss: 6.3714\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.0009 - reconstruction_loss: 81.5973 - kl_loss: 6.4036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8919\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.1861 - reconstruction_loss: 53.0620 - kl_loss: 11.1242\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 140.6582 - reconstruction_loss: 129.5190 - kl_loss: 11.1393\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0759\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 149.2438 - reconstruction_loss: 136.6082 - kl_loss: 12.6356\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102.7040 - reconstruction_loss: 90.0576 - kl_loss: 12.6463\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.6410 - reconstruction_loss: 81.1190 - kl_loss: 7.5219\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.1778 - reconstruction_loss: 44.6249 - kl_loss: 7.5529\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4687\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 127.8665 - reconstruction_loss: 118.8443 - kl_loss: 9.0222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.0452 - reconstruction_loss: 82.9971 - kl_loss: 9.0481\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9738\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.9963 - reconstruction_loss: 39.6743 - kl_loss: 14.3220\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.6072 - reconstruction_loss: 27.2662 - kl_loss: 14.3410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 99.8250 - reconstruction_loss: 92.4586 - kl_loss: 7.3664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 133.0942 - reconstruction_loss: 125.7032 - kl_loss: 7.3910\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5314\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 128.0611 - reconstruction_loss: 117.3984 - kl_loss: 10.6627\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.3064 - reconstruction_loss: 82.6117 - kl_loss: 10.6947\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2270\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0639\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 44.8848 - reconstruction_loss: 30.1429 - kl_loss: 14.7419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.6723 - reconstruction_loss: 22.9069 - kl_loss: 14.7654\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0155\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8443\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.1305 - reconstruction_loss: 118.2853 - kl_loss: 6.8452\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 899us/step - loss: 130.5941 - reconstruction_loss: 123.7315 - kl_loss: 6.8627\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0327\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 79.8109 - reconstruction_loss: 71.4710 - kl_loss: 8.3399\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 105.1238 - reconstruction_loss: 96.7572 - kl_loss: 8.3666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6057\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 135.7126 - reconstruction_loss: 119.7473 - kl_loss: 15.9653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 971us/step - loss: 105.6805 - reconstruction_loss: 89.6929 - kl_loss: 15.9875\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8770\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8147\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 203.1292 - reconstruction_loss: 196.6053 - kl_loss: 6.5239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 135.9983 - reconstruction_loss: 129.4548 - kl_loss: 6.5435\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5038\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4494\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 84.9537 - reconstruction_loss: 74.7096 - kl_loss: 10.2441\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 99.6158 - reconstruction_loss: 89.3239 - kl_loss: 10.2919\n",
      "Success in episode 17 at time step 200 with reward -218.93079025871606\n",
      "Episode 18\n",
      "[0.8250278  0.5650921  0.72398025]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.2071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1798\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 96.1738 - reconstruction_loss: 86.5671 - kl_loss: 9.6067\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 105.1012 - reconstruction_loss: 95.4633 - kl_loss: 9.6378\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4282\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.2097 - reconstruction_loss: 117.4543 - kl_loss: 7.7554\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.0074 - reconstruction_loss: 114.1745 - kl_loss: 7.8329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8408\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8727\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.1949 - reconstruction_loss: 101.4348 - kl_loss: 15.7602\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 113.1786 - reconstruction_loss: 97.3959 - kl_loss: 15.7827\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5470\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5155\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.9994 - reconstruction_loss: 107.2278 - kl_loss: 9.7716\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 967us/step - loss: 101.2393 - reconstruction_loss: 91.4320 - kl_loss: 9.8073\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7055\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 114.7936 - reconstruction_loss: 106.9700 - kl_loss: 7.8235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 960us/step - loss: 100.3239 - reconstruction_loss: 92.4534 - kl_loss: 7.8706\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1877\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1898\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 104.4371 - reconstruction_loss: 86.7223 - kl_loss: 17.7148\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 962us/step - loss: 195.2837 - reconstruction_loss: 177.5354 - kl_loss: 17.7483\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9121\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.6778 - reconstruction_loss: 71.2544 - kl_loss: 8.4234\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 893us/step - loss: 150.0518 - reconstruction_loss: 141.6097 - kl_loss: 8.4421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1786\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1556\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.0638 - reconstruction_loss: 56.0709 - kl_loss: 9.9928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.0533 - reconstruction_loss: 124.0344 - kl_loss: 10.0188\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8075\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7662\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.0184 - reconstruction_loss: 86.3638 - kl_loss: 15.6545\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.3076 - reconstruction_loss: 95.6449 - kl_loss: 15.6627\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3156\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3106\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 266.4935 - reconstruction_loss: 257.0025 - kl_loss: 9.4910\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.0142 - reconstruction_loss: 91.5386 - kl_loss: 9.4756\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1934\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.3524 - reconstruction_loss: 106.1675 - kl_loss: 8.1850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 942us/step - loss: 106.0996 - reconstruction_loss: 97.8878 - kl_loss: 8.2118\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1930\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.1809 - reconstruction_loss: 98.7359 - kl_loss: 16.4450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.9704 - reconstruction_loss: 72.5313 - kl_loss: 16.4391\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4629\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4700\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 91.4273 - reconstruction_loss: 82.4046 - kl_loss: 9.0227\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.8330 - reconstruction_loss: 82.8089 - kl_loss: 9.0240\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3011\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2990\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 57.5124 - reconstruction_loss: 47.6726 - kl_loss: 9.8398\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.7916 - reconstruction_loss: 86.9490 - kl_loss: 9.8426\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8187\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7748\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.8637 - reconstruction_loss: 67.1745 - kl_loss: 15.6891\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.6242 - reconstruction_loss: 99.9339 - kl_loss: 15.6903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1081\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 84.7667 - reconstruction_loss: 76.0316 - kl_loss: 8.7351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 928us/step - loss: 73.3118 - reconstruction_loss: 64.5686 - kl_loss: 8.7431\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0944\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0685\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103.6217 - reconstruction_loss: 92.2943 - kl_loss: 11.3274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.7694 - reconstruction_loss: 91.4036 - kl_loss: 11.3659\n",
      "Success in episode 18 at time step 200 with reward -209.8701308354713\n",
      "Episode 19\n",
      "[-0.9999575   0.00922239  0.21631965]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3904\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.5481 - reconstruction_loss: 7.4582 - kl_loss: 13.0899\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.3012 - reconstruction_loss: 25.1834 - kl_loss: 13.1177\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6987\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51.4368 - reconstruction_loss: 39.5374 - kl_loss: 11.8993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.8614 - reconstruction_loss: 65.9330 - kl_loss: 11.9284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8132\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7543\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.5880 - reconstruction_loss: 58.9047 - kl_loss: 9.6833\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.4724 - reconstruction_loss: 39.7587 - kl_loss: 9.7137\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.6450 - reconstruction_loss: 13.3104 - kl_loss: 14.3346\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.5303 - reconstruction_loss: 69.1639 - kl_loss: 14.3664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2248\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 76.2651 - reconstruction_loss: 66.9289 - kl_loss: 9.3362\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.7880 - reconstruction_loss: 48.4189 - kl_loss: 9.3691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2981\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3023\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.5442 - reconstruction_loss: 23.6883 - kl_loss: 14.8559\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.2016 - reconstruction_loss: 18.3056 - kl_loss: 14.8960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3595\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.9657 - reconstruction_loss: 17.5858 - kl_loss: 12.3799\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.9661 - reconstruction_loss: 14.5614 - kl_loss: 12.4047\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2883\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2698\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 91.5127 - reconstruction_loss: 82.4791 - kl_loss: 9.0337\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.5213 - reconstruction_loss: 59.4632 - kl_loss: 9.0581\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9574\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9512\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.5086 - reconstruction_loss: 84.1412 - kl_loss: 16.3675\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.0064 - reconstruction_loss: 36.6268 - kl_loss: 16.3796\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2736\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2220\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102.6717 - reconstruction_loss: 91.8039 - kl_loss: 10.8678\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.9907 - reconstruction_loss: 58.0932 - kl_loss: 10.8975\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0878\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9856\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 123.1203 - reconstruction_loss: 113.6327 - kl_loss: 9.4876\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 945us/step - loss: 117.6600 - reconstruction_loss: 108.1533 - kl_loss: 9.5067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0345\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9935\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.8721 - reconstruction_loss: 64.5215 - kl_loss: 17.3507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 160.1024 - reconstruction_loss: 142.7418 - kl_loss: 17.3605\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6699\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6483\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.8490 - reconstruction_loss: 77.6942 - kl_loss: 10.1548\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.1967 - reconstruction_loss: 124.0173 - kl_loss: 10.1794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0624\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9751\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 155.3754 - reconstruction_loss: 148.3257 - kl_loss: 7.0497\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.3172 - reconstruction_loss: 109.2519 - kl_loss: 7.0653\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5711\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 82.5766 - reconstruction_loss: 62.7389 - kl_loss: 19.8377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.8092 - reconstruction_loss: 51.9479 - kl_loss: 19.8613\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7422\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 154.2329 - reconstruction_loss: 144.9572 - kl_loss: 9.2756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 145.6936 - reconstruction_loss: 136.4008 - kl_loss: 9.2928\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4595\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 71.7232 - reconstruction_loss: 59.4517 - kl_loss: 12.2715\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.1754 - reconstruction_loss: 65.8978 - kl_loss: 12.2777\n",
      "Success in episode 19 at time step 200 with reward -245.65401997968428\n",
      "Episode 20\n",
      "[ 0.408846   -0.9126034  -0.10648535]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6218\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 139.3142 - reconstruction_loss: 120.2395 - kl_loss: 19.0747\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 958us/step - loss: 144.2101 - reconstruction_loss: 125.1165 - kl_loss: 19.0936\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4496\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 129.5260 - reconstruction_loss: 118.5658 - kl_loss: 10.9602\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 290.5826 - reconstruction_loss: 279.5945 - kl_loss: 10.9881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.2843\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.1738\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 157.1782 - reconstruction_loss: 145.0793 - kl_loss: 12.0988\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 179.3110 - reconstruction_loss: 167.1955 - kl_loss: 12.1155\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 159.6803 - reconstruction_loss: 141.3325 - kl_loss: 18.3479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 999us/step - loss: 176.3478 - reconstruction_loss: 158.0103 - kl_loss: 18.3374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8980\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 276.6183 - reconstruction_loss: 264.1013 - kl_loss: 12.5169\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 239.7272 - reconstruction_loss: 227.2034 - kl_loss: 12.5238\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1121\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 255.6476 - reconstruction_loss: 239.1419 - kl_loss: 16.5056\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 193.6117 - reconstruction_loss: 177.1164 - kl_loss: 16.4953\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6789\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 180.8494 - reconstruction_loss: 167.2521 - kl_loss: 13.5973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 197.3817 - reconstruction_loss: 183.7845 - kl_loss: 13.5973\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2535\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 300.0815 - reconstruction_loss: 288.5367 - kl_loss: 11.5447\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 265.0916 - reconstruction_loss: 253.5525 - kl_loss: 11.5392\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1112\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 204.4339 - reconstruction_loss: 185.5840 - kl_loss: 18.8500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 240.9256 - reconstruction_loss: 222.1119 - kl_loss: 18.8137\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9867\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 238.8632 - reconstruction_loss: 230.2120 - kl_loss: 8.6512\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 231.6230 - reconstruction_loss: 222.9545 - kl_loss: 8.6685\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3230\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 185.2381 - reconstruction_loss: 166.8615 - kl_loss: 18.3766\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 196.6207 - reconstruction_loss: 178.2831 - kl_loss: 18.3377\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0052\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7336\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 310.6480 - reconstruction_loss: 301.2399 - kl_loss: 9.4081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 249.0182 - reconstruction_loss: 239.6169 - kl_loss: 9.4013\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 185.2172 - reconstruction_loss: 170.0388 - kl_loss: 15.1784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 209.6256 - reconstruction_loss: 194.4613 - kl_loss: 15.1644\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3885\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2001\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 197.7586 - reconstruction_loss: 177.7170 - kl_loss: 20.0416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 211.4517 - reconstruction_loss: 191.4375 - kl_loss: 20.0143\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5381\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3111\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 226.3167 - reconstruction_loss: 217.9458 - kl_loss: 8.3709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 268.9606 - reconstruction_loss: 260.6019 - kl_loss: 8.3587\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0048\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 249.5749 - reconstruction_loss: 231.0300 - kl_loss: 18.5450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 202.4759 - reconstruction_loss: 183.9469 - kl_loss: 18.5290\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0529\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 4.8737\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 196.3521 - reconstruction_loss: 182.3223 - kl_loss: 14.0298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 194.8068 - reconstruction_loss: 180.8068 - kl_loss: 14.0000\n",
      "Success in episode 20 at time step 200 with reward -203.89188320923245\n",
      "Episode 21\n",
      "[-0.98203224  0.18871309  0.8740571 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8092\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.7755 - reconstruction_loss: 29.4536 - kl_loss: 13.3219\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.2848 - reconstruction_loss: 29.9839 - kl_loss: 13.3009\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6207\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7018\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.4310 - reconstruction_loss: 36.7387 - kl_loss: 13.6924\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.4628 - reconstruction_loss: 27.7775 - kl_loss: 13.6853\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2907\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3053\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.6224 - reconstruction_loss: 22.1179 - kl_loss: 11.5046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.6760 - reconstruction_loss: 24.1660 - kl_loss: 11.5100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0966\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0757\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.3612 - reconstruction_loss: 57.9266 - kl_loss: 10.4346\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.5169 - reconstruction_loss: 61.0716 - kl_loss: 10.4454\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6211\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5319\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.6871 - reconstruction_loss: 6.6297 - kl_loss: 14.0574\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.3131 - reconstruction_loss: 20.2566 - kl_loss: 14.0565\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9582\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6839\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62.8253 - reconstruction_loss: 50.0852 - kl_loss: 12.7402\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.0226 - reconstruction_loss: 67.2637 - kl_loss: 12.7589\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5114\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.7000 - reconstruction_loss: 48.7310 - kl_loss: 12.9691\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 899us/step - loss: 55.8841 - reconstruction_loss: 42.9160 - kl_loss: 12.9681\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9171\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7234\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.6947 - reconstruction_loss: 24.6762 - kl_loss: 15.0185\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.4113 - reconstruction_loss: 7.4221 - kl_loss: 14.9892\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3983\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.3018 - reconstruction_loss: 82.7652 - kl_loss: 9.5367\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.0301 - reconstruction_loss: 74.5042 - kl_loss: 9.5259\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7924\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7663\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58.7489 - reconstruction_loss: 42.9230 - kl_loss: 15.8258\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 862us/step - loss: 33.8354 - reconstruction_loss: 18.0144 - kl_loss: 15.8210\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9422\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.1787 - reconstruction_loss: 12.3593 - kl_loss: 9.8194\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.8352 - reconstruction_loss: 13.0172 - kl_loss: 9.8180\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3590\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3359\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.0829 - reconstruction_loss: 113.7702 - kl_loss: 8.3127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 103.1536 - reconstruction_loss: 94.8427 - kl_loss: 8.3109\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7484\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6406\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.0760 - reconstruction_loss: 43.2495 - kl_loss: 17.8265\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.1451 - reconstruction_loss: 21.3192 - kl_loss: 17.8259\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2457\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.4136 - reconstruction_loss: 58.0592 - kl_loss: 12.3544\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.7734 - reconstruction_loss: 38.3932 - kl_loss: 12.3802\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0700\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9253\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.3904 - reconstruction_loss: 113.4706 - kl_loss: 8.9198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 887us/step - loss: 186.6086 - reconstruction_loss: 177.6895 - kl_loss: 8.9192\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5656\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4642\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.1716 - reconstruction_loss: 10.8389 - kl_loss: 15.3328\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.6795 - reconstruction_loss: 35.3459 - kl_loss: 15.3336\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3125\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2444\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 60.3440 - reconstruction_loss: 47.8912 - kl_loss: 12.4528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 50.2170 - reconstruction_loss: 37.7814 - kl_loss: 12.4356\n",
      "Success in episode 21 at time step 200 with reward -254.13164917798346\n",
      "Episode 22\n",
      "[ 0.9494638   0.31387654 -0.4946285 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.7383 - reconstruction_loss: 67.3744 - kl_loss: 17.3639\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.5394 - reconstruction_loss: 40.2067 - kl_loss: 17.3327\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5240\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 215.8290 - reconstruction_loss: 202.7453 - kl_loss: 13.0837\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 269.0428 - reconstruction_loss: 255.9511 - kl_loss: 13.0917\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 113.9864 - reconstruction_loss: 92.5295 - kl_loss: 21.4570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 158.6703 - reconstruction_loss: 137.2402 - kl_loss: 21.4300\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 14.3476\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 14.1022\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 169.9811 - reconstruction_loss: 160.0770 - kl_loss: 9.9042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 177.0822 - reconstruction_loss: 167.1566 - kl_loss: 9.9256\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6870\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 186.8662 - reconstruction_loss: 171.9830 - kl_loss: 14.8832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 182.7997 - reconstruction_loss: 167.9525 - kl_loss: 14.8472\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7908\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.5998 - reconstruction_loss: 90.0321 - kl_loss: 18.5677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.0498 - reconstruction_loss: 92.4659 - kl_loss: 18.5839\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4789\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 194.0712 - reconstruction_loss: 182.0955 - kl_loss: 11.9757\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 187.3328 - reconstruction_loss: 175.3576 - kl_loss: 11.9752\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8189\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6079\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.1198 - reconstruction_loss: 52.9949 - kl_loss: 20.1249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 98.0879 - reconstruction_loss: 77.9677 - kl_loss: 20.1202\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3326\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1892\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 196.7128 - reconstruction_loss: 189.5349 - kl_loss: 7.1778\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 224.4724 - reconstruction_loss: 217.2654 - kl_loss: 7.2070\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7459\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.6228 - reconstruction_loss: 71.1905 - kl_loss: 14.4323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.4122 - reconstruction_loss: 103.9417 - kl_loss: 14.4704\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2411\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2057\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 181.8819 - reconstruction_loss: 168.6239 - kl_loss: 13.2580\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 195.8895 - reconstruction_loss: 182.6389 - kl_loss: 13.2507\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0952\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1514\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 149.2825 - reconstruction_loss: 129.0552 - kl_loss: 20.2273\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 137.4995 - reconstruction_loss: 117.2554 - kl_loss: 20.2440\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0097\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9424\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 198.6848 - reconstruction_loss: 187.5226 - kl_loss: 11.1622\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 177.5570 - reconstruction_loss: 166.3978 - kl_loss: 11.1592\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8105\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 176.9932 - reconstruction_loss: 159.5251 - kl_loss: 17.4682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 150.4305 - reconstruction_loss: 132.9579 - kl_loss: 17.4726\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3140\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 198.3988 - reconstruction_loss: 189.8604 - kl_loss: 8.5385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 885us/step - loss: 147.9872 - reconstruction_loss: 139.4370 - kl_loss: 8.5502\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6917\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6420\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 151.5674 - reconstruction_loss: 134.5101 - kl_loss: 17.0573\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 193.2184 - reconstruction_loss: 176.1628 - kl_loss: 17.0556\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7997\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7249\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 159.8384 - reconstruction_loss: 145.3555 - kl_loss: 14.4829\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 162.4526 - reconstruction_loss: 147.9271 - kl_loss: 14.5255\n",
      "Success in episode 22 at time step 200 with reward -203.6278329815451\n",
      "Episode 23\n",
      "[0.8721973  0.4891543  0.92967325]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.0929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 117.1292 - reconstruction_loss: 106.0183 - kl_loss: 11.1110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.2739 - reconstruction_loss: 97.1607 - kl_loss: 11.1133\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3801\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 105.8960 - reconstruction_loss: 93.8041 - kl_loss: 12.0919\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.1925 - reconstruction_loss: 86.9812 - kl_loss: 12.2113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1825\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.4752 - reconstruction_loss: 71.9481 - kl_loss: 17.5271\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.3852 - reconstruction_loss: 102.8370 - kl_loss: 17.5482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8701\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 212.7794 - reconstruction_loss: 203.4515 - kl_loss: 9.3279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 174.5626 - reconstruction_loss: 165.2040 - kl_loss: 9.3586\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.0962 - reconstruction_loss: 135.5099 - kl_loss: 19.5863\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 127.5022 - reconstruction_loss: 107.8783 - kl_loss: 19.6239\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0766\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9832\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 228.1873 - reconstruction_loss: 217.8618 - kl_loss: 10.3255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 219.3826 - reconstruction_loss: 209.0401 - kl_loss: 10.3425\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6989\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.9160 - reconstruction_loss: 77.6828 - kl_loss: 12.2333\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 923us/step - loss: 78.8147 - reconstruction_loss: 66.5177 - kl_loss: 12.2970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1322\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.1198 - reconstruction_loss: 73.3232 - kl_loss: 18.7965\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.5637 - reconstruction_loss: 87.7830 - kl_loss: 18.7807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6325\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6245\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.4743 - reconstruction_loss: 70.7380 - kl_loss: 11.7363\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.0131 - reconstruction_loss: 68.2552 - kl_loss: 11.7580\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8962\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9019\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 145.8504 - reconstruction_loss: 137.8911 - kl_loss: 7.9593\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 145.0145 - reconstruction_loss: 137.0473 - kl_loss: 7.9671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7371\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7116\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.2239 - reconstruction_loss: 31.2569 - kl_loss: 18.9670\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.9793 - reconstruction_loss: 31.0110 - kl_loss: 18.9682\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2931\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2098\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.3867 - reconstruction_loss: 89.6583 - kl_loss: 10.7284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.7081 - reconstruction_loss: 72.9555 - kl_loss: 10.7526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1782\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1314\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 158.8565 - reconstruction_loss: 149.5968 - kl_loss: 9.2597\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 164.1277 - reconstruction_loss: 154.8543 - kl_loss: 9.2734\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1798\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 124.7406 - reconstruction_loss: 112.2635 - kl_loss: 12.4772\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.4395 - reconstruction_loss: 59.9320 - kl_loss: 12.5075\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5445\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.4888 - reconstruction_loss: 101.9767 - kl_loss: 17.5121\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.9057 - reconstruction_loss: 91.3738 - kl_loss: 17.5319\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3592\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 144.5095 - reconstruction_loss: 133.6625 - kl_loss: 10.8469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 140.6137 - reconstruction_loss: 129.7547 - kl_loss: 10.8589\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5171\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 124.2807 - reconstruction_loss: 110.5920 - kl_loss: 13.6887\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.2700 - reconstruction_loss: 107.5384 - kl_loss: 13.7316\n",
      "Success in episode 23 at time step 200 with reward -175.09791252010348\n",
      "Episode 24\n",
      "[-0.98607713  0.16628854  0.06141466]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7908\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 52.3417 - reconstruction_loss: 41.3861 - kl_loss: 10.9556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 929us/step - loss: 49.3171 - reconstruction_loss: 38.3568 - kl_loss: 10.9603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7537\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.8094 - reconstruction_loss: 37.3617 - kl_loss: 12.4477\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.0505 - reconstruction_loss: 9.5681 - kl_loss: 12.4824\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0915\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.4686 - reconstruction_loss: 70.5098 - kl_loss: 10.9588\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 981us/step - loss: 56.5290 - reconstruction_loss: 45.5307 - kl_loss: 10.9983\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.7909 - reconstruction_loss: 22.6044 - kl_loss: 13.1864\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.3388 - reconstruction_loss: 9.1236 - kl_loss: 13.2152\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1543\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8956\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.3871 - reconstruction_loss: 12.9100 - kl_loss: 12.4771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 959us/step - loss: 26.4918 - reconstruction_loss: 13.9742 - kl_loss: 12.5176\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2899\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2696\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 63.8214 - reconstruction_loss: 54.5033 - kl_loss: 9.3181\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 63.9018 - reconstruction_loss: 54.5652 - kl_loss: 9.3366\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2288\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9789\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.6630 - reconstruction_loss: 19.1772 - kl_loss: 16.4857\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.4767 - reconstruction_loss: 12.9872 - kl_loss: 16.4896\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.6392 - reconstruction_loss: 54.2265 - kl_loss: 10.4127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 86.8098 - reconstruction_loss: 76.3912 - kl_loss: 10.4186\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9457\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9474\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.8631 - reconstruction_loss: 21.4527 - kl_loss: 14.4104\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.0685 - reconstruction_loss: 18.6436 - kl_loss: 14.4249\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0310\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6890\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.4198 - reconstruction_loss: 14.9821 - kl_loss: 13.4377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.1492 - reconstruction_loss: 86.7275 - kl_loss: 13.4217\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1551\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2623\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.5534 - reconstruction_loss: 82.4643 - kl_loss: 11.0891\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 889us/step - loss: 103.7266 - reconstruction_loss: 92.6304 - kl_loss: 11.0961\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6959\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5940\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.8585 - reconstruction_loss: 26.6376 - kl_loss: 14.2209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 850us/step - loss: 36.9468 - reconstruction_loss: 22.7282 - kl_loss: 14.2186\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2837\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.8197 - reconstruction_loss: 42.1019 - kl_loss: 11.7178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.5839 - reconstruction_loss: 38.8728 - kl_loss: 11.7110\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3369\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1217\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.6280 - reconstruction_loss: 76.1918 - kl_loss: 11.4362\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 959us/step - loss: 86.9378 - reconstruction_loss: 75.4941 - kl_loss: 11.4437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9784\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.0111 - reconstruction_loss: 22.4476 - kl_loss: 16.5635\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.1928 - reconstruction_loss: 36.6344 - kl_loss: 16.5584\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6889\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4550\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.5765 - reconstruction_loss: 69.8723 - kl_loss: 11.7042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.6459 - reconstruction_loss: 53.9357 - kl_loss: 11.7103\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.8105\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.4957 - reconstruction_loss: 39.0096 - kl_loss: 12.4860\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.9005 - reconstruction_loss: 39.4170 - kl_loss: 12.4835\n",
      "Success in episode 24 at time step 200 with reward -260.5656501449898\n",
      "Episode 25\n",
      "[ 0.6396038  -0.7687048  -0.30887517]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 115.7078 - reconstruction_loss: 95.7796 - kl_loss: 19.9282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.2736 - reconstruction_loss: 99.3352 - kl_loss: 19.9384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1987\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3167\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.9836 - reconstruction_loss: 109.6006 - kl_loss: 10.3830\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 141.1854 - reconstruction_loss: 130.7764 - kl_loss: 10.4090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7684\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 147.2729 - reconstruction_loss: 128.7533 - kl_loss: 18.5196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 162.5728 - reconstruction_loss: 144.0554 - kl_loss: 18.5174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9421\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.4142 - reconstruction_loss: 89.0558 - kl_loss: 12.3584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 95.7963 - reconstruction_loss: 83.3986 - kl_loss: 12.3977\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2417\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1875\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.5949 - reconstruction_loss: 103.0929 - kl_loss: 22.5020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 950us/step - loss: 127.0593 - reconstruction_loss: 104.5329 - kl_loss: 22.5264\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.7248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.2495 - reconstruction_loss: 102.3354 - kl_loss: 11.9141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 162.6587 - reconstruction_loss: 150.7330 - kl_loss: 11.9257\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.9598\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8313\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 43.4683 - reconstruction_loss: 24.8445 - kl_loss: 18.6239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.1699 - reconstruction_loss: 40.5145 - kl_loss: 18.6554\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0890\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 203.4513 - reconstruction_loss: 192.0776 - kl_loss: 11.3736\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 189.3509 - reconstruction_loss: 177.9718 - kl_loss: 11.3791\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 163.1943 - reconstruction_loss: 141.9197 - kl_loss: 21.2746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 159.2608 - reconstruction_loss: 138.0118 - kl_loss: 21.2491\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8402\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.6135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 115.7021 - reconstruction_loss: 100.1147 - kl_loss: 15.5874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.4263 - reconstruction_loss: 88.8208 - kl_loss: 15.6055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9176\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5804\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 89.6083 - reconstruction_loss: 68.2868 - kl_loss: 21.3215\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.5349 - reconstruction_loss: 44.2527 - kl_loss: 21.2822\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0385\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 174.8102 - reconstruction_loss: 165.5289 - kl_loss: 9.2813\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 184.5131 - reconstruction_loss: 175.2158 - kl_loss: 9.2973\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8831\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.5451 - reconstruction_loss: 70.8585 - kl_loss: 19.6866\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 923us/step - loss: 128.3584 - reconstruction_loss: 108.6937 - kl_loss: 19.6647\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5963\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 127.7861 - reconstruction_loss: 113.5113 - kl_loss: 14.2748\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 970us/step - loss: 161.4193 - reconstruction_loss: 147.1288 - kl_loss: 14.2905\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1216\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9230\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.3630 - reconstruction_loss: 70.9675 - kl_loss: 19.3955\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 939us/step - loss: 103.8669 - reconstruction_loss: 84.5130 - kl_loss: 19.3539\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7855\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 160.3917 - reconstruction_loss: 146.0939 - kl_loss: 14.2978\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.9873 - reconstruction_loss: 120.6676 - kl_loss: 14.3197\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 125.8150 - reconstruction_loss: 109.4578 - kl_loss: 16.3572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.4841 - reconstruction_loss: 101.1820 - kl_loss: 16.3021\n",
      "Success in episode 25 at time step 200 with reward -173.51887364193655\n",
      "Episode 26\n",
      "[0.7647414  0.6443373  0.89830625]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7184\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 140.2344 - reconstruction_loss: 129.2759 - kl_loss: 10.9586\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 994us/step - loss: 121.9127 - reconstruction_loss: 111.0175 - kl_loss: 10.8952\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4707\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5247\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.2277 - reconstruction_loss: 70.2845 - kl_loss: 13.9432\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.9765 - reconstruction_loss: 68.9502 - kl_loss: 14.0263\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8561\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.8833 - reconstruction_loss: 50.9480 - kl_loss: 18.9353\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 76.0621 - reconstruction_loss: 57.1227 - kl_loss: 18.9394\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6373\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 11.5438\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.6469 - reconstruction_loss: 46.7452 - kl_loss: 12.9018\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 977us/step - loss: 67.1571 - reconstruction_loss: 54.2750 - kl_loss: 12.8821\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0718\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9998\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 135.8506 - reconstruction_loss: 124.7703 - kl_loss: 11.0803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 910us/step - loss: 122.1874 - reconstruction_loss: 111.0596 - kl_loss: 11.1278\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.5534\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.3990\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 50.2633 - reconstruction_loss: 31.9975 - kl_loss: 18.2659\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.6703 - reconstruction_loss: 25.3671 - kl_loss: 18.3032\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1963\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0602\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 67.4932 - reconstruction_loss: 53.5186 - kl_loss: 13.9745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 952us/step - loss: 121.5267 - reconstruction_loss: 107.5794 - kl_loss: 13.9473\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0856\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 124.3325 - reconstruction_loss: 113.8345 - kl_loss: 10.4980\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.8989 - reconstruction_loss: 118.4112 - kl_loss: 10.4877\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.1527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.7387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 60.1991 - reconstruction_loss: 43.9900 - kl_loss: 16.2090\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.5878 - reconstruction_loss: 41.3526 - kl_loss: 16.2352\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5545\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73.1782 - reconstruction_loss: 53.4099 - kl_loss: 19.7683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.1223 - reconstruction_loss: 62.3887 - kl_loss: 19.7335\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.0766 - reconstruction_loss: 96.1799 - kl_loss: 9.8968\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 82.6997 - reconstruction_loss: 72.8390 - kl_loss: 9.8607\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7777\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.5696 - reconstruction_loss: 40.3882 - kl_loss: 16.1814\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.2463 - reconstruction_loss: 52.0672 - kl_loss: 16.1790\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5773\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.6286 - reconstruction_loss: 34.9519 - kl_loss: 16.6767\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.8961 - reconstruction_loss: 38.2632 - kl_loss: 16.6329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4840\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3533\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.7888 - reconstruction_loss: 56.8034 - kl_loss: 9.9854\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 946us/step - loss: 66.6014 - reconstruction_loss: 56.6564 - kl_loss: 9.9451\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2132\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.1614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.5282 - reconstruction_loss: 118.9686 - kl_loss: 12.5596\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 96.1911 - reconstruction_loss: 83.6208 - kl_loss: 12.5703\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1236\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.5746 - reconstruction_loss: 44.3462 - kl_loss: 21.2283\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.4261 - reconstruction_loss: 36.1950 - kl_loss: 21.2312\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6059\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 5.4265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.4478 - reconstruction_loss: 66.0505 - kl_loss: 14.3973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.8798 - reconstruction_loss: 63.4709 - kl_loss: 14.4088\n",
      "Success in episode 26 at time step 200 with reward -199.39836851262626\n",
      "Episode 27\n",
      "[-0.95205647 -0.30592227 -0.5305455 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7733\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.5732 - reconstruction_loss: 5.6164 - kl_loss: 11.9569\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.5099 - reconstruction_loss: 19.5585 - kl_loss: 11.9514\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8360\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6002\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.0014 - reconstruction_loss: 14.4556 - kl_loss: 12.5458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.6225 - reconstruction_loss: 11.0476 - kl_loss: 12.5749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0901\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.0898 - reconstruction_loss: 51.7075 - kl_loss: 13.3823\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.1151 - reconstruction_loss: 36.7547 - kl_loss: 13.3604\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5597\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5888\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 45.2349 - reconstruction_loss: 31.1192 - kl_loss: 14.1157\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.4707 - reconstruction_loss: 12.3555 - kl_loss: 14.1152\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0409\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.9461 - reconstruction_loss: 52.2146 - kl_loss: 9.7315\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.0438 - reconstruction_loss: 56.3032 - kl_loss: 9.7406\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6445\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.7064 - reconstruction_loss: 19.8207 - kl_loss: 16.8857\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.9349 - reconstruction_loss: 16.0629 - kl_loss: 16.8720\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9599\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9361\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38.0729 - reconstruction_loss: 24.4952 - kl_loss: 13.5778\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.2612 - reconstruction_loss: 43.6636 - kl_loss: 13.5976\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8306\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6128\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.3557 - reconstruction_loss: 91.2630 - kl_loss: 11.0928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.5987 - reconstruction_loss: 69.5065 - kl_loss: 11.0921\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5969\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.7317 - reconstruction_loss: 33.8789 - kl_loss: 17.8528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 917us/step - loss: 48.9750 - reconstruction_loss: 31.1302 - kl_loss: 17.8448\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.0460 - reconstruction_loss: 55.3233 - kl_loss: 10.7228\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 920us/step - loss: 65.3372 - reconstruction_loss: 54.6030 - kl_loss: 10.7342\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6881\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5627\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.3243 - reconstruction_loss: 120.2592 - kl_loss: 11.0652\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.9577 - reconstruction_loss: 114.8879 - kl_loss: 11.0698\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7462\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.0825 - reconstruction_loss: 18.4326 - kl_loss: 17.6499\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.3374 - reconstruction_loss: 21.6994 - kl_loss: 17.6380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1960\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9269\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73.7647 - reconstruction_loss: 61.9066 - kl_loss: 11.8581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.7232 - reconstruction_loss: 44.8468 - kl_loss: 11.8763\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3827\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 135.3062 - reconstruction_loss: 124.1864 - kl_loss: 11.1199\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.2849 - reconstruction_loss: 120.1578 - kl_loss: 11.1271\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7812\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.2343 - reconstruction_loss: 33.0503 - kl_loss: 20.1840\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.8658 - reconstruction_loss: 35.6931 - kl_loss: 20.1727\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9270\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5433\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.3022 - reconstruction_loss: 99.8309 - kl_loss: 10.4712\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102.6594 - reconstruction_loss: 92.1630 - kl_loss: 10.4964\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6708\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 4.5643\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62.4567 - reconstruction_loss: 48.9244 - kl_loss: 13.5323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.1808 - reconstruction_loss: 50.6431 - kl_loss: 13.5377\n",
      "Success in episode 27 at time step 200 with reward -229.65404996946353\n",
      "Episode 28\n",
      "[-0.6625834  -0.74898815 -0.4002887 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7286\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6310\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 57.2002 - reconstruction_loss: 43.4123 - kl_loss: 13.7878\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.2065 - reconstruction_loss: 23.3965 - kl_loss: 13.8100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6150\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 67.4196 - reconstruction_loss: 55.6476 - kl_loss: 11.7720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.6181 - reconstruction_loss: 57.8634 - kl_loss: 11.7547\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9516\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8985\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.5803 - reconstruction_loss: 29.1745 - kl_loss: 13.4058\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 989us/step - loss: 38.1803 - reconstruction_loss: 24.7399 - kl_loss: 13.4403\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5735\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5359\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.4616 - reconstruction_loss: 43.6102 - kl_loss: 15.8514\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.6798 - reconstruction_loss: 42.8460 - kl_loss: 15.8338\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7361\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 106.3178 - reconstruction_loss: 95.9000 - kl_loss: 10.4177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.4580 - reconstruction_loss: 98.0518 - kl_loss: 10.4062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54.4320 - reconstruction_loss: 35.0461 - kl_loss: 19.3859\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 911us/step - loss: 41.9989 - reconstruction_loss: 22.5852 - kl_loss: 19.4137\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9858\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.0724 - reconstruction_loss: 78.9501 - kl_loss: 12.1223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.5873 - reconstruction_loss: 89.4723 - kl_loss: 12.1150\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8393\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.8022 - reconstruction_loss: 74.6680 - kl_loss: 10.1342\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.1072 - reconstruction_loss: 51.9685 - kl_loss: 10.1387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7375\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 55.7763 - reconstruction_loss: 42.1956 - kl_loss: 13.5807\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.2999 - reconstruction_loss: 46.7048 - kl_loss: 13.5952\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2262\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.9415 - reconstruction_loss: 63.4291 - kl_loss: 18.5124\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 946us/step - loss: 100.3160 - reconstruction_loss: 81.8164 - kl_loss: 18.4996\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.0040\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5625\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 104.2616 - reconstruction_loss: 93.6826 - kl_loss: 10.5791\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.7637 - reconstruction_loss: 60.1540 - kl_loss: 10.6097\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7767\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 150.5659 - reconstruction_loss: 140.6381 - kl_loss: 9.9278\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 956us/step - loss: 122.2772 - reconstruction_loss: 112.3258 - kl_loss: 9.9513\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0823\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0622\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.2284 - reconstruction_loss: 59.0936 - kl_loss: 16.1348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62.5636 - reconstruction_loss: 46.4199 - kl_loss: 16.1436\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7884\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6874\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.9778 - reconstruction_loss: 97.8258 - kl_loss: 19.1520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.0399 - reconstruction_loss: 87.8910 - kl_loss: 19.1489\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.2706\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.9273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62.7432 - reconstruction_loss: 50.0330 - kl_loss: 12.7102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.0609 - reconstruction_loss: 67.3254 - kl_loss: 12.7355\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6980\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.5709 - reconstruction_loss: 53.8339 - kl_loss: 16.7370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.8045 - reconstruction_loss: 29.0635 - kl_loss: 16.7410\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9848\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8941\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 71.6069 - reconstruction_loss: 57.3959 - kl_loss: 14.2110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.9017 - reconstruction_loss: 61.7139 - kl_loss: 14.1878\n",
      "Success in episode 28 at time step 200 with reward -174.17042053654788\n",
      "Episode 29\n",
      "[-0.44826522  0.8939006   0.8914521 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1290\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0082\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.6132 - reconstruction_loss: 55.8843 - kl_loss: 10.7289\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.8434 - reconstruction_loss: 47.1027 - kl_loss: 10.7407\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6634\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4674\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.1476 - reconstruction_loss: 3.5297 - kl_loss: 14.6179\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.9290 - reconstruction_loss: 15.3113 - kl_loss: 14.6177\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0872\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7756\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 32.0337 - reconstruction_loss: 21.2645 - kl_loss: 10.7692\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.6126 - reconstruction_loss: 23.8325 - kl_loss: 10.7802\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.3544 - reconstruction_loss: 17.2662 - kl_loss: 14.0883\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.8837 - reconstruction_loss: 17.7838 - kl_loss: 14.0998\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6447\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.6685 - reconstruction_loss: 8.8058 - kl_loss: 12.8627\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.9782 - reconstruction_loss: 5.1087 - kl_loss: 12.8695\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1312\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9289\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.1963 - reconstruction_loss: 55.7681 - kl_loss: 10.4282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.3739 - reconstruction_loss: 54.9396 - kl_loss: 10.4343\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4018\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4509\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.4818 - reconstruction_loss: 8.4081 - kl_loss: 15.0737\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.8170 - reconstruction_loss: 7.7412 - kl_loss: 15.0758\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3893\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1305\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.9221 - reconstruction_loss: 28.4543 - kl_loss: 14.4679\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.3822 - reconstruction_loss: 17.9239 - kl_loss: 14.4583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9699\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 55.1966 - reconstruction_loss: 41.6143 - kl_loss: 13.5823\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 948us/step - loss: 58.4800 - reconstruction_loss: 44.9052 - kl_loss: 13.5748\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0712\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.3100 - reconstruction_loss: 27.3565 - kl_loss: 14.9535\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 983us/step - loss: 36.7961 - reconstruction_loss: 21.8367 - kl_loss: 14.9594\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7606\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6444\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.4777 - reconstruction_loss: 45.6078 - kl_loss: 10.8699\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.6456 - reconstruction_loss: 46.7534 - kl_loss: 10.8922\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3245\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.1440 - reconstruction_loss: 26.3217 - kl_loss: 12.8223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.6662 - reconstruction_loss: 30.8587 - kl_loss: 12.8076\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0523\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.8858 - reconstruction_loss: 43.9317 - kl_loss: 15.9541\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.4103 - reconstruction_loss: 42.4484 - kl_loss: 15.9619\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5944\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3909\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.4455 - reconstruction_loss: 39.1893 - kl_loss: 12.2562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.2131 - reconstruction_loss: 43.9573 - kl_loss: 12.2558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1310\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1289\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 78.3369 - reconstruction_loss: 65.2768 - kl_loss: 13.0600\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.5678 - reconstruction_loss: 71.5125 - kl_loss: 13.0553\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1437\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 101.8524 - reconstruction_loss: 83.2693 - kl_loss: 18.5831\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 76.9049 - reconstruction_loss: 58.3342 - kl_loss: 18.5708\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 46.0203 - reconstruction_loss: 32.6509 - kl_loss: 13.3693\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.3113 - reconstruction_loss: 32.9634 - kl_loss: 13.3479\n",
      "Success in episode 29 at time step 200 with reward -235.65545947462695\n",
      "Episode 30\n",
      "[-0.6776738   0.73536265  0.65418327]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0303\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9733\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 43.9551 - reconstruction_loss: 34.1421 - kl_loss: 9.8130\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.3591 - reconstruction_loss: 32.5640 - kl_loss: 9.7951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2807\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2930\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.1296 - reconstruction_loss: 13.6743 - kl_loss: 15.4553\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: 20.0752 - reconstruction_loss: 4.6226 - kl_loss: 15.4527\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6799\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6412\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.5644 - reconstruction_loss: 21.9526 - kl_loss: 12.6118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.6531 - reconstruction_loss: 14.0783 - kl_loss: 12.5747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1734\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.7484 - reconstruction_loss: 46.9473 - kl_loss: 12.8011\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.7557 - reconstruction_loss: 43.9531 - kl_loss: 12.8025\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7357\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.9812 - reconstruction_loss: 27.3018 - kl_loss: 18.6794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.9641 - reconstruction_loss: 27.3140 - kl_loss: 18.6500\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4856\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.9562 - reconstruction_loss: 37.4103 - kl_loss: 11.5459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 891us/step - loss: 64.0042 - reconstruction_loss: 52.4838 - kl_loss: 11.5204\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4123\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3841\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 71.2307 - reconstruction_loss: 60.1175 - kl_loss: 11.1132\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.1450 - reconstruction_loss: 41.0161 - kl_loss: 11.1290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0823\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.6148 - reconstruction_loss: 100.9907 - kl_loss: 16.6241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 818us/step - loss: 129.0452 - reconstruction_loss: 112.4257 - kl_loss: 16.6195\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3456\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.4853 - reconstruction_loss: 36.0580 - kl_loss: 12.4273\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 61.4114 - reconstruction_loss: 48.9746 - kl_loss: 12.4368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0448\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0081\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.3929 - reconstruction_loss: 108.8265 - kl_loss: 12.5664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 885us/step - loss: 120.2254 - reconstruction_loss: 107.6352 - kl_loss: 12.5902\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1901\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1157\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 67.9835 - reconstruction_loss: 50.6542 - kl_loss: 17.3293\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.8516 - reconstruction_loss: 35.5372 - kl_loss: 17.3144\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0731\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9505\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.0025 - reconstruction_loss: 118.1019 - kl_loss: 13.9006\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 972us/step - loss: 151.4826 - reconstruction_loss: 137.5626 - kl_loss: 13.9200\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8571\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.4581 - reconstruction_loss: 62.9151 - kl_loss: 16.5430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.6118 - reconstruction_loss: 57.0602 - kl_loss: 16.5515\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.1575\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.2840\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.7307 - reconstruction_loss: 103.6562 - kl_loss: 18.0745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.9515 - reconstruction_loss: 110.8889 - kl_loss: 18.0626\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8136\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 113.1970 - reconstruction_loss: 100.1938 - kl_loss: 13.0031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.7452 - reconstruction_loss: 65.7202 - kl_loss: 13.0250\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1737\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.2909 - reconstruction_loss: 51.5207 - kl_loss: 13.7702\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 63.7992 - reconstruction_loss: 50.0068 - kl_loss: 13.7924\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4713\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2952\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73.9444 - reconstruction_loss: 59.8625 - kl_loss: 14.0819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.4871 - reconstruction_loss: 57.4174 - kl_loss: 14.0698\n",
      "Success in episode 30 at time step 200 with reward -208.3953439510243\n",
      "Episode 31\n",
      "[-0.21608812 -0.97637385  0.61105627]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8947\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.6293 - reconstruction_loss: 48.9997 - kl_loss: 16.6297\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.9986 - reconstruction_loss: 58.4179 - kl_loss: 16.5806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0089\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 60.0921 - reconstruction_loss: 48.4181 - kl_loss: 11.6739\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.8968 - reconstruction_loss: 43.1753 - kl_loss: 11.7215\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8516\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 114.9089 - reconstruction_loss: 103.8554 - kl_loss: 11.0535\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 137.5676 - reconstruction_loss: 126.5191 - kl_loss: 11.0485\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.0740\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7993\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75.5346 - reconstruction_loss: 59.0728 - kl_loss: 16.4618\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.9203 - reconstruction_loss: 51.4581 - kl_loss: 16.4622\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7427\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 136.8493 - reconstruction_loss: 120.2683 - kl_loss: 16.5810\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 132.9929 - reconstruction_loss: 116.4225 - kl_loss: 16.5704\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5280\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.7161 - reconstruction_loss: 42.3481 - kl_loss: 14.3680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.4894 - reconstruction_loss: 45.0823 - kl_loss: 14.4072\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8854\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.7908 - reconstruction_loss: 33.5238 - kl_loss: 17.2670\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.5294 - reconstruction_loss: 35.2678 - kl_loss: 17.2617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.5791\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.4606\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.1146 - reconstruction_loss: 62.2286 - kl_loss: 18.8860\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.4546 - reconstruction_loss: 48.5714 - kl_loss: 18.8832\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6355\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 98.8173 - reconstruction_loss: 83.7104 - kl_loss: 15.1069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.5061 - reconstruction_loss: 78.3938 - kl_loss: 15.1124\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1947\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 78.3683 - reconstruction_loss: 64.0596 - kl_loss: 14.3086\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.9988 - reconstruction_loss: 76.6729 - kl_loss: 14.3259\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0541\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1096\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 135.1703 - reconstruction_loss: 124.3753 - kl_loss: 10.7949\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.2655 - reconstruction_loss: 123.4676 - kl_loss: 10.7980\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5695\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4720\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.8296 - reconstruction_loss: 23.3650 - kl_loss: 17.4645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.8999 - reconstruction_loss: 32.4400 - kl_loss: 17.4598\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.8656 - reconstruction_loss: 61.5394 - kl_loss: 19.3262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.0937 - reconstruction_loss: 68.7686 - kl_loss: 19.3251\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8835\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.7682 - reconstruction_loss: 35.8104 - kl_loss: 12.9578\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.3097 - reconstruction_loss: 14.3526 - kl_loss: 12.9571\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0740\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1001\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.9284 - reconstruction_loss: 93.6903 - kl_loss: 11.2381\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.9223 - reconstruction_loss: 90.6913 - kl_loss: 11.2310\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9333\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 47.1388 - reconstruction_loss: 32.2824 - kl_loss: 14.8564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.3723 - reconstruction_loss: 30.5134 - kl_loss: 14.8589\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1438\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 10.0466\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 74.1621 - reconstruction_loss: 59.1436 - kl_loss: 15.0185\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.7561 - reconstruction_loss: 60.7463 - kl_loss: 15.0098\n",
      "Success in episode 31 at time step 200 with reward -168.84937667118487\n",
      "Episode 32\n",
      "[ 0.97509533  0.2217862  -0.5011937 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3337\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.2957 - reconstruction_loss: 17.2346 - kl_loss: 18.0611\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.6281 - reconstruction_loss: 21.6286 - kl_loss: 17.9995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3343\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0175\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.8994 - reconstruction_loss: 26.3275 - kl_loss: 18.5718\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.9229 - reconstruction_loss: 18.3763 - kl_loss: 18.5466\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.2088\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.9180\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 88.2585 - reconstruction_loss: 69.2939 - kl_loss: 18.9646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.0544 - reconstruction_loss: 64.0926 - kl_loss: 18.9618\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8802\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6532\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.0347 - reconstruction_loss: 75.5633 - kl_loss: 14.4715\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 996us/step - loss: 66.3293 - reconstruction_loss: 51.8535 - kl_loss: 14.4757\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4394\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.3110 - reconstruction_loss: 80.0129 - kl_loss: 12.2982\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.0916 - reconstruction_loss: 79.8020 - kl_loss: 12.2896\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6871\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.7178 - reconstruction_loss: 51.0364 - kl_loss: 14.6814\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68.5864 - reconstruction_loss: 53.8837 - kl_loss: 14.7027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6799\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5918\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.3005 - reconstruction_loss: 50.2778 - kl_loss: 19.0227\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.5300 - reconstruction_loss: 46.4960 - kl_loss: 19.0340\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4518\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4134\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.5903 - reconstruction_loss: 33.7091 - kl_loss: 11.8813\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.3617 - reconstruction_loss: 42.4824 - kl_loss: 11.8792\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.9784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.7800\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102.8094 - reconstruction_loss: 93.5373 - kl_loss: 9.2721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.4089 - reconstruction_loss: 89.1372 - kl_loss: 9.2716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0974\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.6462 - reconstruction_loss: 28.3029 - kl_loss: 16.3433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 956us/step - loss: 47.8009 - reconstruction_loss: 31.4448 - kl_loss: 16.3561\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0500\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.2561 - reconstruction_loss: 65.7506 - kl_loss: 18.5055\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 947us/step - loss: 77.8974 - reconstruction_loss: 59.3924 - kl_loss: 18.5050\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1156\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.7657 - reconstruction_loss: 30.1746 - kl_loss: 12.5911\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.2896 - reconstruction_loss: 29.7013 - kl_loss: 12.5883\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1955\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 126.7572 - reconstruction_loss: 115.1085 - kl_loss: 11.6487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 142.9266 - reconstruction_loss: 131.2808 - kl_loss: 11.6458\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9906\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9528\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.5113 - reconstruction_loss: 53.5438 - kl_loss: 16.9675\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.0039 - reconstruction_loss: 47.0119 - kl_loss: 16.9920\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6263\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.0885 - reconstruction_loss: 100.3989 - kl_loss: 15.6897\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.9330 - reconstruction_loss: 103.2573 - kl_loss: 15.6757\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.1741\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.3098\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 98.8513 - reconstruction_loss: 82.3933 - kl_loss: 16.4579\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.4149 - reconstruction_loss: 111.9481 - kl_loss: 16.4667\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0554\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8745\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.1871 - reconstruction_loss: 54.6331 - kl_loss: 15.5540\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.1204 - reconstruction_loss: 57.5589 - kl_loss: 15.5615\n",
      "Success in episode 32 at time step 200 with reward -162.97597869045296\n",
      "Episode 33\n",
      "[ 0.23869132 -0.9710955  -0.2587703 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.7578 - reconstruction_loss: 66.1338 - kl_loss: 17.6240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.7793 - reconstruction_loss: 66.1892 - kl_loss: 17.5901\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9267\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.6559 - reconstruction_loss: 34.1591 - kl_loss: 14.4968\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 909us/step - loss: 37.9536 - reconstruction_loss: 23.4011 - kl_loss: 14.5526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.4414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.2128\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 179.8081 - reconstruction_loss: 168.4602 - kl_loss: 11.3479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 189.9612 - reconstruction_loss: 178.6004 - kl_loss: 11.3608\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.9322\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.3580\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 91.8713 - reconstruction_loss: 70.3971 - kl_loss: 21.4742\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.6063 - reconstruction_loss: 95.1446 - kl_loss: 21.4617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.6402\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.6011\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 177.4704 - reconstruction_loss: 164.6129 - kl_loss: 12.8575\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 171.0887 - reconstruction_loss: 158.2246 - kl_loss: 12.8640\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.0937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.7309\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.5152 - reconstruction_loss: 29.8539 - kl_loss: 19.6612\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.8821 - reconstruction_loss: 28.2585 - kl_loss: 19.6235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.3207\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.6420\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 198.5197 - reconstruction_loss: 188.0957 - kl_loss: 10.4240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 194.7225 - reconstruction_loss: 184.2906 - kl_loss: 10.4319\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.4628\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.4238\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 146.5973 - reconstruction_loss: 129.7947 - kl_loss: 16.8026\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 148.4146 - reconstruction_loss: 131.6506 - kl_loss: 16.7640\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.6357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.8726\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 149.1258 - reconstruction_loss: 130.6754 - kl_loss: 18.4504\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 151.0630 - reconstruction_loss: 132.6284 - kl_loss: 18.4346\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.5033\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 193.7530 - reconstruction_loss: 180.3408 - kl_loss: 13.4122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 195.7583 - reconstruction_loss: 182.3652 - kl_loss: 13.3931\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.3145\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.1213\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.8663 - reconstruction_loss: 32.1598 - kl_loss: 19.7064\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.1683 - reconstruction_loss: 28.5076 - kl_loss: 19.6607\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 175.6339 - reconstruction_loss: 162.5426 - kl_loss: 13.0914\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 174.6147 - reconstruction_loss: 161.5417 - kl_loss: 13.0730\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.2072\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0355\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103.8197 - reconstruction_loss: 84.3707 - kl_loss: 19.4490\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.9126 - reconstruction_loss: 35.5355 - kl_loss: 19.3772\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5737\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 179.9440 - reconstruction_loss: 167.0163 - kl_loss: 12.9277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 190.8231 - reconstruction_loss: 177.9218 - kl_loss: 12.9012\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.0807\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 14.0396\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.3763 - reconstruction_loss: 25.0660 - kl_loss: 18.3103\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.0336 - reconstruction_loss: 24.7654 - kl_loss: 18.2682\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5237\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 197.5438 - reconstruction_loss: 185.4815 - kl_loss: 12.0622\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 194.0750 - reconstruction_loss: 182.0426 - kl_loss: 12.0325\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 117.6997 - reconstruction_loss: 102.2521 - kl_loss: 15.4476\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.9402 - reconstruction_loss: 102.5565 - kl_loss: 15.3838\n",
      "Success in episode 33 at time step 200 with reward -202.75851108411106\n",
      "Episode 34\n",
      "[-0.78195864  0.6233303   0.509071  ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4400\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8023\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.0051 - reconstruction_loss: 45.2323 - kl_loss: 10.7728\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.7416 - reconstruction_loss: 40.0029 - kl_loss: 10.7387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3554\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.9721 - reconstruction_loss: 8.3938 - kl_loss: 14.5783\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.9280 - reconstruction_loss: 23.3635 - kl_loss: 14.5645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.1847\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.4432\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 31.4834 - reconstruction_loss: 19.7585 - kl_loss: 11.7249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.7438 - reconstruction_loss: 18.9860 - kl_loss: 11.7578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5291\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.2144 - reconstruction_loss: 59.6615 - kl_loss: 12.5529\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54.0373 - reconstruction_loss: 41.5092 - kl_loss: 12.5281\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0526\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.6379\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.5001 - reconstruction_loss: 42.8994 - kl_loss: 18.6007\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.2417 - reconstruction_loss: 41.6639 - kl_loss: 18.5778\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9991\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 47.3258 - reconstruction_loss: 33.9503 - kl_loss: 13.3755\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 964us/step - loss: 64.0877 - reconstruction_loss: 50.7229 - kl_loss: 13.3648\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3869\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9843\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.2021 - reconstruction_loss: 98.1253 - kl_loss: 10.0768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.6671 - reconstruction_loss: 101.5910 - kl_loss: 10.0760\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.3071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.9502\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.6827 - reconstruction_loss: 50.7084 - kl_loss: 14.9744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 982us/step - loss: 42.7615 - reconstruction_loss: 27.7930 - kl_loss: 14.9685\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4281\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4837\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.8786 - reconstruction_loss: 79.2175 - kl_loss: 15.6612\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.1140 - reconstruction_loss: 66.4608 - kl_loss: 15.6532\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1731\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9157\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.1037 - reconstruction_loss: 38.4400 - kl_loss: 12.6638\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.4045 - reconstruction_loss: 34.7403 - kl_loss: 12.6643\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8701\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 97.6649 - reconstruction_loss: 87.3507 - kl_loss: 10.3141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.3966 - reconstruction_loss: 89.0915 - kl_loss: 10.3051\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.3908\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9934\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.5300 - reconstruction_loss: 31.3469 - kl_loss: 15.1831\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.8210 - reconstruction_loss: 31.6554 - kl_loss: 15.1656\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0332\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1326\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 136.3970 - reconstruction_loss: 122.5173 - kl_loss: 13.8798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 127.4741 - reconstruction_loss: 113.6008 - kl_loss: 13.8733\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 62.3773\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 61.2150\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 159.3243 - reconstruction_loss: 143.9700 - kl_loss: 15.3544\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.6786 - reconstruction_loss: 144.3399 - kl_loss: 15.3386\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.4303\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.2870\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 173.4555 - reconstruction_loss: 159.8477 - kl_loss: 13.6078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.5578 - reconstruction_loss: 159.9427 - kl_loss: 13.6152\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 54.9731\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.1900\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 168.7579 - reconstruction_loss: 151.6054 - kl_loss: 17.1526\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 178.8404 - reconstruction_loss: 161.6864 - kl_loss: 17.1540\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 13.1056\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6934\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.1570 - reconstruction_loss: 69.6945 - kl_loss: 13.4625\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.4440 - reconstruction_loss: 69.9811 - kl_loss: 13.4629\n",
      "Success in episode 34 at time step 200 with reward -205.86947274339664\n",
      "Episode 35\n",
      "[-0.6612538   0.75016224 -0.19869274]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7010\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6594\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.5833 - reconstruction_loss: 29.4637 - kl_loss: 12.1195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.2341 - reconstruction_loss: 30.1261 - kl_loss: 12.1080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0179\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8600\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.8085 - reconstruction_loss: 3.6218 - kl_loss: 14.1867\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.3434 - reconstruction_loss: 13.1633 - kl_loss: 14.1801\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.6639\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 32.2864 - reconstruction_loss: 18.4373 - kl_loss: 13.8491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.8371 - reconstruction_loss: 21.0190 - kl_loss: 13.8181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9087\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1582\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.7938 - reconstruction_loss: 75.3782 - kl_loss: 10.4156\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.7085 - reconstruction_loss: 77.3141 - kl_loss: 10.3944\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.6519\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.4069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.6870 - reconstruction_loss: 24.9921 - kl_loss: 14.6949\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.1256 - reconstruction_loss: 34.4560 - kl_loss: 14.6697\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4714\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 95.1196 - reconstruction_loss: 81.4334 - kl_loss: 13.6861\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.2968 - reconstruction_loss: 76.6030 - kl_loss: 13.6938\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.1619\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.0501\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.1844 - reconstruction_loss: 27.5246 - kl_loss: 13.6598\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 46.7918 - reconstruction_loss: 33.1368 - kl_loss: 13.6550\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0314\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 130.9048 - reconstruction_loss: 119.8183 - kl_loss: 11.0866\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 126.4524 - reconstruction_loss: 115.3718 - kl_loss: 11.0806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.5076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.2499\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.9660 - reconstruction_loss: 24.0876 - kl_loss: 14.8785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.3176 - reconstruction_loss: 19.4542 - kl_loss: 14.8634\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.0530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 18.6614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 94.0609 - reconstruction_loss: 78.1196 - kl_loss: 15.9413\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.2519 - reconstruction_loss: 72.3269 - kl_loss: 15.9250\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.1587\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.9168\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 161.2439 - reconstruction_loss: 149.3117 - kl_loss: 11.9322\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 147.4408 - reconstruction_loss: 135.5134 - kl_loss: 11.9275\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.5015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.9295\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 134.0526 - reconstruction_loss: 116.0923 - kl_loss: 17.9603\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 135.7853 - reconstruction_loss: 117.8305 - kl_loss: 17.9548\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.4787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.2179\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.9232 - reconstruction_loss: 111.6896 - kl_loss: 11.2336\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 103.4802 - reconstruction_loss: 92.2295 - kl_loss: 11.2507\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 54.9787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.5531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 96.2746 - reconstruction_loss: 81.3092 - kl_loss: 14.9653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.4185 - reconstruction_loss: 74.4794 - kl_loss: 14.9391\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.9425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.6719\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.3016 - reconstruction_loss: 108.5212 - kl_loss: 13.7804\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 125.7583 - reconstruction_loss: 111.9644 - kl_loss: 13.7939\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.7711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.7191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.8206 - reconstruction_loss: 106.7689 - kl_loss: 15.0518\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: 122.7183 - reconstruction_loss: 107.6854 - kl_loss: 15.0328\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 17.5567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1233\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.8487 - reconstruction_loss: 70.0435 - kl_loss: 13.8052\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.4593 - reconstruction_loss: 66.6570 - kl_loss: 13.8023\n",
      "Success in episode 35 at time step 200 with reward -196.36466651114893\n",
      "Episode 36\n",
      "[-0.2190349   0.975717   -0.84238774]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.5478 - reconstruction_loss: 61.2119 - kl_loss: 8.3358\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 866us/step - loss: 72.6671 - reconstruction_loss: 64.3350 - kl_loss: 8.3321\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8918\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.0389 - reconstruction_loss: 4.4258 - kl_loss: 14.6131\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.7487 - reconstruction_loss: 8.1542 - kl_loss: 14.5944\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.9373 - reconstruction_loss: 15.9707 - kl_loss: 11.9666\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.7741 - reconstruction_loss: 15.8162 - kl_loss: 11.9578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.2998 - reconstruction_loss: 80.7154 - kl_loss: 11.5844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 85.9022 - reconstruction_loss: 74.3228 - kl_loss: 11.5795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.2670\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.1848\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.4530 - reconstruction_loss: 17.9978 - kl_loss: 12.4553\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.1698 - reconstruction_loss: 19.7477 - kl_loss: 12.4221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0028\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.9759\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 99.6737 - reconstruction_loss: 82.8181 - kl_loss: 16.8556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 96.7395 - reconstruction_loss: 79.8903 - kl_loss: 16.8492\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.1682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.9742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.9374 - reconstruction_loss: 16.1811 - kl_loss: 14.7562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.1757 - reconstruction_loss: 17.4019 - kl_loss: 14.7738\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.1130\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.9582\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75.7221 - reconstruction_loss: 59.8285 - kl_loss: 15.8935\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: 70.9290 - reconstruction_loss: 55.0537 - kl_loss: 15.8753\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.4303\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.9906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 174.8325 - reconstruction_loss: 159.9435 - kl_loss: 14.8890\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.6991 - reconstruction_loss: 150.8203 - kl_loss: 14.8788\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5678\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4530\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 194.9018 - reconstruction_loss: 181.9901 - kl_loss: 12.9117\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 198.9760 - reconstruction_loss: 186.0927 - kl_loss: 12.8833\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.1600\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.2585\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 79.7142 - reconstruction_loss: 63.8761 - kl_loss: 15.8381\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.9135 - reconstruction_loss: 69.1172 - kl_loss: 15.7962\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8608\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 199.3619 - reconstruction_loss: 187.5072 - kl_loss: 11.8547\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 199.6128 - reconstruction_loss: 187.7837 - kl_loss: 11.8291\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.4112\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.4038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.6843 - reconstruction_loss: 50.1118 - kl_loss: 16.5725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.8497 - reconstruction_loss: 40.3265 - kl_loss: 16.5231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5448\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 170.2501 - reconstruction_loss: 158.4564 - kl_loss: 11.7937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 170.8567 - reconstruction_loss: 159.0787 - kl_loss: 11.7780\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.3084\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.8964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.7060 - reconstruction_loss: 30.3094 - kl_loss: 19.3966\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.7614 - reconstruction_loss: 33.3962 - kl_loss: 19.3652\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8700\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5606\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.5944 - reconstruction_loss: 118.4963 - kl_loss: 13.0981\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 967us/step - loss: 138.7451 - reconstruction_loss: 125.6850 - kl_loss: 13.0601\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 10.3482\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9218\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.4901 - reconstruction_loss: 76.8409 - kl_loss: 13.6492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.8800 - reconstruction_loss: 78.2895 - kl_loss: 13.5905\n",
      "Success in episode 36 at time step 200 with reward -198.63204936905086\n",
      "Episode 37\n",
      "[-0.3335876  -0.9427191   0.01817325]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1765\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.2157 - reconstruction_loss: 12.5839 - kl_loss: 17.6318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 846us/step - loss: 24.8225 - reconstruction_loss: 7.2446 - kl_loss: 17.5779\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.4178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.8147 - reconstruction_loss: 25.6722 - kl_loss: 11.1426\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.9455 - reconstruction_loss: 11.7936 - kl_loss: 11.1519\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0523\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8336\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 85.1586 - reconstruction_loss: 73.2912 - kl_loss: 11.8674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.3183 - reconstruction_loss: 71.4539 - kl_loss: 11.8644\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3403\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.9584 - reconstruction_loss: 39.1465 - kl_loss: 14.8119\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.3006 - reconstruction_loss: 42.5042 - kl_loss: 14.7964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6704\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3529\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.9349 - reconstruction_loss: 31.1928 - kl_loss: 11.7421\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.9844 - reconstruction_loss: 31.2789 - kl_loss: 11.7055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 119.4591 - reconstruction_loss: 109.4859 - kl_loss: 9.9732\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.3444 - reconstruction_loss: 106.3902 - kl_loss: 9.9541\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8362\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7320\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38.0117 - reconstruction_loss: 23.2009 - kl_loss: 14.8107\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.4365 - reconstruction_loss: 38.6244 - kl_loss: 14.8121\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6876\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.3534 - reconstruction_loss: 63.0429 - kl_loss: 16.3105\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 952us/step - loss: 74.8553 - reconstruction_loss: 58.5464 - kl_loss: 16.3089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3639\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1192\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.8156 - reconstruction_loss: 38.5495 - kl_loss: 10.2661\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.0253 - reconstruction_loss: 54.7878 - kl_loss: 10.2375\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8550\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.8788\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.8425 - reconstruction_loss: 95.7822 - kl_loss: 11.0602\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 110.5751 - reconstruction_loss: 99.5314 - kl_loss: 11.0436\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8707\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.2388 - reconstruction_loss: 37.6030 - kl_loss: 14.6357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 900us/step - loss: 46.6234 - reconstruction_loss: 31.9860 - kl_loss: 14.6374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7295\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7751\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63.9904 - reconstruction_loss: 49.4550 - kl_loss: 14.5354\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 980us/step - loss: 73.1553 - reconstruction_loss: 58.6321 - kl_loss: 14.5232\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7089\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4643\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.4636 - reconstruction_loss: 24.3801 - kl_loss: 12.0835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 888us/step - loss: 25.4653 - reconstruction_loss: 13.3827 - kl_loss: 12.0826\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8380\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8116\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 129.7338 - reconstruction_loss: 118.4393 - kl_loss: 11.2945\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.8436 - reconstruction_loss: 113.5499 - kl_loss: 11.2936\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0292\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.8185 - reconstruction_loss: 94.1160 - kl_loss: 14.7025\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.0533 - reconstruction_loss: 87.3487 - kl_loss: 14.7045\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5738\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 146.5110 - reconstruction_loss: 134.0242 - kl_loss: 12.4867\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 138.1324 - reconstruction_loss: 125.6460 - kl_loss: 12.4865\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9820\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8590\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 71.1745 - reconstruction_loss: 58.1379 - kl_loss: 13.0366\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.0484 - reconstruction_loss: 57.0339 - kl_loss: 13.0145\n",
      "Success in episode 37 at time step 200 with reward -191.14170080603358\n",
      "Episode 38\n",
      "[ 0.2455276  -0.9693896  -0.34025776]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7134\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.5338 - reconstruction_loss: 46.6115 - kl_loss: 13.9223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 61.3704 - reconstruction_loss: 47.4800 - kl_loss: 13.8904\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1705\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9254\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.6824 - reconstruction_loss: 24.6238 - kl_loss: 11.0587\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.0588 - reconstruction_loss: 22.0281 - kl_loss: 11.0307\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2517\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 130.6852 - reconstruction_loss: 119.6861 - kl_loss: 10.9990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.1365 - reconstruction_loss: 117.1574 - kl_loss: 10.9791\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.3184\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.9510\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.8625 - reconstruction_loss: 4.0396 - kl_loss: 15.8228\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.3028 - reconstruction_loss: 6.4431 - kl_loss: 15.8596\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.1151\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.1786\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.3726 - reconstruction_loss: 7.3002 - kl_loss: 17.0724\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.8975 - reconstruction_loss: 8.7877 - kl_loss: 17.1099\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7887\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9877\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 150.5193 - reconstruction_loss: 141.8623 - kl_loss: 8.6570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 138.6065 - reconstruction_loss: 129.9482 - kl_loss: 8.6583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8642\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.2398 - reconstruction_loss: 26.4391 - kl_loss: 15.8008\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.0455 - reconstruction_loss: 74.2513 - kl_loss: 15.7942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7521\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 101.3355 - reconstruction_loss: 86.0354 - kl_loss: 15.3001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.1052 - reconstruction_loss: 77.8012 - kl_loss: 15.3040\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1630\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0325\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.1223 - reconstruction_loss: 84.7305 - kl_loss: 11.3918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.1436 - reconstruction_loss: 65.7423 - kl_loss: 11.4012\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6958\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7016\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 163.5047 - reconstruction_loss: 154.4525 - kl_loss: 9.0522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 164.1477 - reconstruction_loss: 155.1013 - kl_loss: 9.0464\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7493\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6335\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.3738 - reconstruction_loss: 21.4516 - kl_loss: 14.9222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.8708 - reconstruction_loss: 17.9442 - kl_loss: 14.9266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8883\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.7504 - reconstruction_loss: 52.3760 - kl_loss: 13.3744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.7897 - reconstruction_loss: 44.4181 - kl_loss: 13.3716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4344\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4188\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.2319 - reconstruction_loss: 34.3313 - kl_loss: 9.9005\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.9285 - reconstruction_loss: 46.0173 - kl_loss: 9.9112\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4172\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.9580 - reconstruction_loss: 103.4108 - kl_loss: 9.5473\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.8367 - reconstruction_loss: 103.2808 - kl_loss: 9.5559\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7935\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.3354 - reconstruction_loss: 25.7746 - kl_loss: 14.5608\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.7249 - reconstruction_loss: 13.1657 - kl_loss: 14.5592\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0242\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0040\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75.3124 - reconstruction_loss: 61.6053 - kl_loss: 13.7071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.0129 - reconstruction_loss: 52.2956 - kl_loss: 13.7173\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5644\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4777\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75.1105 - reconstruction_loss: 62.3149 - kl_loss: 12.7956\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.5201 - reconstruction_loss: 61.7149 - kl_loss: 12.8053\n",
      "Success in episode 38 at time step 200 with reward -165.9854313924796\n",
      "Episode 39\n",
      "[ 0.9293027   0.369319   -0.17839521]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.1209\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.2324 - reconstruction_loss: 26.0038 - kl_loss: 14.2286\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.6229 - reconstruction_loss: 29.3672 - kl_loss: 14.2557\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8970\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9075\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 143.5869 - reconstruction_loss: 133.4629 - kl_loss: 10.1240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 152.5269 - reconstruction_loss: 142.4055 - kl_loss: 10.1214\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6454\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38.7935 - reconstruction_loss: 21.7358 - kl_loss: 17.0577\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 958us/step - loss: 23.0012 - reconstruction_loss: 5.9410 - kl_loss: 17.0602\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3488\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 132.6155 - reconstruction_loss: 121.2489 - kl_loss: 11.3666\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 131.5824 - reconstruction_loss: 120.2097 - kl_loss: 11.3727\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.9520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.9073\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.1597 - reconstruction_loss: 25.8191 - kl_loss: 14.3406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.2043 - reconstruction_loss: 20.8522 - kl_loss: 14.3521\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5816\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5623\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 156.8750 - reconstruction_loss: 142.5984 - kl_loss: 14.2765\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 154.7252 - reconstruction_loss: 140.4427 - kl_loss: 14.2825\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.9054\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.1817\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 97.9066 - reconstruction_loss: 84.2718 - kl_loss: 13.6348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.7204 - reconstruction_loss: 91.0848 - kl_loss: 13.6356\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.1724\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.6145\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.5668 - reconstruction_loss: 151.1101 - kl_loss: 14.4567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 928us/step - loss: 158.3884 - reconstruction_loss: 143.9279 - kl_loss: 14.4605\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1808\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0350\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.3271 - reconstruction_loss: 54.4530 - kl_loss: 12.8741\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 949us/step - loss: 74.5284 - reconstruction_loss: 61.6510 - kl_loss: 12.8774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.0973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 44.6670\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 118.6612 - reconstruction_loss: 103.7918 - kl_loss: 14.8693\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.2418 - reconstruction_loss: 95.3778 - kl_loss: 14.8640\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.9652\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 113.2100 - reconstruction_loss: 98.0748 - kl_loss: 15.1352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.7986 - reconstruction_loss: 99.6632 - kl_loss: 15.1354\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 72.0736\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 69.3338\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.9915 - reconstruction_loss: 77.3866 - kl_loss: 13.6049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.5349 - reconstruction_loss: 76.9379 - kl_loss: 13.5970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9029\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8105\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.6355 - reconstruction_loss: 110.7161 - kl_loss: 11.9194\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.2643 - reconstruction_loss: 116.3341 - kl_loss: 11.9302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.6205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.5349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.0089 - reconstruction_loss: 68.1664 - kl_loss: 12.8425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 801us/step - loss: 93.9961 - reconstruction_loss: 81.1729 - kl_loss: 12.8232\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8164\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.4075 - reconstruction_loss: 106.0647 - kl_loss: 15.3428\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 130.2710 - reconstruction_loss: 114.9184 - kl_loss: 15.3526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5501\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6815\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.4226 - reconstruction_loss: 35.2547 - kl_loss: 13.1679\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 969us/step - loss: 54.7360 - reconstruction_loss: 41.5530 - kl_loss: 13.1830\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 17.7093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1771\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.4368 - reconstruction_loss: 76.6297 - kl_loss: 13.8071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.3134 - reconstruction_loss: 80.5133 - kl_loss: 13.8001\n",
      "Success in episode 39 at time step 200 with reward -145.05140309167325\n",
      "Episode 40\n",
      "[-0.9168867  -0.39914754  0.5586455 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0094\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3720\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20.3188 - reconstruction_loss: 8.0464 - kl_loss: 12.2724\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.6782 - reconstruction_loss: 5.3983 - kl_loss: 12.2799\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7834\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.8741 - reconstruction_loss: 6.9198 - kl_loss: 10.9543\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.2427 - reconstruction_loss: 4.2892 - kl_loss: 10.9536\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4551\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.7371 - reconstruction_loss: 65.8876 - kl_loss: 9.8495\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.4509 - reconstruction_loss: 58.6189 - kl_loss: 9.8320\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0163\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8474\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.8004 - reconstruction_loss: 5.8438 - kl_loss: 15.9566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23.3397 - reconstruction_loss: 7.4075 - kl_loss: 15.9322\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9900\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6888\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.4828 - reconstruction_loss: 24.3581 - kl_loss: 13.1247\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.9910 - reconstruction_loss: 21.8562 - kl_loss: 13.1348\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4467\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 130.6419 - reconstruction_loss: 121.1645 - kl_loss: 9.4774\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 961us/step - loss: 129.5268 - reconstruction_loss: 120.0617 - kl_loss: 9.4650\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.2068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.3073 - reconstruction_loss: 26.7604 - kl_loss: 13.5469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.9541 - reconstruction_loss: 12.4228 - kl_loss: 13.5313\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5976\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.3708 - reconstruction_loss: 116.7336 - kl_loss: 14.6372\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 139.6128 - reconstruction_loss: 124.9848 - kl_loss: 14.6280\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.6843\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.1666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 91.2352 - reconstruction_loss: 79.6214 - kl_loss: 11.6138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.6013 - reconstruction_loss: 80.9987 - kl_loss: 11.6026\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3174\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2323\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 118.4773 - reconstruction_loss: 103.5161 - kl_loss: 14.9612\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 120.1116 - reconstruction_loss: 105.1441 - kl_loss: 14.9675\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4191\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4621\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.6058 - reconstruction_loss: 28.4923 - kl_loss: 13.1135\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.0571 - reconstruction_loss: 34.9494 - kl_loss: 13.1077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.8027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.1546\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.2691 - reconstruction_loss: 75.3490 - kl_loss: 14.9201\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 94.8606 - reconstruction_loss: 79.9384 - kl_loss: 14.9222\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0620\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.5338 - reconstruction_loss: 77.9103 - kl_loss: 11.6234\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 79.5738 - reconstruction_loss: 67.9349 - kl_loss: 11.6389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.2249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.9485\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.5849 - reconstruction_loss: 22.8585 - kl_loss: 13.7264\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.9855 - reconstruction_loss: 19.2574 - kl_loss: 13.7281\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.5996\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.2346\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 95.1540 - reconstruction_loss: 81.5386 - kl_loss: 13.6153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.2986 - reconstruction_loss: 79.6772 - kl_loss: 13.6214\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.6518\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.3479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.5103 - reconstruction_loss: 22.4072 - kl_loss: 12.1031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.2252 - reconstruction_loss: 26.1079 - kl_loss: 12.1173\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7932\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4023\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.8459 - reconstruction_loss: 53.0501 - kl_loss: 12.7958\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.3640 - reconstruction_loss: 53.5615 - kl_loss: 12.8025\n",
      "Success in episode 40 at time step 200 with reward -185.0984499061107\n",
      "Episode 41\n",
      "[ 0.96832895 -0.24967788 -0.33166498]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8035\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 82.0036 - reconstruction_loss: 68.0710 - kl_loss: 13.9327\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.2232 - reconstruction_loss: 69.2409 - kl_loss: 13.9823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3790\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.4620 - reconstruction_loss: 61.8997 - kl_loss: 12.5623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.7014 - reconstruction_loss: 45.1540 - kl_loss: 12.5475\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.6398\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.6117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.4375 - reconstruction_loss: 23.6384 - kl_loss: 14.7990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.2242 - reconstruction_loss: 26.4441 - kl_loss: 14.7801\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.0899\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.4601\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 205.3246 - reconstruction_loss: 194.6428 - kl_loss: 10.6819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 210.3461 - reconstruction_loss: 199.6546 - kl_loss: 10.6915\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.8114\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.6222\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 167.9297 - reconstruction_loss: 151.7764 - kl_loss: 16.1533\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 181.8684 - reconstruction_loss: 165.7520 - kl_loss: 16.1164\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.8173\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.3360 - reconstruction_loss: 146.1497 - kl_loss: 13.1863\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 169.6173 - reconstruction_loss: 156.4274 - kl_loss: 13.1899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.4313\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.6725\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 192.2256 - reconstruction_loss: 180.8665 - kl_loss: 11.3591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 191.1739 - reconstruction_loss: 179.8653 - kl_loss: 11.3085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.9217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 26.5895\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.3113 - reconstruction_loss: 117.3121 - kl_loss: 16.9992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 824us/step - loss: 132.0328 - reconstruction_loss: 115.0958 - kl_loss: 16.9370\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.7684\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.1013\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 213.0730 - reconstruction_loss: 199.9840 - kl_loss: 13.0891\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 213.0865 - reconstruction_loss: 199.9958 - kl_loss: 13.0906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.0227\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.0426\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.3896 - reconstruction_loss: 60.0316 - kl_loss: 15.3580\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.7192 - reconstruction_loss: 57.4633 - kl_loss: 15.2559\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.5645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.5386\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 171.9727 - reconstruction_loss: 161.0643 - kl_loss: 10.9085\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 960us/step - loss: 174.1388 - reconstruction_loss: 163.2182 - kl_loss: 10.9207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.1238\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.8522\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 136.7156 - reconstruction_loss: 122.1798 - kl_loss: 14.5358\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 135.2725 - reconstruction_loss: 120.8187 - kl_loss: 14.4538\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1334\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 148.4335 - reconstruction_loss: 134.5399 - kl_loss: 13.8936\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 935us/step - loss: 142.8353 - reconstruction_loss: 128.9545 - kl_loss: 13.8807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7426\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 177.5031 - reconstruction_loss: 166.1767 - kl_loss: 11.3264\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 170.2842 - reconstruction_loss: 159.0095 - kl_loss: 11.2747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4194\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.7636 - reconstruction_loss: 84.7762 - kl_loss: 15.9874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.3687 - reconstruction_loss: 84.4105 - kl_loss: 15.9582\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.7609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.2222\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 199.1268 - reconstruction_loss: 186.9684 - kl_loss: 12.1584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 197.8280 - reconstruction_loss: 185.6857 - kl_loss: 12.1423\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 14.6772\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 13.7400\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 125.3938 - reconstruction_loss: 112.3258 - kl_loss: 13.0680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 123.3342 - reconstruction_loss: 110.3159 - kl_loss: 13.0183\n",
      "Success in episode 41 at time step 200 with reward -194.93295574279006\n",
      "Episode 42\n",
      "[0.5573196  0.83029807 0.68913054]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0800\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 142.4665 - reconstruction_loss: 132.9964 - kl_loss: 9.4701\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.4219 - reconstruction_loss: 124.9509 - kl_loss: 9.4709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2216\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4608\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 31.3094 - reconstruction_loss: 18.2664 - kl_loss: 13.0431\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.3556 - reconstruction_loss: 16.3390 - kl_loss: 13.0167\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6269\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6200\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.2065 - reconstruction_loss: 54.6714 - kl_loss: 13.5351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.2411 - reconstruction_loss: 53.7263 - kl_loss: 13.5148\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0379\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38.8792 - reconstruction_loss: 28.7344 - kl_loss: 10.1448\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.4896 - reconstruction_loss: 46.3649 - kl_loss: 10.1247\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9827\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1555\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 129.3428 - reconstruction_loss: 118.5421 - kl_loss: 10.8007\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.8651 - reconstruction_loss: 111.0596 - kl_loss: 10.8055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0650\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9839\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38.3695 - reconstruction_loss: 24.6637 - kl_loss: 13.7058\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.0290 - reconstruction_loss: 21.3347 - kl_loss: 13.6943\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.6908 - reconstruction_loss: 65.8686 - kl_loss: 14.8222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.3133 - reconstruction_loss: 73.4937 - kl_loss: 14.8196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7635\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2388\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.6971 - reconstruction_loss: 23.7792 - kl_loss: 10.9179\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.2385 - reconstruction_loss: 35.3327 - kl_loss: 10.9058\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9497\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0737\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 131.4076 - reconstruction_loss: 120.9637 - kl_loss: 10.4439\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 135.2722 - reconstruction_loss: 124.8268 - kl_loss: 10.4454\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1166\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.6480 - reconstruction_loss: 23.3949 - kl_loss: 12.2530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 975us/step - loss: 32.0390 - reconstruction_loss: 19.7927 - kl_loss: 12.2463\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8935\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.3655 - reconstruction_loss: 48.9514 - kl_loss: 15.4141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.4521 - reconstruction_loss: 64.0420 - kl_loss: 15.4101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 55.6888 - reconstruction_loss: 44.5710 - kl_loss: 11.1178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 935us/step - loss: 64.2803 - reconstruction_loss: 53.1499 - kl_loss: 11.1304\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4464\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4905\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 141.2489 - reconstruction_loss: 129.2056 - kl_loss: 12.0433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 131.7227 - reconstruction_loss: 119.6795 - kl_loss: 12.0431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.5877\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.8228\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38.2085 - reconstruction_loss: 24.9834 - kl_loss: 13.2251\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 868us/step - loss: 32.9143 - reconstruction_loss: 19.6661 - kl_loss: 13.2482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 143.6611 - reconstruction_loss: 130.5925 - kl_loss: 13.0685\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 147.8218 - reconstruction_loss: 134.7507 - kl_loss: 13.0711\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.5205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.2798\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.8624 - reconstruction_loss: 22.6370 - kl_loss: 13.2255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.0900 - reconstruction_loss: 16.8342 - kl_loss: 13.2557\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2778\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.8358 - reconstruction_loss: 57.4809 - kl_loss: 12.3549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.6563 - reconstruction_loss: 59.3044 - kl_loss: 12.3519\n",
      "Success in episode 42 at time step 200 with reward -169.68004209951354\n",
      "Episode 43\n",
      "[0.42581937 0.90480816 0.23849344]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8265\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7407\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.3727 - reconstruction_loss: 91.8752 - kl_loss: 9.4975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 993us/step - loss: 103.6952 - reconstruction_loss: 94.2281 - kl_loss: 9.4670\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1573\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 31.9491 - reconstruction_loss: 19.1937 - kl_loss: 12.7554\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.8060 - reconstruction_loss: 18.0818 - kl_loss: 12.7242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1858\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2765\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.3373 - reconstruction_loss: 52.1909 - kl_loss: 14.1463\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.7191 - reconstruction_loss: 51.5809 - kl_loss: 14.1382\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8786\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0348\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.2546 - reconstruction_loss: 26.1193 - kl_loss: 11.1353\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.7872 - reconstruction_loss: 22.6374 - kl_loss: 11.1498\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.9518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.5369 - reconstruction_loss: 92.3635 - kl_loss: 10.1734\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102.7704 - reconstruction_loss: 92.6171 - kl_loss: 10.1533\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0183\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.8671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.6297 - reconstruction_loss: 9.8232 - kl_loss: 12.8065\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.9895 - reconstruction_loss: 10.2023 - kl_loss: 12.7872\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3998\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.7986 - reconstruction_loss: 42.0604 - kl_loss: 13.7382\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 887us/step - loss: 57.0331 - reconstruction_loss: 43.3091 - kl_loss: 13.7240\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6906\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5597\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.2079 - reconstruction_loss: 56.1029 - kl_loss: 9.1050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.8474 - reconstruction_loss: 60.7654 - kl_loss: 9.0820\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8051\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6802\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 46.8258 - reconstruction_loss: 34.4384 - kl_loss: 12.3874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.1144 - reconstruction_loss: 32.7455 - kl_loss: 12.3689\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0268\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 63.2790 - reconstruction_loss: 49.2237 - kl_loss: 14.0552\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 908us/step - loss: 71.5948 - reconstruction_loss: 57.5558 - kl_loss: 14.0390\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5483\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.3165 - reconstruction_loss: 17.7911 - kl_loss: 10.5254\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.1373 - reconstruction_loss: 19.6340 - kl_loss: 10.5034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1371\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 99.6609 - reconstruction_loss: 90.6325 - kl_loss: 9.0284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 961us/step - loss: 100.6385 - reconstruction_loss: 91.6284 - kl_loss: 9.0100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1736\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.4084 - reconstruction_loss: 15.1481 - kl_loss: 12.2603\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.3156 - reconstruction_loss: 12.0808 - kl_loss: 12.2347\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1326\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.5690 - reconstruction_loss: 52.6845 - kl_loss: 12.8844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.4627 - reconstruction_loss: 40.5724 - kl_loss: 12.8903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6256\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5222\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.2845 - reconstruction_loss: 30.1268 - kl_loss: 10.1576\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.6104 - reconstruction_loss: 32.4534 - kl_loss: 10.1570\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4063\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3458\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.8810 - reconstruction_loss: 44.8149 - kl_loss: 12.0661\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 901us/step - loss: 60.0918 - reconstruction_loss: 48.0424 - kl_loss: 12.0494\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4465\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3309\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 55.9664 - reconstruction_loss: 44.5796 - kl_loss: 11.3867\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54.5525 - reconstruction_loss: 43.1857 - kl_loss: 11.3668\n",
      "Success in episode 43 at time step 200 with reward -188.38330930541542\n",
      "Episode 44\n",
      "[0.7691024  0.6391256  0.61758256]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6686\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.9845 - reconstruction_loss: 83.2783 - kl_loss: 9.7062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 94.4439 - reconstruction_loss: 84.7183 - kl_loss: 9.7256\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7375\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.9567 - reconstruction_loss: 28.5105 - kl_loss: 13.4462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 46.6142 - reconstruction_loss: 33.2132 - kl_loss: 13.4010\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7133\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.3108 - reconstruction_loss: 48.5513 - kl_loss: 13.7595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.5098 - reconstruction_loss: 40.7738 - kl_loss: 13.7360\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8230\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.0396\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.3955 - reconstruction_loss: 43.3491 - kl_loss: 11.0465\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.0822 - reconstruction_loss: 34.0186 - kl_loss: 11.0636\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3190\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 106.4018 - reconstruction_loss: 97.3045 - kl_loss: 9.0973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.8663 - reconstruction_loss: 98.7920 - kl_loss: 9.0742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4326\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.4207 - reconstruction_loss: 38.5789 - kl_loss: 14.8418\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.8302 - reconstruction_loss: 41.0006 - kl_loss: 14.8296\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2021\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 67.8587 - reconstruction_loss: 56.7342 - kl_loss: 11.1244\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.5290 - reconstruction_loss: 66.4003 - kl_loss: 11.1287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0439\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 130.8943 - reconstruction_loss: 121.9403 - kl_loss: 8.9540\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 128.3139 - reconstruction_loss: 119.3529 - kl_loss: 8.9610\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.0748\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.8821\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.1406 - reconstruction_loss: 20.6495 - kl_loss: 13.4912\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.8314 - reconstruction_loss: 22.3451 - kl_loss: 13.4864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6726\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.2553\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.4689 - reconstruction_loss: 68.3030 - kl_loss: 12.1659\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.3555 - reconstruction_loss: 65.1940 - kl_loss: 12.1615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.5578\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.1915\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.4473 - reconstruction_loss: 73.8824 - kl_loss: 13.5649\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.4585 - reconstruction_loss: 70.8991 - kl_loss: 13.5594\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.0576\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.2006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 148.2370 - reconstruction_loss: 137.5673 - kl_loss: 10.6697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 154.5875 - reconstruction_loss: 143.9331 - kl_loss: 10.6544\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6698\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.6943 - reconstruction_loss: 21.2094 - kl_loss: 12.4850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.3124 - reconstruction_loss: 23.8277 - kl_loss: 12.4847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 96.3481 - reconstruction_loss: 82.1156 - kl_loss: 14.2325\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.1519 - reconstruction_loss: 77.9241 - kl_loss: 14.2278\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 44.1328 - reconstruction_loss: 32.8819 - kl_loss: 11.2509\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.5940 - reconstruction_loss: 31.3614 - kl_loss: 11.2326\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.2727\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.1688\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.7896 - reconstruction_loss: 121.6959 - kl_loss: 10.0937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.0669 - reconstruction_loss: 117.9767 - kl_loss: 10.0902\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 9.7007\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 74.9739 - reconstruction_loss: 63.0483 - kl_loss: 11.9255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.0568 - reconstruction_loss: 63.1446 - kl_loss: 11.9122\n",
      "Success in episode 44 at time step 200 with reward -184.40074432495427\n",
      "Episode 45\n",
      "[-0.9233098  -0.38405594 -0.26733246]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3476\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1481\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.7424 - reconstruction_loss: 7.5798 - kl_loss: 11.1626\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.5813 - reconstruction_loss: 8.4006 - kl_loss: 11.1807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.2653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.2787 - reconstruction_loss: 5.3152 - kl_loss: 9.9635\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.3076 - reconstruction_loss: 13.3499 - kl_loss: 9.9577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0372\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.2748\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.9929 - reconstruction_loss: 18.7476 - kl_loss: 10.2453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.1410 - reconstruction_loss: 20.9003 - kl_loss: 10.2407\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4242\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2618\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.8278 - reconstruction_loss: 9.0380 - kl_loss: 10.7898\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 991us/step - loss: 18.5752 - reconstruction_loss: 7.7864 - kl_loss: 10.7887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23.2673 - reconstruction_loss: 11.1142 - kl_loss: 12.1531\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.2753 - reconstruction_loss: 8.1385 - kl_loss: 12.1368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5368\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3595\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 71.6887 - reconstruction_loss: 63.7165 - kl_loss: 7.9721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.0083 - reconstruction_loss: 64.0413 - kl_loss: 7.9670\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0253\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.4857 - reconstruction_loss: 16.4819 - kl_loss: 12.0037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.3270 - reconstruction_loss: 18.3317 - kl_loss: 11.9953\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2856\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.0635\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 84.7812 - reconstruction_loss: 71.8706 - kl_loss: 12.9107\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 95.6210 - reconstruction_loss: 82.7221 - kl_loss: 12.8989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.7338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.4032\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.6566 - reconstruction_loss: 22.3868 - kl_loss: 13.2698\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.3763 - reconstruction_loss: 17.0950 - kl_loss: 13.2814\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7113\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8698\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.7249 - reconstruction_loss: 105.4607 - kl_loss: 10.2642\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.5165 - reconstruction_loss: 105.2451 - kl_loss: 10.2714\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9941\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0570\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.2033 - reconstruction_loss: 21.4268 - kl_loss: 11.7765\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.1348 - reconstruction_loss: 22.3623 - kl_loss: 11.7725\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.9806\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.7644\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.6237 - reconstruction_loss: 111.5961 - kl_loss: 13.0277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 123.0462 - reconstruction_loss: 110.0150 - kl_loss: 13.0312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 92.4920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 90.7629\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 147.9340 - reconstruction_loss: 136.9775 - kl_loss: 10.9565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 129.0521 - reconstruction_loss: 118.1004 - kl_loss: 10.9517\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.3245\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.9843\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 142.9105 - reconstruction_loss: 127.9958 - kl_loss: 14.9147\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 139.2747 - reconstruction_loss: 124.3489 - kl_loss: 14.9258\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 85.9673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 83.7495\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 167.4583 - reconstruction_loss: 155.0199 - kl_loss: 12.4384\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 956us/step - loss: 186.7321 - reconstruction_loss: 174.3078 - kl_loss: 12.4243\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.4349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.2402\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 133.8554 - reconstruction_loss: 123.2313 - kl_loss: 10.6241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 129.1790 - reconstruction_loss: 118.5291 - kl_loss: 10.6499\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.0295\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.4834\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 76.2858 - reconstruction_loss: 64.7908 - kl_loss: 11.4950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.2511 - reconstruction_loss: 61.7644 - kl_loss: 11.4867\n",
      "Success in episode 45 at time step 200 with reward -212.09233314600306\n",
      "Episode 46\n",
      "[ 0.7419203  0.6704881 -0.5084424]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.1155\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.7402\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.6897 - reconstruction_loss: 33.5221 - kl_loss: 12.1676\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.6280 - reconstruction_loss: 30.4203 - kl_loss: 12.2077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3173\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86.4927 - reconstruction_loss: 73.9218 - kl_loss: 12.5709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 833us/step - loss: 96.2545 - reconstruction_loss: 83.7002 - kl_loss: 12.5543\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.9184\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.5044\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.8023 - reconstruction_loss: 4.6571 - kl_loss: 14.1452\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.0900 - reconstruction_loss: 6.9003 - kl_loss: 14.1897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0789\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 156.1178 - reconstruction_loss: 146.0527 - kl_loss: 10.0651\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 986us/step - loss: 160.1461 - reconstruction_loss: 150.0808 - kl_loss: 10.0653\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6663\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5517\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.4894 - reconstruction_loss: 21.4798 - kl_loss: 13.0097\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.6790 - reconstruction_loss: 21.6879 - kl_loss: 12.9911\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.9262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8032\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.9082 - reconstruction_loss: 99.0640 - kl_loss: 12.8442\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.7130 - reconstruction_loss: 97.8819 - kl_loss: 12.8311\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.9720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.3816\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 146.8282 - reconstruction_loss: 135.5663 - kl_loss: 11.2619\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 139.3746 - reconstruction_loss: 128.0918 - kl_loss: 11.2828\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3125\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 145.1972 - reconstruction_loss: 131.4169 - kl_loss: 13.7803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 148.3499 - reconstruction_loss: 134.5754 - kl_loss: 13.7745\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7577\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5619\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.5023 - reconstruction_loss: 67.8283 - kl_loss: 10.6740\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.5809 - reconstruction_loss: 62.8820 - kl_loss: 10.6990\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.1738 - reconstruction_loss: 118.3103 - kl_loss: 12.8636\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 142.0784 - reconstruction_loss: 129.2302 - kl_loss: 12.8482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1379\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1939\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73.3463 - reconstruction_loss: 62.1072 - kl_loss: 11.2391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.7489 - reconstruction_loss: 60.4736 - kl_loss: 11.2753\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.9617\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.9765\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 87.5078 - reconstruction_loss: 75.2026 - kl_loss: 12.3052\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.8771 - reconstruction_loss: 76.5649 - kl_loss: 12.3122\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.2356\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.0159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.9341 - reconstruction_loss: 60.9029 - kl_loss: 12.0312\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.0638 - reconstruction_loss: 62.0351 - kl_loss: 12.0287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1096\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 129.2407 - reconstruction_loss: 120.2513 - kl_loss: 8.9894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 120.6139 - reconstruction_loss: 111.6305 - kl_loss: 8.9833\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.5960\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.5549\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.9891 - reconstruction_loss: 15.1941 - kl_loss: 12.7950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.4485 - reconstruction_loss: 22.6457 - kl_loss: 12.8028\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 40.9502\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.8107\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.4914 - reconstruction_loss: 81.6706 - kl_loss: 10.8208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.3754 - reconstruction_loss: 82.5386 - kl_loss: 10.8368\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.6164\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.1684\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.7900 - reconstruction_loss: 75.6164 - kl_loss: 12.1736\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.7786 - reconstruction_loss: 73.5928 - kl_loss: 12.1859\n",
      "Success in episode 46 at time step 200 with reward -178.77093749815188\n",
      "Episode 47\n",
      "[0.63481116 0.7726673  0.6376549 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9436\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.1445\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 128.3087 - reconstruction_loss: 119.3925 - kl_loss: 8.9162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 125.3845 - reconstruction_loss: 116.4635 - kl_loss: 8.9210\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.9933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.6958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.4800 - reconstruction_loss: 38.8850 - kl_loss: 14.5950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.7549 - reconstruction_loss: 26.1778 - kl_loss: 14.5771\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.5818\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.1245\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.1925 - reconstruction_loss: 42.8672 - kl_loss: 14.3253\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.6472 - reconstruction_loss: 44.3857 - kl_loss: 14.2615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4285\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 124.1361 - reconstruction_loss: 114.5148 - kl_loss: 9.6213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 119.1933 - reconstruction_loss: 109.5482 - kl_loss: 9.6451\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.5579\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.9496\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.2662 - reconstruction_loss: 3.6168 - kl_loss: 13.6494\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 895us/step - loss: 16.6475 - reconstruction_loss: 3.0372 - kl_loss: 13.6103\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.7091\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.8923\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.3113 - reconstruction_loss: 28.1199 - kl_loss: 12.1914\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.9935 - reconstruction_loss: 22.8237 - kl_loss: 12.1698\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.7247\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.5953\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.2076 - reconstruction_loss: 58.5081 - kl_loss: 12.6994\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73.9104 - reconstruction_loss: 61.2122 - kl_loss: 12.6983\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5547\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58.2928 - reconstruction_loss: 47.9810 - kl_loss: 10.3118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.6526 - reconstruction_loss: 48.3485 - kl_loss: 10.3041\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.5401\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 26.5308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.6430 - reconstruction_loss: 82.6173 - kl_loss: 10.0257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.4721 - reconstruction_loss: 83.4690 - kl_loss: 10.0031\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.8858\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.6000\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 109.2275 - reconstruction_loss: 95.9628 - kl_loss: 13.2647\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 109.3733 - reconstruction_loss: 96.1278 - kl_loss: 13.2455\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.1096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.1192\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 181.9844 - reconstruction_loss: 169.9912 - kl_loss: 11.9932\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 184.6622 - reconstruction_loss: 172.6842 - kl_loss: 11.9779\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9530\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.6880 - reconstruction_loss: 22.3125 - kl_loss: 13.3755\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.3828 - reconstruction_loss: 18.0164 - kl_loss: 13.3664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.8997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 79.0057 - reconstruction_loss: 65.8141 - kl_loss: 13.1916\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.9754 - reconstruction_loss: 59.7826 - kl_loss: 13.1928\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 55.6829 - reconstruction_loss: 43.8521 - kl_loss: 11.8308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 61.0536 - reconstruction_loss: 49.2252 - kl_loss: 11.8284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1504\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 127.6424 - reconstruction_loss: 119.1055 - kl_loss: 8.5369\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 129.1971 - reconstruction_loss: 120.6598 - kl_loss: 8.5373\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.4672\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.9046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.5756 - reconstruction_loss: 47.2799 - kl_loss: 14.2957\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.9454 - reconstruction_loss: 43.6577 - kl_loss: 14.2877\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.7509\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.3601\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 78.6140 - reconstruction_loss: 66.8122 - kl_loss: 11.8018\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 81.8279 - reconstruction_loss: 70.0460 - kl_loss: 11.7819\n",
      "Success in episode 47 at time step 200 with reward -164.43200251570156\n",
      "Episode 48\n",
      "[ 0.81510097 -0.57931894 -0.5408507 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0010\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.8673\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.3138 - reconstruction_loss: 108.1264 - kl_loss: 14.1873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.8411 - reconstruction_loss: 108.6560 - kl_loss: 14.1850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.0992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.5174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.2254 - reconstruction_loss: 69.7673 - kl_loss: 10.4581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.5574 - reconstruction_loss: 67.1053 - kl_loss: 10.4521\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.4078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.6845\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 111.4009 - reconstruction_loss: 99.2903 - kl_loss: 12.1106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.5907 - reconstruction_loss: 105.4814 - kl_loss: 12.1093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2266\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2818\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 116.2630 - reconstruction_loss: 104.8212 - kl_loss: 11.4418\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.2066 - reconstruction_loss: 108.7602 - kl_loss: 11.4464\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 55.2067\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 54.5199\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.2375 - reconstruction_loss: 9.6171 - kl_loss: 12.6204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.4067 - reconstruction_loss: 24.7863 - kl_loss: 12.6204\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.2022\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.3102\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 52.1486 - reconstruction_loss: 40.6758 - kl_loss: 11.4728\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.2607 - reconstruction_loss: 47.7764 - kl_loss: 11.4842\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3448\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.5214 - reconstruction_loss: 107.6344 - kl_loss: 13.8870\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.7341 - reconstruction_loss: 104.8504 - kl_loss: 13.8837\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.8193\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1630\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.8126 - reconstruction_loss: 60.0929 - kl_loss: 10.7198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 955us/step - loss: 72.0177 - reconstruction_loss: 61.2684 - kl_loss: 10.7493\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.9396\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.2279\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.5181 - reconstruction_loss: 71.7383 - kl_loss: 10.7798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 81.3582 - reconstruction_loss: 70.5752 - kl_loss: 10.7831\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9759\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7753\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 107.4125 - reconstruction_loss: 95.2057 - kl_loss: 12.2068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101.6598 - reconstruction_loss: 89.4478 - kl_loss: 12.2120\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.1530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.1417\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.5352 - reconstruction_loss: 21.4707 - kl_loss: 12.0645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.9106 - reconstruction_loss: 9.7919 - kl_loss: 12.1187\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2930\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2698\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 145.8397 - reconstruction_loss: 135.1713 - kl_loss: 10.6683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 147.2357 - reconstruction_loss: 136.5682 - kl_loss: 10.6675\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.4782\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.1155\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.5630 - reconstruction_loss: 7.1678 - kl_loss: 11.3952\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.8017 - reconstruction_loss: 10.4017 - kl_loss: 11.3999\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.2995\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.9520\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.6757 - reconstruction_loss: 10.6622 - kl_loss: 14.0135\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.4928 - reconstruction_loss: 8.4547 - kl_loss: 14.0382\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.2340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.6908\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 151.3265 - reconstruction_loss: 143.0197 - kl_loss: 8.3068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 900us/step - loss: 152.0310 - reconstruction_loss: 143.7087 - kl_loss: 8.3223\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8126\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.3906 - reconstruction_loss: 27.3917 - kl_loss: 11.9989\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.6278 - reconstruction_loss: 18.6347 - kl_loss: 11.9931\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.5103\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.0907\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 74.4741 - reconstruction_loss: 62.5894 - kl_loss: 11.8847\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.1228 - reconstruction_loss: 65.2530 - kl_loss: 11.8699\n",
      "Success in episode 48 at time step 200 with reward -129.59656075226823\n",
      "Episode 49\n",
      "[ 0.11362666 -0.99352354 -0.909075  ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4432\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 15.3736\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.8000 - reconstruction_loss: 53.6979 - kl_loss: 13.1021\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 72.3082 - reconstruction_loss: 59.2385 - kl_loss: 13.0697\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1547\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3211\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.9728 - reconstruction_loss: 33.3951 - kl_loss: 10.5777\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 835us/step - loss: 37.1439 - reconstruction_loss: 26.6205 - kl_loss: 10.5234\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3547\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 90.0657 - reconstruction_loss: 79.1656 - kl_loss: 10.9001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.6920 - reconstruction_loss: 83.7858 - kl_loss: 10.9063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.9746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.3640\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.7405 - reconstruction_loss: 20.8386 - kl_loss: 11.9019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 31.3540 - reconstruction_loss: 19.4537 - kl_loss: 11.9003\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3269\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 143.3512 - reconstruction_loss: 131.6226 - kl_loss: 11.7285\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 137.5900 - reconstruction_loss: 125.8780 - kl_loss: 11.7119\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 109.1720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 107.2073\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 118.8790 - reconstruction_loss: 108.7761 - kl_loss: 10.1029\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 127.2612 - reconstruction_loss: 117.1569 - kl_loss: 10.1043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.7071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.2950\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.1264 - reconstruction_loss: 108.2840 - kl_loss: 13.8424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.3181 - reconstruction_loss: 106.4786 - kl_loss: 13.8395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 84.0426\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 81.0647\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 109.6365 - reconstruction_loss: 97.1157 - kl_loss: 12.5207\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.9986 - reconstruction_loss: 104.4746 - kl_loss: 12.5240\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0413\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.0579 - reconstruction_loss: 77.6430 - kl_loss: 12.4150\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.7796 - reconstruction_loss: 72.3629 - kl_loss: 12.4167\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 75.6584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 72.8352\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 71.3082 - reconstruction_loss: 60.8140 - kl_loss: 10.4942\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.4372 - reconstruction_loss: 63.9437 - kl_loss: 10.4935\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2632\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1429\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 104.5697 - reconstruction_loss: 91.3992 - kl_loss: 13.1705\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.0154 - reconstruction_loss: 86.8309 - kl_loss: 13.1845\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.9322\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 40.7437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.8568 - reconstruction_loss: 17.9713 - kl_loss: 10.8855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.8756 - reconstruction_loss: 17.9683 - kl_loss: 10.9073\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6029\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.7751 - reconstruction_loss: 146.9120 - kl_loss: 8.8631\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 156.4027 - reconstruction_loss: 147.5461 - kl_loss: 8.8566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.6643\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.0019\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 72.6148 - reconstruction_loss: 58.3653 - kl_loss: 14.2495\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 63.8330 - reconstruction_loss: 49.5979 - kl_loss: 14.2351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0643\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2216\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 168.8417 - reconstruction_loss: 159.7007 - kl_loss: 9.1410\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 167.5291 - reconstruction_loss: 158.3835 - kl_loss: 9.1456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.6088\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.0379\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.5726 - reconstruction_loss: 9.7385 - kl_loss: 12.8341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.0658 - reconstruction_loss: 8.2614 - kl_loss: 12.8044\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.4087\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.0527\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.3587 - reconstruction_loss: 69.7407 - kl_loss: 11.6180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.5221 - reconstruction_loss: 70.9115 - kl_loss: 11.6106\n",
      "Success in episode 49 at time step 200 with reward -166.41202516492248\n",
      "Episode 50\n",
      "[-0.96281755 -0.27015257  0.6750068 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.7430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 15.9442\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.8443 - reconstruction_loss: 15.9926 - kl_loss: 11.8517\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.9673 - reconstruction_loss: 11.1082 - kl_loss: 11.8591\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.2117\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8332\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.2527 - reconstruction_loss: 7.7331 - kl_loss: 10.5196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.5889 - reconstruction_loss: 12.0520 - kl_loss: 10.5369\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.1039\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.8605\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.6544 - reconstruction_loss: 55.9119 - kl_loss: 8.7425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.5880 - reconstruction_loss: 58.8465 - kl_loss: 8.7415\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7497\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.0500 - reconstruction_loss: 13.6628 - kl_loss: 13.3873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.4414 - reconstruction_loss: 13.0673 - kl_loss: 13.3741\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5007\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1796\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23.8673 - reconstruction_loss: 13.9894 - kl_loss: 9.8779\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.2077 - reconstruction_loss: 9.3301 - kl_loss: 9.8777\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.0892\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.3501\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.6152 - reconstruction_loss: 103.1428 - kl_loss: 9.4724\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 104.1764 - reconstruction_loss: 94.7130 - kl_loss: 9.4634\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.4121\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.1621\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.6860 - reconstruction_loss: 13.4637 - kl_loss: 11.2223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.1013 - reconstruction_loss: 27.8885 - kl_loss: 11.2128\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8719\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6352\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.5911 - reconstruction_loss: 51.9799 - kl_loss: 12.6112\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 72.5583 - reconstruction_loss: 59.9609 - kl_loss: 12.5973\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8807\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.1025\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.3334 - reconstruction_loss: 11.3509 - kl_loss: 10.9825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.8518 - reconstruction_loss: 12.8769 - kl_loss: 10.9749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1251\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 132.3913 - reconstruction_loss: 124.1724 - kl_loss: 8.2189\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.5556 - reconstruction_loss: 126.3438 - kl_loss: 8.2117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.1793\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.7518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.0487 - reconstruction_loss: 4.2792 - kl_loss: 11.7696\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.4772 - reconstruction_loss: 10.7069 - kl_loss: 11.7702\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7436\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6982\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 60.5331 - reconstruction_loss: 49.9634 - kl_loss: 10.5697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 962us/step - loss: 61.1640 - reconstruction_loss: 50.5942 - kl_loss: 10.5697\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4275\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2616\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.1796 - reconstruction_loss: 80.1889 - kl_loss: 11.9907\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 94.4976 - reconstruction_loss: 82.5027 - kl_loss: 11.9949\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 64.7257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 63.4402\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 85.5131 - reconstruction_loss: 74.7423 - kl_loss: 10.7709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.9788 - reconstruction_loss: 77.2024 - kl_loss: 10.7764\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4426\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5004\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.4754 - reconstruction_loss: 62.2569 - kl_loss: 13.2185\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 830us/step - loss: 88.3474 - reconstruction_loss: 75.1196 - kl_loss: 13.2278\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.6983\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.8933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.1858 - reconstruction_loss: 25.1531 - kl_loss: 12.0327\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.3993 - reconstruction_loss: 24.3576 - kl_loss: 12.0418\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.9352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4255\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.3528 - reconstruction_loss: 48.5412 - kl_loss: 10.8116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.6135 - reconstruction_loss: 48.8149 - kl_loss: 10.7986\n",
      "Success in episode 50 at time step 200 with reward -205.01370035444683\n",
      "Episode 51\n",
      "[-0.999995    0.00317331 -0.49946418]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.4857\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.7763\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.4541 - reconstruction_loss: 31.9399 - kl_loss: 10.5142\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.3415 - reconstruction_loss: 23.8445 - kl_loss: 10.4970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1183\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1835\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.3329 - reconstruction_loss: 7.7250 - kl_loss: 10.6079\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.4150 - reconstruction_loss: 5.8302 - kl_loss: 10.5848\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1361\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21.0919 - reconstruction_loss: 10.4554 - kl_loss: 10.6365\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 950us/step - loss: 24.4120 - reconstruction_loss: 13.8034 - kl_loss: 10.6087\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6264\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 45.2879 - reconstruction_loss: 36.0381 - kl_loss: 9.2498\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 43.1468 - reconstruction_loss: 33.9190 - kl_loss: 9.2278\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5888\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3596\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.2632 - reconstruction_loss: 14.9245 - kl_loss: 12.3388\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.5275 - reconstruction_loss: 18.2145 - kl_loss: 12.3130\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1218\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1695\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.6736 - reconstruction_loss: 6.7372 - kl_loss: 10.9364\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.1195 - reconstruction_loss: 5.2046 - kl_loss: 10.9149\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.7668\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.3227\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.7366 - reconstruction_loss: 63.3100 - kl_loss: 9.4266\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.0542 - reconstruction_loss: 63.6374 - kl_loss: 9.4168\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7905\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6731\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.3914 - reconstruction_loss: 15.9930 - kl_loss: 13.3984\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.7054 - reconstruction_loss: 17.3170 - kl_loss: 13.3884\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0630\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.3561\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.7972 - reconstruction_loss: 22.1985 - kl_loss: 11.5987\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.8634 - reconstruction_loss: 25.2768 - kl_loss: 11.5867\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 125.9579 - reconstruction_loss: 117.7473 - kl_loss: 8.2106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.4306 - reconstruction_loss: 111.2325 - kl_loss: 8.1981\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4191\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.9643 - reconstruction_loss: 22.8417 - kl_loss: 11.1226\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.8979 - reconstruction_loss: 20.7695 - kl_loss: 11.1284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1010\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 85.5438 - reconstruction_loss: 72.8606 - kl_loss: 12.6832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.7412 - reconstruction_loss: 70.0514 - kl_loss: 12.6898\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4473\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2033\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.6947 - reconstruction_loss: 16.0127 - kl_loss: 10.6820\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 968us/step - loss: 31.8319 - reconstruction_loss: 21.1624 - kl_loss: 10.6695\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.5382\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 126.2351 - reconstruction_loss: 117.0725 - kl_loss: 9.1626\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.1878 - reconstruction_loss: 107.0168 - kl_loss: 9.1710\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.5979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.2179\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.8496 - reconstruction_loss: 10.7277 - kl_loss: 14.1219\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.0394 - reconstruction_loss: 21.9402 - kl_loss: 14.0992\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.8828\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.3104\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.2973 - reconstruction_loss: 145.1278 - kl_loss: 10.1695\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.6889 - reconstruction_loss: 155.5482 - kl_loss: 10.1408\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8435\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 52.3802 - reconstruction_loss: 41.4671 - kl_loss: 10.9131\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 953us/step - loss: 53.5949 - reconstruction_loss: 42.6900 - kl_loss: 10.9048\n",
      "Success in episode 51 at time step 200 with reward -217.93478359476177\n",
      "Episode 52\n",
      "[0.8494275 0.5277053 0.7711015]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.2578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 130.5063 - reconstruction_loss: 121.6957 - kl_loss: 8.8106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.8248 - reconstruction_loss: 124.0483 - kl_loss: 8.7765\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0087\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.3919 - reconstruction_loss: 27.4509 - kl_loss: 12.9410\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.3718 - reconstruction_loss: 31.4755 - kl_loss: 12.8963\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.3861\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.8909\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.6929 - reconstruction_loss: 22.0950 - kl_loss: 12.5979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.7832 - reconstruction_loss: 18.3065 - kl_loss: 12.4767\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2923\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0642\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 164.3734 - reconstruction_loss: 153.7677 - kl_loss: 10.6057\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.8377 - reconstruction_loss: 155.2359 - kl_loss: 10.6018\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.2466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.6093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.9650 - reconstruction_loss: 9.6108 - kl_loss: 11.3542\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.6393 - reconstruction_loss: 18.3211 - kl_loss: 11.3181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0970\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0309\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 98.9806 - reconstruction_loss: 85.6180 - kl_loss: 13.3626\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.9180 - reconstruction_loss: 89.5589 - kl_loss: 13.3591\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.2875\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.1477 - reconstruction_loss: 20.5417 - kl_loss: 11.6060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.6892 - reconstruction_loss: 29.1102 - kl_loss: 11.5790\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.6429\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.5637 - reconstruction_loss: 155.8536 - kl_loss: 9.7101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 943us/step - loss: 167.5453 - reconstruction_loss: 157.8340 - kl_loss: 9.7113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1354\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0304\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.8516 - reconstruction_loss: 25.7622 - kl_loss: 11.0893\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 899us/step - loss: 23.1988 - reconstruction_loss: 12.1214 - kl_loss: 11.0774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.9962\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.7761\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.1789 - reconstruction_loss: 58.5281 - kl_loss: 11.6508\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.0371 - reconstruction_loss: 62.3949 - kl_loss: 11.6422\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1536\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.8019 - reconstruction_loss: 94.8942 - kl_loss: 9.9077\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.7950 - reconstruction_loss: 94.8787 - kl_loss: 9.9163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 58.6711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 57.8324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.9474 - reconstruction_loss: 58.4501 - kl_loss: 9.4973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.4199 - reconstruction_loss: 51.9127 - kl_loss: 9.5072\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6296\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6559\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.3565 - reconstruction_loss: 86.3112 - kl_loss: 13.0453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.9565 - reconstruction_loss: 87.9157 - kl_loss: 13.0408\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.6940\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.5239\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.5980 - reconstruction_loss: 26.6595 - kl_loss: 10.9385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.5054 - reconstruction_loss: 22.5346 - kl_loss: 10.9709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.7046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.4302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.4459 - reconstruction_loss: 90.5025 - kl_loss: 11.9433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.3217 - reconstruction_loss: 95.3780 - kl_loss: 11.9437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6738\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6723\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.4915 - reconstruction_loss: 62.6781 - kl_loss: 9.8134\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.6433 - reconstruction_loss: 59.8076 - kl_loss: 9.8357\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.7051\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.3956\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75.1386 - reconstruction_loss: 63.9927 - kl_loss: 11.1459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.8685 - reconstruction_loss: 63.6877 - kl_loss: 11.1808\n",
      "Success in episode 52 at time step 200 with reward -143.59825780484798\n",
      "Episode 53\n",
      "[ 0.9879827  -0.15456465 -0.7200617 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 13.0732\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 105.9737 - reconstruction_loss: 94.0109 - kl_loss: 11.9627\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.4746 - reconstruction_loss: 94.5341 - kl_loss: 11.9404\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.7432\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.3740\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.5129 - reconstruction_loss: 84.9000 - kl_loss: 9.6128\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.4844 - reconstruction_loss: 74.8027 - kl_loss: 9.6817\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.2784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.7174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 115.5653 - reconstruction_loss: 102.5333 - kl_loss: 13.0320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 113.9956 - reconstruction_loss: 100.9697 - kl_loss: 13.0259\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9541\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6439\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.7934 - reconstruction_loss: 54.4342 - kl_loss: 12.3592\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 895us/step - loss: 65.8374 - reconstruction_loss: 53.4104 - kl_loss: 12.4270\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.7275\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.6578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.6835 - reconstruction_loss: 26.8606 - kl_loss: 12.8229\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.4324 - reconstruction_loss: 20.5663 - kl_loss: 12.8661\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9125\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8947\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 118.1509 - reconstruction_loss: 107.6262 - kl_loss: 10.5246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.8664 - reconstruction_loss: 109.3515 - kl_loss: 10.5148\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0898\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.3608 - reconstruction_loss: 12.2743 - kl_loss: 10.0865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 873us/step - loss: 19.4831 - reconstruction_loss: 9.3976 - kl_loss: 10.0855\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6954\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73.4600 - reconstruction_loss: 60.9756 - kl_loss: 12.4844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.3004 - reconstruction_loss: 59.8286 - kl_loss: 12.4717\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.0689\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.7294\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.6145 - reconstruction_loss: 16.5654 - kl_loss: 11.0491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.6340 - reconstruction_loss: 19.5698 - kl_loss: 11.0641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4319\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 148.2219 - reconstruction_loss: 137.7849 - kl_loss: 10.4370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 149.0995 - reconstruction_loss: 138.6714 - kl_loss: 10.4282\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9845\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6812\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 44.1986 - reconstruction_loss: 31.7457 - kl_loss: 12.4530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.7366 - reconstruction_loss: 36.2883 - kl_loss: 12.4483\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.4994\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.2063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 156.3554 - reconstruction_loss: 147.6562 - kl_loss: 8.6991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 152.4669 - reconstruction_loss: 143.7790 - kl_loss: 8.6878\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.2984\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.0132 - reconstruction_loss: 26.7305 - kl_loss: 10.2826\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 803us/step - loss: 31.8558 - reconstruction_loss: 21.5764 - kl_loss: 10.2794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.0573\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.4009\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.5831 - reconstruction_loss: 72.2527 - kl_loss: 11.3304\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.2373 - reconstruction_loss: 70.9058 - kl_loss: 11.3315\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.5630\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.3517\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.6609 - reconstruction_loss: 80.0382 - kl_loss: 10.6227\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 82.7058 - reconstruction_loss: 72.0878 - kl_loss: 10.6180\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.0842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.0988\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.7559 - reconstruction_loss: 54.3619 - kl_loss: 10.3939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.9243 - reconstruction_loss: 58.5410 - kl_loss: 10.3833\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.3549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.0008\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.2259 - reconstruction_loss: 65.9858 - kl_loss: 11.2402\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.7867 - reconstruction_loss: 66.5447 - kl_loss: 11.2420\n",
      "Success in episode 53 at time step 200 with reward -173.1915736205493\n",
      "Episode 54\n",
      "[-0.92938805  0.36910418  0.12131725]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6258\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.9316 - reconstruction_loss: 31.5714 - kl_loss: 10.3601\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.9376 - reconstruction_loss: 32.5646 - kl_loss: 10.3730\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6005\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.2298 - reconstruction_loss: 5.4006 - kl_loss: 11.8292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.8496 - reconstruction_loss: 4.0181 - kl_loss: 11.8316\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 8.8702\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4184\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.1057 - reconstruction_loss: 4.5025 - kl_loss: 10.6032\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.2368 - reconstruction_loss: 3.6598 - kl_loss: 10.5771\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.3035\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.5460\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.2885 - reconstruction_loss: 58.9747 - kl_loss: 10.3138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.7730 - reconstruction_loss: 56.4719 - kl_loss: 10.3012\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5425\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.3651 - reconstruction_loss: 11.2795 - kl_loss: 13.0856\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.6108 - reconstruction_loss: 11.5393 - kl_loss: 13.0715\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.9178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.2390\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.8035 - reconstruction_loss: 16.1288 - kl_loss: 10.6747\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.4260 - reconstruction_loss: 13.7830 - kl_loss: 10.6430\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.6205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.1606\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 139.8955 - reconstruction_loss: 129.3040 - kl_loss: 10.5915\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 139.6108 - reconstruction_loss: 129.0347 - kl_loss: 10.5760\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.7655\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.5472\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.2824 - reconstruction_loss: 13.9412 - kl_loss: 11.3412\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.3392 - reconstruction_loss: 19.9950 - kl_loss: 11.3441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1296\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5228\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 109.1816 - reconstruction_loss: 97.0293 - kl_loss: 12.1523\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.6282 - reconstruction_loss: 94.4847 - kl_loss: 12.1434\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 68.0922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 67.1813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.3027 - reconstruction_loss: 16.2388 - kl_loss: 12.0639\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.6885 - reconstruction_loss: 21.6290 - kl_loss: 12.0595\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 60.6300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 58.7258\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.3153 - reconstruction_loss: 49.1616 - kl_loss: 12.1537\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.9119 - reconstruction_loss: 48.7738 - kl_loss: 12.1381\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.7864\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.5664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 166.9433 - reconstruction_loss: 157.9986 - kl_loss: 8.9446\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 851us/step - loss: 174.1323 - reconstruction_loss: 165.2013 - kl_loss: 8.9310\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 78.8716\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 78.6060\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 150.5132 - reconstruction_loss: 140.4393 - kl_loss: 10.0739\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 147.2506 - reconstruction_loss: 137.1970 - kl_loss: 10.0536\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.4429\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.3413\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 106.5267 - reconstruction_loss: 95.3129 - kl_loss: 11.2138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.2855 - reconstruction_loss: 89.1032 - kl_loss: 11.1822\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 78.0583\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 75.6120\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 170.8557 - reconstruction_loss: 161.0412 - kl_loss: 9.8145\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 171.7610 - reconstruction_loss: 161.9657 - kl_loss: 9.7953\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.5515\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.5864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68.8995 - reconstruction_loss: 56.2123 - kl_loss: 12.6872\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.0617 - reconstruction_loss: 54.4140 - kl_loss: 12.6477\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.7022\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.6299\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75.5250 - reconstruction_loss: 64.6464 - kl_loss: 10.8786\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.2736 - reconstruction_loss: 64.4258 - kl_loss: 10.8478\n",
      "Success in episode 54 at time step 200 with reward -192.85744094059632\n",
      "Episode 55\n",
      "[ 0.4364673  -0.89972013  0.6448227 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2261\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.1514\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.4423 - reconstruction_loss: 67.1948 - kl_loss: 12.2475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 87.4398 - reconstruction_loss: 75.2125 - kl_loss: 12.2273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.6104\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.5306\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.8753 - reconstruction_loss: 17.7397 - kl_loss: 10.1356\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.1650 - reconstruction_loss: 18.1018 - kl_loss: 10.0633\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.7156\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 147.4552 - reconstruction_loss: 138.4455 - kl_loss: 9.0098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 146.9632 - reconstruction_loss: 137.9761 - kl_loss: 8.9871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.3746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9051\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.3203 - reconstruction_loss: 18.4187 - kl_loss: 11.9016\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.5035 - reconstruction_loss: 23.6632 - kl_loss: 11.8403\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.0108\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.8894\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 95.7790 - reconstruction_loss: 86.8703 - kl_loss: 8.9088\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 96.3884 - reconstruction_loss: 87.5070 - kl_loss: 8.8814\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.5852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 29.9741\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.2629 - reconstruction_loss: 98.3886 - kl_loss: 11.8743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 103.6647 - reconstruction_loss: 91.8158 - kl_loss: 11.8490\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 74.6648\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 73.9803\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 175.4926 - reconstruction_loss: 167.4740 - kl_loss: 8.0186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 177.0577 - reconstruction_loss: 169.0566 - kl_loss: 8.0011\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.0459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.6769\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.1626 - reconstruction_loss: 12.8057 - kl_loss: 11.3570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 893us/step - loss: 25.7443 - reconstruction_loss: 14.4078 - kl_loss: 11.3365\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.4403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.1518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.6302 - reconstruction_loss: 50.8066 - kl_loss: 10.8236\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.8074 - reconstruction_loss: 53.9921 - kl_loss: 10.8152\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6442\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 84.0134 - reconstruction_loss: 72.7525 - kl_loss: 11.2609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.3831 - reconstruction_loss: 79.1338 - kl_loss: 11.2493\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.1280\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.6356\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.7822 - reconstruction_loss: 73.0602 - kl_loss: 10.7220\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.3233 - reconstruction_loss: 60.5638 - kl_loss: 10.7596\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.3377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.5668\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.8180 - reconstruction_loss: 111.7702 - kl_loss: 10.0478\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.9549 - reconstruction_loss: 105.9191 - kl_loss: 10.0358\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3968\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.0782\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.5438 - reconstruction_loss: 56.9821 - kl_loss: 9.5617\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.0830 - reconstruction_loss: 47.4829 - kl_loss: 9.6002\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 88.2165\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 85.3128\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.3880 - reconstruction_loss: 6.3201 - kl_loss: 9.0680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.4099 - reconstruction_loss: 14.3154 - kl_loss: 9.0945\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.6323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8110\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 159.8338 - reconstruction_loss: 150.6960 - kl_loss: 9.1379\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 165.1411 - reconstruction_loss: 155.9839 - kl_loss: 9.1573\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.9623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.1956\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.4939 - reconstruction_loss: 24.1070 - kl_loss: 11.3869\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.8948 - reconstruction_loss: 26.5097 - kl_loss: 11.3852\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.1983\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.6538\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.8294 - reconstruction_loss: 67.5443 - kl_loss: 10.2851\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.0887 - reconstruction_loss: 66.7789 - kl_loss: 10.3099\n",
      "Success in episode 55 at time step 200 with reward -153.97665343865273\n",
      "Episode 56\n",
      "[ 0.34350178 -0.939152    0.28776592]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5052\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8838\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 101.2155 - reconstruction_loss: 89.1329 - kl_loss: 12.0826\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.0150 - reconstruction_loss: 85.9202 - kl_loss: 12.0948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.6663\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.9414\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.1800 - reconstruction_loss: 38.9364 - kl_loss: 10.2436\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.0278 - reconstruction_loss: 41.7407 - kl_loss: 10.2871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.9292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.1530\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 96.6367 - reconstruction_loss: 86.8340 - kl_loss: 9.8027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.2406 - reconstruction_loss: 83.4338 - kl_loss: 9.8068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4175\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 84.6811 - reconstruction_loss: 73.3613 - kl_loss: 11.3198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78.4782 - reconstruction_loss: 67.1283 - kl_loss: 11.3499\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 61.0214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 60.8692\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.8710 - reconstruction_loss: 18.8173 - kl_loss: 11.0538\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.4710 - reconstruction_loss: 20.3714 - kl_loss: 11.0996\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 149.7550 - reconstruction_loss: 141.7765 - kl_loss: 7.9785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 147.4324 - reconstruction_loss: 139.4514 - kl_loss: 7.9810\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.4442\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.2265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.1216 - reconstruction_loss: 20.0689 - kl_loss: 10.0527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.9465 - reconstruction_loss: 18.8938 - kl_loss: 10.0527\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 123.3959 - reconstruction_loss: 111.3374 - kl_loss: 12.0585\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.5315 - reconstruction_loss: 113.4636 - kl_loss: 12.0679\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0870\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9534\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.6156 - reconstruction_loss: 54.0453 - kl_loss: 10.5704\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.1645 - reconstruction_loss: 48.5767 - kl_loss: 10.5878\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.6850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.8369\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 113.2393 - reconstruction_loss: 102.7277 - kl_loss: 10.5116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.2767 - reconstruction_loss: 103.7615 - kl_loss: 10.5151\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0006\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7438\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.5696 - reconstruction_loss: 59.5275 - kl_loss: 11.0421\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.2764 - reconstruction_loss: 56.2020 - kl_loss: 11.0743\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 60.3609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 57.1939\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 115.8062 - reconstruction_loss: 104.2959 - kl_loss: 11.5103\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 946us/step - loss: 111.9961 - reconstruction_loss: 100.4848 - kl_loss: 11.5113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 58.3936\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 55.5059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.8731 - reconstruction_loss: 31.3308 - kl_loss: 10.5423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.7683 - reconstruction_loss: 30.2042 - kl_loss: 10.5641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0258\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7819\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 135.0453 - reconstruction_loss: 124.4459 - kl_loss: 10.5994\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 133.1986 - reconstruction_loss: 122.6040 - kl_loss: 10.5946\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4135\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3740\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.2352 - reconstruction_loss: 16.8936 - kl_loss: 10.3416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.9841 - reconstruction_loss: 20.6390 - kl_loss: 10.3451\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8867\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 108.5065 - reconstruction_loss: 96.1652 - kl_loss: 12.3412\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.8147 - reconstruction_loss: 92.4687 - kl_loss: 12.3460\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.4863\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.0758\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.2513 - reconstruction_loss: 66.4409 - kl_loss: 10.8104\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.7795 - reconstruction_loss: 66.9687 - kl_loss: 10.8108\n",
      "Success in episode 56 at time step 200 with reward -169.7514967093595\n",
      "Episode 57\n",
      "[-0.94386625 -0.33032793  0.3198428 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.0121\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.7380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.0619 - reconstruction_loss: 5.9714 - kl_loss: 11.0906\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 975us/step - loss: 17.8400 - reconstruction_loss: 6.7370 - kl_loss: 11.1029\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7833\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.1784\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.3608 - reconstruction_loss: 5.6810 - kl_loss: 10.6798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.2296 - reconstruction_loss: 0.5716 - kl_loss: 10.6580\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6412\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58.6054 - reconstruction_loss: 49.2400 - kl_loss: 9.3654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 841us/step - loss: 65.0631 - reconstruction_loss: 55.7145 - kl_loss: 9.3486\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.9989\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.1788 - reconstruction_loss: 1.3837 - kl_loss: 11.7951\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.7310 - reconstruction_loss: 8.9697 - kl_loss: 11.7613\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4685\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2147\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.2363 - reconstruction_loss: 42.0126 - kl_loss: 9.2237\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.3401 - reconstruction_loss: 44.1559 - kl_loss: 9.1842\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4548\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2831\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.9389 - reconstruction_loss: 25.7153 - kl_loss: 11.2236\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.0197 - reconstruction_loss: 24.8225 - kl_loss: 11.1972\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.5669\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.1868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.3089 - reconstruction_loss: 26.7977 - kl_loss: 10.5111\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.2789 - reconstruction_loss: 24.7941 - kl_loss: 10.4848\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7360\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.3503 - reconstruction_loss: 40.4915 - kl_loss: 8.8587\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.6846 - reconstruction_loss: 40.8429 - kl_loss: 8.8417\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2382\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1267\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.6762 - reconstruction_loss: 56.0016 - kl_loss: 8.6746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 967us/step - loss: 66.5041 - reconstruction_loss: 57.8424 - kl_loss: 8.6617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.0721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.7939\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68.3011 - reconstruction_loss: 57.3536 - kl_loss: 10.9475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.2821 - reconstruction_loss: 56.3544 - kl_loss: 10.9277\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9599\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.4324 - reconstruction_loss: 35.4806 - kl_loss: 8.9518\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.6328 - reconstruction_loss: 27.6753 - kl_loss: 8.9575\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3606\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.5355 - reconstruction_loss: 93.6296 - kl_loss: 8.9059\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.8127 - reconstruction_loss: 89.9036 - kl_loss: 8.9091\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.6723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5115\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.4987 - reconstruction_loss: 24.8143 - kl_loss: 10.6844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.6311 - reconstruction_loss: 26.9529 - kl_loss: 10.6782\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 132.6320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 129.8944\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.8402 - reconstruction_loss: 3.8683 - kl_loss: 11.9719\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.9271 - reconstruction_loss: 3.9632 - kl_loss: 11.9639\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 138.9868\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 132.4205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.7556 - reconstruction_loss: 4.9095 - kl_loss: 11.8461\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.1047 - reconstruction_loss: 2.2850 - kl_loss: 11.8198\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.0440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.3270\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.0272 - reconstruction_loss: 29.9832 - kl_loss: 10.0439\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.3642 - reconstruction_loss: 30.3431 - kl_loss: 10.0211\n",
      "Success in episode 57 at time step 200 with reward -181.21883549203403\n",
      "Episode 58\n",
      "[ 0.6173188  -0.78671306  0.55691826]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.7900\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.9802 - reconstruction_loss: 102.7772 - kl_loss: 10.2030\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 110.8694 - reconstruction_loss: 100.6672 - kl_loss: 10.2022\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.0409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 28.3130\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.7607 - reconstruction_loss: 49.9992 - kl_loss: 9.7615\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 60.7109 - reconstruction_loss: 51.0135 - kl_loss: 9.6974\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.6059\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.1635\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 146.9039 - reconstruction_loss: 135.9384 - kl_loss: 10.9655\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 147.4283 - reconstruction_loss: 136.4583 - kl_loss: 10.9700\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.1289\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.1086\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 84.8450 - reconstruction_loss: 74.7694 - kl_loss: 10.0756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86.4461 - reconstruction_loss: 76.4036 - kl_loss: 10.0425\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.0118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.8055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.3404 - reconstruction_loss: 106.2442 - kl_loss: 11.0962\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.4185 - reconstruction_loss: 109.3185 - kl_loss: 11.1000\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.8498\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.2620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66.0251 - reconstruction_loss: 56.5369 - kl_loss: 9.4882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.3141 - reconstruction_loss: 58.8435 - kl_loss: 9.4706\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9025\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4999\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 150.3228 - reconstruction_loss: 138.9247 - kl_loss: 11.3981\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 135.7832 - reconstruction_loss: 124.3724 - kl_loss: 11.4108\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 80.1814\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 78.2954\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.9452 - reconstruction_loss: 112.4475 - kl_loss: 9.4976\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 120.9973 - reconstruction_loss: 111.4936 - kl_loss: 9.5038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 79.9402 - reconstruction_loss: 68.4216 - kl_loss: 11.5187\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.8045 - reconstruction_loss: 73.2679 - kl_loss: 11.5367\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.0849\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.0976\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 107.3851 - reconstruction_loss: 95.9287 - kl_loss: 11.4564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111.4837 - reconstruction_loss: 100.0127 - kl_loss: 11.4709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1579\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.8455 - reconstruction_loss: 69.7324 - kl_loss: 11.1131\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.4421 - reconstruction_loss: 58.3002 - kl_loss: 11.1419\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.1255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.6625\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102.0547 - reconstruction_loss: 89.1987 - kl_loss: 12.8561\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98.9095 - reconstruction_loss: 86.0277 - kl_loss: 12.8818\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8778\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2559\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 88.4962 - reconstruction_loss: 78.0177 - kl_loss: 10.4786\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 85.3860 - reconstruction_loss: 74.8588 - kl_loss: 10.5272\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.5398\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3890\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.7760 - reconstruction_loss: 81.3649 - kl_loss: 12.4111\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.2558 - reconstruction_loss: 74.8096 - kl_loss: 12.4462\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.7350\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.8981\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 76.3093 - reconstruction_loss: 66.1692 - kl_loss: 10.1401\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 76.5028 - reconstruction_loss: 66.3126 - kl_loss: 10.1903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.0532\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.8203\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 89.7971 - reconstruction_loss: 76.9224 - kl_loss: 12.8746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 99.8624 - reconstruction_loss: 86.9665 - kl_loss: 12.8959\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.7664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.1585\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 86.5243 - reconstruction_loss: 75.3049 - kl_loss: 11.2193\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.5268 - reconstruction_loss: 72.2245 - kl_loss: 11.3023\n",
      "Success in episode 58 at time step 200 with reward -199.69693795711214\n",
      "Episode 59\n",
      "[-0.02177209  0.99976295 -0.89607   ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7710\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.7133\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.4132 - reconstruction_loss: 123.3303 - kl_loss: 8.0829\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 125.5232 - reconstruction_loss: 117.4047 - kl_loss: 8.1185\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.3123\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.1793 - reconstruction_loss: 15.7273 - kl_loss: 9.4520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.3109 - reconstruction_loss: 14.8427 - kl_loss: 9.4682\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.0558\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.6827\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.7289 - reconstruction_loss: 26.2410 - kl_loss: 12.4879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.9146 - reconstruction_loss: 24.4016 - kl_loss: 12.5129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.2136\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.4856\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.7041 - reconstruction_loss: 22.6943 - kl_loss: 11.0099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 885us/step - loss: 35.4159 - reconstruction_loss: 24.3399 - kl_loss: 11.0760\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3369\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4000\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 163.7140 - reconstruction_loss: 153.8574 - kl_loss: 9.8566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 169.4931 - reconstruction_loss: 159.6331 - kl_loss: 9.8600\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0923\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5557\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23.3299 - reconstruction_loss: 14.8365 - kl_loss: 8.4934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.7991 - reconstruction_loss: 13.3062 - kl_loss: 8.4929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2355\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8400\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.3943 - reconstruction_loss: 28.1006 - kl_loss: 14.2937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.9109 - reconstruction_loss: 26.6023 - kl_loss: 14.3086\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.6453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.4054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.4126 - reconstruction_loss: 30.3239 - kl_loss: 13.0887\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 930us/step - loss: 45.5143 - reconstruction_loss: 32.4323 - kl_loss: 13.0820\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3026\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1423\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 137.3986 - reconstruction_loss: 129.2419 - kl_loss: 8.1566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 137.8638 - reconstruction_loss: 129.7062 - kl_loss: 8.1576\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5353\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4732\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.5270 - reconstruction_loss: 6.3778 - kl_loss: 10.1491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.5750 - reconstruction_loss: 12.4292 - kl_loss: 10.1458\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.9956\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8778\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.3636 - reconstruction_loss: 10.2841 - kl_loss: 13.0795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.0683 - reconstruction_loss: 9.9934 - kl_loss: 13.0749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.0090\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.7813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 99.2714 - reconstruction_loss: 87.7227 - kl_loss: 11.5487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.1728 - reconstruction_loss: 81.6779 - kl_loss: 11.4949\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.5545\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 24.3270\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 143.2887 - reconstruction_loss: 132.0692 - kl_loss: 11.2195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.0048 - reconstruction_loss: 116.7675 - kl_loss: 11.2372\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 75.1571\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 71.7724\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.9455 - reconstruction_loss: 60.0651 - kl_loss: 14.8805\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.6549 - reconstruction_loss: 57.8883 - kl_loss: 14.7665\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.4832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.0985\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 243.9307 - reconstruction_loss: 235.5630 - kl_loss: 8.3677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 241.8360 - reconstruction_loss: 233.5225 - kl_loss: 8.3135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.9140\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 63.3295 - reconstruction_loss: 50.1571 - kl_loss: 13.1724\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.9258 - reconstruction_loss: 59.8154 - kl_loss: 13.1104\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8032\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.1625\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 76.9551 - reconstruction_loss: 66.0644 - kl_loss: 10.8907\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.9017 - reconstruction_loss: 65.0904 - kl_loss: 10.8113\n",
      "Success in episode 59 at time step 200 with reward -163.20946029433463\n",
      "Episode 60\n",
      "[ 0.48632503 -0.873778   -0.44495067]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5194\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3102\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.4942 - reconstruction_loss: 70.6821 - kl_loss: 10.8121\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.7083 - reconstruction_loss: 66.9274 - kl_loss: 10.7808\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2133\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53.7044 - reconstruction_loss: 43.6160 - kl_loss: 10.0884\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 956us/step - loss: 47.8946 - reconstruction_loss: 37.9463 - kl_loss: 9.9483\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.1944\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.7881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 89.3077 - reconstruction_loss: 76.3072 - kl_loss: 13.0005\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.3041 - reconstruction_loss: 75.3273 - kl_loss: 12.9768\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.4986\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.3897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.9588 - reconstruction_loss: 12.1967 - kl_loss: 9.7621\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 952us/step - loss: 24.4160 - reconstruction_loss: 14.7528 - kl_loss: 9.6632\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.9853\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 32.8778\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 194.5901 - reconstruction_loss: 185.7332 - kl_loss: 8.8570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 199.6104 - reconstruction_loss: 190.7773 - kl_loss: 8.8330\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 101.4074\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 101.3921\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 123.6114 - reconstruction_loss: 113.3516 - kl_loss: 10.2597\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 954us/step - loss: 131.4916 - reconstruction_loss: 121.2839 - kl_loss: 10.2077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.1220\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.7547\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 163.3181 - reconstruction_loss: 153.6266 - kl_loss: 9.6915\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 160.0491 - reconstruction_loss: 150.3743 - kl_loss: 9.6748\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.7380\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.2620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.6308 - reconstruction_loss: 16.9730 - kl_loss: 10.6579\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.3404 - reconstruction_loss: 18.7330 - kl_loss: 10.6074\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.0769\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 28.8270\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.8652 - reconstruction_loss: 145.9044 - kl_loss: 9.9609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 154.6542 - reconstruction_loss: 144.7092 - kl_loss: 9.9450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 133.7933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 131.6625\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 114.4449 - reconstruction_loss: 103.0857 - kl_loss: 11.3591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.3979 - reconstruction_loss: 101.0821 - kl_loss: 11.3158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.8379\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.0558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 216.1363 - reconstruction_loss: 207.0018 - kl_loss: 9.1345\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 215.7517 - reconstruction_loss: 206.6294 - kl_loss: 9.1223\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.1919 - reconstruction_loss: 23.8086 - kl_loss: 11.3833\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.1369 - reconstruction_loss: 25.7865 - kl_loss: 11.3504\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.1083\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.5405\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 125.0555 - reconstruction_loss: 116.3608 - kl_loss: 8.6946\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.0671 - reconstruction_loss: 116.3760 - kl_loss: 8.6911\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.6058\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.3044\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 117.3666 - reconstruction_loss: 105.4192 - kl_loss: 11.9474\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.1073 - reconstruction_loss: 109.2022 - kl_loss: 11.9051\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 63.5527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 61.6938\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 197.4014 - reconstruction_loss: 189.5273 - kl_loss: 7.8741\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 195.5670 - reconstruction_loss: 187.6976 - kl_loss: 7.8694\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.8234\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.4592\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 47.7072 - reconstruction_loss: 37.6910 - kl_loss: 10.0162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 939us/step - loss: 43.5752 - reconstruction_loss: 33.5869 - kl_loss: 9.9884\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.5050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.9266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 101.5577 - reconstruction_loss: 91.6606 - kl_loss: 9.8971\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 100.6711 - reconstruction_loss: 90.7925 - kl_loss: 9.8786\n",
      "Success in episode 60 at time step 200 with reward -171.23827050241678\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "daifa, results_one = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=60, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9999847]]\n",
      "[[-0.99998605]]\n",
      "[[-0.9999225]]\n",
      "[[0.788844]]\n",
      "[[-0.24475694]]\n",
      "[[-0.9998107]]\n",
      "[[-0.9997041]]\n",
      "[[0.8476998]]\n",
      "[[0.98380876]]\n",
      "[[-0.9971517]]\n",
      "[[-0.99813217]]\n",
      "[[0.64551556]]\n",
      "[[0.4988195]]\n",
      "[[-0.9986258]]\n",
      "[[-0.99775445]]\n",
      "[[0.99499846]]\n",
      "[[-0.72516656]]\n",
      "[[-0.99972457]]\n",
      "[[-0.99950093]]\n",
      "[[-0.25013044]]\n",
      "[[0.93849057]]\n",
      "[[-0.9979411]]\n",
      "[[-0.99878967]]\n",
      "[[0.9952538]]\n",
      "[[0.7131172]]\n",
      "[[-0.99540454]]\n",
      "[[-0.48355168]]\n",
      "[[0.95396745]]\n",
      "[[-0.94364625]]\n",
      "[[-0.96951824]]\n",
      "[[0.9957447]]\n",
      "[[0.10688989]]\n",
      "[[-0.9665973]]\n",
      "[[0.49411416]]\n",
      "[[0.97614944]]\n",
      "[[0.97789913]]\n",
      "[[0.9827886]]\n",
      "[[0.98413116]]\n",
      "[[0.99011475]]\n",
      "[[0.9828187]]\n",
      "[[0.97657555]]\n",
      "[[0.98228425]]\n",
      "[[0.9653112]]\n",
      "[[0.9875387]]\n",
      "[[0.9883943]]\n",
      "[[0.9793135]]\n",
      "[[0.9781578]]\n",
      "[[0.9630074]]\n",
      "[[0.9763142]]\n",
      "[[0.99007535]]\n",
      "[[0.9842757]]\n",
      "[[0.9721533]]\n",
      "[[0.9821666]]\n",
      "[[0.9814092]]\n",
      "[[0.98306584]]\n",
      "[[0.99023795]]\n",
      "[[0.9791569]]\n",
      "[[0.9788779]]\n",
      "[[0.9795013]]\n",
      "[[0.9768523]]\n",
      "[[0.98871064]]\n",
      "[[0.98828876]]\n",
      "[[0.97850287]]\n",
      "[[0.9802462]]\n",
      "[[0.98008794]]\n",
      "[[0.98008466]]\n",
      "[[0.98812616]]\n",
      "[[0.97965544]]\n",
      "[[0.9749614]]\n",
      "[[0.9798452]]\n",
      "[[0.96590567]]\n",
      "[[0.9805898]]\n",
      "[[0.9900359]]\n",
      "[[0.97393423]]\n",
      "[[0.9786027]]\n",
      "[[0.988356]]\n",
      "[[0.97877884]]\n",
      "[[0.9871774]]\n",
      "[[0.9895926]]\n",
      "[[0.9746892]]\n",
      "[[0.98032683]]\n",
      "[[0.9760925]]\n",
      "[[0.9824375]]\n",
      "[[0.9893422]]\n",
      "[[0.98578835]]\n",
      "[[0.98198205]]\n",
      "[[0.9784535]]\n",
      "[[0.95634645]]\n",
      "[[0.9871879]]\n",
      "[[0.98833597]]\n",
      "[[0.97406274]]\n",
      "[[0.97213477]]\n",
      "[[0.9848026]]\n",
      "[[0.97114164]]\n",
      "[[0.98524815]]\n",
      "[[0.983465]]\n",
      "[[0.97383094]]\n",
      "[[0.9828328]]\n",
      "[[0.97901076]]\n",
      "[[0.98637456]]\n",
      "[[0.9844319]]\n",
      "[[0.9721524]]\n",
      "[[0.8896776]]\n",
      "[[0.9695223]]\n",
      "[[0.8311914]]\n",
      "[[-0.5527002]]\n",
      "[[0.9904342]]\n",
      "[[0.9796143]]\n",
      "[[0.41769254]]\n",
      "[[0.9651671]]\n",
      "[[0.6116186]]\n",
      "[[-0.8177495]]\n",
      "[[0.98787093]]\n",
      "[[0.9714284]]\n",
      "[[-0.11300003]]\n",
      "[[0.95937586]]\n",
      "[[0.71255594]]\n",
      "[[-0.9579139]]\n",
      "[[0.9764971]]\n",
      "[[0.9803756]]\n",
      "[[-0.05495203]]\n",
      "[[0.9351261]]\n",
      "[[0.95166403]]\n",
      "[[-0.9748119]]\n",
      "[[0.42810932]]\n",
      "[[0.99274355]]\n",
      "[[0.28074574]]\n",
      "[[0.5929957]]\n",
      "[[0.96524286]]\n",
      "[[-0.9591534]]\n",
      "[[-0.7874925]]\n",
      "[[0.99522376]]\n",
      "[[0.13757747]]\n",
      "[[-0.95960426]]\n",
      "[[0.90967834]]\n",
      "[[0.85695076]]\n",
      "[[-0.9977449]]\n",
      "[[0.95215636]]\n",
      "[[0.97324055]]\n",
      "[[-0.89212525]]\n",
      "[[-0.45736533]]\n",
      "[[0.95324147]]\n",
      "[[-0.75433433]]\n",
      "[[0.01832323]]\n",
      "[[0.99499583]]\n",
      "[[0.8668431]]\n",
      "[[-0.11906034]]\n",
      "[[0.96774185]]\n",
      "[[0.14454468]]\n",
      "[[-0.65764505]]\n",
      "[[0.9944335]]\n",
      "[[0.9209492]]\n",
      "[[-0.23517638]]\n",
      "[[0.96358657]]\n",
      "[[0.08435409]]\n",
      "[[-0.86884385]]\n",
      "[[0.9921837]]\n",
      "[[0.93454486]]\n",
      "[[0.29985344]]\n",
      "[[0.9612177]]\n",
      "[[0.19053435]]\n",
      "[[-0.9725688]]\n",
      "[[0.9911404]]\n",
      "[[0.95172465]]\n",
      "[[-0.57323825]]\n",
      "[[0.94759107]]\n",
      "[[0.91121006]]\n",
      "[[-0.98115677]]\n",
      "[[0.8307166]]\n",
      "[[0.9911094]]\n",
      "[[-0.9934997]]\n",
      "[[0.9350391]]\n",
      "[[0.9909985]]\n",
      "[[-0.8398287]]\n",
      "[[0.38190776]]\n",
      "[[0.9673013]]\n",
      "[[-0.9399555]]\n",
      "[[-0.6140207]]\n",
      "[[0.9933433]]\n",
      "[[0.22955443]]\n",
      "[[-0.83043385]]\n",
      "[[0.95285624]]\n",
      "[[0.44225153]]\n",
      "[[-0.9782277]]\n",
      "[[0.89781374]]\n",
      "[[0.98870736]]\n",
      "[[-0.7068501]]\n",
      "[[0.6630776]]\n",
      "[[0.96890056]]\n",
      "[[-0.9619745]]\n",
      "[[-0.9246114]]\n",
      "[[0.9947403]]\n",
      "[[-0.7228531]]\n",
      "[[-0.9898862]]\n",
      "[[0.52119815]]\n",
      "[[0.96650815]]\n",
      "[[-0.99863213]]\n",
      "[[-0.9922956]]\n",
      "[[0.9941723]]\n",
      "[[0.5865174]]\n",
      "[[-0.99800587]]\n",
      "[[-0.7028311]]\n",
      "[[0.968714]]\n",
      "[[-0.96171635]]\n",
      "[[-0.99997836]]\n",
      "[[-0.99998814]]\n",
      "[[-0.99980134]]\n",
      "[[0.99497175]]\n",
      "[[-0.5989261]]\n",
      "[[-0.99998134]]\n",
      "[[-0.99999136]]\n",
      "[[-0.9999497]]\n",
      "[[0.9931848]]\n",
      "[[0.31014678]]\n",
      "[[-0.9999118]]\n",
      "[[-0.9999441]]\n",
      "[[-0.9997511]]\n",
      "[[0.8874331]]\n",
      "[[-0.43953833]]\n",
      "[[-0.9998313]]\n",
      "[[-0.99884474]]\n",
      "[[0.98974776]]\n",
      "[[0.61698073]]\n",
      "[[-0.9998608]]\n",
      "[[-0.9967917]]\n",
      "[[0.95336497]]\n",
      "[[-0.74614066]]\n",
      "[[-0.9987576]]\n",
      "[[0.9516201]]\n",
      "[[0.9858463]]\n",
      "[[-0.90964377]]\n",
      "[[-0.66814995]]\n",
      "[[0.96005934]]\n",
      "[[-0.6963509]]\n",
      "[[0.35947302]]\n",
      "[[0.9942505]]\n",
      "[[0.9379837]]\n",
      "[[0.76232845]]\n",
      "[[0.9394006]]\n",
      "[[0.9640553]]\n",
      "[[0.9305585]]\n",
      "[[-0.15649882]]\n",
      "[[0.9933223]]\n",
      "[[0.9835805]]\n",
      "[[0.8892118]]\n",
      "[[0.9722416]]\n",
      "[[0.8659665]]\n",
      "[[-0.44196013]]\n",
      "[[0.99344563]]\n",
      "[[0.9461716]]\n",
      "[[0.6516843]]\n",
      "[[0.9530697]]\n",
      "[[0.2884402]]\n",
      "[[-0.49713895]]\n",
      "[[0.99438757]]\n",
      "[[0.95802224]]\n",
      "[[0.22913456]]\n",
      "[[0.97021854]]\n",
      "[[0.26719162]]\n",
      "[[-0.96854323]]\n",
      "[[0.9881713]]\n",
      "[[0.96221846]]\n",
      "[[-0.71516144]]\n",
      "[[0.94387376]]\n",
      "[[0.92917573]]\n",
      "[[-0.9732692]]\n",
      "[[0.9637028]]\n",
      "[[0.98982483]]\n",
      "[[0.05553629]]\n",
      "[[0.90342015]]\n",
      "[[0.9552474]]\n",
      "[[-0.97662956]]\n",
      "[[0.98552483]]\n",
      "[[0.9896658]]\n",
      "[[0.98786]]\n",
      "[[0.98401916]]\n",
      "[[0.9829183]]\n",
      "[[0.9835998]]\n",
      "[[0.9849859]]\n",
      "[[0.9904888]]\n",
      "[[0.9865144]]\n",
      "[[0.9821833]]\n",
      "[[0.98370755]]\n",
      "[[0.98710036]]\n",
      "[[0.9909974]]\n",
      "[[0.99115276]]\n",
      "[[0.98544055]]\n",
      "[[0.9828807]]\n",
      "[[0.9796773]]\n",
      "[[0.984533]]\n",
      "[[0.98700833]]\n",
      "[[0.9889934]]\n",
      "[[0.98365456]]\n",
      "[[0.97912055]]\n",
      "[[0.9860406]]\n",
      "[[0.99061984]]\n",
      "[[0.9895817]]\n",
      "[[0.98451585]]\n",
      "[[0.9781664]]\n",
      "[[0.9820947]]\n",
      "[[0.98801196]]\n",
      "[[0.98938465]]\n",
      "[[0.9868459]]\n",
      "[[0.9834287]]\n",
      "[[0.986759]]\n",
      "[[0.985344]]\n",
      "[[-0.99994284]]\n",
      "[[-0.99020684]]\n",
      "[[0.9950607]]\n",
      "[[-0.9951122]]\n",
      "[[-0.9999773]]\n",
      "[[-0.9999789]]\n",
      "[[-0.99997944]]\n",
      "[[-0.9998738]]\n",
      "[[-0.8747521]]\n",
      "[[0.9634111]]\n",
      "[[-0.9963231]]\n",
      "[[-0.99973834]]\n",
      "[[0.9736117]]\n",
      "[[0.9728913]]\n",
      "[[-0.968212]]\n",
      "[[-0.7606275]]\n",
      "[[0.9651991]]\n",
      "[[-0.8221512]]\n",
      "[[-0.92987233]]\n",
      "[[0.99386144]]\n",
      "[[0.7507981]]\n",
      "[[-0.79239476]]\n",
      "[[0.92301005]]\n",
      "[[0.28988105]]\n",
      "[[-0.9582805]]\n",
      "[[0.9632984]]\n",
      "[[0.9877097]]\n",
      "[[0.21212266]]\n",
      "[[0.8943238]]\n",
      "[[0.9592284]]\n",
      "[[-0.97672844]]\n",
      "[[-0.25066447]]\n",
      "[[0.99330217]]\n",
      "[[-0.76422507]]\n",
      "[[-0.99974436]]\n",
      "[[-0.9183098]]\n",
      "[[0.96127486]]\n",
      "[[-0.9631557]]\n",
      "[[-0.9837006]]\n",
      "[[0.98943955]]\n",
      "[[0.9129568]]\n",
      "[[-0.9246588]]\n",
      "[[0.8518119]]\n",
      "[[0.88991237]]\n",
      "[[-0.99852425]]\n",
      "[[-0.96247363]]\n",
      "[[0.99552363]]\n",
      "[[-0.88883656]]\n",
      "[[-0.9995818]]\n",
      "[[-0.99325335]]\n",
      "[[0.9581174]]\n",
      "[[-0.72327995]]\n",
      "[[-0.9982154]]\n",
      "[[0.9493682]]\n",
      "[[0.9869463]]\n",
      "[[-0.8121063]]\n",
      "[[-0.4536612]]\n",
      "[[0.9598225]]\n",
      "[[-0.72589946]]\n",
      "[[-0.5842282]]\n",
      "[[0.9941483]]\n",
      "[[0.75343186]]\n",
      "[[-0.6725522]]\n",
      "[[0.9581489]]\n",
      "[[0.44359347]]\n",
      "[[-0.9638259]]\n",
      "[[0.991582]]\n",
      "[[0.9638169]]\n",
      "[[-0.37259796]]\n",
      "[[0.9217346]]\n",
      "[[0.91889364]]\n",
      "[[-0.98512167]]\n",
      "[[0.90991956]]\n",
      "[[0.9914067]]\n",
      "[[0.41380775]]\n",
      "[[0.7434]]\n",
      "[[0.97138137]]\n",
      "[[-0.96187514]]\n",
      "[[-0.32611343]]\n",
      "[[0.99384356]]\n",
      "[[-0.38317463]]\n",
      "[[-0.94048595]]\n",
      "[[0.95262235]]\n",
      "[[0.591259]]\n",
      "[[-0.9948845]]\n",
      "[[0.8618408]]\n",
      "[[0.99181426]]\n",
      "[[-0.4034188]]\n",
      "[[-0.18931808]]\n",
      "[[0.9682817]]\n",
      "[[-0.34751084]]\n",
      "[[-0.15030622]]\n",
      "[[0.993441]]\n",
      "[[0.9069017]]\n",
      "[[0.5579219]]\n",
      "[[0.9641077]]\n",
      "[[-0.59721553]]\n",
      "[[-0.5724646]]\n",
      "[[0.9937111]]\n",
      "[[0.7520978]]\n",
      "[[-0.835816]]\n",
      "[[0.9560994]]\n",
      "[[-0.9998803]]\n",
      "[[0.9154014]]\n",
      "[[0.97371924]]\n",
      "[[-0.99537903]]\n",
      "[[-0.97311985]]\n",
      "[[0.93785393]]\n",
      "[[0.449229]]\n",
      "[[-0.9989329]]\n",
      "[[-0.9086268]]\n",
      "[[0.9916629]]\n",
      "[[-0.96332234]]\n",
      "[[-0.9998174]]\n",
      "[[-0.9974394]]\n",
      "[[0.90710133]]\n",
      "[[-0.2010279]]\n",
      "[[-0.99967575]]\n",
      "[[-0.99575377]]\n",
      "[[0.99523133]]\n",
      "[[-0.8912915]]\n",
      "[[-0.9999151]]\n",
      "[[-0.9999709]]\n",
      "[[-0.9996084]]\n",
      "[[0.903203]]\n",
      "[[-0.41669202]]\n",
      "[[-0.9997749]]\n",
      "[[-0.9972423]]\n",
      "[[0.9939475]]\n",
      "[[-0.5614237]]\n",
      "[[-0.9999574]]\n",
      "[[-0.999944]]\n",
      "[[-0.9960096]]\n",
      "[[0.9117732]]\n",
      "[[-0.7846347]]\n",
      "[[-0.9993842]]\n",
      "[[0.98489636]]\n",
      "[[0.98809505]]\n",
      "[[0.9880843]]\n",
      "[[0.9825187]]\n",
      "[[0.9834071]]\n",
      "[[0.97795355]]\n",
      "[[0.986919]]\n",
      "[[0.9870902]]\n",
      "[[0.9848811]]\n",
      "[[0.97936255]]\n",
      "[[0.9861163]]\n",
      "[[0.98043627]]\n",
      "[[0.98641706]]\n",
      "[[0.9855978]]\n",
      "[[0.979963]]\n",
      "[[0.9833409]]\n",
      "[[0.97169626]]\n",
      "[[0.98146194]]\n",
      "[[0.9900536]]\n",
      "[[0.9825755]]\n",
      "[[0.9803351]]\n",
      "[[0.9839557]]\n",
      "[[0.98289675]]\n",
      "[[0.9917468]]\n",
      "[[0.9875176]]\n",
      "[[0.984493]]\n",
      "[[0.9831868]]\n",
      "[[0.9846445]]\n",
      "[[0.9895067]]\n",
      "[[0.9872184]]\n",
      "[[0.9838097]]\n",
      "[[0.97811913]]\n",
      "[[0.9845558]]\n",
      "[[0.9871863]]\n",
      "[[0.98208094]]\n",
      "[[0.99138683]]\n",
      "[[0.98466295]]\n",
      "[[0.98040843]]\n",
      "[[0.9787398]]\n",
      "[[0.96359295]]\n",
      "[[0.9850162]]\n",
      "[[0.9873978]]\n",
      "[[0.979136]]\n",
      "[[0.9813601]]\n",
      "[[0.98001814]]\n",
      "[[0.97731483]]\n",
      "[[0.99102074]]\n",
      "[[0.98491085]]\n",
      "[[0.97554827]]\n",
      "[[0.98145586]]\n",
      "[[0.9815576]]\n",
      "[[0.984364]]\n",
      "[[0.98982066]]\n",
      "[[0.9831876]]\n",
      "[[0.9787526]]\n",
      "[[0.9793425]]\n",
      "[[0.9753083]]\n",
      "[[0.9891583]]\n",
      "[[0.9855773]]\n",
      "[[0.9812636]]\n",
      "[[0.9730198]]\n",
      "[[0.9801848]]\n",
      "[[0.9759519]]\n",
      "[[0.99123293]]\n",
      "[[0.980624]]\n",
      "[[0.9758492]]\n",
      "[[0.98373824]]\n",
      "[[0.9817477]]\n",
      "[[-0.99536794]]\n",
      "[[0.98416275]]\n",
      "[[0.9429733]]\n",
      "[[-0.9322572]]\n",
      "[[0.41771924]]\n",
      "[[0.9548433]]\n",
      "[[-0.99521697]]\n",
      "[[-0.9375799]]\n",
      "[[0.9957345]]\n",
      "[[-0.715551]]\n",
      "[[-0.99577034]]\n",
      "[[-0.8811924]]\n",
      "[[0.9637542]]\n",
      "[[-0.9643033]]\n",
      "[[-0.9476854]]\n",
      "[[0.9954344]]\n",
      "[[0.43025115]]\n",
      "[[-0.9432436]]\n",
      "[[0.9154562]]\n",
      "[[0.80391854]]\n",
      "[[-0.99878824]]\n",
      "[[-0.9501294]]\n",
      "[[0.9943784]]\n",
      "[[-0.9463897]]\n",
      "[[-0.9997871]]\n",
      "[[-0.9942684]]\n",
      "[[0.9445633]]\n",
      "[[-0.82415855]]\n",
      "[[-0.99797875]]\n",
      "[[0.9590074]]\n",
      "[[0.97679734]]\n",
      "[[-0.94559854]]\n",
      "[[-0.34492034]]\n",
      "[[0.9624781]]\n",
      "[[-0.99937326]]\n",
      "[[0.81001663]]\n",
      "[[0.9875992]]\n",
      "[[-0.989177]]\n",
      "[[-0.9615725]]\n",
      "[[0.9415706]]\n",
      "[[0.4924756]]\n",
      "[[-0.9986702]]\n",
      "[[-0.2635655]]\n",
      "[[0.99327886]]\n",
      "[[-0.9703979]]\n",
      "[[-0.9931018]]\n",
      "[[0.493766]]\n",
      "[[0.9515153]]\n",
      "[[-0.99944687]]\n",
      "[[-0.99933875]]\n",
      "[[0.9877077]]\n",
      "[[0.56398004]]\n",
      "[[-0.9955984]]\n",
      "[[-0.90836596]]\n",
      "[[0.9364111]]\n",
      "[[-0.8753389]]\n",
      "[[-0.9618807]]\n",
      "[[0.9911854]]\n",
      "[[0.9255226]]\n",
      "[[-0.9198694]]\n",
      "[[0.93114376]]\n",
      "[[0.9193295]]\n",
      "[[-0.99266803]]\n",
      "[[-0.10613681]]\n",
      "[[0.9938147]]\n",
      "[[-0.89833033]]\n",
      "[[-0.98174626]]\n",
      "[[0.8596838]]\n",
      "[[0.9848055]]\n",
      "[[0.982969]]\n",
      "[[0.9872758]]\n",
      "[[0.9837776]]\n",
      "[[0.98416394]]\n",
      "[[0.9884924]]\n",
      "[[0.98801583]]\n",
      "[[0.98705405]]\n",
      "[[0.984856]]\n",
      "[[0.98403907]]\n",
      "[[0.98342305]]\n",
      "[[0.9876243]]\n",
      "[[0.9907196]]\n",
      "[[0.9864351]]\n",
      "[[0.98393744]]\n",
      "[[0.98272663]]\n",
      "[[0.9869712]]\n",
      "[[0.98884535]]\n",
      "[[0.98488647]]\n",
      "[[0.98301446]]\n",
      "[[0.9849996]]\n",
      "[[0.9807685]]\n",
      "[[0.98594385]]\n",
      "[[0.988589]]\n",
      "[[0.9816857]]\n",
      "[[0.98273623]]\n",
      "[[0.98222077]]\n",
      "[[0.9865074]]\n",
      "[[0.9836199]]\n",
      "[[0.9855455]]\n",
      "[[0.9853967]]\n",
      "[[0.9835208]]\n",
      "[[0.98848724]]\n",
      "[[0.98819923]]\n",
      "[[0.94461405]]\n",
      "[[0.96667814]]\n",
      "[[0.934215]]\n",
      "[[0.5010231]]\n",
      "[[0.9897519]]\n",
      "[[0.98354435]]\n",
      "[[0.9186729]]\n",
      "[[0.96198964]]\n",
      "[[0.8431261]]\n",
      "[[0.734983]]\n",
      "[[0.99197817]]\n",
      "[[0.97507465]]\n",
      "[[0.9243356]]\n",
      "[[0.96682006]]\n",
      "[[0.7927411]]\n",
      "[[0.5863539]]\n",
      "[[0.9944501]]\n",
      "[[0.9628941]]\n",
      "[[0.9271212]]\n",
      "[[0.9725527]]\n",
      "[[0.57934797]]\n",
      "[[0.83132106]]\n",
      "[[0.9940167]]\n",
      "[[0.96644646]]\n",
      "[[0.95176166]]\n",
      "[[0.9739347]]\n",
      "[[0.31072253]]\n",
      "[[0.8584089]]\n",
      "[[0.9922584]]\n",
      "[[0.95098853]]\n",
      "[[0.9504478]]\n",
      "[[0.97167027]]\n",
      "[[0.23558874]]\n",
      "[[0.97088146]]\n",
      "[[-0.99998593]]\n",
      "[[-0.99999094]]\n",
      "[[-0.9999855]]\n",
      "[[-0.99997616]]\n",
      "[[-0.99988025]]\n",
      "[[0.39187327]]\n",
      "[[0.20248905]]\n",
      "[[-0.999914]]\n",
      "[[-0.9998652]]\n",
      "[[0.9697815]]\n",
      "[[0.97372764]]\n",
      "[[-0.99653715]]\n",
      "[[-0.9698614]]\n",
      "[[0.94265497]]\n",
      "[[-0.08776999]]\n",
      "[[-0.99909234]]\n",
      "[[0.19493543]]\n",
      "[[0.99234444]]\n",
      "[[-0.9286845]]\n",
      "[[-0.9756375]]\n",
      "[[0.93001485]]\n",
      "[[0.6711209]]\n",
      "[[-0.997818]]\n",
      "[[-0.8849919]]\n",
      "[[0.994865]]\n",
      "[[-0.9565212]]\n",
      "[[-0.99958265]]\n",
      "[[-0.98041344]]\n",
      "[[0.93536055]]\n",
      "[[-0.57408774]]\n",
      "[[-0.9992623]]\n",
      "[[-0.15481487]]\n",
      "[[0.9872012]]\n",
      "[[-0.9962238]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "        reward  timesteps  num_actions\n0  -174.290924        204           34\n1  -264.419006        204           34\n2  -264.804138        204           34\n3  -208.613525        204           34\n4  -208.576080        204           34\n5  -201.111496        204           34\n6  -170.493958        204           34\n7  -208.788696        204           34\n8  -288.000275        204           34\n9  -180.362473        204           34\n10 -193.342239        204           34\n11 -212.440369        204           34\n12 -159.031158        204           34\n13 -275.806274        204           34\n14 -266.870819        204           34\n15 -194.056747        204           34\n16 -193.951981        204           34\n17 -295.212250        204           34\n18 -229.594681        204           34\n19 -161.852615        204           34",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-174.290924</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-264.419006</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-264.804138</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-208.613525</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-208.576080</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-201.111496</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-170.493958</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-208.788696</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-288.000275</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-180.362473</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-193.342239</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-212.440369</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-159.031158</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-275.806274</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-266.870819</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-194.056747</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-193.951981</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-295.212250</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-229.594681</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-161.852615</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x176aab2b0>]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA/KElEQVR4nO3deXykZZno/d9VS/bKvnYnnTS9dwO90qyyI4ggqHhERRl1Bt8R56jj6xmYmTPjMpzjmU1n3uMGijKOCCiowAgKyI42dDe9L/TeSWff96RSdb9/1PNUKpWqSiWpSirV1/fz6U+SJ1WV+2noq+5c93VftxhjUEoplV4c8z0ApZRSiafBXSml0pAGd6WUSkMa3JVSKg1pcFdKqTTkmu8BAJSWlpq6urr5HoZSSi0oO3bsaDfGlEX6XkoE97q6OrZv3z7fw1BKqQVFRE5F+96UaRkRyRKRN0Vkt4jsF5GvWte/IiJnRGSX9efGkOfcKyJHReSwiFyfmNtQSikVr3hm7iPA1caYfhFxA6+JyDPW975pjPnn0AeLyFrgdmAdsAh4XkRWGmN8iRy4Ukqp6KacuZuAfutLt/Un1rbWW4BHjDEjxpgTwFFg66xHqpRSKm5xVcuIiFNEdgGtwHPGmG3Wtz4nIntE5EERKbKuLQbqQ57eYF0Lf827RGS7iGxva2ub+R0opZSaJK7gbozxGWM2ANXAVhE5F/gusAzYADQB/2I9XCK9RITXvN8Ys8UYs6WsLOJir1JKqRmaVp27MaYbeAm4wRjTYgV9P/AA46mXBqAm5GnVQOPsh6qUUipe8VTLlIlIofV5NnAtcEhEqkIe9n5gn/X5k8DtIpIpIkuBFcCbCR21UkqpmOKplqkCHhIRJ4E3g8eMMU+LyE9EZAOBlMtJ4DMAxpj9IvIYcAAYA+5OVqVMY/cQj7x5mvdvqmZpaW4yfoRSSi1IUwZ3Y8weYGOE6x+P8Zz7gPtmN7SpdQ6M8u+/P8q6xQUa3JVSKsSC7i2Tn+UGoHfIO88jUUqp1LKwg3t24BeP3uGxeR6JUkqllgUd3D06c1dKqYgWdHB3OgRPpoveYQ3uSikVakEHd4D8bDc9OnNXSqkJFnxw92S56B3SnLtSSoVa8ME9P9utaRmllAqz8IN7llsXVJVSKsyCD+4F2W76tBRSKaUmWPDBPT/bpTN3pZQKs/CDe5abvpExfP5Y54copdTZZeEH9+zARqY+XVRVSqmghR/cs6wWBFoOqZRSQQs+uBdYM3cth1RKqXELPrjbaRldVFVKqXELP7hbzcO0BYFSSo1b+ME92PZXg7tSStnSILjbaRldUFVKKduCD+55GS4cktoz97/55V5+t795voehlDqLLPjg7nAInhTuLzM65ufhN0/z+0Ot8z0UpdRZZMEHdwjk3VN1QbW1bxhjdMFXKTW30iO4Z7lT9hzVpp5hALoHNbgrpeZO+gT3FJ0Z28FdZ+5KqbmUFsG9IIUP7GjqHgI0uCul5taUwV1EskTkTRHZLSL7ReSr1vViEXlORI5YH4tCnnOviBwVkcMicn0ybwDstr+pnZbR4K6UmkvxzNxHgKuNMeuBDcANInIRcA/wgjFmBfCC9TUisha4HVgH3AB8R0ScSRh7UCDnnprBs6knMHPvHxljzOef59Eopc4WUwZ3E9Bvfem2/hjgFuAh6/pDwK3W57cAjxhjRowxJ4CjwNZEDjpcfrabwVEf3hQMns3WzB1I2UVfpVT6iSvnLiJOEdkFtALPGWO2ARXGmCYA62O59fDFQH3I0xusa+GveZeIbBeR7W1tbbO4hdC2v6k3e2/sGSbLHfhr7h4cnefRKKXOFnEFd2OMzxizAagGtorIuTEeLpFeIsJr3m+M2WKM2VJWVhbXYKMpyLHb/qbWzHh0zE97/wirKjyA5t2VUnNnWtUyxphu4CUCufQWEakCsD7aWzAbgJqQp1UDjbMdaCx2Z8hYM/evPLmfx3c0JHMYk9gbmFZX5gPQrcFdKTVH4qmWKRORQuvzbOBa4BDwJHCn9bA7gV9bnz8J3C4imSKyFFgBvJngcU+QP8WBHcYYHn2rngdePZ7MYUxiV8qsqgzM3FMxbaSUSk+uOB5TBTxkVbw4gMeMMU+LyB+Ax0Tk08Bp4EMAxpj9IvIYcAAYA+42xviSM/yAqXq6942MMeT1cai5j8buIRYVZidzOEF2cF9dpWkZpdTcmjK4G2P2ABsjXO8AronynPuA+2Y9ujgFe7pHqXVv7R2vWHnpcBsfvXDJnIzL3sC0xk7LaAsCpdQcSZsdqhA9LdPSOxL8fC67Mzb1DJOX6aIoN4OcDKfO3JVScyYtgnu224nLIVFz2i3WzP2y5aW8frSdYW9Ss0RBTT1DVBZkAVCY7dbgrpSaM2kR3EWE/Bj9ZeyZ+4cvqGHI6+PNE51zMq7mnmGqrOCen+3WtIxSas6kRXCHwEamaDn3lt5AeuTaNRVkuhxzlpppDAnuBdmp27lSKZV+0ie4x0h7tPYNU56fSXaGk0uWlfDi4VaMmbSvKqHsDUxVBYHKnMIcTcsopeZO2gT3WG1/W3tHqPAEZtBXry7nVMcgJ9oHkjqelt7ABqbQmXv3kLYfUErNjbQJ7rEO7GjpG6YiPxOAK1cFWuAkOzXTbC3iVlk19QW6oKqUmkPpE9yzXRF7yxhjaOkdoSI/MIOuKc5hRXkeLx5ObnC3NzDZM/fCnAyGvf45q9RRSp3d0ie4R5m59wx5GR3zU24FdwikZt480Un/SPIajdkbmEKrZUBbECil5kb6BPdsNyNjk2fGdhmknZYBuGp1OV6f4bUj7XG9tjGG9v6RqR8Ywt7A5LFaI9gbrTQ1o5SaC+kT3O2e7mGLqvYGpoqQmfvm2iI8WS5ejDPv/uTuRrb8w/P8798cZHQsvgNBmnqGgrN2CGxiAu0MqZSaG+kT3INpj4mplmBw94wHWrfTweUryuIuidzf2AvA9185zm3feyOuSpvmnuHg7lQImbnrRial1BxIv+AeNnNv7QukU8pD0jIQSM209o0EA3cs9Z2DnFOWy/fu2MypjkFu+vdXeXxHQ8w3hsaeYRYVjHefLMzRtIxSau6kT3CPcmBHS+8wBdlustwTz+i+clXg9Kd4UjP1XYPUFOVww7mVPPP5d7FucQFf+vluvvjorojnttobmCLN3DUto5SaC2kT3Austr/hM+OW3uEJi6m20rxMzq8u4NWjUy+qNnQNUV0UmIUvKszmZ392EX9x9XJ+tauR3+5vnvR4ewPTosLx4O6Zoue8UkolUtoE9+DMfTg85z4yYTE11JrKfI63xc6f9w176R70UlOcE7zmdAhfuHYlBdluXjo8+XBvewNTZUhaxukQq/+NBnelVPKlT3CPUkfe2jtMuSdycK8tzaG9fyRmvXt9Z6BevaYoZ8J1p0N414pSXn6nDb9/Yu690apxX1Qw8ecW5LjpHtQWBEqp5Eub4J7ldpLhckxYUPX7Da19I5MWU21LS3IBOBmj+qWhaxAgmJYJdeWqctr6RjjQNHFRtrnHnrmHBXdtQaCUmiNpE9zB3qU6PgvvHBxlzG+o8EQO7rVWcD/VMRj1Neu7rJl7cc6k712xMrAo+1JYK4PwDUy2wuwMDe5KqTmRXsE92zVh5h5pA1Oo2pJAwD7ZEX3mXt85SG6Gk6Ic96TvlXkyOW9xwaS8e/gGJlugM6QGd6VU8qVXcA/rL9Paa9e4Rw7uuZkuyj2ZU6RlhqgpzkFEIn7/ylVl7DzdNWFzUvgGpuD49MAOpdQcSavgHn7aUWufPXOPnJYBqCvJjZmWaegajJhvt125qhy/gVeOjM/ewzcw2ewDO5J9UIhSSqVVcA+cozqec7ebhpVFyblDIDUTLS1jjKG+c5Dqosn5dtuGmkIKc8ZLIiNtYLIVZLvx+gyDo9r2VymVXOkV3MPqyFt6hynOzSDT5Yz6nLrSXFr7RhiIUA7ZPehlYNQXcTHVFiiJLAuWREbawGTTzpBKqbkyZXAXkRoReVFEDorIfhH5vHX9KyJyRkR2WX9uDHnOvSJyVEQOi8j1ybyBUPnWUXt22qOld4TyGLN2CKRlIHLFTH2MMshQV64so70/UBIZaQOTLdgZUpuHKaWSzBXHY8aALxljdoqIB9ghIs9Z3/umMeafQx8sImuB24F1wCLgeRFZaYxJei4iPyuQ9hjy+sjJcNHaNxy1UsZmV8yc6hhg7aL8Cd+LtoEp3BUhfWqWWK8XvoEJdOaulJo7U87cjTFNxpid1ud9wEFgcYyn3AI8YowZMcacAI4CWxMx2KkUhLX9jdZXJlRdaWDmfiJC3t2eudcUx565231qXnqnLeoGJhjfRavBXSmVbNPKuYtIHbAR2GZd+pyI7BGRB0WkyLq2GKgPeVoDEd4MROQuEdkuItvb2ib3Z5mJ/OzxAzt8fkNbX/S+Mra8TBeleZmcap+clmnoGqQwxz1pM1IkV64s4+3TXRxq7sMTYQMThLb91RYESqnkiju4i0ge8DjwBWNML/BdYBmwAWgC/sV+aISnT6r9M8bcb4zZYozZUlZWNt1xRxTa9rejfwS/iV7jHqouSsVMfefQlPl22xVWSeQz+5oiztpB0zJKqbkTV3AXETeBwP5TY8wTAMaYFmOMzxjjBx5gPPXSANSEPL0aaEzckKMLPbAjeHbqFAuqEGhDEDG4W33c42GXRA57/VQVRn5DyMt04XSIBnelVNLFUy0jwA+Bg8aYfw25XhXysPcD+6zPnwRuF5FMEVkKrADeTNyQo7PPUe0Z8k7ZeiDU0tIcWnpHGBwdL4c0xnDG2p0aD6dDuHxF4DeQqig/U0QCLQi0WkYplWTxVMtcCnwc2Csiu6xrfw18REQ2EEi5nAQ+A2CM2S8ijwEHCFTa3D0XlTIwcUHV3igUT3C3G4id7hxkdWWgYqatb4SRMX/caRkItCJ4cncjVRFq3EPHqDN3pVSyTRncjTGvETmP/psYz7kPuG8W45oRT0jO3es3iEBpXsaUz6sLaf1rB/dgpUycaRkItCIozs3g/OqCqI/R4K6UmgvxzNwXjAyXg2y3k95hL33DY5TkZuJyTr2sUFtqd4ccr5hpCLb6jX/mXpybwY6/vTZqkzEIBPcuPbBDKZVkadV+AKy2v0NjcdW4B5+T5aYkN4NTIYuq9Z327tT4Z+5AzMAOOnNXSs2N9AvuWe5gtUw8+XZbbUkOJ9pDg/sQpXmZZLmj96WZCbszpFJKJVPaBXd7ZhxoPRDfzB0mt/5t6B6cVkpmuuMLP3cVAh0lh7RjpFIqAdIuuOdnu+kcGKVjYDTqwdiR1JXm0tQzzLA3EFzrO4emtZgar4JsN8ZAX4QulP/rNwf50PffSPjPVEqdfdIvuGe5ONE+gDHxlUHaxhuIDeLzGxq749+dOh3BXaoRat1fP9rO8bbop0IppVS80qpaBgIz95ExPxD7BKZwwXLIjgFyM52M+U3cG5imI1oLgr5hL0fb+jEGhr2+hOf6lVJnlzScuY837JrOzD201j1YBpmktAxMDu57z/Rgn76npZJKqdlKu+BuB0+A8mnM3Aty3BTluDnZMRgsg0zGgmphTmBTVXdYZ8jd9T3BzzsHNLgrpWYnDdMygVtyOoSS3PiDOwTaEJzqGKDMk4kIVEU4TWm2os3cd9V3BT/vGtBSSaXU7KTdzN1Oy5TlZeJ0xN5QFG5paa6VlhmkKj+LDFfi/3qiBffd9T2ss06C6tS0jFJqltIvuFvBczqLqbbakhwae4Y51tpPdRIWUwGy3A4yXI4J1TLNPcM09w5z1apyALo0LaOUmqX0C+7WzD2eQzrC2Yuqe8/0JKUMEsbb/obO3Hc3dANw+coyRGaWczfG8Mu3G3T3q1IKSMPgXjCLmbt9nqrfJKdSxlYYHtzru3E5hPOrCyiwNmFN1/7GXr746G7u/ulOxnz+RA5XKbUApW9wn8buVFtdyXhAT0aNuy38wI7dDd2srvKQ5XZSnJMxo5z7oeY+AF472s4//e5wwsaqlFqY0i+457j5x9vO58MX1Ez94DCFORnBN4dkpWVgYmdIv9+wp76H9dWFABTlZswo5364uZcMl4OPXriE7798nKf3zMnJhkqpFJV2wR3gv22pmVHOHcZn70mduYd0hjzePkDfyBjrawoBKMrJmFFa5lBzHyvK8/jKzevYUlvEl3++h4NNvYkctlJqAUnL4D4bdaW5uJ1C5QzfHOIROnPfXd8NBA7YBijOndlhHu+09LGq0kOGy8F37thEfraLz/xkB91aVqnUWUmDe5iPX1TLX92weto18tNRkO2mf2QMr8/P7oZucjOcLCvLA+y0jBdjJrcEjqZ7cJSW3hFWV3oAKPdk8d07NtPUM8R/f2QXvgjthZVS6U2De5gtdcX86bvOSerPKMweP+t1d30351cXBt9MSnIzGPX5GZhGX3d7MXVlhSd4bdOSIr52y7m88k4bD287lcDRK6UWAg3u86AgJxDc2/pHONDUG8y3QyDnDtPbyHTYCu724d62j2xdQmV+FrtC+tYopc4OGtzngV2R84djHXh9hg01BcHvFecGgvt0FlUPt/RRkO2OWNtfW5Iz4WxYpdTZQYP7PCjIDgTwl99pA5g4c7eD+zQWQg8397GqwhPxcO66klxOhhwfqJQ6O2hwnwf2zP2Pxzso92ROqMwpnmZaxhjDO82BSplIaktzaO8foT/CsX5KqfQ1ZXAXkRoReVFEDorIfhH5vHW9WESeE5Ej1seikOfcKyJHReSwiFyfzBtYiOzgPuz1s76mcMKMu2iaaZkz3UP0jYxFDe52v5zTOntX6qwSz8x9DPiSMWYNcBFwt4isBe4BXjDGrABesL7G+t7twDrgBuA7IqJnxoUIPVBkQ0hKBgJnwDodEnet+zst9mJq5OC+pNg+G1bz7kqdTaYM7saYJmPMTuvzPuAgsBi4BXjIethDwK3W57cAjxhjRowxJ4CjwNYEj3tBy3A5yMkIvN/ZbQdsImLtUo2vu6NdBrmiIkpaxtpxq3l3pc4u08q5i0gdsBHYBlQYY5og8AYAlFsPWwzUhzytwboW/lp3ich2Edne1tY2g6EvbPbs/bzqgknfK851x51zP9zcx6KCrAm/DYTyZLkpzcvQmbtSZ5m4g7uI5AGPA18wxsRqWhJpa+ekLZLGmPuNMVuMMVvKysriHUbaKMh2s6wsN2JQnk5/mcMxFlNttSW5nNTgrtRZJa4zVEXETSCw/9QY84R1uUVEqowxTSJSBbRa1xuA0JaM1YC2KAzzJ5fURT3Grzg3gyOt/VO+htfn51hbP1euKo/5uNqSHP54rGNG41RKLUzxVMsI8EPgoDHmX0O+9SRwp/X5ncCvQ67fLiKZIrIUWAG8mbghp4fbty7hA5uqI34v3ra/J9oH8PoMqyrzYj6utjiXxp5hhr3xtzRQSi1s8aRlLgU+DlwtIrusPzcC3wCuE5EjwHXW1xhj9gOPAQeAZ4G7jTEaVaahOCeDrsFR/FM0/LIXU1dV5Md8XF1pYFG1vjN5i6oDWkevVEqZMi1jjHmNyHl0gGuiPOc+4L5ZjOusVpybgd9A77CXQmtTUyTvNPfhdAjLynNjvl6tVet+smMwalXNbNR3DnLVP7/Eo5+5mM21RVM/QSmVdLpDNQXF21/mUHMfS0tzyXTF3kZgH0CSrIqZ+s5BxvyG0526aKtUqtDgnoLsXapTbWQ63NI7ZaUMjB8feCpJte7d1sEjfcOamlEqVWhwT0F2f5lYG5kGRsao7xxidZxpltqSnKSVQ/ZocFcq5WhwT0FFuYHa91gVM3bbgXhm7hDIuydr5m4Hd21OplTq0OCegorjaPtrH9ARb3CvK8mhoWuQ0TH/7AcYpnvQnrnH1zJBKZV8GtxTULbbSabLEXPmfqi5j5wMJzVFOXG9Zm1JLn4T6CKZaMGZu6ZllEoZGtxTkIhQnBu7BcE7LX2sqPDgiPMg77pgA7HE5917NeeuVMrR4J6iiqyNTNEETl+KvTM1VG0S+7p3DwXG2ac5d6VShgb3FBVr5t7WN0LHwCirKmPvTA1VmpdBToYzKTN3TcsolXo0uKeootwMugYjL1Dai6nRDuiIRESSVjETXFAd0QVVpVKFBvcUVZzjpqN/JOL3DjYFOi5PJ7hDIO+uM3elzg4a3FNUUW4GvcNjeH2TSxcPNvVSmZ9FSV7mtF6ztiSX+s5BfFM0JJsOn98EF1L7R8YwJnGvrZSaOQ3uKarEqnXvjpCaOdDUy5qq6TcAqyvJweszNPUkrhzSrpQp82Ti9RlGklBHr5SaPg3uKSpaf5lhr4+jrf2sXRT/YqptSbCBWOLy7nZKZnFhNqDlkEqlCg3uKWq8v8zE4H60tZ8xv2Ft1eSzV6dSF2z9m7i8ux3cq4vs4K6LqkqlAg3uKSo4cw8L7gcaA4upM5m5V+ZnkeFyJHTm3h0M7oHfCrS/jFKpQYN7iorWX+ZAUy85GU5qi+NrOxDK4RBqi3M42Z68mbtWzCiVGjS4p6jCnMidIQ809rKmKj/utgPhaktyOZ3A4/bCg3uvBnelUoIG9xSV6XKSl+ma0NPdGMPBpl7WVk0/JWOz+7onqmSxx/rNIjhz17SMUilBg3sKK8p1T6iWaegaom9kbEb5dltdSQ7DXj+tfZE3SE1Xz5CXLLeD4txAzb0uqCqVGjS4p7DinIn9ZfZbi6lrZjVztypmEpR37xnyUpidQV5m4Kx1zbkrlRo0uKewQH+Z8eB+oKkXh8CqOI/Wi8Quh0xUxUz3oJeCbDcZLgeZLoemZZRKERrcU1j4zP1AYy/nlOWRneGc8WsuKszC5RBOdSZu5l6QHVj89WS5dEFVqRShwT2FFeVmTKiWme1iKoDL6aCmOIcTCUzLFOTYwd2tM3elUsSUwV1EHhSRVhHZF3LtKyJyRkR2WX9uDPnevSJyVEQOi8j1yRr42aA4N4OBUR/DXh/dg6Oc6R6a1WKqbUV5HoestsGzFTpzz8t00T/FgupfPrqLB187kZCfrZSKLp6Z+4+BGyJc/6YxZoP15zcAIrIWuB1YZz3nOyIy8xzCWa44pL/MAavN72xn7hBYkD3ZPsDQqG/WrxWelpmqt8wLh1r55dtnZv1zlVKxTRncjTGvAJ1xvt4twCPGmBFjzAngKLB1FuM7qxWF9Jc5kIBKGduaqnz8Bg63zG72PjrmZ3DUR2HozD1GWmbM56dnyMvBpt6EvLEopaKbTc79cyKyx0rbFFnXFgP1IY9psK5NIiJ3ich2Edne1tY2i2Gkr+DMfcDLwaY+yjyZlHmm18M9Ertd8CHrt4GZsnen2jn3vClm7nYfmjG/YU9D96x+tlIqtpkG9+8Cy4ANQBPwL9b1SHviI26FNMbcb4zZYozZUlZWNsNhpLfi3EDQ7LTSMolIyQDUFOWQm+EMnug0U8Hgbs3c87PcMTcxhS4Ov13fPaufrZSKbUbB3RjTYozxGWP8wAOMp14agJqQh1YDjbMb4tnLTsu09g5ztLUvIYupEGggtroqn4NNs0vLhAd3Oy0TrbVBaFnnzlNds/rZSqnYZhTcRaQq5Mv3A3YlzZPA7SKSKSJLgRXAm7Mb4tmrINuNCGw70YnXZxI2c4fA+asHm3tn1WOmZygQrEMXVP0GBqPk0+0NWasrPew83a1H8imVRPGUQv4M+AOwSkQaROTTwD+KyF4R2QNcBXwRwBizH3gMOAA8C9xtjNGVsxlyOR0UZLv547EOYGY93KNZU5VP3/AYZ7pnfuSePXMvtH7DyMuyWhBEWVTtso4MvHp1Oe39IzR0Je64P6XURK6pHmCM+UiEyz+M8fj7gPtmMyg1rjgng+PtA2S7ncHWAYlgV90cbOoLHrQxXT2Dk9MyEDhqryLC+5CdlrlmTQXfeekYO093UTODvvRKqanpDtUUZ5/ItLrKg3OGPdwjWV0ZqJiZzaKqXf2Sb83Y87MCQT7aomrXwCg5GU7WVxeQ7Xby9unuGf9spVRsGtxTnL2omsh8O0BupovakhwONc88uPcMefFkunA5A/8bTZWW6RwcpSgnA5fTwfnVBew8rYuqSiWLBvcUZ5dDJmLzUrg1lbOrmOkZ8pJvpWRgYlomkq6B0WDt/qbaIg409jLs1SUZpZJBg3uKs9MyiVxMta2pyudkxwCDozNr9tUzON56AALVMhC9p3vnoDd4fOCmJUWM+Q17z/TM6GcrpWLT4J7ilpflUZjjDubIE2l1lQdjmHETsZ6h8WAN4Mm0cu5R0jLdg+Mz941LCgGtd1cqWTS4p7jbNlfzh3uuISdjysKmabPz+IdmmJoJbRoG4zn3aAuqnQOjwTWE0rxMlhTnaN5dqSTR4J7iRGRWh3PEUl2UjSfTNeOKme6w4O50CDkZzohpGa/PT9/wWHDmDrBpSaFuZlIqSTS4n8VEhNVVnhkFd2PMhIM6bNE6Q9q7U4tCg3ttEW19I7PaSKWUikyD+1ludWU+h5r78PunN3se9voZHfNPmLlD9J7uXQOBVE1xTujMPdBMdKfWuyuVcBrcz3JrqvLpH5l+G4LwpmG2vCx3xAVVe3dqUchMf1Wlhyy3QxdVlUoCDe5nObu3+4FppmaCfWWyMyZcz89yRVxQ7Y6QlnE7HZxfXajtf5VKAg3uZ7lVlR5Ept+GwA7Wk2buma6IC6qd1uNDF1QhkJo50Nijm5mUSjAN7me5nAwXdSW50w7uUdMy0RZUrbRMYdgC7KYlhXh9hn26mUmphNLgrlhT5Zn2Rqbxdr/hC6ruiAuqnQNe8jJdZLomlnVuDC6qat5dqUTS4K5YU5nPqY7BmIdbh7ODe/6kBdXAzD28+qZrcJSi3ImPBSjzZFJTnM3OU93TH7iasY7+EQam8d9bLTwa3BWrrZ2qh6fRIbJnyItDwJM5cees3f63P6xfTeju1HDrqgo40jq7I//U9Hz0gW187akD8z0MlUQa3FVIxUz8AdbuCOkI6zFvd4YMX1TtHowe3KuLsjnTPRTXTlXdzTp7w14f77T2sbuhe76HopJIg7ticWE2+VkuDk1jUbU7rCOkLVpP986QpmGTfn5RNsNePx0hB2hH8uKhVjZ+/bkJB22r6TvW1o8xcLxtgDGff76Ho5JEg7uy2hDks6s+/j4vPUNeCiMEd0+U05i6BrxRZ+6LC7MBODPFmao7T3fRPejltaPtcY1xvgxFOSA8VRxt7Qdg1OfnZMfgPI9GJYsGdwXAe8+rYn9jL7/d3xzX48MP6rBFOrBjZMxH/8hY8OCRcPYZrlPtkj3dGQhEb6RwcG/pHWbD137Hs/vi+3ucD8faBoKfH2nRtY50pcFdAfCxC5ewutLD158+GNfMM7zdr80TIS3TbR2kXRQjLQNTz9zrreD++rHUDe7bT3YxMubnmX1N8z2UqI619lOZn4UIvNPSP9/DUUmiwV0B4HI6+Or71nGme4hvv3h0yseHH9Rh82RNnrmP95WJHNwLst14Ml00dMVOEdR3DZHhclDfORQM9KnGXqR89Uj7tJuxzZWjrf2cuzif6qJs3tEqpbSlwV0FXXhOCbduWMT9rxznRPtA1McF2/3GSMuEVst0TRHcITB7j5WWGRr10dY3wnvOrQTg9RRNzeyq78YhgTe0fY2pt+t2zOfnRPsAy8rzWFnu0bRMGtPgrib46xvXkOFy8NWn9kddXO0fGcPnNxGDe26GC5GJC6pdVlomWrUMBMohG2KkZexZ/dWryynzZPL6sY647mcu+fyBNgo3nleFCLx8uG2+hzRJfdcQoz4/y8vyWFHh4UT7AF6tmElLUwZ3EXlQRFpFZF/ItWIReU5Ejlgfi0K+d6+IHBWRwyJyfbIGrpKjPD+LL1y7gpcOt/HcgZaIj4nWERLA4RDyMlwT2v52BjtCRl5QhUDFTKyce70V3GuKc7hkWQl/ONaecjXvR1v7GRz1cfXqcs5bXMDL76RecD9mVcosK89jZUUeXp/hZIzf0tTCFc/M/cfADWHX7gFeMMasAF6wvkZE1gK3A+us53xHRJJzRpxKmjsvqWNlRR5fe/pAxG6N0VoP2PKyXDNKy/SNjAVfO9xpq2RvSXEOly4rpb1/lMMpllLYbbUuXl9TyOUryni7vjvq/cyXo22B4L68PI+VFYHNa7qomp6mDO7GmFeAzrDLtwAPWZ8/BNwacv0RY8yIMeYEcBTYmpihqrnidjr46vvOpaFriO++dGzS93sGI3eEtIWfxtQ5MIony4XbGf1/t2A5ZJTZe33XENluJyW5GVy6ohSA14+mVmpmV0M3niwXS0tyuWJVGT6/SbmyzaOt/ZR7MsnPcrOsLM+qmEmtN8mzSffgaNJ+A51pzr3CGNMEYH0st64vBupDHtdgXZtERO4Ske0isr2tLfV+fT3bXbyshJvXL+J7Lx+jN2xDUrSOkLbwtr9dMVoP2OyNTNEqZk53DlJTnI2IsLgwm7qSnJQLnLvru1lfXYjDIWysKcST5Uq51MzR1n6Wl+cBkJ3hZElxjvb1mUd3/HAbf/Gzt5Py2oleUJUI1yK+LRlj7jfGbDHGbCkrK0vwMFQi/MkldYyM+Xk+LPcerZe7zRN21F7nwGjUGndbsNY9SsVMfecgS4pzgl9fsryUbSc6U2b7/LDXx6HmPtbXFACB0tJLl5XyyjttKbM2YIzhWFs/y8rygtdWlHs0LTNPjrf1s+9MLxtqCpPy+jMN7i0iUgVgfWy1rjcANSGPqwYaZz48NZ82LSlkcWE2T+2e+J+we6qZe9hRe92DXoqjPNZWkptBltsRMS1jjKGhayiYugG4dFkp/SNj7G5IjXLD/Y09+PyG9dWFwWtXrCqjsWc4uN1/vrX1jdA3PBacuQOsrMjjZPsAo2Op8SZ5NnlqdxMicNP5i5Ly+jMN7k8Cd1qf3wn8OuT67SKSKSJLgRXAm7MbopovIsJ7z6/i1SPtwWP1IDBzdzuFbHfktXJP2FF78czc7XRLpHLIrkEv/SNj1ITM3C9eVgKkTiuC3fWBN5n1IbOwy1cGfiNNldSM/SYzMbh7GPObmPsaVOIZY3hy9xm21hVTWZCVlJ8RTynkz4A/AKtEpEFEPg18A7hORI4A11lfY4zZDzwGHACeBe42xqR2FyUV083nL2LMbyb0nLE3MIlEysJNXlDtGhyleIqcO8DiopyIaRl7N2poWqY4N4O1Vfkp04pgd0M3lflZVOSP/0NdXJjN8vK81AnubZOD+4qKwOe6qDq3DjX3caxtgJvXJ2fWDvFVy3zEGFNljHEbY6qNMT80xnQYY64xxqywPnaGPP4+Y8wyY8wqY8wzSRu5mhPnLs6ntiSHp/eM90rpidLu15aX6WbI62PM52fY62Nw1DflzB3G+7qHG69xz55w/dLlJew81Z0SXRh313cH8+2hrlhZxrYTnXGPsaN/hOae4RmN4XTHIBu/9ju2HY9cRXS0tR9PpotyT2bw2rKyPByiDcTm2lO7G3E6JLjjOhl0h6qKSUS46fwq3jjWQUf/CBC9aZjN7uk+MOKja3DqGnfb4sJsOgdGGQw7xcnuBlkTknOHwKLqqM/P9lPhlbpzq3twlJMdgxNSMrYrVpYxOubnjydil22OjPn43svHuPwfX+TWb78+o12jj22vp2vQy6Nv1Uf8/rG2fs4pz5vwG1eW20ltSW7K7RlIZ8YYntrTyKXLSynJy5z6CTOkwV1N6abzF+HzG56x2thOFdzt5mG9w95g07Bo7X5DVUfpDlnfOURJbga5YUf6ba0rxuWQea93txd1N4Qsptq2Li0m0+XglRipmRcPtXLDt17lG88cYmlZLs29w/z+UGvUx0fi9xt++fYZAH53oCXi5rOjrf0sD6mUsa0oz+NIEipmjDE8s7eJfWd6UqZiKBXsbuihvnOIm8+vSurP0eCuprS60sOyslye3hOomukeGqUwxkzcPle1f2RsvN1vnDN3gIaw1ExD1yDVxTmTHp+b6WLjkkLemOe8++76bkTg3OrJaZkst5OLzimJmHc/3NzHp378Fp/88VsI8ONPXsCvPnspFfmZPPLm6WmNYduJTs50D3Hb5mr6R8Ym/bzeYS8tvSMT8u22VZUeTnYMRHxDmI0HXz/Jn/90Jzf9f69xw7de5XsvH5txyimdPLW7kQyng3evS15KBjS4qzgEUjOL2Haik5be4Slz7uOnMY2FzNzjybkHAnh4xczpzkFqirIjPYVLlpWy90xPcNfsfNjT0M2ysjzysyL/nVyxsozjbQOcbB/gjWPtfP3pA1z1zy9x/bdeYdvxDv76xtU8+4XLuXJVOS6ngw9truGld9qmPLwk1OM7G8jLdPH3N6+lODdjwhoJjPeUiRTcV1R48FvH7iXKq0fauO+/DvDutRX8w63nkpvp5BvPHOLib7zAHT/Yxo5TXQn7WQuJ3294ek8jV6wqi/lvKBE0uKu43Ly+CmPg6T1N9A6PRe0rA6HnqHrHc+5xBPdyTyZup0xIy/j8hsbuoQmVMqHetaIUY+DFw9NLYySKMYZd9T0T6tvD2SWR1/7ry3z0gW385A+nWFKcw9dvWcdLX76Kuy5fRoZr/J/ihy8IbBV5LEruPNzg6BjP7G3ivedV4clyc8O5lbxwsGXCIq5dBrmsLHfS81daFTOJ2ql6sn2Azz38NisrPHzzwxu446Janvjspbz4/17JX1y9gqOt/Xzih9t4+/TZF+DfOtlJS+9IUqtkbBrcVVyWl3tYXenhZ1a6INL5qbbQo/bsmXusx9scDqGqYGLFTHPvMF6fmVDjHmrTkiIWF2bz+M6GuO8lkRp7hmnvH4lYKWNbVpbLf9tSzfs3LuZ7d2zm7b+7joc+tZWPX1xHmWfyglpNcQ6XLS/l59vr8cVx4Mdv9zczMOrjA5sCnT5uOr+KwVHfhLz9sbYBMpyOiG+SS0tzcTokIeWQfcNe/vQ/tuMQeOATWyaskywtzeUvr1vJrz93KaWeTO588E0ONMZ/KPtC4fcbHt52OuKBMk/taSTb7eTaNeURnplYGtxV3G5evyg4A4z1K2V+yGlMXQOj5Ge5cMVoGhYq0Nd9/B+F3Q0yvFLG5nAIH9y0mNeOttPUE38aI1GCnSBjzNxFhH+8bT3/9KH13HBu5aSF4Ug+snUJjT3DMRdibU/sPENNcTYX1BUDcOHSEso8mcE1EgjM3OtKcyL+d8h0OakryZl1GwK/3/DFR3dxon2Ab39sU9Q35Ir8LP7z0xeSm+ni4z/cFnEHr9fn55dvN/DoW9Nbe4jlcHMft333DT7+w208tr1+Us+kRNl5uou//uVe3vNvr/LoW6eDi8ljPj+/2dvMNWvKycmY+v+B2dLgruJ2U8jqfjylkP0jY3QOeuPKt9vC+7rbNe7R0jIAH9hUjTEEq0Xm0u76bjKcDlZXeRL6uteuqaA0L4OHp1hYbe4Z5rWj7bx/YzUOR6DE0ekQbjy3kt8fag02cDvW1h8x325bWTH7U5n+9bl3eP5gK39/81ouWVYa87E1xTn8559eiAjc8YNtwVnusNfHf/7xFFf980t88dHd3PPE3oS0b3h8RwO3fPs1TnYMcqpjkP/xiz1s+frz3PUf23l6T2NC90rssaqnVlV6+KvH9/Lph7bT2jvMG8c66BwYnZOUDGhwV9NQW5LLeYsD6YdofWUAst1OnA6hf3iM7sGpWw+EWlyUTWvfCCNjgX9sDZ2DOASqCqNv0a4rzWVLbRGP72iY85K7XfXdrFmUT6YrsccWZLgcfHBzNb8/1Eprb/QKk1++fQZj4AMbJzZfvWn9IkbG/LxwsIWRMR+nOgYmNAwLt6LCw6nOwRlVzOw708Of/cd2/u+LR/nI1ho+flFtXM9bVpbHf3zqQgZHx/jYD7bx3ZcCdf5/+6t9lOZl8s0PryfD6eAHrx6f9phsw14f9z6xhy/9fDcba4r4zecv4+UvX8mv7r6UOy6q5e36bj738Nts+vpz/D8/2cEv326Y1IPf6/Ozv7GHR948zc+3T70OsvdMDxX5mfz8Mxfzdzet5fWj7bz7W6/wL8+9gyfTxRUr56ZRYvJ/N1Bp5eb1Vew90xNzNi4i5GUGmod1DoxSmR9/7wy7Yqape5i60lxOdw5SVZAdsxc8wG2bq7nnib3sbuhJWpe9cPaxerdtrk7K699+wRK+//Jxfr6jgbuvWj7p+8YYntjZwJbaIupKJy6Ubl5SRGV+Fk/tbmJ1ZT5+E7lSxrayIg9j7MOzo68fhNrf2MO/PX+E3x1oIT/LxZeuW8lnrlgWtS1FJGsX5fPQp7Zyxw+28X+ePcTF55TwzQ9v4JJlJYgIb53s4hfbG/jLd6+k3DO9HiynOgb48//cyYGmXu6+ahlfvHZlMC21oaaQDTWF/M1717DteAfP7m/mt/ubeXZ/My6HcPGyEpYU57CvsZeDTb0TGqu9a0VZzH4we8/0cN7iAhwO4VOXLeXylWV86bFd7K7v5gObFpMVpSdTomlwV9PyiYvrqCvJ5ZwYs0AILKr2jQRy7muq8uN+/fG+7kPUleZS3xW9UibUjedX8fdP7ufxHQ2zDu4n2gd46XArRTkZFOcG/pTkZZDlctLaN0JTzxDNPcMca+tnYNQXcWdqIiwtzeXic0p45K3T/PkVy4JpF9u+M70cae3nf73/vEnPdTgCTd9+8odTwcW7WDN3+1SmI619Uwb3zoFR7n1iD7/d34Iny8UXr13JJy+ri1oKOpWNS4r41d2XMhjh7/LP3nUOP3vzNA+9cZIvX7867tfs6B/hff/3dQAe/JMtXL26IuLjnA7hkuWlXLK8lK/cvI7dDd08u7+Z3+1vYdfpbtYuyufOi2s5d3EBLoeDux/eyY5TXbw3ygak/pExjrX1c3NIp8fl5Xk8/ueX8OtdjVy2Ina6KpE0uKtpyXI749p8YTcP6xwcnVbOPbhLtTuQg63vHOTKVVP/Gpuf5eb6dZU8ubuRv71pzazSJP/w9AFeiHOHaHVR9pT55dm4fWsNn39kF68fa+ddKyb+PTy+s4EMlyNqoLnp/Cp++NoJ7n/lOCKxg3tdSS4uh8S1qPrtF4/ywsFWPn/NCj512dKE1GuvqIi8ZrG0NJfr11bykz+c4rNXLo9rMRoC6aqeIS//9d8vY92i+H4TcTiEjUuK2LikiHvfs2bS970+P1luR8zgfqCxF2PgvOqJExqXM5Bmm0sa3FVSeLJctPWNMOz1x8zPh6ssyMIhgRYEw14frX0jUStlwn1wczVP7m7khYOt3HjezLZ2D46O8drRdj564RI+delSOgdG6RwYoXPAy5DXR7knk6qCLCoLsij3ZE2oT0+G69dVUpjj5sHXTlCYnUF2hoPsDBcZTgdP7m7kurUVUYPrhppCqouyOd4+QHVRNtkZ0d/wMlwOlpbmTrmoOuz18fjOBq5fV8kXr1s5q3uL111XnMOz+5t55K16Pn3Z0ikfb4zhse31bFxSGHdgj4fb6eD86kJ2xKjP33smsJgab2ormTS4q6TwZLmDZYLxtPu1uZ0OKvOzaOgaCpZELimJL7hftryUivxMHt/REDG4H23toyI/K7iDNpLXjrQzMubnpvOqYuao50qW28mHNlfzwKsnePHw5LLID26KeIolMN6P//svH485a7dtri3i17sa6YrRf/83e5voHvTysQuXxH8Ts7RpSREX1BXx4Gsn+MTFtVOuv+xp6OGdlsjpqtnaXFvEA68cZ9jri5g732u1fp7u+kAyaLWMSoq8TBcdA/HvTg21uCibhu4h6jsDJZHVcc7cnQ7h1o2LeemdNtr6RoLXx3x+/um3h7jum6/wP3+1L+ZrPH8wkEe+YGnxtMacTF969yoe/tMLeeATW/i32zfwfz54Hl+5eS3/+wPnceXK2Jth7NxvPG9Un75sKUNeHz9642TUxzy87XRgLcA6LGWufObyZZzpHuI3e5umfOxj2+vJcju4aX3iG3NtXlLEmN8Eyx3D7T3TkxKzdtDgrpLErnWH+PrKhLJr3YOtfosj95WJ5LZN1fj8hl/vCtS8N3YP8ZEH/si3XzxGZX4Wz+xrjrp5xe83/P5QK1euKp9ydjiXstxOLlleynVrK7hlw2I+fMES/uTSpXxk65JJi6zh1i3K58vXr+L2C2piPg4Cee/r11Xw49dPTDjg3Ha4uY/tp7r46NYl06qISYSrV5ezrCyX7718PGa567DXx5O7G3nPuVUzXuCNZVNtEUDENtP9I2Mcbx8IlgvPt9T5P1ilFU9IcI+nI2So6qIcmnuHOdE+QJbbQdk0el6vqPCwvrqAx3ee4fkDLdz4769yoLGXb314A9+7YzMjY37+a0/k2d+uhm7a+0fnZGv4XBER7r5qedQFy3CfvXI5vcNj/PSPpyZ97+Ftp8iYh4VBCCx23nX5ORxs6uW1GEcr/nZ/M33DY3xoS3LGWJybwTllueyM0Phs/5kejIHzI3QHnQ8a3FVSeDJnMXMvysbnN2w/1UlNUc60Z4kf3FzNwaZe/vQ/trOoIJun/uIybt24mPOrC1henscvdkTuQ/P8gRZcDpky1ZHO1tcU8q4VpTzw6okJG5qGRn088fYZbjyvctr/PRPl1o2LKfNkcv8r0Tc1/Xx7A9VF2Vy0NHlpo81LithxqmvSbxCptJgKGtxVktjNw0RityqIxK5139/YG7U/SSzvW7+I2pIc7ry4lic+e0mwJl9EuG1zNTtOdUU8EPr5gy1cUFdMwTSqe9LRZ69cTnv/yITdmE/taaRveIyPXhjf7tNkyHQ5+fRlS3n1SDsPb5vclqGha5DXj7Xzoc01U6arZmNzbRFdg95J/w/tPdNDVUFWxGZw80GDu0oKuyKlINuNc5r/0Oxad2Ni95SJpjAng5e/fBVfveXcSRUN79+4GIfAE2FdJE93DPJOSz/Xro282eVsctE5xWxaUsj3Xj4ePO7vp9tOs6I8jwvqiuZ1bJ++bClXrCzjf/5636Q2z4/vCKyzfHBz9AqiRNhs5d3De9Kn0mIqaHBXSWIvqE6nDNK2qHB8AbU6yiEdM1WRn8VlK8p4YucZ/CHtdJ8/2AKQVvn2mbLz9Ge6h3hyVyP7zvSwu76bj1449wup4dxOB9/+2CZWVXi4+6c72WelQvx+w8931HPpstK4q6tmKnAwi2tCcO8bDszkU2UxFTS4qySxF1SnWwYJgeqQUmsRdSZpmanctrmaM91D/PH4+Nmrzx9sYWVFHrUlkw+zOBtdvbqc1ZUevvPSUX667RSZLgcf2Dj3C6mR5GW6+NEnL6Aw280nf/wWDV2D/PFEBw1dQ0lbSA3lcAiba4smBPf9wZ2pGtxVmvNkBtIy062UsS22ZuwzSctM5d1rK/BkufiFlZrpGfLy5olOrlmjKRmbPXs/1jbAI2/Vc/P6RSm1FlGRn8WPP7WVYa+PT/7oLX70+kk8WS6uT/K5pLbNtUUcae0PHu9o/wahM3eV9oJpmdyZBQQ7HZOMmXuW28lN51fx7L5mBqzDpMf8hms1uE9w43lV1JXkYAx8dA53pMZrZYWH79+xmZMdAzx3oIX3rV80Zx0X7Xr3nfWB2fuehsBiauk0ynaTbVbBXUROisheEdklItuta8Ui8pyIHLE+zu8KjJoXwbTMDGfuFy4tZn1NYbDqJtFu21zN4KiP3+xt4vkDLZTmZcxZq+CFwukQvnrLuXzy0jo2pujfzSXLS/mn29ZTkpvBx+awkmd9dSFOh7DjZCC477Pa/KaSRPzLucoYE7qr4B7gBWPMN0TkHuvrv0rAz1ELSH6Wm6IcN8tm2J/lExfX8YmL6xI7qBCblhSxtDSXR9+q53BLHzesq5x2Vc/Z4IqVZXN2uMRM3bpxMe9bvyip5Y/hcjNdrKnysONUF33DXo63D/D+jcmt0pmuZKRlbgEesj5/CLg1CT9DpbgMl4M37rmG2zalxiJcOBHhAxsXs/1UF33DY1oCucDNZWC3baktZld9N7vrrc1LKbSYCrMP7gb4nYjsEJG7rGsVxpgmAOtjxNoyEblLRLaLyPa2tqkPAVYLT3aGc17+0cXrA5urEQm8Eb1rDg9RUOlhU20RQ14fP98R2OyVbmmZS40xjSJSDjwnIofifaIx5n7gfoAtW7bM7cGXShHYCXvDukqy3c45OY1epRd7M9Nv9jaxKMUWU2GWwd0Y02h9bBWRXwJbgRYRqTLGNIlIFRDfkTZKzYPv3rF5voegFqhFBVlU5mfR3DucUvXtthmnZUQkV0Q89ufAu4F9wJPAndbD7gR+PdtBKqVUqhGR4Ow91VIyMLucewXwmojsBt4E/ssY8yzwDeA6ETkCXGd9rZRSaccO7qnUU8Y247SMMeY4sD7C9Q7gmtkMSimlFoJbNiyiqWeIi86Z25Op4qGrSEopNUMleZn8zXvXzvcwItL2A0oplYY0uCulVBrS4K6UUmlIg7tSSqUhDe5KKZWGNLgrpVQa0uCulFJpSIO7UkqlITFm/hsyikgbcGoWL1EKtE/5qIUhne4F0ut+0uleIL3uJ53uBeK/n1pjTMTTVFIiuM+WiGw3xmyZ73EkQjrdC6TX/aTTvUB63U863Qsk5n40LaOUUmlIg7tSSqWhdAnu98/3ABIone4F0ut+0uleIL3uJ53uBRJwP2mRc1dKKTVRuszclVJKhdDgrpRSaWhBB3cRuUFEDovIURG5Z77HM10i8qCItIrIvpBrxSLynIgcsT4WzecY4yUiNSLyoogcFJH9IvJ56/pCvZ8sEXlTRHZb9/NV6/qCvB8AEXGKyNsi8rT19UK+l5MisldEdonIduvagrwfESkUkV+IyCHr38/FibiXBRvcRcQJfBt4D7AW+IiIpOaRKNH9GLgh7No9wAvGmBXAC9bXC8EY8CVjzBrgIuBu67/HQr2fEeBqY8x6YANwg4hcxMK9H4DPAwdDvl7I9wJwlTFmQ0g9+EK9n38DnjXGrCZwdOlBEnEvxpgF+Qe4GPhtyNf3AvfO97hmcB91wL6Qrw8DVdbnVcDh+R7jDO/r1wQOSF/w9wPkADuBCxfq/QDVVpC4GnjaurYg78Ua70mgNOzagrsfIB84gVXcksh7WbAzd2AxUB/ydYN1baGrMMY0AVgfy+d5PNMmInXARmAbC/h+rDTGLqAVeM4Ys5Dv51vA/wD8IdcW6r0AGOB3IrJDRO6yri3E+zkHaAN+ZKXMfiAiuSTgXhZycJcI17Suc56JSB7wOPAFY0zvfI9nNowxPmPMBgKz3q0icu48D2lGROQmoNUYs2O+x5JAlxpjNhFIy94tIpfP94BmyAVsAr5rjNkIDJCgdNJCDu4NQE3I19VA4zyNJZFaRKQKwPrYOs/jiZuIuAkE9p8aY56wLi/Y+7EZY7qBlwisjyzE+7kUeJ+InAQeAa4Wkf9kYd4LAMaYRutjK/BLYCsL834agAbrt0KAXxAI9rO+l4Uc3N8CVojIUhHJAG4HnpznMSXCk8Cd1ud3EshdpzwREeCHwEFjzL+GfGuh3k+ZiBRan2cD1wKHWID3Y4y51xhTbYypI/Dv5PfGmDtYgPcCICK5IuKxPwfeDexjAd6PMaYZqBeRVdala4ADJOJe5ntBYZaLETcC7wDHgL+Z7/HMYPw/A5oAL4F38E8DJQQWvo5YH4vne5xx3stlBNJie4Bd1p8bF/D9nA+8bd3PPuDvrOsL8n5C7utKxhdUF+S9EMhT77b+7Lf/7S/g+9kAbLf+X/sVUJSIe9H2A0oplYYWclpGKaVUFBrclVIqDWlwV0qpNKTBXSml0pAGd6WUSkMa3JVSKg1pcFdKqTT0/wMCbfT0ziqQHwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results_one.VFE_post_run)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "daifa.train_vae = False\n",
    "daifa.model_vae.show_training = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[0.09571323 0.99540895 0.4442995 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7121\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8400\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 14.1267\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1174\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8256\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.2671\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.1093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6156\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5771\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0265\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3656\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2031\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9094\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8529\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.8066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.2960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3149\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 46.5217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.7884\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 64.5753\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 63.1881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.6160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.0624\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 34.4151\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.0488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 35.7451\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.0920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.3152\n",
      "Success in episode 1 at time step 200 with reward -146.24990862816924\n",
      "Episode 2\n",
      "[ 0.07769445 -0.9969772   0.97278094]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.5830\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.8406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.2808\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7710\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0272\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3689\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4203\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8039\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5097\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.9569\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0290\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.9460\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.4903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.0849\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 22.3367\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2028\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.8848\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 22.9174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.4385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.2752\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.4562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.0670\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7253\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8554\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.9192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.9371\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.0492\n",
      "Success in episode 2 at time step 200 with reward -144.26989631949647\n",
      "Episode 3\n",
      "[0.61065173 0.79189926 0.83033407]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.8816\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 7.5190\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6976\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 67.4143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 67.9232\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0875\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9820\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.1535\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.3220\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8291\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8524\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 102.0980\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 99.1679\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7272\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6277\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 108.8843\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 105.2556\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6223\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.2318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.9920\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8648\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 87.5018\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 84.6515\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2657\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3224\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.3050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 32.5640\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7157\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.3346\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.8803\n",
      "Success in episode 3 at time step 200 with reward -173.39537405772273\n",
      "Episode 4\n",
      "[ 0.0680659  -0.99768084  0.2882985 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.9949\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.4551\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.7543\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.8420\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.9808\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.5453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.6763\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7853\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5785\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3431\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4353\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6039\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8773\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.6863\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.7062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.1702\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 156.3249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 153.6223\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 73.5347\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 71.9011\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0082\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0358\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8371\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1164\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.5051\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.1718\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.1667\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.5868\n",
      "Success in episode 4 at time step 200 with reward -166.0716523715925\n",
      "Episode 5\n",
      "[ 0.6694796  -0.74283046  0.36801407]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3430\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3119\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1969\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.6366\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.6639\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.1837\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.2098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7882\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.8628\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.7159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 86.4585\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 85.3206\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8157\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 94.4950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 92.0329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.7660\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.0822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 55.2219\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.2913\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.2504\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 111.8100\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 109.2540\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6877\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5846\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 68.0105\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 68.1463\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.0453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.6024\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.9569\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.4110\n",
      "Success in episode 5 at time step 200 with reward -177.2113014855819\n",
      "Episode 6\n",
      "[-0.7222441   0.69163823  0.14211687]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5332\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7614\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.9459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.6885\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5119\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6561\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7417\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.8570\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.9485\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.2015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.1867\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.1009\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 2.3568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.4490\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 4.6632\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4810\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.1947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 5.1432\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.8725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.6436\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.6713\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.0932\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.5822\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1927\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.1297\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 15.5391\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1752\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9851\n",
      "Success in episode 6 at time step 200 with reward -182.74709800505866\n",
      "Episode 7\n",
      "[-0.1897561  -0.98183125  0.35365564]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.7466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.6946\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.1185\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2756\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6950\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 59.3673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 58.4370\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9395\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9734\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.8514\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.5849\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 61.2020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 59.5078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8673\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.0408\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 18.6038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.5356\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.6053\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2335\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.1850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.8815\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 21.3236\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.5671\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 42.8169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.4815\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 25.9445\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 65.8287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 64.2479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3216\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.7335\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.1597\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.6669\n",
      "Success in episode 7 at time step 200 with reward -146.53630499653636\n",
      "Episode 8\n",
      "[0.5563779 0.8309294 0.64184  ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1875\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.8428\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 26.5255\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2659\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2718\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.8644\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.4672\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.9754\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.8146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.5176\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.4937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.9899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8381\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.9251\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.9388\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.1902\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 21.5223\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.6784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.6695\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5718\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5874\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.1049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0131\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3941\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.5458\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2510\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0445\n",
      "Success in episode 8 at time step 200 with reward -169.678279498827\n",
      "Episode 9\n",
      "[-0.0054356  -0.9999852  -0.08560528]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.6334\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.9679\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9426\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.3070\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.7615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1289\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7681\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 55.6768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 54.8125\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 67.6278\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 67.3681\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9373\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.5688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.4531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.1733\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.9404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.4861\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.8421\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.3676\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5631\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.6954\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.7258\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9757\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9581\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.0464\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.9622\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.8918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.6642\n",
      "Success in episode 9 at time step 200 with reward -141.92871413968706\n",
      "Episode 10\n",
      "[0.5844306  0.81144375 0.82857573]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.6851\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.5018\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.1391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.1794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.9936\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.6479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4599\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.0208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.2657\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.3560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2790\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 40.4886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.5236\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.7663\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.9789\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 123.1898\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 120.9820\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7672\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1456\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.7305\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.1464\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 43.2430\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7088\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7981\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.2740\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8899\n",
      "Success in episode 10 at time step 200 with reward -175.3071083589534\n",
      "Episode 11\n",
      "[ 0.31771794 -0.94818527  0.32989892]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6710\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4212\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 78.9511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 79.6301\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.4975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.3265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.6298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 18.1986\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6452\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7140\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8410\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6226\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9033\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7805\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8375\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8047\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4252\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1656\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6634\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.3141\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3133\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3057\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 72.0747\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 70.8353\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0713\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9213\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5685\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.5037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.9096\n",
      "Success in episode 11 at time step 200 with reward -167.26623597116821\n",
      "Episode 12\n",
      "[ 0.2036931  -0.9790348  -0.27860343]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2900\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.2503\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7238\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2801\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8232\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.0376\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.0614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7233\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.8417\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 27.4900\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0999\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4372\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9656\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 93.4470\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 90.1957\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.3414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.1654\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2224\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3644\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.7576\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.5207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7510\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 79.9739\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 77.3520\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.4713\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.4812\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1907\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7693\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 18.7839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.3813\n",
      "Success in episode 12 at time step 200 with reward -146.52639578768563\n",
      "Episode 13\n",
      "[ 0.8055938   0.59246826 -0.3699466 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1417\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2909\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0213\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.1253\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.8971\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6220\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7905\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 101.5424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 100.6914\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8296\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6737\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 67.9454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 65.4689\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.0181\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.1819\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.1525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.1105\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.7187\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.1802\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.5205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.5429\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.8885\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.3520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.6748\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.7852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.0676\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.9041\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.1880\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.2348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.7222\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.5791\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.1641\n",
      "Success in episode 13 at time step 200 with reward -172.98424149983126\n",
      "Episode 14\n",
      "[ 0.7876127  -0.61617064  0.24429409]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.9790\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 19.3170\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.7114\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.0260\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3337\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0882\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7717\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 116.0171\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 116.3313\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.1971\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.9090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3970\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.7574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.4841\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.9365\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.8615\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.3540\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4202\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1126\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.4456\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.8222\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0368\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9313\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.6590\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.6597\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1460\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.5392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.0227\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.4373\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.2438\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.0082\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.6053\n",
      "Success in episode 14 at time step 200 with reward -152.90343180781298\n",
      "Episode 15\n",
      "[ 0.82244503  0.56884456 -0.048824  ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.1410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6949\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5684\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1818\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5583\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.5310\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.3756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.5335\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0593\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.2942\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.9996\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7849\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7571\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.7101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 28.6668\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1245\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1750\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0465\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9786\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6456\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.9082\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 30.5929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5197\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.6801\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.3781\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.9577\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6627\n",
      "Success in episode 15 at time step 200 with reward -170.02320334483093\n",
      "Episode 16\n",
      "[ 0.06186089 -0.9980848  -0.11236466]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2666\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.5147\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7625\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5964\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3224\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.4517\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1269\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.8347\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.3806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.5178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 45.1854\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.3277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.8678\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.9105\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.0448\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.6543\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.0603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.6582\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.1929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.0771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.9914\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.1627\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.5617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.3305\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.4213\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7279\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4651\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4922\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.1908\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.9429\n",
      "Success in episode 16 at time step 200 with reward -166.62506398645135\n",
      "Episode 17\n",
      "[-0.9813862 -0.1920447  0.3557799]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.8100\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 31.2992\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.0959\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5091\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.3588\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3009\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.2514\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1404\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.6196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.6989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0362\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.2961\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.8044\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.9036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.7302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.0874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.2716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9976\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9623\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.2088\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.3173\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2892\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7833\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5018\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.0304\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.2647\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 32.6391\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.7776\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.3512\n",
      "Success in episode 17 at time step 200 with reward -196.4872676447894\n",
      "Episode 18\n",
      "[ 0.972934   -0.23108311  0.01983112]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2461\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0286\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3862\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.4580\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.2639\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5364\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3920\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2526\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.4313\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7736\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6542\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8267\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7370\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.4578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.4955\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.8392\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6559\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.7173\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8830\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.8583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.1352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.1932\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4238\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2408\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.1928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.9394\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8775\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.0920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 24.9107\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0695\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.8832\n",
      "Success in episode 18 at time step 200 with reward -177.54381595809255\n",
      "Episode 19\n",
      "[-0.0766677  0.9970567  0.4759225]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4178\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2111\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.9785\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.6240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.5818\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 89.8892\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 87.6398\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.7816\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 47.0382\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5586\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5756\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.0520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.1197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8823\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9501\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8494\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1237\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9807\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4509\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0058\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8313\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1034\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 16.7173\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.8424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.3926\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2586\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0423\n",
      "Success in episode 19 at time step 200 with reward -165.74838857310053\n",
      "Episode 20\n",
      "[ 0.73259234 -0.6806677   0.7166661 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6423\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.5634\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 22.2637\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.2745\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6811\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.4847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 94.4422\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 93.9450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 57.3625\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.7090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4977\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7752\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0985\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4438\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2331\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2113\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0803\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5821\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6827\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8436\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.9134\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8149\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6082\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.5634\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5702\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4816\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8361\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7302\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.7788\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.6395\n",
      "Success in episode 20 at time step 200 with reward -149.48577804224394\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "daifa, results_two = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# # make the HABIT ACTION NET\n",
    "# habit_net = HabitualAction(latent_dim, 1, [16, 16], train_epochs=2, show_training=True)\n",
    "# habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "#\n",
    "# daifa.habit_action_model = habit_net\n",
    "#\n",
    "# actor_model = get_actor(latent_dim, 1)\n",
    "# critic_model = get_critic(latent_dim, 1)\n",
    "#\n",
    "# target_actor = get_actor(latent_dim, 1)\n",
    "# target_critic = get_critic(latent_dim, 1)\n",
    "#\n",
    "# # Making the weights equal initially\n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "#\n",
    "# critic_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "# actor_optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "#\n",
    "# habit_net = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau=0.005, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9999956]]\n",
      "[[-0.9999276]]\n",
      "[[0.9717017]]\n",
      "[[-0.9992425]]\n",
      "[[-0.9999996]]\n",
      "[[-0.9070271]]\n",
      "[[0.9934823]]\n",
      "[[-0.99998575]]\n",
      "[[-0.9999999]]\n",
      "[[-0.99995726]]\n",
      "[[0.96853244]]\n",
      "[[-0.99973077]]\n",
      "[[-0.9999995]]\n",
      "[[-0.9987053]]\n",
      "[[0.99550587]]\n",
      "[[-0.99998015]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.9999598]]\n",
      "[[0.9614532]]\n",
      "[[-0.99989307]]\n",
      "[[-1.]]\n",
      "[[-0.99993795]]\n",
      "[[0.99822277]]\n",
      "[[-0.999492]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.9999989]]\n",
      "[[0.08209892]]\n",
      "[[-0.9147555]]\n",
      "[[-0.9999975]]\n",
      "[[-0.9926654]]\n",
      "[[0.99749386]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.99980557]]\n",
      "[[0.997134]]\n",
      "[[-0.9999912]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.9999996]]\n",
      "[[0.9968242]]\n",
      "[[-0.9182594]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.9999851]]\n",
      "[[0.9980948]]\n",
      "[[-0.9999706]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.99932474]]\n",
      "[[0.9721099]]\n",
      "[[-0.9999932]]\n",
      "[[-0.9999997]]\n",
      "[[-0.95777595]]\n",
      "[[0.99579597]]\n",
      "[[-0.9999931]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.98789483]]\n",
      "[[0.7865886]]\n",
      "[[-0.9999995]]\n",
      "[[-0.9999993]]\n",
      "[[-0.99999934]]\n",
      "[[-0.39880198]]\n",
      "[[0.98633057]]\n",
      "[[-0.99997514]]\n",
      "[[-0.9999986]]\n",
      "[[-0.9910445]]\n",
      "[[0.9718688]]\n",
      "[[-0.99998194]]\n",
      "[[-0.99998915]]\n",
      "[[0.9943338]]\n",
      "[[0.576213]]\n",
      "[[-0.99932164]]\n",
      "[[0.04013797]]\n",
      "[[0.9610137]]\n",
      "[[-0.9999848]]\n",
      "[[-0.99983245]]\n",
      "[[0.9976138]]\n",
      "[[-0.9415957]]\n",
      "[[-0.9998865]]\n",
      "[[-0.96895653]]\n",
      "[[0.9702456]]\n",
      "[[-0.999901]]\n",
      "[[-0.9997694]]\n",
      "[[0.99614114]]\n",
      "[[-0.59479284]]\n",
      "[[-0.99934334]]\n",
      "[[-0.20541593]]\n",
      "[[0.8921975]]\n",
      "[[-0.99996275]]\n",
      "[[-0.99951917]]\n",
      "[[0.9979631]]\n",
      "[[-0.95988744]]\n",
      "[[-0.9998856]]\n",
      "[[-0.73357624]]\n",
      "[[-0.99981964]]\n",
      "[[-0.30885366]]\n",
      "[[0.73069173]]\n",
      "[[-0.99998087]]\n",
      "[[-0.99912494]]\n",
      "[[0.99755234]]\n",
      "[[-0.9867451]]\n",
      "[[-0.99992603]]\n",
      "[[-0.7041926]]\n",
      "[[0.9686564]]\n",
      "[[-0.99997884]]\n",
      "[[-0.9998606]]\n",
      "[[0.99791574]]\n",
      "[[-0.5930599]]\n",
      "[[-0.99991494]]\n",
      "[[0.06718258]]\n",
      "[[0.9387522]]\n",
      "[[-0.9999809]]\n",
      "[[-0.9999376]]\n",
      "[[0.9962859]]\n",
      "[[-0.29472348]]\n",
      "[[-0.9999483]]\n",
      "[[-0.99057883]]\n",
      "[[0.9632474]]\n",
      "[[-0.9998591]]\n",
      "[[-0.9998727]]\n",
      "[[0.9959834]]\n",
      "[[0.23513052]]\n",
      "[[-0.99541414]]\n",
      "[[0.93786293]]\n",
      "[[-0.7769086]]\n",
      "[[-0.9999557]]\n",
      "[[0.5962132]]\n",
      "[[0.99305046]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.9973103]]\n",
      "[[0.9954074]]\n",
      "[[-0.99999464]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.99999774]]\n",
      "[[0.9979823]]\n",
      "[[-0.99923235]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.99562573]]\n",
      "[[0.9939697]]\n",
      "[[-0.9999933]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.99957526]]\n",
      "[[0.994537]]\n",
      "[[-0.999997]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.06288289]]\n",
      "[[0.9863208]]\n",
      "[[-0.9999992]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.99652916]]\n",
      "[[-0.9576817]]\n",
      "[[-1.]]\n",
      "[[0.99134016]]\n",
      "[[0.98999316]]\n",
      "[[0.9903632]]\n",
      "[[0.9917839]]\n",
      "[[0.99070877]]\n",
      "[[0.9927132]]\n",
      "[[0.9906444]]\n",
      "[[0.99291366]]\n",
      "[[0.9916224]]\n",
      "[[0.9947032]]\n",
      "[[0.99260575]]\n",
      "[[0.9918383]]\n",
      "[[0.98713815]]\n",
      "[[0.9941741]]\n",
      "[[0.99394184]]\n",
      "[[0.9930869]]\n",
      "[[0.9907639]]\n",
      "[[0.9904016]]\n",
      "[[0.99171525]]\n",
      "[[0.99395865]]\n",
      "[[0.99234146]]\n",
      "[[0.9921015]]\n",
      "[[0.9871289]]\n",
      "[[0.98891336]]\n",
      "[[0.99041295]]\n",
      "[[0.99370444]]\n",
      "[[0.9928709]]\n",
      "[[0.99263525]]\n",
      "[[0.986326]]\n",
      "[[0.9902078]]\n",
      "[[0.9910443]]\n",
      "[[0.99372005]]\n",
      "[[0.9932685]]\n",
      "[[0.9854873]]\n",
      "[[-0.9994103]]\n",
      "[[0.596455]]\n",
      "[[0.26605684]]\n",
      "[[-0.99999386]]\n",
      "[[-0.99933714]]\n",
      "[[0.9980128]]\n",
      "[[-0.97784096]]\n",
      "[[-0.9999708]]\n",
      "[[-0.9501872]]\n",
      "[[0.97102237]]\n",
      "[[-0.9999298]]\n",
      "[[-0.9998591]]\n",
      "[[0.9961595]]\n",
      "[[0.17976071]]\n",
      "[[-0.998833]]\n",
      "[[0.9128761]]\n",
      "[[-0.6446633]]\n",
      "[[-0.99995816]]\n",
      "[[-0.13995789]]\n",
      "[[0.9929673]]\n",
      "[[-0.9942221]]\n",
      "[[-0.9137519]]\n",
      "[[0.9757693]]\n",
      "[[-0.99669755]]\n",
      "[[-0.99800223]]\n",
      "[[0.9968231]]\n",
      "[[0.28914866]]\n",
      "[[-0.9481416]]\n",
      "[[0.9820952]]\n",
      "[[-0.95581454]]\n",
      "[[-0.9995417]]\n",
      "[[0.995074]]\n",
      "[[0.980031]]\n",
      "[[0.6626351]]\n",
      "[[0.9916003]]\n",
      "[[0.9922122]]\n",
      "[[0.9911479]]\n",
      "[[0.9890499]]\n",
      "[[0.99185693]]\n",
      "[[0.99191546]]\n",
      "[[0.99139965]]\n",
      "[[0.992837]]\n",
      "[[0.9915667]]\n",
      "[[0.98766536]]\n",
      "[[0.99105626]]\n",
      "[[0.99311143]]\n",
      "[[0.9921823]]\n",
      "[[0.9923008]]\n",
      "[[0.9931401]]\n",
      "[[0.9899731]]\n",
      "[[0.9918045]]\n",
      "[[0.99260926]]\n",
      "[[0.9928806]]\n",
      "[[0.99088323]]\n",
      "[[0.99208647]]\n",
      "[[0.9917896]]\n",
      "[[0.99140614]]\n",
      "[[0.99047637]]\n",
      "[[0.99242425]]\n",
      "[[0.9926297]]\n",
      "[[0.99270236]]\n",
      "[[0.9935064]]\n",
      "[[0.99124366]]\n",
      "[[0.99224615]]\n",
      "[[0.9931939]]\n",
      "[[0.9921612]]\n",
      "[[0.993222]]\n",
      "[[0.9927053]]\n",
      "[[-0.98058444]]\n",
      "[[0.9471023]]\n",
      "[[0.5257675]]\n",
      "[[-0.9999932]]\n",
      "[[-0.997658]]\n",
      "[[0.99775386]]\n",
      "[[-0.9945362]]\n",
      "[[-0.9999488]]\n",
      "[[-0.3952415]]\n",
      "[[0.9622718]]\n",
      "[[-0.99998957]]\n",
      "[[-0.99998933]]\n",
      "[[0.997829]]\n",
      "[[-0.5017566]]\n",
      "[[-0.999989]]\n",
      "[[-0.9947158]]\n",
      "[[0.9656434]]\n",
      "[[-0.9999139]]\n",
      "[[-0.9999348]]\n",
      "[[0.99628973]]\n",
      "[[0.9710356]]\n",
      "[[-0.9931586]]\n",
      "[[0.95043325]]\n",
      "[[-0.9275891]]\n",
      "[[-0.99961036]]\n",
      "[[0.9927542]]\n",
      "[[0.9871744]]\n",
      "[[-0.14946884]]\n",
      "[[0.9868808]]\n",
      "[[-0.98160774]]\n",
      "[[-0.9978142]]\n",
      "[[0.99738824]]\n",
      "[[0.9521215]]\n",
      "[[-0.30480537]]\n",
      "[[-1.]]\n",
      "[[-0.99979585]]\n",
      "[[0.9793984]]\n",
      "[[-0.9998692]]\n",
      "[[-0.9999997]]\n",
      "[[-0.9891793]]\n",
      "[[0.9963561]]\n",
      "[[-0.9999909]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.99996346]]\n",
      "[[0.9719703]]\n",
      "[[-0.9999525]]\n",
      "[[-1.]]\n",
      "[[-0.9999135]]\n",
      "[[0.99825835]]\n",
      "[[-0.99994695]]\n",
      "[[-0.9999998]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.99999934]]\n",
      "[[-0.9679901]]\n",
      "[[0.77771527]]\n",
      "[[-0.9999963]]\n",
      "[[-0.9999952]]\n",
      "[[0.9957155]]\n",
      "[[0.09076411]]\n",
      "[[-0.99995434]]\n",
      "[[-0.9607932]]\n",
      "[[0.9774277]]\n",
      "[[-0.9998655]]\n",
      "[[-0.99982214]]\n",
      "[[0.99697804]]\n",
      "[[0.5485399]]\n",
      "[[0.9914636]]\n",
      "[[0.99215055]]\n",
      "[[0.9913149]]\n",
      "[[0.9929244]]\n",
      "[[0.98915315]]\n",
      "[[0.9910347]]\n",
      "[[0.99131644]]\n",
      "[[0.99273145]]\n",
      "[[0.99026036]]\n",
      "[[0.99287814]]\n",
      "[[0.9915789]]\n",
      "[[0.9918995]]\n",
      "[[0.99237126]]\n",
      "[[0.99267644]]\n",
      "[[0.99243826]]\n",
      "[[0.9929074]]\n",
      "[[0.9919681]]\n",
      "[[0.99247235]]\n",
      "[[0.9929115]]\n",
      "[[0.9894603]]\n",
      "[[0.98891824]]\n",
      "[[0.9930501]]\n",
      "[[0.99215597]]\n",
      "[[0.9914875]]\n",
      "[[0.99066895]]\n",
      "[[0.9915249]]\n",
      "[[0.9923414]]\n",
      "[[0.99198085]]\n",
      "[[0.99195886]]\n",
      "[[0.9912091]]\n",
      "[[0.9919198]]\n",
      "[[0.99006355]]\n",
      "[[0.9930564]]\n",
      "[[0.989615]]\n",
      "[[0.97545105]]\n",
      "[[0.986065]]\n",
      "[[0.5781098]]\n",
      "[[-0.9614636]]\n",
      "[[0.99721324]]\n",
      "[[0.98097193]]\n",
      "[[0.96629083]]\n",
      "[[0.96518713]]\n",
      "[[-0.999772]]\n",
      "[[-0.5830214]]\n",
      "[[0.9972043]]\n",
      "[[-0.8314626]]\n",
      "[[-0.8859075]]\n",
      "[[0.9778806]]\n",
      "[[-0.99663633]]\n",
      "[[-0.9985093]]\n",
      "[[0.9975322]]\n",
      "[[0.77355665]]\n",
      "[[-0.48832294]]\n",
      "[[0.97875994]]\n",
      "[[-0.9977879]]\n",
      "[[-0.99823433]]\n",
      "[[0.9975592]]\n",
      "[[0.07103013]]\n",
      "[[-0.90456337]]\n",
      "[[0.9835086]]\n",
      "[[-0.99522465]]\n",
      "[[-0.9956738]]\n",
      "[[0.9972602]]\n",
      "[[0.84811836]]\n",
      "[[-0.13702068]]\n",
      "[[0.9781095]]\n",
      "[[-0.9991127]]\n",
      "[[-0.99718654]]\n",
      "[[0.65381444]]\n",
      "[[0.9804255]]\n",
      "[[-0.99315006]]\n",
      "[[-0.9986981]]\n",
      "[[0.99575204]]\n",
      "[[0.8070336]]\n",
      "[[-0.2353354]]\n",
      "[[0.9806789]]\n",
      "[[-0.99791884]]\n",
      "[[-0.99973404]]\n",
      "[[0.99646914]]\n",
      "[[0.34794024]]\n",
      "[[-0.9686668]]\n",
      "[[0.97445047]]\n",
      "[[-0.98463744]]\n",
      "[[-0.9970687]]\n",
      "[[0.99695903]]\n",
      "[[0.9496135]]\n",
      "[[0.13018511]]\n",
      "[[0.9796521]]\n",
      "[[-0.9989628]]\n",
      "[[-0.99529296]]\n",
      "[[0.99774945]]\n",
      "[[-0.17117248]]\n",
      "[[-0.9779213]]\n",
      "[[0.98033035]]\n",
      "[[-0.8360334]]\n",
      "[[-0.99941057]]\n",
      "[[0.99540865]]\n",
      "[[0.98304194]]\n",
      "[[-0.35787418]]\n",
      "[[0.984455]]\n",
      "[[-0.94969976]]\n",
      "[[-0.99732447]]\n",
      "[[0.99446535]]\n",
      "[[0.9934363]]\n",
      "[[0.99229753]]\n",
      "[[0.98393416]]\n",
      "[[0.9818307]]\n",
      "[[0.9906159]]\n",
      "[[0.9931503]]\n",
      "[[0.99116355]]\n",
      "[[0.9809796]]\n",
      "[[0.99278444]]\n",
      "[[0.9914876]]\n",
      "[[0.9935263]]\n",
      "[[0.99152154]]\n",
      "[[0.9914438]]\n",
      "[[0.98667735]]\n",
      "[[0.99380064]]\n",
      "[[0.9932912]]\n",
      "[[0.9927308]]\n",
      "[[0.99208194]]\n",
      "[[0.9903127]]\n",
      "[[0.98812383]]\n",
      "[[0.99417245]]\n",
      "[[0.9926114]]\n",
      "[[0.9928854]]\n",
      "[[0.99019206]]\n",
      "[[0.99053204]]\n",
      "[[0.98081464]]\n",
      "[[0.9937816]]\n",
      "[[0.99259216]]\n",
      "[[0.9895187]]\n",
      "[[0.98054975]]\n",
      "[[0.98728204]]\n",
      "[[0.98562324]]\n",
      "[[0.9925726]]\n",
      "[[0.988562]]\n",
      "[[0.98698664]]\n",
      "[[0.99062186]]\n",
      "[[0.99185425]]\n",
      "[[0.9891113]]\n",
      "[[0.99301296]]\n",
      "[[0.99196905]]\n",
      "[[0.9926684]]\n",
      "[[0.9935211]]\n",
      "[[0.99315107]]\n",
      "[[0.98851615]]\n",
      "[[0.9876156]]\n",
      "[[0.9919955]]\n",
      "[[0.99245715]]\n",
      "[[0.9918613]]\n",
      "[[0.9929058]]\n",
      "[[0.9905214]]\n",
      "[[0.99294955]]\n",
      "[[0.99176383]]\n",
      "[[0.991802]]\n",
      "[[0.992458]]\n",
      "[[0.99044037]]\n",
      "[[0.99173546]]\n",
      "[[0.9920225]]\n",
      "[[0.9929447]]\n",
      "[[0.99113625]]\n",
      "[[0.99342275]]\n",
      "[[0.9912129]]\n",
      "[[0.9899729]]\n",
      "[[0.9923832]]\n",
      "[[0.99189544]]\n",
      "[[0.99304503]]\n",
      "[[0.9922958]]\n",
      "[[0.9902736]]\n",
      "[[-0.99722]]\n",
      "[[0.99676305]]\n",
      "[[0.9798636]]\n",
      "[[0.5618189]]\n",
      "[[0.9773204]]\n",
      "[[-0.9962047]]\n",
      "[[-0.998412]]\n",
      "[[0.99702686]]\n",
      "[[0.05131394]]\n",
      "[[-0.768127]]\n",
      "[[0.9816179]]\n",
      "[[-0.96827847]]\n",
      "[[-0.9995762]]\n",
      "[[0.99634326]]\n",
      "[[0.9012597]]\n",
      "[[-0.59074956]]\n",
      "[[0.9815795]]\n",
      "[[-0.9948168]]\n",
      "[[-0.9954806]]\n",
      "[[0.9972543]]\n",
      "[[0.67890817]]\n",
      "[[-0.2095784]]\n",
      "[[0.975301]]\n",
      "[[-0.99818486]]\n",
      "[[-0.9956839]]\n",
      "[[0.997387]]\n",
      "[[-0.33992454]]\n",
      "[[-0.9550683]]\n",
      "[[0.97930956]]\n",
      "[[-0.9457797]]\n",
      "[[-0.99973565]]\n",
      "[[0.9969789]]\n",
      "[[0.9819376]]\n",
      "[[-0.4540285]]\n",
      "[[-0.9999998]]\n",
      "[[-0.9999809]]\n",
      "[[0.95006096]]\n",
      "[[-0.99967754]]\n",
      "[[-0.9999996]]\n",
      "[[-0.9997258]]\n",
      "[[0.99800533]]\n",
      "[[-0.99975485]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.99999094]]\n",
      "[[0.95483404]]\n",
      "[[-0.99975324]]\n",
      "[[-1.]]\n",
      "[[-0.99977535]]\n",
      "[[0.9982746]]\n",
      "[[-0.99977994]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.99818075]]\n",
      "[[0.95870245]]\n",
      "[[-0.9999877]]\n",
      "[[-0.99999917]]\n",
      "[[0.69043756]]\n",
      "[[0.9867923]]\n",
      "[[-0.99874187]]\n",
      "[[-0.99427325]]\n",
      "[[0.96209675]]\n",
      "[[-0.99803764]]\n",
      "[[-0.99987626]]\n",
      "[[0.9956053]]\n",
      "[[0.9756478]]\n",
      "[[-0.96607023]]\n",
      "[[0.9919364]]\n",
      "[[0.988558]]\n",
      "[[0.98452824]]\n",
      "[[0.98307174]]\n",
      "[[0.9922443]]\n",
      "[[0.9935942]]\n",
      "[[0.9913543]]\n",
      "[[0.9876469]]\n",
      "[[0.9875843]]\n",
      "[[0.9878279]]\n",
      "[[0.99373597]]\n",
      "[[0.99232745]]\n",
      "[[0.99151284]]\n",
      "[[0.9910585]]\n",
      "[[0.9894474]]\n",
      "[[0.993972]]\n",
      "[[0.99433774]]\n",
      "[[0.9899983]]\n",
      "[[0.9912197]]\n",
      "[[0.9877419]]\n",
      "[[0.9856098]]\n",
      "[[0.9928988]]\n",
      "[[0.9917747]]\n",
      "[[0.98567784]]\n",
      "[[0.9824789]]\n",
      "[[0.98961765]]\n",
      "[[0.9932623]]\n",
      "[[0.9944845]]\n",
      "[[0.9895257]]\n",
      "[[0.9881349]]\n",
      "[[0.982895]]\n",
      "[[0.98667604]]\n",
      "[[0.994106]]\n",
      "[[0.99388665]]\n",
      "[[-0.99673015]]\n",
      "[[0.8233708]]\n",
      "[[0.5765084]]\n",
      "[[-0.99996156]]\n",
      "[[-0.9996815]]\n",
      "[[0.9979614]]\n",
      "[[-0.99046]]\n",
      "[[-0.9999891]]\n",
      "[[-0.9947474]]\n",
      "[[0.97390455]]\n",
      "[[-0.999892]]\n",
      "[[-0.99993443]]\n",
      "[[0.9945045]]\n",
      "[[0.90757555]]\n",
      "[[-0.99373347]]\n",
      "[[0.94366634]]\n",
      "[[-0.66240615]]\n",
      "[[-0.9998963]]\n",
      "[[0.93589044]]\n",
      "[[0.9949223]]\n",
      "[[-0.72116387]]\n",
      "[[0.96033055]]\n",
      "[[0.60401464]]\n",
      "[[-0.9997733]]\n",
      "[[-0.30517176]]\n",
      "[[0.9966557]]\n",
      "[[-0.9041638]]\n",
      "[[-0.7924118]]\n",
      "[[0.9791181]]\n",
      "[[-0.99777585]]\n",
      "[[-0.9919738]]\n",
      "[[0.99770564]]\n",
      "[[-0.20254517]]\n",
      "[[-0.8813301]]\n",
      "[[0.982489]]\n",
      "[[0.9867766]]\n",
      "[[0.9169877]]\n",
      "[[-0.98802274]]\n",
      "[[0.9944051]]\n",
      "[[0.989325]]\n",
      "[[0.890174]]\n",
      "[[0.9777005]]\n",
      "[[-0.98699486]]\n",
      "[[-0.7373467]]\n",
      "[[0.9969475]]\n",
      "[[-0.28695467]]\n",
      "[[0.2656028]]\n",
      "[[0.9724999]]\n",
      "[[-0.99930614]]\n",
      "[[-0.99632305]]\n",
      "[[0.99779165]]\n",
      "[[-0.17951089]]\n",
      "[[-0.9945369]]\n",
      "[[0.9699918]]\n",
      "[[-0.9424733]]\n",
      "[[-0.9999199]]\n",
      "[[0.9931764]]\n",
      "[[0.9873573]]\n",
      "[[-0.7134901]]\n",
      "[[0.9823069]]\n",
      "[[-0.9099994]]\n",
      "[[-0.99882835]]\n",
      "[[0.994983]]\n",
      "[[0.98091024]]\n",
      "[[0.7956416]]\n",
      "[[0.97866446]]\n",
      "[[-0.9997863]]\n",
      "[[-0.9951887]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "        reward  timesteps  num_actions\n0  -162.710556        204           34\n1  -129.821579        204           34\n2  -196.523254        204           34\n3  -197.150375        204           34\n4  -128.037262        204           34\n5  -293.907532        204           34\n6  -204.023666        204           34\n7  -294.456818        204           34\n8  -203.279236        204           34\n9  -161.626801        204           34\n10 -294.251831        204           34\n11 -221.038376        204           34\n12 -211.679779        204           34\n13 -292.024139        204           34\n14 -294.141479        204           34\n15 -212.784073        204           34\n16 -166.360718        204           34\n17 -291.163910        204           34\n18 -202.655289        204           34\n19 -219.489807        204           34",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-162.710556</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-129.821579</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-196.523254</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-197.150375</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-128.037262</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-293.907532</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-204.023666</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-294.456818</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-203.279236</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-161.626801</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-294.251831</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-221.038376</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-211.679779</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-292.024139</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-294.141479</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-212.784073</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-166.360718</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-291.163910</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-202.655289</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-219.489807</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.89174557 -0.45253718  0.61515385]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 3.2820\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 3.1470\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3905\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9514\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6656\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5802\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3105\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8678\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.8799\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3703\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.4646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 17.2302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.5014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 15.9832\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9415\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.5729\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1847\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.9789\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.0932\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 40.4328\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 92.4252\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 90.4853\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.9867\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.3650\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 68.7394\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 66.4427\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.2069\n",
      "Success in episode 1 at time step 200 with reward -212.5362943435096\n",
      "Episode 2\n",
      "[0.9567418  0.2909383  0.51680875]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.7835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8010\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.1499\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.7780\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.5394\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.6948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2712\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.0939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.5032\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.6645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9630\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0547\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2921\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1992\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2112\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6198\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4026\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.2508\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 62.4553\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 60.7482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1753\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9590\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.1001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.5425\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1654\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 72.7390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 72.6133\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.2409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.8527\n",
      "Success in episode 2 at time step 200 with reward -168.26775845542755\n",
      "Episode 3\n",
      "[ 0.849417   -0.52772224 -0.25149977]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9631\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9986\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.1471\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.2195\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.5102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.4681\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0286\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9323\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 58.2983\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 57.1264\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6833\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6025\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.5287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.6476\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 85.5770\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 82.9093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 76.2427\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 71.7733\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.1360\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.0855\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9248\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.3564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.6183\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6830\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7910\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.1315\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.1859\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6727\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.6226\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.2362\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.2031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.7195\n",
      "Success in episode 3 at time step 200 with reward -136.87014286423127\n",
      "Episode 4\n",
      "[ 0.16212894 -0.98676956  0.94493914]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9772\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7452\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.4894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 35.1205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.3657\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.8779\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7996\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4293\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6499\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7195\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0225\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6178\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0749\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9719\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3112\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2281\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4333\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.2557\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.1682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.2820\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3736\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.4492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.9299\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0094\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7885\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.7615\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.1173\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1092\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8949\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4092\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0595\n",
      "Success in episode 4 at time step 200 with reward -164.49943162522885\n",
      "Episode 5\n",
      "[-0.5834134  -0.81217533 -0.795447  ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1741\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8263\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8215\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7435\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1540\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1053\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8609\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6017\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8542\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0142\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8970\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.7193\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.2314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1889\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2806\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2254\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.0322\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 43.3989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.4945\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.9093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.2074\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8540\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5790\n",
      "Success in episode 5 at time step 200 with reward -183.43980762916343\n",
      "Episode 6\n",
      "[ 0.9696636   0.24444316 -0.97849035]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 63.0939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 61.8895\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5904\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6298\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.2993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 30.8169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.5602\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.9730\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.4294\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4033\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3319\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.5734\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.8868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.8688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.7562\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4533\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7451\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6963\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9164\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7988\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.7298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.4364\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7665\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.1770\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.7416\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.4264\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.4685\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.4613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.1861\n",
      "Success in episode 6 at time step 200 with reward -154.94527651069328\n",
      "Episode 7\n",
      "[ 0.71353185 -0.7006228  -0.5109903 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.1704\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.5724\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6372\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3670\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5515\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.4658\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.5044\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.8632\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.9763\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5322\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4785\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 116.5697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 115.1469\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9182\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6608\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.4378\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.9069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1499\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.1424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.3450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.3765\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 55.4256\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 55.8658\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.2916\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1691\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 12.6970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.7602\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.8153\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.4416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.4913\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.4140\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.1028\n",
      "Success in episode 7 at time step 200 with reward -149.26401987726885\n",
      "Episode 8\n",
      "[-0.141815   -0.9898932  -0.65811425]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0940\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5604\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8492\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 6.0176\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8353\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8114\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7393\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6636\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7275\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 9.6252\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 9.2285\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0611\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4969\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5789\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5454\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4608\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8559\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8820\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8572\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.2101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.8003\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7279\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2895\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2694\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0908\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9415\n",
      "Success in episode 8 at time step 200 with reward -180.82833032968347\n",
      "Episode 9\n",
      "[0.78390974 0.6208748  0.8235279 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5582\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6595\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7480\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7315\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4172\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4040\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0818\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.9481\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.8697\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8330\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8987\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1884\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.2411\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.4051\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1252\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.8954\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.5676\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2817\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1229\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.3141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3130\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.5388\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 18.7041\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6893\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.7785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.8245\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3533\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2654\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.6666\n",
      "Success in episode 9 at time step 200 with reward -146.62547291058962\n",
      "Episode 10\n",
      "[0.9927792  0.11995634 0.8371262 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.8276\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.6857\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3133\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2134\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.8324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.3713\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2455\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6749\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6333\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.0011\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.8709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3311\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3161\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.2436\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 33.8825\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.1207\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.1347\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.3331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.9878\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7388\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5695\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.2944\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.2080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6375\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8374\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.6184\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.5814\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.4510\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3685\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3332\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2113\n",
      "Success in episode 10 at time step 200 with reward -130.39258794567544\n",
      "Episode 11\n",
      "[ 0.7346119 -0.6784875  0.5377717]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8184\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.1591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.9855\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.5509\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.0374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6694\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.5431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.2783\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 37.0405\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5021\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5714\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.5781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 44.0990\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2291\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0659\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.6794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.5892\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4000\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5322\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.0089\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.4226\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7177\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4002\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1047\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3939\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 60.8830\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 61.1151\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0541\n",
      "Success in episode 11 at time step 200 with reward -147.14403222576755\n",
      "Episode 12\n",
      "[-0.6366802  0.771128   0.2867185]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2381\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2547\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0121\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9048\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6267\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5893\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2733\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1072\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4164\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2814\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2815\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5650\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3728\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0484\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1274\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 58.4465\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 57.4902\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 103.0303\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 98.1902\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.0010\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 38.7684\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.0854\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.4556\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.3462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 48.3287\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.8013\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.0787\n",
      "Success in episode 12 at time step 200 with reward -164.76802190006126\n",
      "Episode 13\n",
      "[-0.9773661   0.21155515  0.1838685 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1392\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8116\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.0869\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9989\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1058\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5175\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2148\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3939\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4493\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.6751\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2426\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9008\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2100\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2013\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0485\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9927\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0869\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9452\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8324\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7252\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9060\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4434\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7728\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7062\n",
      "fast thinking\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4895\n",
      "Success in episode 13 at time step 200 with reward -212.34046135555337\n",
      "Episode 14\n",
      "[0.9547141  0.2975247  0.34283832]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1820\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7415\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.7856\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.2084\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9071\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8801\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.8190\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.6636\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7390\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3750\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.8107\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.9831\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.7938\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.7494\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 128.2001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 126.2184\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2521\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3647\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0217\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0427\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5767\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6832\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.9655\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.6858\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.2220\n",
      "Success in episode 14 at time step 200 with reward -150.7864209755507\n",
      "Episode 15\n",
      "[ 0.86979717 -0.4934094   0.7922203 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3748\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 52.8330\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 51.6283\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 45.9985\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 46.6740\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 42.0388\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.0476\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9369\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8563\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8402\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7382\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.9794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.3409\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6345\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6393\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.2592\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.3321\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 62.5692\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 61.7205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 63.2313\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 60.5940\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.7583\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.0300\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 93.6513\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 94.1147\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.8021\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.9754\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 57.1284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 56.6851\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 27.0020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.4754\n",
      "Success in episode 15 at time step 200 with reward -168.6701374362478\n",
      "Episode 16\n",
      "[0.9934345  0.11440236 0.7300069 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.6380\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.8091\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6374\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3873\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.5363\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.7760\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.0911\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.8815\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7075\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6820\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.7660\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.4708\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5083\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5540\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 62.8406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 62.1220\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1727\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1738\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.0914\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.1535\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5994\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5952\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5470\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4131\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5930\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5638\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9393\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8896\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.3420\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1958\n",
      "Success in episode 16 at time step 200 with reward -149.21670989789465\n",
      "Episode 17\n",
      "[-0.93120366  0.36449933 -0.8998327 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1914\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5378\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1815\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0718\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3865\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1147\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2090\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8829\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8363\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.7557\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.5138\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6828\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0140\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.2805\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.0527\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2253\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0833\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.7987\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4126\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3195\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 54.5805\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 53.7372\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.9453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4260\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 10.3662\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 10.1113\n",
      "Success in episode 17 at time step 200 with reward -190.61783152386525\n",
      "Episode 18\n",
      "[-0.9958316  -0.09121082  0.55556905]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3472\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0347\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1585\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1056\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3736\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0523\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0529\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4938\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3011\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8033\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6328\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4007\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6300\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.0488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9213\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9353\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.8464\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 50.8103\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 49.9739\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 41.1704\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 39.9247\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5299\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.2848\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.0508\n",
      "training on full data\n",
      "1 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 14.0839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 13.7973\n",
      "Success in episode 18 at time step 200 with reward -210.541899678423\n",
      "Episode 19\n",
      "[ 0.80237347 -0.59682226  0.28311107]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8824\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.7917\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.0735\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7365\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6257\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8663\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2943\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3156\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.9763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.5610\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5862\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 4.1456\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5263\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5496\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.0298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.4247\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7656\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7344\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3461\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1379\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.0845\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 23.8308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5608\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4467\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.2599\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.0407\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.5758\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 31.0637\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.8072\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1759\n",
      "Success in episode 19 at time step 200 with reward -169.85962130403846\n",
      "Episode 20\n",
      "[-0.99719167 -0.07489206 -0.4290384 ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7643\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.3842\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9095\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3298\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6998\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9182\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6215\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4793\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3913\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2790\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1762\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.4280\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.9270\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 29.0581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 28.8252\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8689\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8202\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.0046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8125\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2326\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.1709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8155\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 60.6258\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 59.6651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.3434\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.3282\n",
      "training on full data\n",
      "0 34\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9955\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7279\n",
      "Success in episode 20 at time step 200 with reward -195.42676682755885\n"
     ]
    }
   ],
   "source": [
    "daifa.habit_action_model.show_training = False\n",
    "daifa.train_habit_net = True\n",
    "daifa.train_after_exploring = True\n",
    "daifa.use_kl_intrinsic = True\n",
    "daifa.use_kl_extrinsic = False\n",
    "daifa.use_fast_thinking = True\n",
    "daifa.uncertainty_tolerance = 0.1\n",
    "\n",
    "# daifa.tran.show_training = False\n",
    "# daifa.prior_model.show_training = False\n",
    "\n",
    "# train the agent on the env\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "daifa, results_three = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.999985]]\n",
      "[[0.9926074]]\n",
      "[[-0.9981063]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.9902263]]\n",
      "[[0.999153]]\n",
      "[[-0.54729325]]\n",
      "[[0.9755413]]\n",
      "[[0.98625594]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.9771793]]\n",
      "[[0.99796736]]\n",
      "[[-0.999193]]\n",
      "[[-0.79554117]]\n",
      "[[0.9970504]]\n",
      "[[-0.9998214]]\n",
      "[[-1.]]\n",
      "[[-0.9998888]]\n",
      "[[0.9999314]]\n",
      "[[-0.7635011]]\n",
      "[[-0.9999752]]\n",
      "[[-0.9964315]]\n",
      "[[0.9978376]]\n",
      "[[-0.9988771]]\n",
      "[[-1.]]\n",
      "[[-0.99999243]]\n",
      "[[0.9998726]]\n",
      "[[0.68442297]]\n",
      "[[-0.9985651]]\n",
      "[[0.99958545]]\n",
      "[[0.9990507]]\n",
      "[[0.9783091]]\n",
      "[[0.9758151]]\n",
      "[[0.9989933]]\n",
      "[[0.9995645]]\n",
      "[[0.9992406]]\n",
      "[[0.99772114]]\n",
      "[[0.9308463]]\n",
      "[[0.99699825]]\n",
      "[[0.99951404]]\n",
      "[[0.99964595]]\n",
      "[[0.9987143]]\n",
      "[[0.9928902]]\n",
      "[[0.9639039]]\n",
      "[[0.99847186]]\n",
      "[[0.9995326]]\n",
      "[[0.9993045]]\n",
      "[[0.99661386]]\n",
      "[[0.9250229]]\n",
      "[[0.99763757]]\n",
      "[[0.9996322]]\n",
      "[[0.9996329]]\n",
      "[[0.99928516]]\n",
      "[[0.99206525]]\n",
      "[[0.9459597]]\n",
      "[[0.99830794]]\n",
      "[[0.99966735]]\n",
      "[[0.99920183]]\n",
      "[[0.99802566]]\n",
      "[[0.61554104]]\n",
      "[[0.99144995]]\n",
      "[[0.9989991]]\n",
      "[[0.99962515]]\n",
      "[[0.9994061]]\n",
      "[[0.999137]]\n",
      "[[0.99712974]]\n",
      "[[0.9946602]]\n",
      "[[0.999047]]\n",
      "[[0.9995679]]\n",
      "[[0.99934936]]\n",
      "[[0.9985126]]\n",
      "[[0.9922837]]\n",
      "[[0.9977941]]\n",
      "[[0.9995006]]\n",
      "[[0.99936765]]\n",
      "[[0.9989324]]\n",
      "[[0.997943]]\n",
      "[[0.9961147]]\n",
      "[[0.99872583]]\n",
      "[[0.9995239]]\n",
      "[[0.9993261]]\n",
      "[[0.99843377]]\n",
      "[[0.99282134]]\n",
      "[[0.9969209]]\n",
      "[[0.9994046]]\n",
      "[[0.9995162]]\n",
      "[[0.9984354]]\n",
      "[[0.997059]]\n",
      "[[0.99517685]]\n",
      "[[0.9991129]]\n",
      "[[0.99921507]]\n",
      "[[0.99927455]]\n",
      "[[0.9965294]]\n",
      "[[0.99623746]]\n",
      "[[0.9971376]]\n",
      "[[0.99939084]]\n",
      "[[0.9993438]]\n",
      "[[-1.]]\n",
      "[[-0.99276567]]\n",
      "[[0.99986935]]\n",
      "[[-0.99993724]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.9998687]]\n",
      "[[0.19577144]]\n",
      "[[-0.99996823]]\n",
      "[[-0.9979174]]\n",
      "[[0.99515754]]\n",
      "[[-0.9884965]]\n",
      "[[-1.]]\n",
      "[[-0.9999989]]\n",
      "[[0.9997387]]\n",
      "[[0.9850642]]\n",
      "[[-0.9744067]]\n",
      "[[0.99401283]]\n",
      "[[-0.5266945]]\n",
      "[[-1.]]\n",
      "[[-0.9999982]]\n",
      "[[0.9997856]]\n",
      "[[0.9846665]]\n",
      "[[-0.11920349]]\n",
      "[[0.99780726]]\n",
      "[[-0.99752724]]\n",
      "[[-1.]]\n",
      "[[-0.9998338]]\n",
      "[[0.99992836]]\n",
      "[[-0.6744533]]\n",
      "[[-0.9999657]]\n",
      "[[-0.4739152]]\n",
      "[[0.9977312]]\n",
      "[[-1.]]\n",
      "[[-0.9999962]]\n",
      "[[0.99987]]\n",
      "[[0.7092267]]\n",
      "[[-0.9999731]]\n",
      "[[0.07950194]]\n",
      "[[0.9967808]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.9996639]]\n",
      "[[0.95987755]]\n",
      "[[-0.99997777]]\n",
      "[[0.3104182]]\n",
      "[[0.99687386]]\n",
      "[[-0.99999964]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.9906334]]\n",
      "[[0.9990466]]\n",
      "[[-0.99578434]]\n",
      "[[-0.91624665]]\n",
      "[[0.99865276]]\n",
      "[[-0.9998803]]\n",
      "[[-0.9999995]]\n",
      "[[-0.9984482]]\n",
      "[[0.9998599]]\n",
      "[[-0.97174287]]\n",
      "[[-0.9999941]]\n",
      "[[-0.8349653]]\n",
      "[[0.9989094]]\n",
      "[[-0.9999794]]\n",
      "[[-1.]]\n",
      "[[-0.9999805]]\n",
      "[[-0.9999998]]\n",
      "[[0.9954808]]\n",
      "[[0.9997079]]\n",
      "[[0.85541993]]\n",
      "[[0.99715877]]\n",
      "[[-0.9998043]]\n",
      "[[-1.]]\n",
      "[[-0.9723856]]\n",
      "[[0.99981034]]\n",
      "[[-0.9919677]]\n",
      "[[-0.9996372]]\n",
      "[[0.98687077]]\n",
      "[[0.90401745]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.99962014]]\n",
      "[[0.9999032]]\n",
      "[[-0.99946976]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.9998126]]\n",
      "[[0.56895965]]\n",
      "[[-0.9999914]]\n",
      "[[-0.7178563]]\n",
      "[[0.9979466]]\n",
      "[[-0.9999787]]\n",
      "[[-1.]]\n",
      "[[-0.9999802]]\n",
      "[[0.9998837]]\n",
      "[[0.31959036]]\n",
      "[[-0.9999498]]\n",
      "[[0.9978637]]\n",
      "[[0.9983914]]\n",
      "[[0.99946356]]\n",
      "[[0.9992941]]\n",
      "[[0.99814796]]\n",
      "[[0.996105]]\n",
      "[[0.99804056]]\n",
      "[[0.9992132]]\n",
      "[[0.99941117]]\n",
      "[[0.9993093]]\n",
      "[[0.99805725]]\n",
      "[[0.9973697]]\n",
      "[[0.99906343]]\n",
      "[[0.99944913]]\n",
      "[[0.9993977]]\n",
      "[[0.998257]]\n",
      "[[0.99588466]]\n",
      "[[0.99825877]]\n",
      "[[0.9992105]]\n",
      "[[0.9993106]]\n",
      "[[0.9989075]]\n",
      "[[0.9971982]]\n",
      "[[0.9903499]]\n",
      "[[0.9987068]]\n",
      "[[0.9992784]]\n",
      "[[0.99913657]]\n",
      "[[0.9988201]]\n",
      "[[0.99675065]]\n",
      "[[0.9985917]]\n",
      "[[0.99917835]]\n",
      "[[0.9995909]]\n",
      "[[0.99828887]]\n",
      "[[0.9961224]]\n",
      "[[0.99790865]]\n",
      "[[-0.9903178]]\n",
      "[[0.9991763]]\n",
      "[[0.99960595]]\n",
      "[[0.99874955]]\n",
      "[[0.9956756]]\n",
      "[[-0.9996467]]\n",
      "[[0.28120512]]\n",
      "[[0.99979335]]\n",
      "[[0.99853194]]\n",
      "[[0.9989023]]\n",
      "[[0.37185135]]\n",
      "[[-0.99995464]]\n",
      "[[0.9779147]]\n",
      "[[0.99956113]]\n",
      "[[0.9967704]]\n",
      "[[0.998425]]\n",
      "[[-0.99964106]]\n",
      "[[-0.9999597]]\n",
      "[[0.99936986]]\n",
      "[[0.99910516]]\n",
      "[[0.9943959]]\n",
      "[[0.9962313]]\n",
      "[[-0.9999932]]\n",
      "[[-0.99999803]]\n",
      "[[0.99915123]]\n",
      "[[0.9990668]]\n",
      "[[0.918874]]\n",
      "[[0.9988446]]\n",
      "[[-0.99996203]]\n",
      "[[-1.]]\n",
      "[[-0.9854417]]\n",
      "[[0.9995236]]\n",
      "[[-0.9985349]]\n",
      "[[-0.99999756]]\n",
      "[[0.7988431]]\n",
      "[[0.9974232]]\n",
      "[[-0.9998808]]\n",
      "[[-0.99999976]]\n",
      "[[-0.3021031]]\n",
      "[[0.999875]]\n",
      "[[-0.7477565]]\n",
      "[[0.04121238]]\n",
      "[[0.99712014]]\n",
      "[[-0.99996704]]\n",
      "[[-1.]]\n",
      "[[-0.99994177]]\n",
      "[[0.99992216]]\n",
      "[[-0.9822413]]\n",
      "[[-0.9999995]]\n",
      "[[-1.]]\n",
      "[[-0.9999562]]\n",
      "[[0.9955299]]\n",
      "[[-0.99970454]]\n",
      "[[-1.]]\n",
      "[[-0.99999917]]\n",
      "[[0.9993075]]\n",
      "[[0.99637556]]\n",
      "[[-0.9926355]]\n",
      "[[0.97447556]]\n",
      "[[0.98895323]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.9975673]]\n",
      "[[0.99840224]]\n",
      "[[-0.9994554]]\n",
      "[[-0.9504873]]\n",
      "[[0.9980481]]\n",
      "[[0.9996751]]\n",
      "[[0.9977505]]\n",
      "[[0.2633241]]\n",
      "[[0.9935482]]\n",
      "[[0.9995567]]\n",
      "[[0.9993501]]\n",
      "[[0.9990201]]\n",
      "[[0.9790169]]\n",
      "[[-0.50019056]]\n",
      "[[0.99949145]]\n",
      "[[0.9996645]]\n",
      "[[0.9995289]]\n",
      "[[0.9916453]]\n",
      "[[-0.98785657]]\n",
      "[[0.9966415]]\n",
      "[[0.9998521]]\n",
      "[[0.9992064]]\n",
      "[[0.99776953]]\n",
      "[[-0.95482796]]\n",
      "[[-0.31303176]]\n",
      "[[0.99981356]]\n",
      "[[0.99871224]]\n",
      "[[0.9984556]]\n",
      "[[0.67979497]]\n",
      "[[-0.9999785]]\n",
      "[[0.8162439]]\n",
      "[[0.99976605]]\n",
      "[[0.99538285]]\n",
      "[[0.99882317]]\n",
      "[[-0.96062446]]\n",
      "[[-0.99994284]]\n",
      "[[0.9986891]]\n",
      "[[0.9991995]]\n",
      "[[0.9961862]]\n",
      "[[-0.9332154]]\n",
      "[[0.9995143]]\n",
      "[[0.99944913]]\n",
      "[[0.99892694]]\n",
      "[[0.9925231]]\n",
      "[[-0.99969894]]\n",
      "[[0.904196]]\n",
      "[[0.9998798]]\n",
      "[[0.99817127]]\n",
      "[[0.998426]]\n",
      "[[-0.8389556]]\n",
      "[[-0.9974502]]\n",
      "[[0.99960536]]\n",
      "[[0.9984871]]\n",
      "[[0.99861795]]\n",
      "[[0.9925986]]\n",
      "[[-0.9999929]]\n",
      "[[-0.9996458]]\n",
      "[[0.99989796]]\n",
      "[[0.98142415]]\n",
      "[[0.9848214]]\n",
      "[[0.991458]]\n",
      "[[-0.9999997]]\n",
      "[[-1.]]\n",
      "[[-0.99964625]]\n",
      "[[0.99982905]]\n",
      "[[-0.9721963]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.9940934]]\n",
      "[[0.9992288]]\n",
      "[[-0.99978566]]\n",
      "[[-0.99995077]]\n",
      "[[-0.9999985]]\n",
      "[[0.99782354]]\n",
      "[[0.99909467]]\n",
      "[[0.9811593]]\n",
      "[[0.9982423]]\n",
      "[[-0.999864]]\n",
      "[[-0.9999995]]\n",
      "[[0.929067]]\n",
      "[[0.99972326]]\n",
      "[[0.98183656]]\n",
      "[[0.99815845]]\n",
      "[[-0.9983323]]\n",
      "[[-0.9999985]]\n",
      "[[0.75666183]]\n",
      "[[0.999788]]\n",
      "[[0.9893801]]\n",
      "[[0.9975169]]\n",
      "[[-0.97120655]]\n",
      "[[-0.99999946]]\n",
      "[[-0.3080405]]\n",
      "[[0.99978757]]\n",
      "[[0.81632066]]\n",
      "[[0.99340546]]\n",
      "[[0.59377587]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.9941535]]\n",
      "[[0.99977523]]\n",
      "[[-0.99994874]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.98876303]]\n",
      "[[0.9996599]]\n",
      "[[-0.99996656]]\n",
      "[[0.99933785]]\n",
      "[[0.99929816]]\n",
      "[[0.99864626]]\n",
      "[[0.9962098]]\n",
      "[[0.9978297]]\n",
      "[[0.99913716]]\n",
      "[[0.9992738]]\n",
      "[[0.99929476]]\n",
      "[[0.99853295]]\n",
      "[[0.99646395]]\n",
      "[[0.99744576]]\n",
      "[[0.9994185]]\n",
      "[[0.9990013]]\n",
      "[[0.9990416]]\n",
      "[[0.9982557]]\n",
      "[[0.9974745]]\n",
      "[[0.9987999]]\n",
      "[[0.9993421]]\n",
      "[[0.9991451]]\n",
      "[[0.99849826]]\n",
      "[[0.9980846]]\n",
      "[[0.9979304]]\n",
      "[[0.9991722]]\n",
      "[[0.9991043]]\n",
      "[[0.99803865]]\n",
      "[[0.99789447]]\n",
      "[[0.99593335]]\n",
      "[[0.99910873]]\n",
      "[[0.9992335]]\n",
      "[[0.9989518]]\n",
      "[[0.99806476]]\n",
      "[[0.99465334]]\n",
      "[[0.9989469]]\n",
      "[[0.9990957]]\n",
      "[[0.9982525]]\n",
      "[[0.9954924]]\n",
      "[[0.9977099]]\n",
      "[[0.99822575]]\n",
      "[[0.99934745]]\n",
      "[[0.99910223]]\n",
      "[[0.99857837]]\n",
      "[[0.99651456]]\n",
      "[[0.9988671]]\n",
      "[[0.9993333]]\n",
      "[[0.99949586]]\n",
      "[[0.99820894]]\n",
      "[[0.9960519]]\n",
      "[[0.9979126]]\n",
      "[[0.9985019]]\n",
      "[[0.99939936]]\n",
      "[[0.99807197]]\n",
      "[[0.9980273]]\n",
      "[[0.9959675]]\n",
      "[[0.9987696]]\n",
      "[[0.99882305]]\n",
      "[[0.99921024]]\n",
      "[[0.9982182]]\n",
      "[[0.9963137]]\n",
      "[[0.9972667]]\n",
      "[[0.9989773]]\n",
      "[[0.9994023]]\n",
      "[[0.999203]]\n",
      "[[0.99795705]]\n",
      "[[0.9964033]]\n",
      "[[0.9987145]]\n",
      "[[0.9993636]]\n",
      "[[0.999447]]\n",
      "[[0.9983792]]\n",
      "[[0.99919146]]\n",
      "[[0.9977114]]\n",
      "[[0.9959558]]\n",
      "[[0.9986743]]\n",
      "[[0.9994557]]\n",
      "[[0.9994411]]\n",
      "[[0.9985587]]\n",
      "[[0.9960796]]\n",
      "[[0.99521405]]\n",
      "[[0.9993673]]\n",
      "[[0.99954945]]\n",
      "[[0.99878633]]\n",
      "[[0.9980987]]\n",
      "[[0.9960682]]\n",
      "[[0.99839735]]\n",
      "[[0.99938476]]\n",
      "[[0.99957836]]\n",
      "[[0.99892086]]\n",
      "[[0.99614364]]\n",
      "[[0.99872077]]\n",
      "[[0.99915546]]\n",
      "[[0.99954265]]\n",
      "[[0.9987529]]\n",
      "[[0.99774325]]\n",
      "[[0.9956625]]\n",
      "[[0.99913603]]\n",
      "[[0.99910975]]\n",
      "[[0.9994877]]\n",
      "[[0.99868387]]\n",
      "[[0.99550253]]\n",
      "[[0.99748623]]\n",
      "[[0.9994319]]\n",
      "[[0.9995171]]\n",
      "[[0.999274]]\n",
      "[[-0.9683743]]\n",
      "[[0.99581003]]\n",
      "[[-0.5491541]]\n",
      "[[-0.99999875]]\n",
      "[[-0.99998915]]\n",
      "[[0.9998704]]\n",
      "[[0.8622261]]\n",
      "[[-0.96515733]]\n",
      "[[0.997607]]\n",
      "[[-0.99843377]]\n",
      "[[-0.99999946]]\n",
      "[[-0.979079]]\n",
      "[[0.99986064]]\n",
      "[[-0.921056]]\n",
      "[[-0.9998293]]\n",
      "[[0.9594994]]\n",
      "[[0.97734386]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.9983179]]\n",
      "[[0.9997702]]\n",
      "[[-0.9998378]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.9998239]]\n",
      "[[0.9597215]]\n",
      "[[-0.9999844]]\n",
      "[[-0.66523457]]\n",
      "[[0.99572325]]\n",
      "[[-0.99999416]]\n",
      "[[-1.]]\n",
      "[[0.99868083]]\n",
      "[[0.99593806]]\n",
      "[[-0.9983672]]\n",
      "[[-0.8166594]]\n",
      "[[0.9998365]]\n",
      "[[0.99623585]]\n",
      "[[0.99766487]]\n",
      "[[-0.09456395]]\n",
      "[[-0.9999926]]\n",
      "[[-0.91850007]]\n",
      "[[0.9999073]]\n",
      "[[0.74013436]]\n",
      "[[0.98634744]]\n",
      "[[0.96502346]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.99999785]]\n",
      "[[0.9999026]]\n",
      "[[0.38208747]]\n",
      "[[-0.9999713]]\n",
      "[[-0.94430864]]\n",
      "[[0.997656]]\n",
      "[[-0.99998415]]\n",
      "[[-1.]]\n",
      "[[-0.9998498]]\n",
      "[[0.99991935]]\n",
      "[[-0.9718048]]\n",
      "[[-0.99999833]]\n",
      "[[-0.9999993]]\n",
      "[[-0.9231374]]\n",
      "[[0.9971383]]\n",
      "[[-0.99999356]]\n",
      "[[-1.]]\n",
      "[[-0.99999815]]\n",
      "[[0.32692182]]\n",
      "[[0.9985941]]\n",
      "[[0.99962854]]\n",
      "[[0.9994292]]\n",
      "[[0.9969593]]\n",
      "[[-0.8479985]]\n",
      "[[0.9983915]]\n",
      "[[0.9997327]]\n",
      "[[0.9995598]]\n",
      "[[0.9986908]]\n",
      "[[-0.88506585]]\n",
      "[[0.99394965]]\n",
      "[[0.99975824]]\n",
      "[[0.99904877]]\n",
      "[[0.9990364]]\n",
      "[[0.65635103]]\n",
      "[[-0.9294264]]\n",
      "[[0.9995283]]\n",
      "[[0.99948233]]\n",
      "[[0.99929243]]\n",
      "[[0.9853575]]\n",
      "[[-0.9999129]]\n",
      "[[0.7448036]]\n",
      "[[0.99971604]]\n",
      "[[0.99838763]]\n",
      "[[0.9991239]]\n",
      "[[-0.4879852]]\n",
      "[[-0.99926186]]\n",
      "[[0.9992357]]\n",
      "[[0.99925065]]\n",
      "[[0.9978338]]\n",
      "[[0.9970596]]\n",
      "[[-0.9999764]]\n",
      "[[-0.9998207]]\n",
      "[[0.9911203]]\n",
      "[[0.99705416]]\n",
      "[[0.9994948]]\n",
      "[[0.9996544]]\n",
      "[[0.9990297]]\n",
      "[[0.9887983]]\n",
      "[[0.9897528]]\n",
      "[[0.9985794]]\n",
      "[[0.99963146]]\n",
      "[[0.9996162]]\n",
      "[[0.9974294]]\n",
      "[[0.93872386]]\n",
      "[[0.99538225]]\n",
      "[[0.9993992]]\n",
      "[[0.99972945]]\n",
      "[[0.9989299]]\n",
      "[[0.9931077]]\n",
      "[[0.9772108]]\n",
      "[[0.9981769]]\n",
      "[[0.9996825]]\n",
      "[[0.9995182]]\n",
      "[[0.9987441]]\n",
      "[[0.9789628]]\n",
      "[[0.9957226]]\n",
      "[[0.99926287]]\n",
      "[[0.9996007]]\n",
      "[[0.9990933]]\n",
      "[[0.99507207]]\n",
      "[[0.92931026]]\n",
      "[[0.99813336]]\n",
      "[[0.9997102]]\n",
      "[[0.999684]]\n",
      "[[0.99746525]]\n",
      "[[0.9881146]]\n",
      "[[-0.9999943]]\n",
      "[[0.94816184]]\n",
      "[[0.999782]]\n",
      "[[0.9911668]]\n",
      "[[0.99844146]]\n",
      "[[-0.99412173]]\n",
      "[[-0.9999936]]\n",
      "[[0.99499613]]\n",
      "[[0.99957746]]\n",
      "[[0.9915085]]\n",
      "[[0.99769753]]\n",
      "[[-0.9997293]]\n",
      "[[-0.99999744]]\n",
      "[[0.996544]]\n",
      "[[0.9994272]]\n",
      "[[0.97406834]]\n",
      "[[0.99743533]]\n",
      "[[-0.9998976]]\n",
      "[[-0.9999991]]\n",
      "[[0.9652732]]\n",
      "[[0.99956554]]\n",
      "[[0.9874684]]\n",
      "[[0.99819005]]\n",
      "[[-0.9980662]]\n",
      "[[-0.9999995]]\n",
      "[[-0.01065227]]\n",
      "[[0.9997492]]\n",
      "[[0.23346578]]\n",
      "[[0.9875876]]\n",
      "[[0.8936803]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.9999739]]\n",
      "[[0.9999222]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "        reward  timesteps  num_actions\n0  -163.611710        204           34\n1  -267.267273        204           34\n2  -287.636658        204           34\n3  -179.181763        204           34\n4  -174.783951        204           34\n5  -163.229477        204           34\n6  -291.205383        204           34\n7  -205.403137        204           34\n8  -174.532150        204           34\n9  -238.689728        204           34\n10 -197.667770        204           34\n11 -189.093155        204           34\n12 -296.454071        204           34\n13 -290.791626        204           34\n14 -289.532837        204           34\n15 -166.605301        204           34\n16 -193.476517        204           34\n17 -242.480164        204           34\n18 -267.286469        204           34\n19 -204.961929        204           34",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-163.611710</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-267.267273</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-287.636658</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-179.181763</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-174.783951</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-163.229477</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-291.205383</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-205.403137</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-174.532150</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-238.689728</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-197.667770</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-189.093155</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-296.454071</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-290.791626</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-289.532837</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-166.605301</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-193.476517</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-242.480164</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-267.286469</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-204.961929</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x17470aca0>,\n <matplotlib.lines.Line2D at 0x174af0d90>,\n <matplotlib.lines.Line2D at 0x175a4e4f0>]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100), np.zeros(100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(obs_pos)\n",
    "\n",
    "utils = daifa.prior_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(vel_pos)\n",
    "\n",
    "utils = daifa.prior_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(obs_pos)\n",
    "\n",
    "# utils = daifa.habit_action_model.actor_model(latent_mean)\n",
    "utils = daifa.select_fast_thinking_policy(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(vel_pos)\n",
    "\n",
    "# utils = daifa.habit_action_model.actor_model(latent_mean)\n",
    "utils = daifa.select_fast_thinking_policy(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99992335]]\n",
      "[[-0.88178855]]\n",
      "[[0.99789256]]\n",
      "[[-0.9998848]]\n",
      "[[-1.]]\n",
      "[[-0.9999769]]\n",
      "[[0.99991107]]\n",
      "[[-0.16730253]]\n",
      "[[-0.9999739]]\n",
      "[[-0.8034171]]\n",
      "[[0.9970662]]\n",
      "[[-0.9999894]]\n",
      "[[-1.]]\n",
      "[[-0.99990356]]\n",
      "[[0.999879]]\n",
      "[[-0.97488046]]\n",
      "[[-0.9999994]]\n",
      "[[-0.9999995]]\n",
      "[[-0.9999298]]\n",
      "[[0.99215883]]\n",
      "[[-0.9951003]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.9989349]]\n",
      "[[0.9983758]]\n",
      "[[-0.966355]]\n",
      "[[0.9603897]]\n",
      "[[0.9575787]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[0.999734]]\n",
      "[[0.9678278]]\n",
      "[[-0.999788]]\n",
      "[[0.99940175]]\n",
      "[[0.99864036]]\n",
      "[[0.9983579]]\n",
      "[[0.9979407]]\n",
      "[[0.9991464]]\n",
      "[[0.99915755]]\n",
      "[[0.99778235]]\n",
      "[[0.9984719]]\n",
      "[[0.99706304]]\n",
      "[[0.998763]]\n",
      "[[0.9988698]]\n",
      "[[0.9993559]]\n",
      "[[0.9990138]]\n",
      "[[0.99794537]]\n",
      "[[0.99839854]]\n",
      "[[0.99911153]]\n",
      "[[0.9989697]]\n",
      "[[0.9992252]]\n",
      "[[0.9982412]]\n",
      "[[0.9965633]]\n",
      "[[0.99803454]]\n",
      "[[0.9994383]]\n",
      "[[0.99928397]]\n",
      "[[0.99822474]]\n",
      "[[0.99837875]]\n",
      "[[0.998344]]\n",
      "[[0.9986446]]\n",
      "[[0.99930716]]\n",
      "[[0.9991316]]\n",
      "[[0.9982009]]\n",
      "[[0.9972399]]\n",
      "[[0.9988507]]\n",
      "[[0.9991977]]\n",
      "[[0.9992266]]\n",
      "[[0.7610545]]\n",
      "[[0.99936944]]\n",
      "[[0.9997359]]\n",
      "[[0.9990438]]\n",
      "[[0.99532485]]\n",
      "[[0.6572884]]\n",
      "[[0.9957741]]\n",
      "[[0.99969804]]\n",
      "[[0.9997258]]\n",
      "[[0.99891406]]\n",
      "[[0.94040376]]\n",
      "[[0.38200775]]\n",
      "[[0.9993763]]\n",
      "[[0.9997448]]\n",
      "[[0.9992805]]\n",
      "[[0.9952285]]\n",
      "[[-0.93943685]]\n",
      "[[0.9984129]]\n",
      "[[0.99984366]]\n",
      "[[0.99897325]]\n",
      "[[0.9981243]]\n",
      "[[-0.75387]]\n",
      "[[0.98440313]]\n",
      "[[0.9997273]]\n",
      "[[0.99954104]]\n",
      "[[0.9986005]]\n",
      "[[0.66133595]]\n",
      "[[-0.95227736]]\n",
      "[[0.9994654]]\n",
      "[[0.99972373]]\n",
      "[[0.9988063]]\n",
      "[[0.99198824]]\n",
      "[[-0.9997448]]\n",
      "[[-0.81360483]]\n",
      "[[0.98846155]]\n",
      "[[0.998729]]\n",
      "[[-0.9997293]]\n",
      "[[-0.9999994]]\n",
      "[[0.9901023]]\n",
      "[[0.9996195]]\n",
      "[[0.9842945]]\n",
      "[[0.99777156]]\n",
      "[[-0.9990607]]\n",
      "[[-0.9999959]]\n",
      "[[0.9973255]]\n",
      "[[0.9998672]]\n",
      "[[0.98625666]]\n",
      "[[0.998695]]\n",
      "[[-0.9993962]]\n",
      "[[-0.99999744]]\n",
      "[[0.98042065]]\n",
      "[[0.9998296]]\n",
      "[[0.9862935]]\n",
      "[[0.9992096]]\n",
      "[[-0.9998129]]\n",
      "[[-0.99999815]]\n",
      "[[0.994805]]\n",
      "[[0.99963593]]\n",
      "[[0.9870506]]\n",
      "[[0.99905175]]\n",
      "[[-0.9997644]]\n",
      "[[-0.99999756]]\n",
      "[[0.9931796]]\n",
      "[[0.99962646]]\n",
      "[[0.9890735]]\n",
      "[[0.99827486]]\n",
      "[[-0.99973375]]\n",
      "[[-0.9999985]]\n",
      "[[-0.34690377]]\n",
      "[[0.9970365]]\n",
      "[[-0.9937075]]\n",
      "[[-0.9999985]]\n",
      "[[-0.99661165]]\n",
      "[[0.99990404]]\n",
      "[[-0.3368223]]\n",
      "[[-0.99675363]]\n",
      "[[0.9937415]]\n",
      "[[-0.95446336]]\n",
      "[[-1.]]\n",
      "[[-0.9999811]]\n",
      "[[0.99991393]]\n",
      "[[0.774462]]\n",
      "[[-0.8210585]]\n",
      "[[0.99798965]]\n",
      "[[-0.99943346]]\n",
      "[[-1.]]\n",
      "[[-0.99916816]]\n",
      "[[0.9999176]]\n",
      "[[-0.9466849]]\n",
      "[[-0.9999215]]\n",
      "[[0.14903238]]\n",
      "[[0.9968899]]\n",
      "[[-1.]]\n",
      "[[-1.]]\n",
      "[[-0.9999995]]\n",
      "[[0.99963546]]\n",
      "[[0.9573536]]\n",
      "[[-0.9998998]]\n",
      "[[-0.24675551]]\n",
      "[[0.9967874]]\n",
      "[[-0.9999918]]\n",
      "[[-1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "       reward  timesteps  num_actions\n0 -163.081482        204           34\n1 -295.807770        204           34\n2 -253.660629        204           34\n3 -202.541702        204           34\n4 -180.507751        204           34",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-163.081482</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-295.807770</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-253.660629</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-202.541702</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-180.507751</td>\n      <td>204</td>\n      <td>34</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 5, daifa.agent_time_ratio, show_env=True)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.4574806  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "60 167\n",
      "No Success\n",
      "Episode 2\n",
      "[-0.47844926  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "64 167\n",
      "No Success\n",
      "Episode 3\n",
      "[-0.5311618  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "37 147\n",
      "Success in episode 3 at time step 877\n",
      "Episode 4\n",
      "[-0.46539444  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "60 167\n",
      "No Success\n",
      "Episode 5\n",
      "[-0.43320078  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "70 167\n",
      "No Success\n",
      "Episode 6\n",
      "[-0.56359833  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "54 167\n",
      "No Success\n",
      "Episode 7\n",
      "[-0.41705218  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "51 167\n",
      "No Success\n",
      "Episode 8\n",
      "[-0.49579993  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "90 167\n",
      "No Success\n",
      "Episode 9\n",
      "[-0.40739912  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "94 167\n",
      "No Success\n",
      "Episode 10\n",
      "[-0.57973146  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "54 167\n",
      "No Success\n",
      "Episode 11\n",
      "[-0.47232947  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "62 167\n",
      "No Success\n",
      "Episode 12\n",
      "[-0.49276304  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "72 167\n",
      "No Success\n",
      "Episode 13\n",
      "[-0.5930956  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "114 167\n",
      "No Success\n",
      "Episode 14\n",
      "[-0.5572101  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "56 167\n",
      "No Success\n",
      "Episode 15\n",
      "[-0.4525781  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "76 167\n",
      "No Success\n",
      "Episode 16\n",
      "[-0.49785632  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "64 167\n",
      "No Success\n",
      "Episode 17\n",
      "[-0.5918306  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "38 97\n",
      "Success in episode 17 at time step 582\n",
      "Episode 18\n",
      "[-0.50446576  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "46 119\n",
      "Success in episode 18 at time step 711\n",
      "Episode 19\n",
      "[-0.5515545  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "48 167\n",
      "No Success\n",
      "Episode 20\n",
      "[-0.4675508  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "46 167\n",
      "No Success\n",
      "Episode 21\n",
      "[-0.5987304  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "24 84\n",
      "Success in episode 21 at time step 500\n",
      "Episode 22\n",
      "[-0.46599886  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "14 44\n",
      "Success in episode 22 at time step 259\n",
      "Episode 23\n",
      "[-0.5442812  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 26\n",
      "Success in episode 23 at time step 154\n",
      "Episode 24\n",
      "[-0.56831306  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "65 167\n",
      "No Success\n",
      "Episode 25\n",
      "[-0.4211194  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "35 113\n",
      "Success in episode 25 at time step 676\n",
      "Episode 26\n",
      "[-0.48089644  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 44\n",
      "Success in episode 26 at time step 259\n",
      "Episode 27\n",
      "[-0.49131438  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "13 45\n",
      "Success in episode 27 at time step 265\n",
      "Episode 28\n",
      "[-0.4746755  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 27\n",
      "Success in episode 28 at time step 157\n",
      "Episode 29\n",
      "[-0.59761935  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 57\n",
      "Success in episode 29 at time step 342\n",
      "Episode 30\n",
      "[-0.4673799  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "10 27\n",
      "Success in episode 30 at time step 159\n",
      "Episode 31\n",
      "[-0.5692723  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 26\n",
      "Success in episode 31 at time step 153\n",
      "Episode 32\n",
      "[-0.4338846  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "45 134\n",
      "Success in episode 32 at time step 802\n",
      "Episode 33\n",
      "[-0.43634194  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "31 84\n",
      "Success in episode 33 at time step 503\n",
      "Episode 34\n",
      "[-0.57066995  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "41 107\n",
      "Success in episode 34 at time step 638\n",
      "Episode 35\n",
      "[-0.4396064  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "9 29\n",
      "Success in episode 35 at time step 170\n",
      "Episode 36\n",
      "[-0.4796282  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "9 27\n",
      "Success in episode 36 at time step 159\n",
      "Episode 37\n",
      "[-0.502011  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "13 29\n",
      "Success in episode 37 at time step 172\n",
      "Episode 38\n",
      "[-0.48284528  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "14 39\n",
      "Success in episode 38 at time step 234\n",
      "Episode 39\n",
      "[-0.41663125  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 40\n",
      "Success in episode 39 at time step 238\n",
      "Episode 40\n",
      "[-0.52127314  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 28\n",
      "Success in episode 40 at time step 165\n",
      "Episode 41\n",
      "[-0.5059667  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "10 25\n",
      "Success in episode 41 at time step 150\n",
      "Episode 42\n",
      "[-0.5644291  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "25 55\n",
      "Success in episode 42 at time step 326\n",
      "Episode 43\n",
      "[-0.56273377  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "73 159\n",
      "Success in episode 43 at time step 950\n",
      "Episode 44\n",
      "[-0.41874164  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 45\n",
      "Success in episode 44 at time step 269\n",
      "Episode 45\n",
      "[-0.49548292  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "28 44\n",
      "Success in episode 45 at time step 264\n",
      "Episode 46\n",
      "[-0.5138384  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "30 76\n",
      "Success in episode 46 at time step 452\n",
      "Episode 47\n",
      "[-0.4449977  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 27\n",
      "Success in episode 47 at time step 159\n",
      "Episode 48\n",
      "[-0.51948357  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "56 167\n",
      "No Success\n",
      "Episode 49\n",
      "[-0.58224535  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "66 137\n",
      "Success in episode 49 at time step 819\n",
      "Episode 50\n",
      "[-0.40817586  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 33\n",
      "Success in episode 50 at time step 195\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "daifa, results_four = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=50, render_env=False, flip_dynamics=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.43750945  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "67 167\n",
      "No Success\n",
      "Episode 2\n",
      "[-0.5206395  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "6 56\n",
      "Success in episode 2 at time step 334\n",
      "Episode 3\n",
      "[-0.53664637  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 53\n",
      "Success in episode 3 at time step 315\n",
      "Episode 4\n",
      "[-0.4584642  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 53\n",
      "Success in episode 4 at time step 314\n",
      "Episode 5\n",
      "[-0.44199425  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "24 62\n",
      "Success in episode 5 at time step 372\n",
      "Episode 6\n",
      "[-0.5820085  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "25 64\n",
      "Success in episode 6 at time step 381\n",
      "Episode 7\n",
      "[-0.43078417  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 24\n",
      "Success in episode 7 at time step 143\n",
      "Episode 8\n",
      "[-0.548233  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "5 25\n",
      "Success in episode 8 at time step 149\n",
      "Episode 9\n",
      "[-0.45224836  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 25\n",
      "Success in episode 9 at time step 149\n",
      "Episode 10\n",
      "[-0.50875205  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "29 62\n",
      "Success in episode 10 at time step 369\n",
      "Episode 11\n",
      "[-0.44876334  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "40 127\n",
      "Success in episode 11 at time step 761\n",
      "Episode 12\n",
      "[-0.5969452  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "56 129\n",
      "Success in episode 12 at time step 771\n",
      "Episode 13\n",
      "[-0.5480532  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "21 52\n",
      "Success in episode 13 at time step 310\n",
      "Episode 14\n",
      "[-0.505934  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 71\n",
      "Success in episode 14 at time step 424\n",
      "Episode 15\n",
      "[-0.43395558  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "75 167\n",
      "No Success\n",
      "Episode 16\n",
      "[-0.46172836  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "59 98\n",
      "Success in episode 16 at time step 586\n",
      "Episode 17\n",
      "[-0.58230996  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "60 102\n",
      "Success in episode 17 at time step 608\n",
      "Episode 18\n",
      "[-0.5118248  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "73 167\n",
      "No Success\n",
      "Episode 19\n",
      "[-0.50273025  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "39 103\n",
      "Success in episode 19 at time step 616\n",
      "Episode 20\n",
      "[-0.46269357  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "35 86\n",
      "Success in episode 20 at time step 515\n",
      "Episode 21\n",
      "[-0.57779664  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "20 46\n",
      "Success in episode 21 at time step 274\n",
      "Episode 22\n",
      "[-0.45123324  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "9 38\n",
      "Success in episode 22 at time step 227\n",
      "Episode 23\n",
      "[-0.49308443  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "82 122\n",
      "Success in episode 23 at time step 728\n",
      "Episode 24\n",
      "[-0.551804  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "55 99\n",
      "Success in episode 24 at time step 593\n",
      "Episode 25\n",
      "[-0.47908628  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "21 50\n",
      "Success in episode 25 at time step 296\n",
      "Episode 26\n",
      "[-0.46834362  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "30 88\n",
      "Success in episode 26 at time step 523\n",
      "Episode 27\n",
      "[-0.5952358  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "100 167\n",
      "No Success\n",
      "Episode 28\n",
      "[-0.43523842  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 25\n",
      "Success in episode 28 at time step 146\n",
      "Episode 29\n",
      "[-0.44274956  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "1 26\n",
      "Success in episode 29 at time step 153\n",
      "Episode 30\n",
      "[-0.4964754  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 66\n",
      "Success in episode 30 at time step 394\n",
      "Episode 31\n",
      "[-0.56049025  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 35\n",
      "Success in episode 31 at time step 209\n",
      "Episode 32\n",
      "[-0.49966016  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "110 167\n",
      "No Success\n",
      "Episode 33\n",
      "[-0.49757892  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 25\n",
      "Success in episode 33 at time step 149\n",
      "Episode 34\n",
      "[-0.51177806  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 22\n",
      "Success in episode 34 at time step 132\n",
      "Episode 35\n",
      "[-0.47141552  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "6 34\n",
      "Success in episode 35 at time step 203\n",
      "Episode 36\n",
      "[-0.5119726  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "36 81\n",
      "Success in episode 36 at time step 485\n",
      "Episode 37\n",
      "[-0.41683808  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "61 100\n",
      "Success in episode 37 at time step 599\n",
      "Episode 38\n",
      "[-0.40439466  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 23\n",
      "Success in episode 38 at time step 138\n",
      "Episode 39\n",
      "[-0.50185466  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "72 143\n",
      "Success in episode 39 at time step 856\n",
      "Episode 40\n",
      "[-0.40703216  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 25\n",
      "Success in episode 40 at time step 149\n",
      "Episode 41\n",
      "[-0.58549064  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "88 167\n",
      "No Success\n",
      "Episode 42\n",
      "[-0.56644875  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 46\n",
      "Success in episode 42 at time step 274\n",
      "Episode 43\n",
      "[-0.49367848  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "19 47\n",
      "Success in episode 43 at time step 280\n",
      "Episode 44\n",
      "[-0.4626446  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "5 25\n",
      "Success in episode 44 at time step 147\n",
      "Episode 45\n",
      "[-0.5570115  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "100 167\n",
      "No Success\n",
      "Episode 46\n",
      "[-0.58739907  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 36\n",
      "Success in episode 46 at time step 215\n",
      "Episode 47\n",
      "[-0.5529454  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 26\n",
      "Success in episode 47 at time step 154\n",
      "Episode 48\n",
      "[-0.5576318  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "31 63\n",
      "Success in episode 48 at time step 378\n",
      "Episode 49\n",
      "[-0.52270186  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "47 106\n",
      "Success in episode 49 at time step 634\n",
      "Episode 50\n",
      "[-0.49432787  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "96 167\n",
      "No Success\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results_four = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=50, render_env=False, flip_dynamics=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "      reward  timesteps  num_actions\n0  -6.068081       1002          167\n1  -6.230983       1002          167\n2  -6.431071       1002          167\n3  -6.138415       1002          167\n4  -6.204773       1002          167\n5  -6.439107       1002          167\n6  -6.316643       1002          167\n7  -6.352924       1002          167\n8  -6.375400       1002          167\n9  -6.346141       1002          167\n10 -6.284814       1002          167\n11 -6.313875       1002          167\n12 -6.224095       1002          167\n13 -6.106530       1002          167\n14 -6.240360       1002          167\n15 -6.177990       1002          167\n16 -6.252221       1002          167\n17 -6.372750       1002          167\n18 -6.166451       1002          167\n19 -6.136749       1002          167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-6.068081</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-6.230983</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-6.431071</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-6.138415</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-6.204773</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-6.439107</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-6.316643</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-6.352924</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-6.375400</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-6.346141</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-6.284814</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-6.313875</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-6.224095</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-6.106530</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-6.240360</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-6.177990</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-6.252221</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-6.372750</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-6.166451</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-6.136749</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "     reward  timesteps  num_actions\n0 -6.402275       1002          167\n1 -6.180096       1002          167\n2 -6.352802       1002          167\n3 -6.167919       1002          167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-6.402275</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-6.180096</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-6.352802</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-6.167919</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 4, daifa.agent_time_ratio, show_env=False)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_results = pd.concat([results_one, results_two, results_three, results_four])\n",
    "full_results.reset_index(drop=True)\n",
    "full_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "T = np.arange(len(full_results))\n",
    "plt.plot(T, full_results.percent_use_fast_thinking)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(T, full_results.success)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(T, full_results.total_reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(T, full_results.sim_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}