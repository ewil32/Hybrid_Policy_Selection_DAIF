{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lunar Lander"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from vae_recurrent import VAE, create_decoder, create_encoder\n",
    "from transition_gru import TransitionGRU\n",
    "from recurrent_agent import DAIFAgentRecurrent\n",
    "from prior_model import PriorModelBellman\n",
    "from habitual_action_network import HabitualAction, compute_discounted_cumulative_reward\n",
    "from ddpg import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations, test_policy, habit_policy\n",
    "from train_agent import train_single_agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# from identity_vae import IdentityVAE, identity_encoder, identity_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 habitual_action_net,\n",
    "                 given_prior_mean=None,\n",
    "                 given_prior_stddev=None,\n",
    "                 agent_time_ratio=6,\n",
    "                 actions_to_execute_when_exploring=2,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True,\n",
    "                 train_prior_model=True,\n",
    "                 train_habit_net=True,\n",
    "                 train_with_replay=True,\n",
    "                 train_after_exploring=True,\n",
    "                 use_kl_extrinsic=True,\n",
    "                 use_kl_intrinsic=True,\n",
    "                 use_FEEF=True,\n",
    "                 use_fast_thinking=False,\n",
    "                 uncertainty_tolerance=0.05,\n",
    "                 habit_model_type=\"name_of_model\"):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        # parameters for slow policy planning\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        # flags for whether or not we are training models or using pretrained models and when we should train\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "        self.train_habit_net = train_habit_net\n",
    "        self.train_prior = train_prior_model\n",
    "        self.train_with_replay = train_with_replay\n",
    "        self.train_after_exploring = train_after_exploring\n",
    "\n",
    "        # do we use the kl divergence for extrinsic vs intrinsic\n",
    "        self.use_kl_intrinsic = use_kl_intrinsic\n",
    "        self.use_kl_extrinsic = use_kl_extrinsic\n",
    "\n",
    "        # do we use the FEEF or EFE?\n",
    "        self.use_FEEF = use_FEEF\n",
    "\n",
    "        # given prior values\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.tran = tran\n",
    "        self.prior_model = prior_model\n",
    "        self.habit_action_model = habitual_action_net\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "        self.actions_to_execute_when_exploring = actions_to_execute_when_exploring\n",
    "        self.time_step = 0\n",
    "        self.exploring = False\n",
    "\n",
    "        # track the hidden state of the transition gru model so we can use it to train\n",
    "        self.tran_hidden_state = None\n",
    "        self.tran_hidden_state_pre_exploring = None\n",
    "        self.prev_tran_hidden_state = None\n",
    "\n",
    "        # store the full observations for the episode so we can train using replay\n",
    "        self.full_observation_sequence = []\n",
    "        self.full_action_sequence = []\n",
    "        self.full_reward_sequence = []\n",
    "\n",
    "        # store the observations while the agent is in exploration mode\n",
    "        self.exploring_observation_sequence = []\n",
    "        self.exploring_action_sequence = []\n",
    "        self.exploring_reward_sequence = []\n",
    "\n",
    "        # store the observations at the world time scale\n",
    "        self.env_time_scale_observations = []\n",
    "\n",
    "        self.policy_left_to_execute = [None]\n",
    "        self.previous_observation = None\n",
    "        self.action_being_executed = None\n",
    "        self.action_being_executed = [0]*self.tran.action_dim\n",
    "\n",
    "        self.use_fast_thinking = use_fast_thinking\n",
    "        self.habit_model_type = habit_model_type\n",
    "        self.uncertainty_tolerance = uncertainty_tolerance\n",
    "        self.num_fast_thinking_choices = 0\n",
    "\n",
    "\n",
    "    def perceive_and_act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        The function called to have the agent interact with the environment\n",
    "        We assume the agent gets a transformed/noisy observation from the environment and then returns an action\n",
    "\n",
    "        TODO: possibly the agent returns some other information for logging and showing experiments\n",
    "\n",
    "        :param observation:\n",
    "        :param reward:\n",
    "        :param done:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # track the world time scale observation sequence\n",
    "        self.env_time_scale_observations.append(observation)\n",
    "\n",
    "        # if the episode is finished, then do any training on the full data set\n",
    "        if done and self.train_with_replay:\n",
    "\n",
    "            print(\"training on full data\")\n",
    "            print(self.num_fast_thinking_choices, len(self.full_action_sequence))\n",
    "\n",
    "            # add the final observation and reward we observed to the sequences\n",
    "            self.full_observation_sequence.append(observation)\n",
    "            self.full_reward_sequence.append(reward)\n",
    "\n",
    "            # Call the training function on the observation sequences to train everything we need to train\n",
    "            self.train_models(np.vstack(self.full_observation_sequence),\n",
    "                              np.vstack(self.full_action_sequence),\n",
    "                              np.array(self.full_reward_sequence),\n",
    "                              None)\n",
    "\n",
    "\n",
    "        # Otherwise are we at a point where we can reconsider our policy and maybe train the world model\n",
    "        elif self.time_step % self.agent_time_ratio == 0:\n",
    "\n",
    "            # add the observation to the sequence\n",
    "            self.full_observation_sequence.append(observation)\n",
    "\n",
    "            # add the reward only if it's not the first observation\n",
    "            if self.time_step != 0:\n",
    "                self.full_reward_sequence.append(reward)\n",
    "\n",
    "            # We only update the model during the episode when we were exploring using the planning method and we have executed all of the actions in the policy\n",
    "            if self.exploring and len(self.policy_left_to_execute) == 0:\n",
    "\n",
    "                # print(\"f\", self.full_observation_sequence)\n",
    "                # print(\"e\", self.full_observation_sequence[-1*(self.actions_to_execute_when_exploring + 1):])\n",
    "\n",
    "                if self.train_after_exploring:\n",
    "\n",
    "                    # the actions done while exploring were the last self.actions_to_execute_when_exploring\n",
    "                    self.exploring_action_sequence = self.full_action_sequence[-1*self.actions_to_execute_when_exploring:]\n",
    "                    self.exploring_reward_sequence = self.full_reward_sequence[-1*self.actions_to_execute_when_exploring:]\n",
    "                    self.exploring_observation_sequence = self.full_observation_sequence[-1*(self.actions_to_execute_when_exploring + 1):]\n",
    "\n",
    "                    # Call the training function on the observation sequences to train everything we need to train\n",
    "                    self.train_models(np.vstack(self.exploring_observation_sequence),\n",
    "                                      np.vstack(self.exploring_action_sequence),\n",
    "                                      np.array(self.exploring_reward_sequence),\n",
    "                                      self.tran_hidden_state_pre_exploring)\n",
    "\n",
    "                # now we're no longer exploring\n",
    "                self.exploring = False\n",
    "\n",
    "            # print(self.action_being_executed)\n",
    "            # Predict the expected observation based on the previously executed action\n",
    "            action_as_array = np.array(self.action_being_executed).reshape(1, self.tran.action_dim)\n",
    "            expected_observation, self.tran_hidden_state = self.predict_next_observation(self.previous_observation, action_as_array, self.prev_tran_hidden_state)\n",
    "\n",
    "            # Now we select our action. If we aren't exploring then either we act out of habit or we might need to explore\n",
    "            # I think I can check this based on whether or not there are actions left to execute in the current policy\n",
    "            if not self.exploring:\n",
    "\n",
    "                # first observation will have no previous observation\n",
    "                if self.use_fast_thinking and self.previous_observation is None:\n",
    "                    # self.policy_left_to_execute = self.habit_action_model(observation)\n",
    "                    self.policy_left_to_execute = self.select_fast_thinking_policy(observation)\n",
    "                    self.policy_left_to_execute = self.policy_left_to_execute.numpy().tolist()  # tf tensor to list\n",
    "                    print(\"fast thinking\")\n",
    "\n",
    "                # TDOD Fix this to work however it needs to\n",
    "                # we need to see what the generative model now thinks about what the expected current observation is\n",
    "                elif self.use_fast_thinking and np.allclose(observation, expected_observation, atol=self.uncertainty_tolerance):  # within some tolerance\n",
    "\n",
    "                    self.policy_left_to_execute = self.select_fast_thinking_policy(observation)\n",
    "                    # self.policy_left_to_execute = self.policy_left_to_execute + np.random.normal(0, scale=self.habit_action_model.action_std_dev)\n",
    "                    self.policy_left_to_execute = self.policy_left_to_execute.numpy().tolist()\n",
    "\n",
    "                    # self.tran_hidden_state = next_tran_hidden_state\n",
    "                    self.num_fast_thinking_choices += 1\n",
    "                    print(\"fast thinking\")\n",
    "\n",
    "                # the generative model is surprised so we should use the slow deliberation for planning out a policy that balances exploration and exploitation\n",
    "                else:\n",
    "                    # TODO should we actually sample here?\n",
    "                    # print(\"slow thinking\")\n",
    "                    policy = self.select_policy(observation)\n",
    "                    policy = policy.mean().numpy()\n",
    "                    policy = policy.reshape(policy.shape[0], self.tran.action_dim).tolist()\n",
    "                    self.policy_left_to_execute = policy[0: self.actions_to_execute_when_exploring]\n",
    "\n",
    "                    self.tran_hidden_state_pre_exploring = self.tran_hidden_state\n",
    "\n",
    "                    self.exploring = True\n",
    "\n",
    "\n",
    "\n",
    "            # finally update the previous observation and action to be the one we just had/did\n",
    "            self.previous_observation = observation\n",
    "            self.prev_tran_hidden_state = self.tran_hidden_state\n",
    "            self.action_being_executed = self.policy_left_to_execute[0]\n",
    "            self.full_action_sequence.append(self.action_being_executed)\n",
    "            self.policy_left_to_execute.pop(0)\n",
    "\n",
    "        # final updates increment the current timestep and return the action specified by the policy\n",
    "        self.time_step += 1\n",
    "\n",
    "        return self.action_being_executed\n",
    "\n",
    "\n",
    "    def predict_next_observation(self, obs, action, tran_hidden_state):\n",
    "\n",
    "        # TODO: Fix this with the transition hidden states\n",
    "        if obs is None:\n",
    "            return None, None\n",
    "        else:\n",
    "            z_mean, z_std, z = self.model_vae.encoder(obs)\n",
    "            # print(z_mean.shape)\n",
    "            # print(action.shape)\n",
    "            z_mean = z_mean.numpy()\n",
    "            z_plus_action = np.concatenate([z_mean, action], axis=1)\n",
    "            # print(z_mean)\n",
    "            # print(action)\n",
    "            # print(z_plus_action)\n",
    "\n",
    "            z_plus_action = z_plus_action.reshape(1, 1, z_plus_action.shape[1])\n",
    "            # print(z_plus_action)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((z_plus_action, tran_hidden_state))\n",
    "\n",
    "            next_observation = self.model_vae.decoder(next_latent_mean)\n",
    "            # print(next_observation)\n",
    "            return next_observation.numpy(), next_hidden_state\n",
    "\n",
    "\n",
    "    # We use this function to reset the hidden state of the transition model when we want to train on the full data set\n",
    "    def reset_tran_hidden_state(self):\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "\n",
    "    def reset_all_states(self):\n",
    "        self.time_step = 0\n",
    "        self.exploring = False\n",
    "\n",
    "        # track the hidden state of the transition gru model so we can use it to train\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "        # store the full observations for the episode so we can train using replay\n",
    "        # self.complete_observation_sequence = []\n",
    "        self.full_observation_sequence = []\n",
    "        self.full_action_sequence = []\n",
    "        self.full_reward_sequence = []\n",
    "\n",
    "        # store the observations while the agent is in exploration mode\n",
    "        self.exploring_observation_sequence = []\n",
    "        self.exploring_action_sequence = []\n",
    "        self.exploring_reward_sequence = []\n",
    "\n",
    "        self.policy_left_to_execute = []\n",
    "        self.previous_observation = None\n",
    "        self.action_being_executed = [0]*self.tran.action_dim\n",
    "\n",
    "        self.num_fast_thinking_choices = 0\n",
    "\n",
    "\n",
    "    def train_models(self, observations_full, actions, rewards, tran_hidden_state_pre_obs):\n",
    "\n",
    "        pre_observations = observations_full[:-1]\n",
    "        post_observations = observations_full[1:]\n",
    "\n",
    "        # find the actual observed latent states using the vae\n",
    "        pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "        post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "        #### TRAIN THE TRANSITION MODEL ####\n",
    "        if self.train_tran:\n",
    "\n",
    "            num_observations = pre_observations.shape[0]\n",
    "            # observation_dim = pre_observations.shape[1]\n",
    "            action_dim = actions.shape[1]\n",
    "            latent_dim = self.model_vae.latent_dim\n",
    "\n",
    "            # set up the input training data that we use to train the transition model\n",
    "            z_train = np.concatenate([np.array(pre_latent_mean), actions], axis=1)\n",
    "\n",
    "            # we use the sequence to find the right hidden states to use as input\n",
    "            z_train_seq = z_train.reshape((1, num_observations, latent_dim + action_dim))\n",
    "            z_train_singles = z_train.reshape(num_observations, 1, latent_dim + action_dim)\n",
    "\n",
    "            # the previous hidden state is the memory after observing some sequences but it might be None if we're just starting\n",
    "            if tran_hidden_state_pre_obs is None:\n",
    "                tran_hidden_state_pre_obs = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, tran_hidden_state_pre_obs))\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([tran_hidden_state_pre_obs, h_states_for_training], axis=0)\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran.train_epochs, verbose=self.tran.show_training, batch_size=z_train_singles.shape[0])\n",
    "\n",
    "            # now find the new predicted hidden state that we will use for finding the policy\n",
    "            # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "            _, _, final_hidden_state, h_states = self.tran((z_train_seq, tran_hidden_state_pre_obs))\n",
    "            # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "            z_pred, _, _, _ = self.tran((z_train_singles, h_states_for_training))\n",
    "            # print(h_states)\n",
    "            # print(final_hidden_state)\n",
    "            # print(h_states[:, -2, :])\n",
    "            self.prev_tran_hidden_state = h_states[:, -2, :]\n",
    "            self.tran_hidden_state = final_hidden_state\n",
    "\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            # self.model_vae.fit(pre_observations_raw, epochs=self.vae_train_epochs, verbose=self.show_vae_training)\n",
    "            self.model_vae.fit(pre_observations, epochs=self.model_vae.train_epochs, verbose=self.model_vae.show_training, batch_size=pre_observations.shape[0])\n",
    "\n",
    "\n",
    "        #### TRAIN THE PRIOR MODEL ####\n",
    "        # TODO fix how this part should work\n",
    "        if self.train_prior:\n",
    "            # self.prior_model.train(post_observations, rewards, verbose=self.show_prior_training)\n",
    "            # if max(rewards) > 0:\n",
    "                # self.prior_model.train(post_observations, rewards)\n",
    "            self.prior_model.train(post_latent_mean, rewards)\n",
    "\n",
    "\n",
    "        #### TRAIN THE HABIT ACTION NET ####\n",
    "        if self.train_habit_net:\n",
    "\n",
    "            # prior_preferences_mean = tf.convert_to_tensor(self.given_prior_mean, dtype=\"float32\")\n",
    "            # prior_preferences_stddev = tf.convert_to_tensor(self.given_prior_stddev, dtype=\"float32\")\n",
    "            #\n",
    "            # prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "            #\n",
    "            # external_efe = -1 * tf.math.log(prior_dist.prob(post_observations))\n",
    "            # external_efe = external_efe.numpy().reshape(external_efe.shape[0], 1)\n",
    "            #\n",
    "            # one_over_external_efe = 1/external_efe\n",
    "            #\n",
    "            # ten_minus_external_efe = -1*external_efe + 10\n",
    "\n",
    "            # ten_minus_external_efe = ten_minus_external_efe.numpy().reshape(ten_minus_external_efe.shape[0], 1)\n",
    "\n",
    "            # one_over_external_efe = one_over_external_efe.numpy().reshape(one_over_external_efe.shape[0], 1)\n",
    "            # print(one_over_external_efe.shape)\n",
    "\n",
    "            # print(post_observations)\n",
    "            # print(one_over_external_efe)\n",
    "\n",
    "            # obs_utilities = self.prior_model(pre_observations)\n",
    "            # obs_utilities = tf.reduce_sum(obs_utilities, axis=-1)\n",
    "            # obs_utilities = obs_utilities.numpy().reshape(obs_utilities.shape[0], 1)\n",
    "            # # print(obs_utilities)\n",
    "            #\n",
    "            # cum_rewards = compute_discounted_cumulative_reward(obs_utilities, self.habit_action_model.discount_factor)\n",
    "\n",
    "            if self.habit_model_type == \"PG\":\n",
    "                # rewards = rewards.reshape(rewards.shape[0], 1)\n",
    "                # cum_rewards = compute_discounted_cumulative_reward(rewards, self.habit_action_model.discount_factor)\n",
    "                # rewards_to_train_on = cum_rewards\n",
    "\n",
    "                # TODO I think for the final state the V(s_t+1) should be set to 0\n",
    "                # ADVANTAGE\n",
    "                v_state = self.prior_model(pre_latent_mean)\n",
    "                v_plus_one_state = self.prior_model(post_latent_mean)\n",
    "                advantage = rewards + self.prior_model.discount_factor * v_plus_one_state - v_state\n",
    "\n",
    "                # print(advantage)\n",
    "\n",
    "                # DDPG and policy gradient interface with same function\n",
    "                # self.habit_action_model.train(pre_latent_mean, actions, rewards_to_train_on, post_latent_mean)\n",
    "                self.habit_action_model.train(pre_latent_mean, actions, advantage, post_latent_mean)\n",
    "\n",
    "            if self.habit_model_type == \"DDPG\":\n",
    "                self.habit_action_model.train(pre_latent_mean, actions, rewards, post_latent_mean)\n",
    "\n",
    "\n",
    "    def select_fast_thinking_policy(self, observation):\n",
    "\n",
    "        # TODO should you select the mean here?\n",
    "        # _,  _, latent_state = self.model_vae.encoder(observation)\n",
    "        latent_state,  _, _ = self.model_vae.encoder(observation)\n",
    "        action = self.habit_action_model.select_action(latent_state)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "        \"\"\"\n",
    "        :param observation: needs to be [n, observation_dim] shape np array or tf tensor\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO do you take the mean or that latent here?\n",
    "        # get the latent state from this observation\n",
    "        # TODO should I use the mean here?\n",
    "        _,  _, latent_state = self.model_vae.encoder(observation)\n",
    "        # latent_state,  _, _ = self.model_vae.encoder(observation)\n",
    "        # latent_state = latent_state.numpy().reshape((1, latent_state.shape[0]))\n",
    "        # print(latent_state)\n",
    "        # print(latent_state)\n",
    "        # select the policy\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(latent_state)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    # TODO Fix this so we can use different action dimensions\n",
    "    def cem_policy_optimisation(self, latent_z):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros((self.planning_horizon, self.tran.action_dim))\n",
    "        std_best_policies = tf.ones((self.planning_horizon, self.tran.action_dim))\n",
    "\n",
    "        # print(mean_best_policies)\n",
    "        # print(mean_best_policies.shape)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            # print(\"p\", policies.shape)\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "            # policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), latent_z)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            # print(\"POLICIES\", policies)\n",
    "            # print(\"FEEFS\", FEEFs)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by -1 to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape (self.n_policies, latent_dim) when z_t_minus is tensor with shape (1, latent_dim)\n",
    "        prev_latent_mean = tf.squeeze(tf.stack([z_t_minus_one]*self.n_policies, axis=1))\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.tran_hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.tran_hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # print(prev_latent_mean)\n",
    "            # print(policies[:, t, :].shape)\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t, :]], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            # print(tran_input.shape)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        if self.use_FEEF:\n",
    "            return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "        else:\n",
    "            return self.EFE(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "                # Compute the extrinisc approximation with the prior model\n",
    "                else:\n",
    "                    kl_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    kl_extrinsic = tf.reduce_sum(kl_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                kl_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"Extrinsic\", kl_extrinsic)\n",
    "            # print(\"Intrinsic\", kl_intrinsic)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    # TODO Find out how this works with the log probability extrinsic term\n",
    "    def EFE(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        EFEs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack(self.given_prior_mean), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack(self.given_prior_stddev), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    # compute extrinsic prior preferences term\n",
    "                    efe_extrinsic = -1 * tf.math.log(prior_dist.prob(predicted_likelihood))\n",
    "\n",
    "                # TODO Can I use the learned prior model here?\n",
    "                else:\n",
    "                    # efe_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    efe_extrinsic = self.prior_model.extrinsic_kl(predicted_posterior)\n",
    "                    efe_extrinsic = tf.reduce_sum(efe_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                efe_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"EX\")\n",
    "            # print(efe_extrinsic)\n",
    "            # print(\"IN\")\n",
    "            # print(kl_intrinsic)\n",
    "\n",
    "            EFE = efe_extrinsic - kl_intrinsic\n",
    "\n",
    "            EFEs.append(EFE)\n",
    "\n",
    "        return EFEs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Hide GPU from visible devices\n",
    "# tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def identity_encoder(inputs, stddev):\n",
    "\n",
    "    return [tf.convert_to_tensor(inputs, dtype=\"float32\"), stddev * tf.ones_like(inputs, dtype=\"float32\"), tf.convert_to_tensor(inputs, dtype=\"float32\")]\n",
    "\n",
    "\n",
    "def identity_decoder(inputs):\n",
    "    return tf.convert_to_tensor(inputs, dtype=\"float32\")\n",
    "\n",
    "\n",
    "class IdentityVAE(keras.Model):\n",
    "    \"\"\"\n",
    "    Implements the identity mapping with standard deviation as all 1s\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 latent_dim,\n",
    "                 reg_mean=None,\n",
    "                 reg_stddev=None,\n",
    "                 recon_stddev=0.05,\n",
    "                 llik_scaling=1,\n",
    "                 kl_scaling=1,\n",
    "                 train_epochs=1,\n",
    "                 show_training=True,\n",
    "                 **kwargs):\n",
    "\n",
    "        super(IdentityVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.reg_mean = reg_mean\n",
    "        self.reg_stddev = reg_stddev\n",
    "        self.recon_stddev = recon_stddev\n",
    "\n",
    "        self.llik_scaling = llik_scaling\n",
    "        self.kl_scaling = kl_scaling\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        return inputs\n",
    "\n",
    "    def compute_loss(self, x=None):\n",
    "        return 0\n",
    "\n",
    "    def train_step(self, data):\n",
    "        return {\n",
    "            \"total_loss\": 0\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with no prior model FEEF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 0., 1., 1.])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pln_hrzn = 5\n",
    "latent_dim = 8\n",
    "obs_dim = 8\n",
    "action_dim = 2\n",
    "\n",
    "# make the VAE\n",
    "# enc = create_encoder(obs_dim, latent_dim, [200, 100])\n",
    "# dec = create_decoder(latent_dim, obs_dim, [100, 200])\n",
    "# vae = VAE(enc, dec, latent_dim,  [0]*latent_dim, [0.3]*latent_dim, train_epochs=2, show_training=True)\n",
    "# vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "\n",
    "# IdentityVAE, identity_encoder, identity_decoder\n",
    "# identity VAE\n",
    "enc = lambda X: identity_encoder(X, 0.1)\n",
    "dec = lambda X: identity_decoder(X)\n",
    "vae = IdentityVAE(enc, dec, latent_dim)\n",
    "\n",
    "\n",
    "# make the TRANSITION\n",
    "tran = TransitionGRU(latent_dim, action_dim, 2*pln_hrzn*latent_dim, latent_dim, train_epochs=2, show_training=False)\n",
    "tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the HABIT ACTION NET\n",
    "habit_net = HabitualAction(latent_dim, action_dim, [32, 32], train_epochs=2, show_training=False)\n",
    "habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# # # make the HABIT ACTION NET\n",
    "# actor_model = get_actor(2, 1)\n",
    "# critic_model = get_critic(2, 1)\n",
    "# target_actor = get_actor(2, 1)\n",
    "# target_critic = get_critic(2, 1)\n",
    "#\n",
    "# # Making the weights equal initially\n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "# critic_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "# actor_optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "# habit_net = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau=0.005, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)\n",
    "\n",
    "\n",
    "# make the PRIOR NET\n",
    "prior_model = PriorModelBellman(latent_dim, hidden_units=[15, 15], output_dim=1, scaling_factor=1, show_training=True, use_tanh_on_output=False)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0, 0, 0, 0, 0, 0, 1, 1]\n",
    "prior_stddev = [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "prior_stddev = [0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.05, 0.05]\n",
    "\n",
    "observation_max = np.array([1.5, 1.5, 5, 5, 3.14, 5, 1, 1])\n",
    "observation_min = np.array([-1.5, -1.5, -5, -5, -3.14, -5, 0, 0])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = None\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, observation_noise_stddev)  # no noise on prior\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           habit_net,\n",
    "                           given_prior_mean=scaled_prior_mean,\n",
    "                           given_prior_stddev=prior_stddev,\n",
    "                           planning_horizon=pln_hrzn,\n",
    "                           use_kl_extrinsic=True,  # maybe this works\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           train_vae=False,\n",
    "                           train_habit_net=False,\n",
    "                           train_prior_model=False,\n",
    "                           train_tran=True,\n",
    "                           train_after_exploring=True,\n",
    "                           train_with_replay=True,\n",
    "                           use_fast_thinking=False,\n",
    "                           habit_model_type=\"PG\",\n",
    "                           uncertainty_tolerance=0.1)\n",
    "\n",
    "\n",
    "# daifa.train_prior = True\n",
    "# daifa.prior_model.show_training = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[ 0.00761471  1.3990803   0.7712858  -0.52624667 -0.00881689 -0.17470765\n",
      "  0.          0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 11:46:52.884833: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-09-24 11:46:53.022942: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-09-24 11:46:53.080194: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on full data\n",
      "0 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 11:46:54.789400: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-09-24 11:46:54.922138: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-09-24 11:46:54.981268: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in episode 1 at time step 72 with reward -143.95063058734922\n",
      "Episode 2\n",
      "[-0.0023735   1.4125128  -0.24042973  0.07078164  0.00275714  0.05446095\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 11:46:59.077446: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-09-24 11:46:59.267092: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-09-24 11:46:59.351718: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in episode 2 at time step 142 with reward -192.7222330123741\n",
      "Episode 3\n",
      "[ 0.00544357  1.3999424   0.5513607  -0.48790467 -0.00630095 -0.12489128\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 16\n",
      "Success in episode 3 at time step 92 with reward -153.76798881024268\n",
      "Episode 4\n",
      "[ 0.00283566  1.4086992   0.2871912  -0.09871037 -0.00327888 -0.06505306\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 16\n",
      "Success in episode 4 at time step 92 with reward -144.07032559549515\n",
      "Episode 5\n",
      "[-0.00639477  1.4065967  -0.64774734 -0.19216873  0.00741684  0.14672442\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 12\n",
      "Success in episode 5 at time step 67 with reward -161.90767251652855\n",
      "Episode 6\n",
      "[-0.00512247  1.4130161  -0.5188643   0.09313293  0.00594241  0.11753038\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 24\n",
      "Success in episode 6 at time step 141 with reward -111.17239650871042\n",
      "Episode 7\n",
      "[-5.4368970e-04  1.4079202e+00 -5.5087488e-02 -1.3332762e-01\n",
      "  6.3682284e-04  1.2478151e-02  0.0000000e+00  0.0000000e+00]\n",
      "training on full data\n",
      "0 21\n",
      "Success in episode 7 at time step 121 with reward -183.6504238212803\n",
      "Episode 8\n",
      "[ 5.1479338e-04  1.4135011e+00  5.2129589e-02  1.1470886e-01\n",
      " -5.8975641e-04 -1.1808136e-02  0.0000000e+00  0.0000000e+00]\n",
      "training on full data\n",
      "0 20\n",
      "Success in episode 8 at time step 119 with reward -199.35978043640534\n",
      "Episode 9\n",
      "[-6.7920686e-04  1.4181111e+00 -6.8809882e-02  3.1960106e-01\n",
      "  7.9377962e-04  1.5586412e-02  0.0000000e+00  0.0000000e+00]\n",
      "training on full data\n",
      "0 18\n",
      "Success in episode 9 at time step 105 with reward -126.06769838838778\n",
      "Episode 10\n",
      "[ 0.00751114  1.4074628   0.76077574 -0.15368761 -0.00869669 -0.17232713\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 13\n",
      "Success in episode 10 at time step 78 with reward -161.41509428321618\n",
      "Episode 11\n",
      "[ 0.00617714  1.4137071   0.62566715  0.12385727 -0.00715102 -0.141723\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 20\n",
      "Success in episode 11 at time step 117 with reward -124.38193381390032\n",
      "Episode 12\n",
      "[ 0.00465288  1.399933    0.47126698 -0.48831514 -0.00538469 -0.10674886\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 11\n",
      "Success in episode 12 at time step 66 with reward -109.34497460691335\n",
      "Episode 13\n",
      "[ 0.00421991  1.4016347   0.42741615 -0.4126927  -0.00488303 -0.09681603\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 12\n",
      "Success in episode 13 at time step 72 with reward -176.27168674625767\n",
      "Episode 14\n",
      "[-0.00443296  1.4124672  -0.4490323   0.0687461   0.00514355  0.10171237\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 15\n",
      "Success in episode 14 at time step 86 with reward -157.79280247332971\n",
      "Episode 15\n",
      "[-0.0031827   1.413692   -0.32238102  0.12319263  0.00369467  0.07302409\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 18\n",
      "Success in episode 15 at time step 105 with reward -155.4973933658568\n",
      "Episode 16\n",
      "[-0.00631781  1.412191   -0.6399341   0.05646983  0.00732746  0.14495464\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 21\n",
      "Success in episode 16 at time step 123 with reward -122.63418509180052\n",
      "Episode 17\n",
      "[ 0.00223579  1.4187447   0.22643428  0.34775928 -0.00258381 -0.05129073\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 22\n",
      "Success in episode 17 at time step 132 with reward -180.48079845406266\n",
      "Episode 18\n",
      "[ 0.00761652  1.4165756   0.771467    0.2513227  -0.00881895 -0.17474864\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 19\n",
      "Success in episode 18 at time step 112 with reward -181.98747434166393\n",
      "Episode 19\n",
      "[-0.0071867   1.4126587  -0.72794634  0.07725556  0.00833434  0.16489075\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 16\n",
      "Success in episode 19 at time step 91 with reward -179.4885009620977\n",
      "Episode 20\n",
      "[ 0.00601616  1.4021167   0.6093611  -0.39127207 -0.00696449 -0.13802934\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 13\n",
      "Success in episode 20 at time step 74 with reward -182.1755040064417\n",
      "Episode 21\n",
      "[-0.00682936  1.4073342  -0.6917454  -0.15940171  0.00792021  0.15669072\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 14\n",
      "Success in episode 21 at time step 79 with reward -172.0795165410932\n",
      "Episode 22\n",
      "[-1.01900101e-03  1.41976881e+00 -1.03239015e-01  3.93267244e-01\n",
      "  1.18765212e-03  2.33851783e-02  0.00000000e+00  0.00000000e+00]\n",
      "training on full data\n",
      "0 21\n",
      "Success in episode 22 at time step 122 with reward -136.94473072041842\n",
      "Episode 23\n",
      "[ 0.00366669  1.4125013   0.37137777  0.07027307 -0.00424196 -0.08412258\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 17\n",
      "Success in episode 23 at time step 99 with reward -141.78535522699332\n",
      "Episode 24\n",
      "[-0.00211763  1.4128124  -0.21450111  0.08409914  0.00246052  0.04858772\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 20\n",
      "Success in episode 24 at time step 115 with reward -111.04536686283573\n",
      "Episode 25\n",
      "[-0.00382929  1.4165306  -0.38788074  0.24934891  0.00444398  0.08786077\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 16\n",
      "Success in episode 25 at time step 92 with reward -166.50547345018578\n",
      "Episode 26\n",
      "[-0.00632944  1.4129038  -0.6411184   0.08814418  0.00734101  0.14522274\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 14\n",
      "Success in episode 26 at time step 79 with reward -185.9443556473592\n",
      "Episode 27\n",
      "[-0.00789433  1.4146527  -0.7996193   0.16586645  0.00915428  0.18112575\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 16\n",
      "Success in episode 27 at time step 92 with reward -152.7255877681783\n",
      "Episode 28\n",
      "[-0.00553093  1.4057881  -0.56024724 -0.22809717  0.00641586  0.1269045\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 12\n",
      "Success in episode 28 at time step 72 with reward -101.09293874725951\n",
      "Episode 29\n",
      "[ 0.00169077  1.4064288   0.17123239 -0.19961838 -0.0019523  -0.03878667\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 23\n",
      "Success in episode 29 at time step 138 with reward -153.57016377611654\n",
      "Episode 30\n",
      "[ 0.00440254  1.4041477   0.4459072  -0.30100307 -0.00509459 -0.10100462\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 12\n",
      "Success in episode 30 at time step 67 with reward -102.43688612256277\n",
      "Episode 31\n",
      "[ 0.00763006  1.4091923   0.7728325  -0.0768178  -0.00883459 -0.17505814\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 13\n",
      "Success in episode 31 at time step 76 with reward -123.06257369313737\n",
      "Episode 32\n",
      "[ 1.2096405e-03  1.4071163e+00  1.2250900e-01 -1.6906044e-01\n",
      " -1.3948893e-03 -2.7750071e-02  0.0000000e+00  0.0000000e+00]\n",
      "training on full data\n",
      "0 14\n",
      "Success in episode 32 at time step 83 with reward -160.6782664952496\n",
      "Episode 33\n",
      "[ 0.00679197  1.4175948   0.68794125  0.2966341  -0.00786342 -0.15582898\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 11\n",
      "Success in episode 33 at time step 63 with reward -187.9932253541188\n",
      "Episode 34\n",
      "[-0.00684051  1.401651   -0.69287944 -0.41198874  0.00793317  0.15694734\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 13\n",
      "Success in episode 34 at time step 74 with reward -164.69899605672632\n",
      "Episode 35\n",
      "[-0.00354118  1.4006736  -0.3587026  -0.45540974  0.00411018  0.08125149\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 12\n",
      "Success in episode 35 at time step 69 with reward -138.94475988411529\n",
      "Episode 36\n",
      "[-0.00328598  1.4011562  -0.33285204 -0.43394452  0.00381444  0.07539604\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 15\n",
      "Success in episode 36 at time step 88 with reward -176.1930360097423\n",
      "Episode 37\n",
      "[-1.3376236e-03  1.4039325e+00 -1.3549928e-01 -3.1056485e-01\n",
      "  1.5567362e-03  3.0692583e-02  0.0000000e+00  0.0000000e+00]\n",
      "training on full data\n",
      "0 15\n",
      "Success in episode 37 at time step 87 with reward -86.08781206083198\n",
      "Episode 38\n",
      "[ 0.00390635  1.406908    0.39565435 -0.17832215 -0.00451968 -0.08962151\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 17\n",
      "Success in episode 38 at time step 99 with reward -118.9706909025915\n",
      "Episode 39\n",
      "[ 0.00758429  1.4143187   0.7682022   0.15102454 -0.0087816  -0.1740091\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 13\n",
      "Success in episode 39 at time step 77 with reward -172.73125765806407\n",
      "Episode 40\n",
      "[-0.0048893   1.4224147  -0.4952558   0.5108579   0.00567235  0.11218293\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 14\n",
      "Success in episode 40 at time step 83 with reward -152.35663693267276\n",
      "Episode 41\n",
      "[ 1.8453598e-04  1.4014328e+00  1.8683970e-02 -4.2165488e-01\n",
      " -2.0712549e-04 -4.2322255e-03  0.0000000e+00  0.0000000e+00]\n",
      "training on full data\n",
      "0 15\n",
      "Success in episode 41 at time step 87 with reward -147.7957206474176\n",
      "Episode 42\n",
      "[-1.22547150e-03  1.42054784e+00 -1.24150194e-01  4.27894861e-01\n",
      "  1.42688770e-03  2.81218477e-02  0.00000000e+00  0.00000000e+00]\n",
      "training on full data\n",
      "0 20\n",
      "Success in episode 42 at time step 117 with reward -184.44632952714923\n",
      "Episode 43\n",
      "[-0.00628662  1.4141083  -0.63678765  0.14168034  0.00729148  0.14424199\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 16\n",
      "Success in episode 43 at time step 95 with reward -182.00166293464278\n",
      "Episode 44\n",
      "[ 0.00271072  1.4029912   0.27455464 -0.35239282 -0.00313428 -0.06219057\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 12\n",
      "Success in episode 44 at time step 70 with reward -165.42266220831164\n",
      "Episode 45\n",
      "[-1.2913704e-03  1.4087728e+00 -1.3082497e-01 -9.5436886e-02\n",
      "  1.5032374e-03  2.9633790e-02  0.0000000e+00  0.0000000e+00]\n",
      "training on full data\n",
      "0 19\n",
      "Success in episode 45 at time step 113 with reward -66.05802316362568\n",
      "Episode 46\n",
      "[-3.7870408e-04  1.4205317e+00 -3.8369469e-02  4.2718905e-01\n",
      "  4.4555429e-04  8.6912662e-03  0.0000000e+00  0.0000000e+00]\n",
      "training on full data\n",
      "0 21\n",
      "Success in episode 46 at time step 126 with reward -163.25222808707517\n",
      "Episode 47\n",
      "[-0.0018362   1.4102148  -0.18600531 -0.03134434  0.00213452  0.04213301\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 16\n",
      "Success in episode 47 at time step 91 with reward -108.3896606161087\n",
      "Episode 48\n",
      "[-0.00411577  1.4014142  -0.41689548 -0.4224984   0.0047759   0.09443299\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 9\n",
      "Success in episode 48 at time step 53 with reward -144.11712760575045\n",
      "Episode 49\n",
      "[ 0.00682955  1.4118975   0.6917435   0.04342063 -0.00790693 -0.15669015\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 14\n",
      "Success in episode 49 at time step 82 with reward -117.99241357495373\n",
      "Episode 50\n",
      "[-0.00667582  1.4201173  -0.6762043   0.40873578  0.00774238  0.15317056\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 29\n",
      "Success in episode 50 at time step 172 with reward -143.38802847683877\n",
      "Episode 51\n",
      "[ 0.00215054  1.4057535   0.21780494 -0.22962786 -0.00248509 -0.04933599\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 20\n",
      "Success in episode 51 at time step 118 with reward -155.26815718540894\n",
      "Episode 52\n",
      "[-0.00779152  1.4005051  -0.78921205 -0.46291044  0.00903519  0.17876816\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 14\n",
      "Success in episode 52 at time step 83 with reward -106.53498771521075\n",
      "Episode 53\n",
      "[-2.8800964e-04  1.4152910e+00 -2.9191017e-02  1.9426468e-01\n",
      "  3.4055620e-04  6.6122250e-03  0.0000000e+00  0.0000000e+00]\n",
      "training on full data\n",
      "0 22\n",
      "Success in episode 53 at time step 130 with reward -209.62577521915432\n",
      "Episode 54\n",
      "[-0.00636349  1.4011822  -0.6445732  -0.43282497  0.00738053  0.14600544\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 10\n",
      "Success in episode 54 at time step 58 with reward -144.41260438355337\n",
      "Episode 55\n",
      "[-0.00463228  1.4006326  -0.4692121  -0.45723054  0.00537442  0.10628344\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 24\n",
      "Success in episode 55 at time step 142 with reward -162.80367991183113\n",
      "Episode 56\n",
      "[-0.00445671  1.4019185  -0.45143548 -0.40008098  0.00517103  0.10225683\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 12\n",
      "Success in episode 56 at time step 69 with reward -117.35130764524673\n",
      "Episode 57\n",
      "[-0.00157509  1.4125311  -0.15957303  0.07160039  0.00183211  0.03614568\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 14\n",
      "Success in episode 57 at time step 82 with reward -176.3817879329908\n",
      "Episode 58\n",
      "[-0.00610428  1.4009461  -0.6183113  -0.44330448  0.00708011  0.14005674\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 10\n",
      "Success in episode 58 at time step 58 with reward -136.4373999711244\n",
      "Episode 59\n",
      "[-0.00423889  1.4016085  -0.42937574 -0.4138643   0.00491868  0.09725998\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 12\n",
      "Success in episode 59 at time step 68 with reward -130.76692408963805\n",
      "Episode 60\n",
      "[ 0.0053483   1.408383    0.54172075 -0.11276734 -0.00619067 -0.12270789\n",
      "  0.          0.        ]\n",
      "training on full data\n",
      "0 14\n",
      "Success in episode 60 at time step 83 with reward -101.8352359942655\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "daifa, results_one = train_single_agent(env, daifa, obs_max=observation_max, obs_min=observation_min, observation_noise_stddev=None, num_episodes=60, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x2ebac4400>]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results_one.VFE_post_run)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12643678 0.1768126 ]\n",
      "[0.12770037 0.17442903]\n",
      "[0.1294361  0.17261635]\n",
      "[0.13158748 0.17060891]\n",
      "[0.13326299 0.16883524]\n",
      "[0.13388364 0.16703786]\n",
      "[0.13525562 0.1652819 ]\n",
      "[0.13569395 0.16295421]\n",
      "[0.13658956 0.16116865]\n",
      "[0.13745289 0.15814021]\n",
      "[0.13856505 0.15699579]\n",
      "[0.13972741 0.1559071 ]\n",
      "[0.14118204 0.15431614]\n",
      "[0.14131513 0.15123852]\n",
      "[0.14229223 0.14843044]\n",
      "[0.14303233 0.14599179]\n",
      "[0.14411312 0.1435683 ]\n",
      "[0.14526515 0.14228174]\n",
      "[0.14648373 0.14173934]\n",
      "[0.14871046 0.13994057]\n",
      "[0.15032844 0.13823327]\n",
      "[0.1520063  0.13637826]\n",
      "[0.15327339 0.13544966]\n",
      "[0.15546517 0.13274662]\n",
      "[0.15774284 0.12831914]\n",
      "[0.15996037 0.12473579]\n",
      "[0.16294298 0.12147006]\n",
      "[0.1656171  0.11737504]\n",
      "[0.16766398 0.11367574]\n",
      "[0.16984738 0.10923476]\n",
      "[0.1727904  0.10440448]\n",
      "[0.17596447 0.10012767]\n",
      "[0.17885605 0.09427588]\n",
      "[0.18184501 0.08804055]\n",
      "[0.18469088 0.08382998]\n",
      "[0.18897656 0.07993722]\n",
      "[0.19360839 0.07478351]\n",
      "[0.19808625 0.07059658]\n",
      "[0.20304522 0.06531512]\n",
      "[0.20591204 0.05799587]\n",
      "[0.13090193 0.18360767]\n",
      "[0.12930042 0.17889205]\n",
      "[0.12775193 0.17595007]\n",
      "[0.12707438 0.1737669 ]\n",
      "[0.12641397 0.17044225]\n",
      "[0.12797782 0.17094433]\n",
      "[0.12951109 0.17335449]\n",
      "[0.1308239  0.17301756]\n",
      "[0.13382882 0.16811758]\n",
      "[0.1371514  0.16407219]\n",
      "[0.14075398 0.15999578]\n",
      "[0.1448854 0.1564606]\n",
      "[0.14914201 0.15319644]\n",
      "[0.15443885 0.15023795]\n",
      "[0.15922686 0.14675017]\n",
      "[0.16400391 0.14388481]\n",
      "[0.16819885 0.1411789 ]\n",
      "[0.1737656  0.13845538]\n",
      "[0.17851819 0.13540179]\n",
      "[0.1825138  0.13161574]\n",
      "[0.1884048  0.12893678]\n",
      "[0.19431327 0.12730657]\n",
      "[0.20101883 0.1262916 ]\n",
      "[0.2090693  0.12728623]\n",
      "[0.10906683 0.16790518]\n",
      "[0.11453032 0.1613554 ]\n",
      "[0.12261351 0.16032997]\n",
      "[0.13102211 0.15677252]\n",
      "[0.13774957 0.14582618]\n",
      "[0.14492881 0.13406214]\n",
      "[0.15281656 0.12191833]\n",
      "[0.15986009 0.10821966]\n",
      "[0.16621871 0.09464631]\n",
      "[0.17364465 0.08270644]\n",
      "[0.17791782 0.0722484 ]\n",
      "[0.1836895  0.06130553]\n",
      "[0.18455514 0.04826047]\n",
      "[0.18557152 0.03475282]\n",
      "[0.130336   0.17583823]\n",
      "[0.1304988  0.17491393]\n",
      "[0.13024305 0.17432821]\n",
      "[0.13005006 0.1721098 ]\n",
      "[0.13012083 0.17066686]\n",
      "[0.12996088 0.16853112]\n",
      "[0.12933317 0.16571537]\n",
      "[0.12945917 0.16541815]\n",
      "[0.13019638 0.1648296 ]\n",
      "[0.12921043 0.16290736]\n",
      "[0.1291652  0.16342764]\n",
      "[0.12990275 0.16509078]\n",
      "[0.13012083 0.16597442]\n",
      "[0.13031216 0.16669115]\n",
      "[0.13009472 0.16872442]\n",
      "[0.12950231 0.16985188]\n",
      "[0.13122565 0.16863628]\n",
      "[0.13368414 0.16553462]\n",
      "[0.13753705 0.16288233]\n",
      "[0.14137195 0.16088864]\n",
      "[0.14539416 0.1585334 ]\n",
      "[0.15035832 0.15673643]\n",
      "[0.15565221 0.15564442]\n",
      "[0.16092919 0.15430373]\n",
      "[0.16648032 0.15273842]\n",
      "[0.17232032 0.15167019]\n",
      "[0.1786696  0.14925022]\n",
      "[0.18472867 0.14753583]\n",
      "[0.1911728  0.14677772]\n",
      "[0.20044386 0.14923301]\n",
      "[0.21133243 0.1548639 ]\n",
      "[0.22244544 0.16054364]\n",
      "[0.1253781  0.16687647]\n",
      "[0.12752646 0.16617939]\n",
      "[0.12890254 0.16378699]\n",
      "[0.13129884 0.16452672]\n",
      "[0.1334268  0.16543247]\n",
      "[0.13568977 0.16403437]\n",
      "[0.13763572 0.16129   ]\n",
      "[0.13957846 0.15916593]\n",
      "[0.14296374 0.1575777 ]\n",
      "[0.14513075 0.15821214]\n",
      "[0.14712146 0.15679781]\n",
      "[0.14854217 0.15737101]\n",
      "[0.15029521 0.15596414]\n",
      "[0.15313868 0.15554515]\n",
      "[0.15567915 0.15254812]\n",
      "[0.15864146 0.14904124]\n",
      "[0.16211747 0.14755957]\n",
      "[0.16639517 0.14397028]\n",
      "[0.17062937 0.14183486]\n",
      "[0.17466955 0.13846396]\n",
      "[0.17814244 0.13509162]\n",
      "[0.12068177 0.16401011]\n",
      "[0.12489595 0.16224462]\n",
      "[0.12935978 0.16222839]\n",
      "[0.13351896 0.1628817 ]\n",
      "[0.13775986 0.16044013]\n",
      "[0.14173621 0.15823929]\n",
      "[0.14670317 0.1526689 ]\n",
      "[0.15387665 0.14767216]\n",
      "[0.16073614 0.14237265]\n",
      "[0.16755126 0.13763759]\n",
      "[0.17318532 0.1323947 ]\n",
      "[0.18063153 0.12665968]\n",
      "[0.1880757  0.11998942]\n",
      "[0.19560866 0.11379611]\n",
      "[0.20436327 0.10499816]\n",
      "[0.21380033 0.09692774]\n",
      "[0.13063732 0.17653544]\n",
      "[0.12983412 0.17544346]\n",
      "[0.12962507 0.17445649]\n",
      "[0.12974627 0.17375554]\n",
      "[0.13003887 0.17223571]\n",
      "[0.13073577 0.16975734]\n",
      "[0.13127343 0.1674432 ]\n",
      "[0.13269788 0.16674605]\n",
      "[0.13376455 0.16622965]\n",
      "[0.13431996 0.16514257]\n",
      "[0.1356248  0.16617876]\n",
      "[0.13710795 0.16646206]\n",
      "[0.13836764 0.1668861 ]\n",
      "[0.13976194 0.16629927]\n",
      "[0.14262241 0.16359764]\n",
      "[0.14542162 0.16112947]\n",
      "[0.14887413 0.15906912]\n",
      "[0.15119356 0.15718403]\n",
      "[0.15374525 0.15478233]\n",
      "[0.15610893 0.15168563]\n",
      "[0.15937899 0.1489015 ]\n",
      "[0.16188923 0.1468037 ]\n",
      "[0.16431764 0.14440462]\n",
      "[0.1669908  0.14240828]\n",
      "[0.16952233 0.14037466]\n",
      "[0.17215995 0.13773628]\n",
      "[0.17468205 0.13521066]\n",
      "[0.1768971  0.13301006]\n",
      "[0.17915791 0.13113624]\n",
      "[0.18114865 0.12859783]\n",
      "[0.18394908 0.12742454]\n",
      "[0.18722126 0.12604132]\n",
      "[0.19045019 0.12531312]\n",
      "[0.19427012 0.12342661]\n",
      "[0.19748475 0.12162448]\n",
      "[0.2004876  0.12014191]\n",
      "[0.20363364 0.11893681]\n",
      "[0.20719087 0.11819258]\n",
      "[0.21105774 0.11779562]\n",
      "[0.21496008 0.11712525]\n",
      "[0.21942322 0.11634701]\n",
      "[0.2233755 0.1164116]\n",
      "[0.22774504 0.11668435]\n",
      "[0.23237838 0.11711014]\n",
      "[0.23701224 0.11744055]\n",
      "[0.24145514 0.11737783]\n",
      "[0.24490425 0.1179974 ]\n",
      "[0.24675849 0.11850993]\n",
      "[0.25100422 0.11834662]\n",
      "[0.25395778 0.11923551]\n",
      "[0.25660762 0.11999948]\n",
      "[0.26036248 0.11969602]\n",
      "[0.2637725  0.11978499]\n",
      "[0.26759815 0.12002917]\n",
      "[0.2711637  0.12014648]\n",
      "[0.27488947 0.11906625]\n",
      "[0.12486728 0.17483632]\n",
      "[0.12682164 0.17278613]\n",
      "[0.12885836 0.17058031]\n",
      "[0.13058504 0.16681843]\n",
      "[0.13193609 0.16427843]\n",
      "[0.13380477 0.16278625]\n",
      "[0.13608903 0.16014275]\n",
      "[0.13931534 0.15774976]\n",
      "[0.14187486 0.15430105]\n",
      "[0.14494154 0.15011023]\n",
      "[0.14810218 0.14617467]\n",
      "[0.15233843 0.14372543]\n",
      "[0.15638351 0.1416484 ]\n",
      "[0.15890424 0.13651226]\n",
      "[0.16320491 0.13061294]\n",
      "[0.16647604 0.12418977]\n",
      "[0.16915399 0.11919408]\n",
      "[0.17155284 0.11144959]\n",
      "[0.17632343 0.10225762]\n",
      "[0.18238403 0.09481601]\n",
      "[0.18814003 0.08605023]\n",
      "[0.19330232 0.07462491]\n",
      "[0.2001216  0.06444214]\n",
      "[0.2063958  0.05191385]\n",
      "[0.21815173 0.04097741]\n",
      "[0.23053668 0.03034602]\n",
      "[0.12085035 0.1745744 ]\n",
      "[0.12390801 0.17051794]\n",
      "[0.12733507 0.16758724]\n",
      "[0.13068219 0.16390279]\n",
      "[0.1340789  0.16148321]\n",
      "[0.1380904  0.15822135]\n",
      "[0.14167573 0.1522467 ]\n",
      "[0.14516838 0.14700204]\n",
      "[0.14878531 0.1440922 ]\n",
      "[0.15187179 0.14072163]\n",
      "[0.15409338 0.13511012]\n",
      "[0.15732673 0.13079211]\n",
      "[0.16030438 0.12506703]\n",
      "[0.1634083  0.11802737]\n",
      "[0.16618387 0.11108649]\n",
      "[0.16914137 0.10207985]\n",
      "[0.17197017 0.09143751]\n",
      "[0.17612553 0.08258569]\n",
      "[0.18114138 0.07193714]\n",
      "[0.18677278 0.06228082]\n",
      "[0.1924437  0.05260159]\n",
      "[0.19941552 0.04368982]\n",
      "[0.1221231  0.17834137]\n",
      "[0.12472761 0.1733139 ]\n",
      "[0.12642008 0.16846922]\n",
      "[0.1282109  0.16495523]\n",
      "[0.13021553 0.16113995]\n",
      "[0.13329533 0.1592636 ]\n",
      "[0.13555682 0.15663046]\n",
      "[0.13849966 0.15297763]\n",
      "[0.14081325 0.15033184]\n",
      "[0.14215991 0.14815405]\n",
      "[0.14372928 0.14672516]\n",
      "[0.14587478 0.14755169]\n",
      "[0.14798419 0.14717412]\n",
      "[0.14942202 0.14623068]\n",
      "[0.1515821  0.14486913]\n",
      "[0.15325111 0.14341584]\n",
      "[0.15417986 0.1425166 ]\n",
      "[0.15592082 0.14235115]\n",
      "[0.15637656 0.14089926]\n",
      "[0.15810147 0.139128  ]\n",
      "[0.16018231 0.13390078]\n",
      "[0.16143987 0.12859659]\n",
      "[0.16380133 0.1229079 ]\n",
      "[0.16590016 0.11703608]\n",
      "[0.16822386 0.11153561]\n",
      "[0.17008908 0.10594508]\n",
      "[0.17234157 0.10025842]\n",
      "[0.1736452  0.09425897]\n",
      "[0.17479183 0.08870723]\n",
      "[0.17656033 0.08312516]\n",
      "[0.17840551 0.07771968]\n",
      "[0.17973205 0.07297751]\n",
      "[0.18100415 0.07000953]\n",
      "[0.18419099 0.07285461]\n",
      "[0.18664102 0.07619811]\n",
      "[0.19228978 0.08326388]\n",
      "[0.19883983 0.08773617]\n",
      "[0.20715423 0.09097935]\n",
      "[0.21430807 0.09269644]\n",
      "[0.22189692 0.09318175]\n",
      "[0.2266376  0.09236778]\n",
      "[0.23024696 0.0913107 ]\n",
      "[0.2351973  0.08987032]\n",
      "[0.12712464 0.17662637]\n",
      "[0.12772977 0.17521985]\n",
      "[0.12968145 0.17428313]\n",
      "[0.13056554 0.17184207]\n",
      "[0.13185722 0.16908848]\n",
      "[0.13317585 0.16549826]\n",
      "[0.13480914 0.16426073]\n",
      "[0.13768302 0.16170768]\n",
      "[0.13981892 0.15914659]\n",
      "[0.14222041 0.15583573]\n",
      "[0.14479735 0.15306829]\n",
      "[0.14769603 0.15186022]\n",
      "[0.14984925 0.14876367]\n",
      "[0.15292986 0.14629129]\n",
      "[0.15618406 0.14378956]\n",
      "[0.15881877 0.14182407]\n",
      "[0.16181868 0.13870497]\n",
      "[0.16455226 0.13535385]\n",
      "[0.16710494 0.12942046]\n",
      "[0.16959086 0.12455018]\n",
      "[0.17244563 0.11960687]\n",
      "[0.17637199 0.11382391]\n",
      "[0.1795211  0.10619281]\n",
      "[0.18317427 0.09831754]\n",
      "[0.18612085 0.09151244]\n",
      "[0.18993203 0.08238488]\n",
      "[0.1925477  0.07215378]\n",
      "[0.19436395 0.06183179]\n",
      "[0.19768104 0.05089726]\n",
      "[0.20299357 0.03942751]\n",
      "[0.20550317 0.02809058]\n",
      "[0.13083723 0.16212714]\n",
      "[0.1322652  0.16612487]\n",
      "[0.13551019 0.17163526]\n",
      "[0.139155   0.17587057]\n",
      "[0.14300752 0.18030787]\n",
      "[0.14456053 0.18516862]\n",
      "[0.14577554 0.19094001]\n",
      "[0.14852217 0.1943393 ]\n",
      "[0.15158525 0.19355266]\n",
      "[0.154849   0.19003056]\n",
      "[0.15794216 0.18595771]\n",
      "[0.16101538 0.1812905 ]\n",
      "[0.16258767 0.17841874]\n",
      "[0.16473103 0.17692441]\n",
      "[0.16783376 0.17555033]\n",
      "[0.14466083 0.3950488 ]\n",
      "[0.15337919 0.16602139]\n",
      "[0.15056092 0.17187682]\n",
      "[0.14823595 0.17770866]\n",
      "[0.14594187 0.18568438]\n",
      "[0.14387417 0.19073036]\n",
      "[0.14173485 0.19557387]\n",
      "[0.13878809 0.20151086]\n",
      "[0.13638644 0.20852119]\n",
      "[0.13914417 0.21495557]\n",
      "[0.14330329 0.22366056]\n",
      "[0.14864315 0.23458947]\n",
      "[0.15309355 0.24231385]\n",
      "[0.15906015 0.2484892 ]\n",
      "[0.16486815 0.25271517]\n",
      "[0.17060657 0.25270784]\n",
      "[0.17708974 0.25251007]\n",
      "[0.13656084 0.16808775]\n",
      "[0.13615112 0.1688355 ]\n",
      "[0.13630663 0.16947809]\n",
      "[0.1355591  0.17044535]\n",
      "[0.13624883 0.17263266]\n",
      "[0.13819467 0.17394167]\n",
      "[0.13855638 0.17659587]\n",
      "[0.13886672 0.1788264 ]\n",
      "[0.13865577 0.18248932]\n",
      "[0.13939649 0.18684869]\n",
      "[0.13996132 0.19190344]\n",
      "[0.14145705 0.1948233 ]\n",
      "[0.14225504 0.1996426 ]\n",
      "[0.14319609 0.20510516]\n",
      "[0.14450867 0.20995046]\n",
      "[0.14609341 0.21395668]\n",
      "[0.14701669 0.2192484 ]\n",
      "[0.15008888 0.2186329 ]\n",
      "[0.15289201 0.21773706]\n",
      "[0.15504755 0.2182278 ]\n",
      "[0.15876344 0.2173364 ]\n",
      "[0.16182718 0.21299757]\n",
      "[0.16306654 0.20793018]\n",
      "[0.16428608 0.1997357 ]\n",
      "[0.16601817 0.19093516]\n",
      "[0.16858661 0.17890789]\n",
      "[0.17246339 0.16811427]\n",
      "[0.17578639 0.16213363]\n",
      "[0.18138374 0.15664683]\n",
      "[0.18879862 0.15359737]\n",
      "[0.19428177 0.1432547 ]\n",
      "[0.19640379 0.14495264]\n",
      "[0.16208044 0.15604202]\n",
      "[0.15678102 0.16099237]\n",
      "[0.15253471 0.16843626]\n",
      "[0.14678194 0.17657113]\n",
      "[0.14620773 0.18676469]\n",
      "[0.148786   0.19788761]\n",
      "[0.15222439 0.20948102]\n",
      "[0.15774825 0.222127  ]\n",
      "[0.16118959 0.23091665]\n",
      "[0.16477636 0.2396868 ]\n",
      "[0.16842204 0.2469506 ]\n",
      "[0.17130776 0.25925982]\n",
      "[0.18284126 0.26004392]\n",
      "[0.20227653 0.26461992]\n",
      "[0.22488043 0.27289554]\n",
      "[0.25053972 0.27534142]\n",
      "[0.1417609  0.17081688]\n",
      "[0.1402654  0.17175466]\n",
      "[0.13915057 0.17388867]\n",
      "[0.13875127 0.1766658 ]\n",
      "[0.13870673 0.17712164]\n",
      "[0.13768412 0.17938343]\n",
      "[0.13684408 0.18142997]\n",
      "[0.1361477  0.18520083]\n",
      "[0.13602316 0.18725865]\n",
      "[0.13556346 0.18897954]\n",
      "[0.13510099 0.19139816]\n",
      "[0.13392733 0.19587833]\n",
      "[0.13357964 0.2012007 ]\n",
      "[0.13377504 0.20635138]\n",
      "[0.13436471 0.21179819]\n",
      "[0.13593023 0.21494475]\n",
      "[0.13968766 0.21802478]\n",
      "[0.1463621  0.22246236]\n",
      "[0.15252899 0.22616835]\n",
      "[0.15641971 0.22614205]\n",
      "[0.15955585 0.22403318]\n",
      "[0.10977883 0.17256041]\n",
      "[0.11615232 0.16526522]\n",
      "[0.12367966 0.15983672]\n",
      "[0.13104174 0.15803611]\n",
      "[0.13926063 0.15310945]\n",
      "[0.14692411 0.14500336]\n",
      "[0.1541096  0.13472912]\n",
      "[0.16056883 0.12450401]\n",
      "[0.16664992 0.11175387]\n",
      "[0.17455429 0.09912107]\n",
      "[0.18035327 0.08632044]\n",
      "[0.18522888 0.07501026]\n",
      "[0.19066149 0.06360448]\n",
      "[0.19382364 0.04966053]\n",
      "[0.19634977 0.03599213]\n",
      "[0.19655132 0.01956623]\n",
      "[0.14421952 0.16327481]\n",
      "[0.1436365 0.1650577]\n",
      "[0.14260857 0.16811836]\n",
      "[0.14156508 0.17165864]\n",
      "[0.14009456 0.17490338]\n",
      "[0.13835423 0.17855525]\n",
      "[0.1375705  0.18392746]\n",
      "[0.1367577  0.19127415]\n",
      "[0.13589494 0.19843347]\n",
      "[0.13405906 0.20481682]\n",
      "[0.13305259 0.21047787]\n",
      "[0.13304617 0.21548815]\n",
      "[0.1324803  0.22157599]\n",
      "[0.13242337 0.22752091]\n",
      "[0.13332464 0.2357023 ]\n",
      "[0.13461117 0.24238534]\n",
      "[0.13673203 0.2475572 ]\n",
      "[0.13945171 0.25427344]\n",
      "[0.14100787 0.26357004]\n",
      "[0.14245905 0.27483407]\n",
      "[0.14522047 0.28608784]\n",
      "[0.14678285 0.29510581]\n",
      "[0.146852  0.3058746]\n",
      "[0.14876358 0.3147844 ]\n",
      "[0.15277335 0.32649952]\n",
      "[0.5486151 0.2539867]\n",
      "[0.13740388 0.16903168]\n",
      "[0.13771985 0.17018148]\n",
      "[0.13830528 0.17123975]\n",
      "[0.13796039 0.17245342]\n",
      "[0.13919751 0.17290846]\n",
      "[0.13875073 0.17490987]\n",
      "[0.13791808 0.17648098]\n",
      "[0.13868205 0.17828248]\n",
      "[0.13968831 0.18190779]\n",
      "[0.14097479 0.18560189]\n",
      "[0.1419355  0.18886733]\n",
      "[0.14292236 0.19102387]\n",
      "[0.14225544 0.19513173]\n",
      "[0.1425037  0.19816168]\n",
      "[0.14326386 0.20045273]\n",
      "[0.14447995 0.20194007]\n",
      "[0.14603321 0.20278881]\n",
      "[0.1460363  0.20387194]\n",
      "[0.14757858 0.20302573]\n",
      "[0.14972709 0.20038119]\n",
      "[0.15226482 0.19739933]\n",
      "[0.15372716 0.19410509]\n",
      "[0.15485363 0.19011377]\n",
      "[0.15602556 0.18145595]\n",
      "[0.15830162 0.1732773 ]\n",
      "[0.16125266 0.16562986]\n",
      "[0.16407892 0.16060938]\n",
      "[0.16700275 0.1565424 ]\n",
      "[0.17216006 0.15190789]\n",
      "[0.17813563 0.1474258 ]\n",
      "[0.18367955 0.14229532]\n",
      "[0.19010264 0.13966319]\n",
      "[0.19638383 0.13547242]\n",
      "[0.12787314 0.17723809]\n",
      "[0.12736335 0.17628005]\n",
      "[0.12804547 0.17400593]\n",
      "[0.12853068 0.16971406]\n",
      "[0.13038005 0.1675587 ]\n",
      "[0.13191019 0.16624567]\n",
      "[0.13258894 0.16370587]\n",
      "[0.13351917 0.16167921]\n",
      "[0.13501385 0.1600868 ]\n",
      "[0.13508949 0.15844898]\n",
      "[0.13611771 0.15858984]\n",
      "[0.13665713 0.15827526]\n",
      "[0.13756232 0.15870103]\n",
      "[0.13835232 0.15905686]\n",
      "[0.13944443 0.16008516]\n",
      "[0.14008258 0.15958482]\n",
      "[0.14018574 0.15995769]\n",
      "[0.14022967 0.15826713]\n",
      "[0.14058328 0.15552883]\n",
      "[0.14163052 0.15260658]\n",
      "[0.14169757 0.14945595]\n",
      "[0.142141  0.1471333]\n",
      "[0.14324939 0.14524347]\n",
      "[0.14452271 0.14361984]\n",
      "[0.1442249 0.1416906]\n",
      "[0.14819068 0.1410081 ]\n",
      "[0.15383314 0.13937455]\n",
      "[0.15918013 0.13785645]\n",
      "[0.16528662 0.13688844]\n",
      "[0.17183635 0.13574818]\n",
      "[0.17700845 0.1337552 ]\n",
      "[0.18101238 0.13158792]\n",
      "[0.18582557 0.12991318]\n",
      "[0.19111152 0.13004857]\n",
      "[0.19718964 0.13411309]\n",
      "[0.20375377 0.13838309]\n"
     ]
    },
    {
     "data": {
      "text/plain": "        reward  timesteps  num_actions\n0  -142.683333        240           40\n1  -160.275509        144           24\n2  -144.070711         84           14\n3  -160.240566        192           32\n4  -118.581080        126           21\n5  -132.026663         96           16\n6  -146.894146        336           56\n7  -157.527405        156           26\n8  -147.209805        132           22\n9  -193.534703        258           43\n10 -160.853743        186           31\n11 -105.288829         96           16\n12 -136.328477         96           16\n13 -139.246004        192           32\n14 -129.892468         96           16\n15 -145.157343        126           21\n16 -157.382683         96           16\n17  -77.227643        156           26\n18 -143.709211        198           33\n19 -169.949615        216           36",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-142.683333</td>\n      <td>240</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-160.275509</td>\n      <td>144</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-144.070711</td>\n      <td>84</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-160.240566</td>\n      <td>192</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-118.581080</td>\n      <td>126</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-132.026663</td>\n      <td>96</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-146.894146</td>\n      <td>336</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-157.527405</td>\n      <td>156</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-147.209805</td>\n      <td>132</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-193.534703</td>\n      <td>258</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-160.853743</td>\n      <td>186</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-105.288829</td>\n      <td>96</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-136.328477</td>\n      <td>96</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-139.246004</td>\n      <td>192</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-129.892468</td>\n      <td>96</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-145.157343</td>\n      <td>126</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-157.382683</td>\n      <td>96</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-77.227643</td>\n      <td>156</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-143.709211</td>\n      <td>198</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-169.949615</td>\n      <td>216</td>\n      <td>36</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "daifa.train_vae = False\n",
    "daifa.model_vae.show_training = False\n",
    "\n",
    "daifa.train_tran = True\n",
    "daifa.tran.show_training = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.0074296   1.4090451  -0.7525569  -0.08336726  0.00861586  0.17046544\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.6372\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.0368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.5965\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.6137\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.0651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 9.2782\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.2923\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.0403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 9.4404\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 11.2895\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.2086\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 12.0169\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.5234\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 12.1603\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.3648\n",
      "training on full data\n",
      "0 17\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 13.6504\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 12.9486\n",
      "Success in episode 1 at time step 101 with reward -116.15350677734322\n",
      "Episode 2\n",
      "[-0.00317955  1.39928    -0.32206997 -0.51734483  0.00369108  0.07295362\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.8119\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.6085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.6021\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.2525\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.9461\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.2104\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.4246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.3362\n",
      "training on full data\n",
      "0 12\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 19.7551\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 18.8025\n",
      "Success in episode 2 at time step 69 with reward -110.75353126382032\n",
      "Episode 3\n",
      "[-0.00552044  1.3982916  -0.55918086 -0.5612758   0.00640364  0.12666276\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9856\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.5871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.9445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5459\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.6348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.6878\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.9399\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.1099\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.2505\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.3267\n",
      "training on full data\n",
      "0 12\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 24.3206\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 23.6056\n",
      "Success in episode 3 at time step 68 with reward -75.87519152117473\n",
      "Episode 4\n",
      "[-0.00601692  1.3986174  -0.60946715 -0.54680413  0.00697892  0.13805328\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.4769\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.3679\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.5680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.8170\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.1616\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.4438\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.9687\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.0548\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 159.8273\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 157.7594\n",
      "training on full data\n",
      "0 11\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 44.2300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 42.8127\n",
      "Success in episode 4 at time step 62 with reward -71.41237563288105\n",
      "Episode 5\n",
      "[ 0.00695105  1.4010811   0.70405424 -0.4373019  -0.00804778 -0.1594787\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.1390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.3370\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.8886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.2451\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.1530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.3481\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.5181\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.3080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 10.9475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.2215\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 70.7498\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 69.7656\n",
      "training on full data\n",
      "0 13\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 20.8089\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 20.1008\n",
      "Success in episode 5 at time step 74 with reward -125.2295657152413\n",
      "Episode 6\n",
      "[-0.00750456  1.4111466  -0.760147    0.01003644  0.00870269  0.1721845\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.3475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.6013\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.5411\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.9354\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.5237\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.4591\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.3294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.7278\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.5959\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.5278\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.5989\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.3700\n",
      "training on full data\n",
      "0 14\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 5.2695\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 4.4582\n",
      "Success in episode 6 at time step 83 with reward -156.77014355086555\n",
      "Episode 7\n",
      "[ 0.00541267  1.4173113   0.5482353   0.28404465 -0.0062652  -0.12418345\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.8522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.2964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.1421\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.5791\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 11.6495\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 11.5090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.7313\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 8.0231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.3037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.6158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.2265\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.9772\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.4998\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.9840\n",
      "training on full data\n",
      "0 16\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 37ms/step - kl_loss: 9.4178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 8.5373\n",
      "Success in episode 7 at time step 95 with reward -158.4333861774487\n",
      "Episode 8\n",
      "[ 2.3765564e-04  1.4118960e+00  2.4059964e-02  4.3363884e-02\n",
      " -2.6863834e-04 -5.4499446e-03  0.0000000e+00  0.0000000e+00]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.4745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.2283\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.8647\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.0325\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.1748\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.2868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5490\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.5835\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.1722\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.1477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.9456\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.7796\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.8760\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.4768\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.3407\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2485\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.9345\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7189\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.4465\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.8752\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.8043\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.7361\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.3920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.3816\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.4463\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.3820\n",
      "training on full data\n",
      "0 30\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 56ms/step - kl_loss: 2.0835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 1.7377\n",
      "Success in episode 8 at time step 178 with reward -142.9576388870982\n",
      "Episode 9\n",
      "[-0.00470028  1.4022738  -0.47610274 -0.3842912   0.00545324  0.10784434\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.9353\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.7501\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.5186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.3516\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.6659\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.5216\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.4831\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.2649\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.7648\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.1515\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 11.3598\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 9.9776\n",
      "training on full data\n",
      "0 14\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 5.7184\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 4.7472\n",
      "Success in episode 9 at time step 81 with reward -189.29623553465672\n",
      "Episode 10\n",
      "[-0.00150003  1.4170105  -0.1519442   0.27068588  0.00174487  0.03441762\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.7708\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.9737\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.8780\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.4792\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.6825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.7634\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.9377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.4518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.7760\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.9672\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.5509\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.7495\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8301\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.2509\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.5966\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.3960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5080\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.4835\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 95.5335\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 92.1547\n",
      "training on full data\n",
      "0 21\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 37ms/step - kl_loss: 15.9525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 15.2939\n",
      "Success in episode 10 at time step 124 with reward -143.15653591168297\n",
      "Episode 11\n",
      "[-8.1892015e-04  1.4062606e+00 -8.2966350e-02 -2.0709065e-01\n",
      "  9.5574331e-04  1.8793132e-02  0.0000000e+00  0.0000000e+00]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.7645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.6772\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.7133\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.9369\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.5068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.7144\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.1564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.0270\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.7123\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.3017\n",
      "training on full data\n",
      "0 11\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 21.8513\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 20.8586\n",
      "Success in episode 11 at time step 65 with reward -112.90041732836292\n",
      "Episode 12\n",
      "[-0.00665703  1.4064205  -0.6743051  -0.20000938  0.00772066  0.15274009\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.7095\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.1417\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.4812\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.2934\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7124\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7385\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5587\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.6656\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.2976\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.0260\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.9025\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.4985\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.2546\n",
      "training on full data\n",
      "0 16\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 13ms/step - kl_loss: 12.9825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 12.6871\n",
      "Success in episode 12 at time step 96 with reward -135.78726941894962\n",
      "Episode 13\n",
      "[-0.0062871   1.4095107  -0.63683367 -0.06265526  0.00729199  0.14425221\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.3124\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.8850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.3193\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.9817\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.2590\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.1396\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.5066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.3990\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.2566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.9632\n",
      "training on full data\n",
      "0 11\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 3.7663\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 3.4388\n",
      "Success in episode 13 at time step 62 with reward -154.17991875470364\n",
      "Episode 14\n",
      "[-0.00158491  1.406205   -0.16056368 -0.20956182  0.00184345  0.03637002\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.3333\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.4962\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.8489\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.9595\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.9339\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.9884\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.2787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.2758\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2766\n",
      "training on full data\n",
      "0 13\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 39ms/step - kl_loss: 16.4961\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 16.0748\n",
      "Success in episode 14 at time step 76 with reward -146.30853689857238\n",
      "Episode 15\n",
      "[ 5.7220461e-07  1.4086188e+00  3.4527176e-05 -1.0227836e-01\n",
      "  6.2046975e-06 -7.8179173e-06  0.0000000e+00  0.0000000e+00]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.2268\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.6187\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.6512\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.6772\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.8251\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.2503\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.3911\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5505\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.6123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.5999\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.4751\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.9726\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.6052\n",
      "training on full data\n",
      "0 16\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 14.6753\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 14.3101\n",
      "Success in episode 15 at time step 92 with reward -154.78313369313986\n",
      "Episode 16\n",
      "[-0.00469999  1.406026   -0.47607794 -0.21752797  0.00545293  0.10783876\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.4094\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.2479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.1323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.2023\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.2030\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.3856\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.8899\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0050\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.0375\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.0172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.2660\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.0870\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.4525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.9659\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.0123\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.3276\n",
      "training on full data\n",
      "0 18\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 38ms/step - kl_loss: 13.0019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 12.5583\n",
      "Success in episode 16 at time step 108 with reward -106.62236847933178\n",
      "Episode 17\n",
      "[ 0.00264187  1.4137201   0.2675833   0.12444482 -0.00305453 -0.06061172\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.7655\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9543\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.7975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.1858\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.7319\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.9035\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.4715\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.4273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.4589\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.0801\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.5955\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.0756\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.1765\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.5795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.7280\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.0450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.6523\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.7613\n",
      "training on full data\n",
      "0 19\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 12.1908\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 11.7992\n",
      "Success in episode 17 at time step 114 with reward -194.82135717589637\n",
      "Episode 18\n",
      "[-0.00301676  1.4043584  -0.3055826  -0.29163486  0.00350247  0.06921887\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.8385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.9316\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.2738\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.3496\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.2555\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.2703\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.2749\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.0626\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.2233\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.5682\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 9.6317\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.4301\n",
      "training on full data\n",
      "0 14\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 15.5458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 14.4086\n",
      "Success in episode 18 at time step 81 with reward -117.59483204114667\n",
      "Episode 19\n",
      "[ 0.00190411  1.4178901   0.19284959  0.3097755  -0.00219961 -0.04368336\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.3832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.0409\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.7109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.7302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 8.1018\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.7115\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.6010\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 8.4504\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.9316\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.3866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.1721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 5.4104\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.2419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.5762\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.4334\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.2174\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.8136\n",
      "training on full data\n",
      "0 19\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 4.4478\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.2872\n",
      "Success in episode 19 at time step 113 with reward -179.45357227608662\n",
      "Episode 20\n",
      "[ 0.00317717  1.4043114   0.32180053 -0.2937202  -0.00367477 -0.07289255\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2203\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.0959\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.8339\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.8459\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.6996\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.6881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.3321\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.3613\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.6031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.5644\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.6421\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.6041\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.3492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.2743\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 84.1262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 82.8906\n",
      "training on full data\n",
      "0 18\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 14.3442\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 14.1152\n",
      "Success in episode 20 at time step 105 with reward -153.05863511089507\n",
      "Episode 21\n",
      "[ 4.2047500e-04  1.4037578e+00  4.2568441e-02 -3.1833163e-01\n",
      " -4.8037423e-04 -9.6423756e-03  0.0000000e+00  0.0000000e+00]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.5280\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.5370\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.3647\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.4675\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 0.9412\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.9311\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.3370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.2386\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6655\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.4975\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.2329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.5758\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.0674\n",
      "training on full data\n",
      "0 17\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 3.6983\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 3.6044\n",
      "Success in episode 21 at time step 101 with reward -158.52563277786757\n",
      "Episode 22\n",
      "[ 0.00523329  1.4110546   0.5300513   0.00596954 -0.0060572  -0.12006462\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 37ms/step - kl_loss: 4.2296\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 4.1508\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.0990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.0727\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.2325\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.0129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.9859\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.5864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3910\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.0690\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.5344\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.9508\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.7690\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.3314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.2384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.4619\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.4160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 83.6847\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 81.7699\n",
      "training on full data\n",
      "0 21\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 12.3417\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 12.0206\n",
      "Success in episode 22 at time step 125 with reward -136.8349617016666\n",
      "Episode 23\n",
      "[-1.04904175e-04  1.42141199e+00 -1.06395204e-02  4.66306657e-01\n",
      "  1.28331085e-04  2.40996317e-03  0.00000000e+00  0.00000000e+00]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.3076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.3752\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.8653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.9075\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.1183\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.0764\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.8731\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 1.7218\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0764\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.9318\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.9204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7022\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5417\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7381\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.6329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.6842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5070\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3318\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9174\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.5203\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.1496\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.6537\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9332\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.4033\n",
      "training on full data\n",
      "0 30\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 8.1455\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 7.9606\n",
      "Success in episode 23 at time step 178 with reward -120.18276306686577\n",
      "Episode 24\n",
      "[ 0.00205288  1.4156433   0.20791931  0.20992169 -0.00237198 -0.04709683\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.0548\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 3.0162\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.5459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.6471\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.1190\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.3372\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.6257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.7759\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.1850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.1728\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0152\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.8554\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.8214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.5155\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.2857\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.8948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.3459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8352\n",
      "training on full data\n",
      "0 19\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 13ms/step - kl_loss: 11.6809\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 11.4281\n",
      "Success in episode 24 at time step 113 with reward -99.6775069246745\n",
      "Episode 25\n",
      "[ 0.00172205  1.3991718   0.17441157 -0.52214104 -0.00198865 -0.0395068\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.4891\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.7874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.9731\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.4073\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.5794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.1910\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.2251\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.3313\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.9509\n",
      "training on full data\n",
      "0 11\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 35ms/step - kl_loss: 16.3473\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 16.0483\n",
      "Success in episode 25 at time step 65 with reward -143.10365506990522\n",
      "Episode 26\n",
      "[ 0.00475206  1.4111171   0.481306    0.00875115 -0.00549954 -0.10902289\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.7004\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.4614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.9746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.8735\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.8109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.7704\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.0706\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.1159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.3265\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.2642\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.5470\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.3372\n",
      "training on full data\n",
      "0 14\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 2.4291\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 2.1904\n",
      "Success in episode 26 at time step 81 with reward -139.8768347309085\n",
      "Episode 27\n",
      "[ 0.00192995  1.4090787   0.19546974 -0.08183923 -0.00222955 -0.04427683\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7376\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.0131\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 1.1813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.4808\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.4979\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.9209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.7631\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.7259\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.5132\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.8112\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.3095\n",
      "training on full data\n",
      "0 14\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 11.9430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 11.6325\n",
      "Success in episode 27 at time step 84 with reward -141.1541725354835\n",
      "Episode 28\n",
      "[-4.3621063e-04  1.3998375e+00 -4.4201437e-02 -4.9256599e-01\n",
      "  5.1226333e-04  1.0012238e-02  0.0000000e+00  0.0000000e+00]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.4324\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3805\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.5074\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.5814\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.7188\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.8350\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.1858\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.3017\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2435\n",
      "training on full data\n",
      "0 12\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 17.4340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 17.0752\n",
      "Success in episode 28 at time step 72 with reward -100.59750705619281\n",
      "Episode 29\n",
      "[ 0.00550947  1.4133773   0.5580224   0.10919779 -0.0063772  -0.1264004\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.9211\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.7472\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.6354\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.4621\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0984\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.0993\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.2352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.2085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.2600\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.0863\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.9609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.7338\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.8988\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.5617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.4677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.8171\n",
      "training on full data\n",
      "0 17\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 9.8654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 9.6288\n",
      "Success in episode 29 at time step 100 with reward -157.76498059951817\n",
      "Episode 30\n",
      "[-1.3867378e-03  1.4024256e+00 -1.4047781e-01 -3.7752840e-01\n",
      "  1.6136734e-03  3.1820312e-02  0.0000000e+00  0.0000000e+00]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6025\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6263\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.9287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.0474\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.1571\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.2451\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 2.1669\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.1872\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.2398\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.0064\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.2754\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.5518\n",
      "training on full data\n",
      "0 14\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 4.1573\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 3.4319\n",
      "Success in episode 30 at time step 81 with reward -201.31347139048628\n",
      "Episode 31\n",
      "[ 0.0041625   1.4091312   0.42160583 -0.07951108 -0.00481656 -0.09549996\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.1333\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.1891\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.6502\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 4.1083\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.3412\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.6453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.2572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.3225\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.2276\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.9949\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.7710\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.8764\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6711\n",
      "training on full data\n",
      "0 16\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 13.1056\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 12.3082\n",
      "Success in episode 31 at time step 95 with reward -135.20973527439023\n",
      "Episode 32\n",
      "[-3.3855438e-05  1.4079516e+00 -3.4408174e-03 -1.3193755e-01\n",
      "  4.5980145e-05  7.7939592e-04  0.0000000e+00  0.0000000e+00]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.7811\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.0630\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.0678\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.4390\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.8778\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.4139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.6991\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.0777\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.0417\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 8.4526\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.9100\n",
      "training on full data\n",
      "0 14\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 19.1825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 18.0493\n",
      "Success in episode 32 at time step 84 with reward -103.01533762687006\n",
      "Episode 33\n",
      "[ 0.00372515  1.4075698   0.37729856 -0.1489105  -0.00430969 -0.0854636\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.7705\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.3349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.8713\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.9951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.1903\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.7424\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.0999\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.4524\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.9129\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.8177\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.1796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 9.4271\n",
      "training on full data\n",
      "0 13\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 13ms/step - kl_loss: 10.6755\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 9.6437\n",
      "Success in episode 33 at time step 75 with reward -165.77019945562623\n",
      "Episode 34\n",
      "[-0.00176592  1.4070435  -0.17888983 -0.17229167  0.00205311  0.04052123\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.1946\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.9391\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.7047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.7094\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.2423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.1847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.2579\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.1664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0451\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 2.2409\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.3762\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.4599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6998\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.7404\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.2030\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.1195\n",
      "training on full data\n",
      "0 17\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 15.1388\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 14.1269\n",
      "Success in episode 34 at time step 101 with reward -100.07000367444017\n",
      "Episode 35\n",
      "[-0.00168076  1.4069068  -0.17026272 -0.17836934  0.00195442  0.03856705\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.1325\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7761\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.5873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.9799\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0587\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.6158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.0297\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.3935\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.3713\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.6244\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.5143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.2466\n",
      "training on full data\n",
      "0 14\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 19.3683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 17.7106\n",
      "Success in episode 35 at time step 83 with reward -116.38220661975086\n",
      "Episode 36\n",
      "[ 0.00260334  1.3994367   0.2636825  -0.5103702  -0.00300991 -0.05972806\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2604\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.0137\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.9193\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 1.2792\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6838\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2976\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.1575\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.7242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.4921\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.6192\n",
      "training on full data\n",
      "0 12\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 6.5310\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 5.4341\n",
      "Success in episode 36 at time step 68 with reward -139.40666017189946\n",
      "Episode 37\n",
      "[ 0.00412273  1.4100823   0.41757965 -0.03724121 -0.0047705  -0.09458798\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.7459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.9766\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.8982\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.6465\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.0829\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.2882\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.7149\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.8045\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8927\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.0308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.8577\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.7205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.4061\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.0696\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 36.7891\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 34.6626\n",
      "training on full data\n",
      "0 17\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 6.0209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 5.3171\n",
      "Success in episode 37 at time step 97 with reward -160.13797221834125\n",
      "Episode 38\n",
      "[-0.00594587  1.4179683  -0.60226387  0.31323698  0.00689654  0.13642183\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.7968\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.8628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.1014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.5068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.0795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.4050\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 5.4122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.4313\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.2097\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.7459\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.0512\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.1779\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.2454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.2789\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.8624\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.8724\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.4365\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.7689\n",
      "training on full data\n",
      "0 19\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 2.7389\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 2.5631\n",
      "Success in episode 38 at time step 113 with reward -155.95703563367826\n",
      "Episode 39\n",
      "[-0.0032383   1.4187918  -0.32801855  0.34984767  0.00375916  0.07430115\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.6585\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.4099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2402\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6779\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.5448\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5736\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.9512\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0125\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.0602\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.9476\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.2365\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.8016\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.7298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.1570\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.5250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.8658\n",
      "training on full data\n",
      "0 19\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 3.3377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 3.1933\n",
      "Success in episode 39 at time step 110 with reward -187.72933567377504\n",
      "Episode 40\n",
      "[-0.00759745  1.3989418  -0.76956177 -0.532396    0.00881039  0.17431708\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.4635\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.4119\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.5939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.7183\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.0586\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.1955\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5467\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5986\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.1804\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.0758\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.8405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.5431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.6340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.1776\n",
      "training on full data\n",
      "0 16\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 13ms/step - kl_loss: 20.5123\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 19.6938\n",
      "Success in episode 40 at time step 91 with reward -110.44546867928626\n",
      "Episode 41\n",
      "[ 0.00630274  1.4107788   0.6383793  -0.00630248 -0.00729647 -0.14460246\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.0827\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8461\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.7854\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.1300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.1290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.4556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.3709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.8795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.6197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8191\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.4612\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.7239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.3049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.1398\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 3.5990\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8582\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.2781\n",
      "training on full data\n",
      "0 19\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 13.4550\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 12.6083\n",
      "Success in episode 41 at time step 111 with reward -137.24557048432695\n",
      "Episode 42\n",
      "[-0.00744886  1.4128968  -0.7545052   0.08781883  0.00863815  0.17090656\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.0004\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.6308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.7529\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.4720\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.3828\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.4116\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3563\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3552\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.1118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.0369\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.8701\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.5558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.8226\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.2273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 85.3145\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 83.2017\n",
      "training on full data\n",
      "0 17\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 19.3550\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 17.8347\n",
      "Success in episode 42 at time step 97 with reward -126.59589070871912\n",
      "Episode 43\n",
      "[ 0.00523138  1.4007016   0.5298612  -0.45416558 -0.00605501 -0.12002134\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.3770\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.8350\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.1601\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.9297\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.0693\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.3559\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.4779\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.9227\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.6729\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.9648\n",
      "training on full data\n",
      "0 12\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 25.2412\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 24.5996\n",
      "Success in episode 43 at time step 67 with reward -103.78039278765635\n",
      "Episode 44\n",
      "[-0.00471344  1.4049889  -0.47744125 -0.26361796  0.00546854  0.1081476\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.8854\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.2089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 9.5076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.6374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 10.9616\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 10.4672\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.7869\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.7125\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.4643\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.9630\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.9013\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.3180\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.0121\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.5532\n",
      "training on full data\n",
      "0 16\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 2.5459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 1.8323\n",
      "Success in episode 44 at time step 95 with reward -151.2073522881503\n",
      "Episode 45\n",
      "[-0.00316772  1.4057521  -0.32086867 -0.22969396  0.00367737  0.07268142\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.8188\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.2566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.7648\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.6143\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.4936\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.0508\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.3266\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.5315\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.2639\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.0007\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.7184\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 5.0706\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.4720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.7841\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 92.5471\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 91.3534\n",
      "training on full data\n",
      "0 17\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 22.1404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 21.5433\n",
      "Success in episode 45 at time step 100 with reward -110.03817129849472\n",
      "Episode 46\n",
      "[ 0.00774651  1.4082377   0.7846123  -0.11924163 -0.00896939 -0.17772642\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.7288\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.0051\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5381\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2969\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.1366\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2470\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.5020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.5340\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.4472\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.2059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.4259\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.8977\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.7219\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.8536\n",
      "training on full data\n",
      "0 16\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 3.3349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 2.6454\n",
      "Success in episode 46 at time step 93 with reward -140.35679773645919\n",
      "Episode 47\n",
      "[ 0.00324812  1.4186075   0.3289842   0.34165943 -0.00375698 -0.07451986\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.8865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.9776\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.6296\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.0152\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.5024\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.7292\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 8.3545\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.0181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.9463\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.3139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.5663\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.6382\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.1145\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5283\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.8824\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.1796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.8773\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.9990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.0750\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.9164\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.1438\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.9807\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.8334\n",
      "training on full data\n",
      "0 25\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 60ms/step - kl_loss: 5.6019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 4.6073\n",
      "Success in episode 47 at time step 146 with reward -176.2059958486232\n",
      "Episode 48\n",
      "[-8.288383e-04  1.419384e+00 -8.397256e-02  3.761703e-01  9.672631e-04\n",
      "  1.902099e-02  0.000000e+00  0.000000e+00]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3187\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.7975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.5655\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.5599\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.6505\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.3986\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.9753\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.9027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 9.4536\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 11.6882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.1874\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.7690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.6060\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.4507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.5167\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.0261\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.0840\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.9132\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.8833\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.4597\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.8526\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.9479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6188\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5343\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0342\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.7488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.6826\n",
      "training on full data\n",
      "0 32\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 66ms/step - kl_loss: 8.9776\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 7.3985\n",
      "Success in episode 48 at time step 190 with reward -189.15005041158005\n",
      "Episode 49\n",
      "[-0.00285244  1.4171565  -0.28893605  0.27717024  0.00331204  0.06544826\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.4808\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.9298\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.9101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.7879\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.5740\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.3879\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.3156\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 10.3160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 13.4291\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 13.3123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 14.5176\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 13.1850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 10.0856\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 8.4542\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.4652\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.1771\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2614\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6375\n",
      "training on full data\n",
      "0 20\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 38ms/step - kl_loss: 12.0228\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 13ms/step - kl_loss: 11.0308\n",
      "Success in episode 49 at time step 119 with reward -130.20555404324125\n",
      "Episode 50\n",
      "[ 0.00614471  1.4052744   0.6223755  -0.25093958 -0.00711338 -0.1409774\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.4733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.3265\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.2267\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.2937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.2495\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 7.4836\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.0231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 11.6985\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 11.4695\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 15.3872\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 14.3118\n",
      "training on full data\n",
      "0 14\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 40ms/step - kl_loss: 21.8596\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 20.3052\n",
      "Success in episode 50 at time step 83 with reward -147.7770658771688\n",
      "Episode 51\n",
      "[ 0.00562267  1.4134256   0.56949586  0.11134452 -0.00650842 -0.12899902\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.0551\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.9449\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.8101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.2306\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.0029\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9699\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.5605\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.4053\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.6859\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.1140\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.9419\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.3629\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.7149\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.3499\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.8025\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.3687\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.7302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.5503\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.1786\n",
      "training on full data\n",
      "0 22\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 40ms/step - kl_loss: 11.5183\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 10.7862\n",
      "Success in episode 51 at time step 132 with reward -99.97741102184396\n",
      "Episode 52\n",
      "[-0.00151615  1.4202065  -0.15358092  0.4127262   0.00176357  0.03478833\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5329\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5706\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.2744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.8587\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.0389\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5118\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.3787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.6294\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.7022\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.7820\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.6212\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.5353\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 3.4438\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.1985\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.8073\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.3167\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.7761\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 14.2991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 12.7254\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.5911\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.6728\n",
      "training on full data\n",
      "0 23\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 38ms/step - kl_loss: 2.6320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 2.2830\n",
      "Success in episode 52 at time step 136 with reward -157.51445152023308\n",
      "Episode 53\n",
      "[ 0.00615072  1.3997241   0.62298065 -0.4976167  -0.0071203  -0.14111444\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.6025\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.3010\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.9675\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.9141\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.9631\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.0229\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.6320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.6910\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.2869\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2217\n",
      "training on full data\n",
      "0 12\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 16.1024\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 15.8266\n",
      "Success in episode 53 at time step 69 with reward -146.89137403910934\n",
      "Episode 54\n",
      "[ 0.00509758  1.4012234   0.5163221  -0.43098018 -0.00590011 -0.11695464\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.7539\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.5520\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.8116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.6578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.0127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.0670\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.1508\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2680\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.0339\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 6.9202\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 13.2566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 12.3148\n",
      "training on full data\n",
      "0 13\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 15.2155\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 14.8818\n",
      "Success in episode 54 at time step 73 with reward -178.3077020575067\n",
      "Episode 55\n",
      "[-0.00256586  1.410427   -0.2599098  -0.02192145  0.00298     0.05887336\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.7655\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.8771\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.0057\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.1043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.6207\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.4568\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.6341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.1830\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9468\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.4784\n",
      "training on full data\n",
      "0 12\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 37.4019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 36.2291\n",
      "Success in episode 55 at time step 71 with reward -106.1985425059694\n",
      "Episode 56\n",
      "[ 0.00263948  1.4220779   0.26734072  0.4958969  -0.00305175 -0.0605566\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.0855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.3372\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.5140\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8885\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.4750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.5392\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.6759\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.5982\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.3590\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.1347\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.9697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.5697\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.2959\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.6535\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.0641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.3492\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.8306\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 4.0293\n",
      "training on full data\n",
      "0 20\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 2.3139\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 1.6576\n",
      "Success in episode 56 at time step 116 with reward -164.80799406912402\n",
      "Episode 57\n",
      "[ 2.6521683e-04  1.3989036e+00  2.6847258e-02 -5.3406155e-01\n",
      " -3.0052470e-04 -6.0812593e-03  0.0000000e+00  0.0000000e+00]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.5833\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.2844\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7647\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.9312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.8199\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.1511\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.6864\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9601\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.9250\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.9576\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.6600\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.0398\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.6381\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.4731\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.1791\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 118.4759\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 116.9925\n",
      "training on full data\n",
      "0 19\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 39ms/step - kl_loss: 13.4707\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 13ms/step - kl_loss: 12.8057\n",
      "Success in episode 57 at time step 111 with reward -91.89245753718929\n",
      "Episode 58\n",
      "[-0.00596209  1.409833   -0.60391104 -0.04833563  0.00691539  0.13679492\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.3099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.1358\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.0289\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.2169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.1472\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.3252\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.6818\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.8624\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.1992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.3617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.1587\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.1054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.4082\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.1749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 9.1661\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 8.5449\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 14.0595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 12.7156\n",
      "training on full data\n",
      "0 20\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 6.3918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 5.3486\n",
      "Success in episode 58 at time step 120 with reward -160.78814981406023\n",
      "Episode 59\n",
      "[-7.9221727e-04  1.4121666e+00 -8.0248021e-02  5.5403098e-02\n",
      "  9.2466467e-04  1.8177342e-02  0.0000000e+00  0.0000000e+00]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.0933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.1869\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.8034\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 4.3707\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 5.8882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 6.3804\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 8.6423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 8.9603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 8.9750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 8.7858\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 8.0387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 7.4806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.8661\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 5.2487\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.0874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.6153\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.7986\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6258\n",
      "training on full data\n",
      "0 19\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 2.5190\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 1.9963\n",
      "Success in episode 59 at time step 114 with reward -187.23463635996012\n",
      "Episode 60\n",
      "[-0.0077137   1.4172051  -0.78132296  0.27929842  0.00894496  0.17698124\n",
      "  0.          0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.8293\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.7151\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.4243\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.4914\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.0155\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.1967\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.2030\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.2722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.7771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.8079\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.5891\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 2.5021\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.4393\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 3.2124\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 95.0604\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 93.9098\n",
      "training on full data\n",
      "0 17\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 38ms/step - kl_loss: 14.7459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 14.4543\n",
      "Success in episode 60 at time step 99 with reward -146.01424236577992\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "daifa, results_two = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=60, render_env=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x2b61ce6d0>]"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABBi0lEQVR4nO29d3hj53Xn/zkASIKdwzLkkJxCTtU0tdF41IulSEocyz3yxnFfJYqcdYqb1hsnWUebOPnFju043p9iy45jJ7JsRbIiF9kq1qhLnNH0yhnODDmNvRMECLz7B3BBkLwgAAIkQOB8nocPyYvC95LEF+d+z3nPEWMMiqIoSm7hSPcCFEVRlIVHxV9RFCUHUfFXFEXJQVT8FUVRchAVf0VRlBzEle4FxEt1dbVZtWpVupehKIqyqNi1a1e3MaZm+vFFI/6rVq2ipaUl3ctQFEVZVIjIabvjavsoiqLkICr+iqIoOUjaxF9E7hCRoyLSKiKfS9c6FEVRcpG0iL+IOIFvAHcCG4H3i8jGdKxFURQlF0lX5L8daDXGnDTGeIGHgbvStBZFUZScI13i3wC0R3zfETo2BRG5R0RaRKSlq6trwRanKIqS7aRL/MXm2Iz2osaYB40x24wx22pqZpSpKoqiKHMkXeLfASyP+L4ROJemtWQlzx/r4kzPaLqXoShKhpIu8X8DWCsiTSKSD9wNPJGmtWQln3z4TR56qS3dy1AUJUNJyw5fY8yEiHwCeApwAg8ZYw6mYy3Zysj4BCPjE+lehqIoGUra2jsYY34G/CxdPz+bmfAH8PkNnolAupeiKEqGojt8sxBL9Me8/jSvRFGUTEXFPwvx+IKiPz6h4q8oij0q/lmIFfFbbwKKoijTUfHPQqyIf0zFX1GUKKj4ZyEeX2DKZ0VRlOmo+GchVsSvCV9FUaKh4p+FaMJXUZRYqPhnIZMJX7V9FEWxR8U/CwnX+WvCV1GUKKj4ZyGeUOTvDxh8fo3+FUWZiYp/FuKJ8Pq11l9RFDtU/LOQyCoftX4URbFDxT8LiUz0jmvSV1EUG1T8s5BI20cjf0VR7FDxz0IibR/1/BVFsUPFPwsZn5LwVdtHUZSZqPhnIZrwVRQlFvMm/iLy9yJyRET2ichjIlIRcdv9ItIqIkdF5Pb5WkOuEhntq+2jKIod8xn5/wrYbIzZChwD7gcQkY0EB7ZvAu4A/llEnPO4jpxjzOen1B2c0KniryiKHfMm/saYXxpjrAnirwKNoa/vAh42xowbY9qAVmD7fK0jF/H4/FQU5YW/VhRFmc5Cef4fBX4e+roBaI+4rSN0bAYico+ItIhIS1dX1zwvMXvwTARYUpQf/FoTvoqi2JCU+IvI0yJywObjroj7fB6YAH5gHbJ5KmP3/MaYB40x24wx22pqapJZak7h8fqpCIm/JnwVRbHDlcyDjTG3zna7iHwIeBvwVmOMJfAdwPKIuzUC55JZhzIVz4SfikK1fRRFic58VvvcAXwWeLsxZjTipieAu0WkQESagLXA6/O1jlxkzOunKN9Jvsuhkb+iKLYkFfnH4J+AAuBXIgLwqjHmD4wxB0XkEeAQQTvoPmOMKlQK8fj8uPOcuF0O7e2jKIot8yb+xpg1s9z2APDAfP3sXMfjC+DOc1KY71TbR1EUW3SHb5bhDxi8/gDuPAfuPKfaPoqi2KLin2VYfX0K85y4XRr5K4pij4p/lmH19XHnOXHnO7XOX1EUW1T8swxreHthKOGrto+iKHao+GcZVuRfEPL8x1X8FUWxQcU/y7A8/sI8J4Wa8FUUJQoq/lmGJf7uPCfuPId6/oqi2KLin2VYYq91/oqizIaKf5YRafsUuNT2URTFHhX/LGMsbPtYCV+1fRRFmYmKf5YR6fkX5jnx+gP4A7YdsxVFyWFU/LOM6QnfyGOKoigWKv5ZhpXwLcx34s5zho6p+CuKMhUV/ywj7Pm7HBSGxF+TvoqiTEfFP8vw+PzkOQWX00FB2PbRpK+iKFNR8c8yPL4Ablcw4i9U20dRlCio+GcZYz4/BSHRV89fUZRoqPhnGeM+P4X5wT/rpPir7aMoylTmXfxF5FMiYkSkOuLY/SLSKiJHReT2+V5DLjHm88+wfTThqyjKdOZzgDsishy4DTgTcWwjcDewCagHnhaRdTrEPTV4fH4K8y3bR+v8FUWxZ74j/68AnwEit5jeBTxsjBk3xrQBrcD2eV5HzhAZ+avnryhKNOZN/EXk7cBZY8zeaTc1AO0R33eEjtk9xz0i0iIiLV1dXfO00uzC4wvgzlfxVxRldpKyfUTkaaDO5qbPA/8T+A27h9kcs20+Y4x5EHgQYNu2bdqgJg48Pj9LSwuASNtHE76KokwlKfE3xtxqd1xEtgBNwF4RAWgEdovIdoKR/vKIuzcC55JZhzKJx+cPR/xuTfgqihKFebF9jDH7jTFLjTGrjDGrCAr+FcaYC8ATwN0iUiAiTcBa4PX5WEcu4vEFwlU+eU4HLoeo7aMoygzmtdrHDmPMQRF5BDgETAD3aaVP6hjz+cN2DwSjf7V9FEWZzoKIfyj6j/z+AeCBhfjZuYbH5w8nfCEo/mr7KIoyHd3hm0UEAobxicnePhBM+o6r+CuKMg0V/yxifGKyl7+FO8+JZ0LFX1GUqaj4ZxGeiF7+FoV5Tsa8Kv6KokxFxT+LGIsY4WjhznNowldRlBmo+GcRVuQ/3fbRhK+iKNNR8c8iLJEvcE3z/FX8FUWZhop/FhE5vN3CnecMJ4IVRVEsVPyzCPuEr0MTvoqizEDFP4uI5vlrqaeiKNNR8c8iLNsnstpHSz0VRbFDxT+LCJd6RiR8C0KevzHaEVtRlElU/LOIsOefH9nYLfi1Jn0VRYlExT+L8Nhs8goPcVfrR1GUCFT8s4hwwjdvasIX0KSvoihTUPHPIsZ8fpwOIc850/bRFg+KokSi4p9FRE7xslDbR1EUO1T8s4jpU7wgWO0DavsoijKVeRV/EfkjETkqIgdF5O8ijt8vIq2h226fzzXkEh6ff0pfH5iM/D0a+SuKEsG8jXEUkZuBu4CtxphxEVkaOr4RuBvYBNQDT4vIOp3jmzzjvsCU3b2gCV9FUeyZz8j/XuBvjTHjAMaYztDxu4CHjTHjxpg2oBXYPo/ryBnsbB9N+CqKYsd8iv864HoReU1EnheRq0LHG4D2iPt1hI4pSeLx+TXhqyhKXCRl+4jI00CdzU2fDz33EmAHcBXwiIg0A2Jzf9veAyJyD3APwIoVK5JZak4w5vNTUjD1T6q2j6IodiQl/saYW6PdJiL3Av9pgk1lXheRAFBNMNJfHnHXRuBclOd/EHgQYNu2bdqcJgYeX4Dqkmmev0sjf0VRZjKfts/jwC0AIrIOyAe6gSeAu0WkQESagLXA6/O4jpzB4/NPae0Ak31+tLePoiiRzFu1D/AQ8JCIHAC8wIdCVwEHReQR4BAwAdynlT6pwePzTxnkApDvdCCCjnJUFGUK8yb+xhgv8IEotz0APDBfPztX8fj8M0o9RUR7+iuKMgPd4ZtFjNnYPqDTvBRFmYmKf5ZgjMHjC9iLv8uhdf6KokxBxT9LsBK60zd5AbjzneEpX4qiKKDinzXY9fK3cLucjKv4K4oSgYp/ljBmM8XLwp3n0MhfUZQpqPhnCZanb2f7FOY71fNXFGUKKv5ZQizbR+v8FUWJRMU/S7BsnQI78deEr6Io01DxzxJiJ3zV9lHSy8snujl2cSjdy1BCqPhnCR5N+CoZzp89spevPnM83ctQQqj4ZwlWQtcu8i/MU89fSS8en5/zAx4GRn3pXooSQsU/S7B699hu8gqJf7CvnqIsPGf7xwAY9Kj4Zwoq/lmC1bsnmu0TMOD1q++vpIf23lEABsdU/DMFFf8sYbLO376xW+R9FGWhae8LRv4DKv4Zg4p/ljCZ8LW3fQBt8aCkjQ4r8vdMqP2YIaj4Zwkenx+HBIe3TCc8xF3FX0kT7X1B8fcHDKM6WyIjUPHPEsa8wV7+IjLjNrV9lHTT3jsW/lqtn8xAxT9L8Ez4bcs8YdIK0shfSRftfaNUFecDWvGTKcyb+IvIZSLyqojsEZEWEdkecdv9ItIqIkdF5Pb5WkMuMea1H+QCk7aP1vor6WDQ46N/1MfG+rLg92MTaV6RAvMb+f8d8FfGmMuAL4S+R0Q2AncDm4A7gH8WEXvVUuLGM+G3TfbCZL8fFX8lHVhlnpvqywEt98wU5lP8DVAW+rocOBf6+i7gYWPMuDGmDWgFtts8XkmA8Sjze2HS9lHxV9KB5fdvbgjKgXr+mYFrHp/7j4GnROT/I/gmc03oeAPwasT9OkLHZiAi9wD3AKxYsWLeFpoNRBveDpG2jyZ8lYWno29a5K+ef0aQlPiLyNNAnc1NnwfeCvyJMeZREXkf8G3gVmBmOUrwKmHmQWMeBB4E2LZtmxYHz4LHF5gl4aulnkr6aO8dpaTAxfIlhYB6/plCUuJvjLk12m0i8j3gk6FvfwR8K/R1B7A84q6NTFpCyhwZ8/pZUpRne1uuJ3xfau1maWkBa2tL072UnKS9b4zGJYW4nA5KClxq+2QI8+n5nwNuDH19C2D1cn0CuFtECkSkCVgLvD6P68gJggnf2SP/XLR9jDH80X+8yZd+cSTdS8lZ2ntHWVFZBEB5YZ7aPhnCfHr+/x34qoi4AA8h794Yc1BEHgEOARPAfcaY3AxJU4jHG138C1y5W+ffNTxO74iXA2cH072UnMQYQ0ffGDesqwGg1O3Sap8MYd7E3xjzInBllNseAB6Yr5+di3gmonv+DodQ4HLkZG+f4xeHAbgw6KFraJya0oI0ryi36B72Mubzh/3+Mo38Mwbd4ZsleHzR6/xhsqd/rhE5NvDguYE0riQ3sXr6LA/ZPmXuPAY04ZsRqPhnAcaYWUs9IXdHOR67OERxfvD3cvCcWj8LjbXBa3mk56+2T0Ywn56/skB4/QGMse/lbxEc5Zh7Cd9jF4fZVF9O55CHA2c18l9oOkJ9/BvDto9LbZ8MQSP/LMDjjT7IxcKd58y5yN8Yw7GLQ6yrK2FTfTkH1PZZcNp7R6kuyacoPxhnlrnzGPJM4A/otp10o+KfBVgjHKMlfCE3Pf8Lgx6GPBOsqy1lU0MZ7b1jOkB8gTnTO0rjkqLw92WFwb0owx71/dONin8WMNvwdgt3noPxHLN9joUqfdbVlrI51Frg4HmN/heS9r7JGn8Iev6gLR4yARX/LCDeyD/XbJ/joUqfdbWlbAq1Ez6o9f4LxoQ/wLl+D8srC8PHytxB+0d3+aYfTfhmAbMNb7cozEHb5+iFIapL8qkMDRGpL3er77+AnB/w4A8YltvYPlrxk3408s8CLNunIEadf65F/sc6h1kX0c9nU0O5VvwsINNr/EFtn0xCxT8LiM/2cWRVqee5/jGu/OKv2HW6z/b2QMDQenFoivhvri/nZPcII+OabFwIOkJ9/O0jf/0bpJucFP/h8Qn6RrzpXkbK8IQTvrN7/tnU3uGl1m56Rrz81177hrBn+8cY8fpZW1sSPra5oQxj4PB59f0Xgva+URwCyyrc4WOW56+Rf/rJSfH/X4/t5/3/8mrsOy4ScjHhu/tMPwDPHe20vf14ZzDZuz7S9glV/KTC+jnXP5b0cyw2jl4Y4s6vvhB34NTeO8qy8kLynJMyU5zvwiGa8M0EclL8958d4MiFIS4OetK9lJQwFscmr8I8JxMBg8+fHdbPm2f6cAic7hmlrXtkxu1WmWdkD//asgKqS/I5kGSbh73t/Vzzt8/yxqnepJ5nsfFiazeHzw9y+EJ8v7/2vrEplT4QbDJYpi0eMoKcE3+fP8DpnmAi6tWTPWleTWqwqnhief6R913MDI9PcOziEO+4LDj989c20f+xC0PUlbnDCUYAEWFTfXnSPX6s/5s9oauPXKGtO/iGGm/Q1N47OsXvtyhz5zGom7zSTs6J/+meESZCW8tfOZEl4j8RX7UPZMdAl73t/QQMvP2yepprinnuaNeM+xzrHJri91tsbijj+MWhpN4E97T3A3A0omNoLmBdYZ0fiC3+Hp+fzqHxKRu8LMoKdZpXJpBz4t/aGfwHri9388oii/wHRn3c/5/7Z3iuHq8fkcmhLXa4s2iU45tnghU+ly9fws3rl/LqyZ5wuSuAP2BonVbmabG5vpyJgJnS6jlRLPFP5jkWI21dwdfOxTjE32rottxO/N2ZafsMenz87/86xJ89shdjsr/3UM6J/4mu4KXr3dtXcLpndFEl7n5+4Dz/8foZfryrY8pxz0QAt8uJiER9bDaJ/+4z/ayuKaa8KI+b1tfgnQjwysnu8O3tvaN4fIEpyV6LzQ1W0ndu1s/FQQ/nBzwU5zs5dnGIQI40KBvz+jkXEv14Iv/JGv/CGbdl2ihHYwxP7jvHrf/wPA+91MajuzvYnwP7QXJP/DuHWVbu5q2XLAUWl/XzQmtQ4J7cN7W8ccw7+yAXALfL8vwXt+1jjOHNM31csWIJANubKinMc/LrCOvHisjtbJ/GJYWUuV1z3ulrRf1v21qPxxcIi1y2c6onGPWLBBvmxSLcxz+a558hdf5nekb58Hfe4BP//iZLywr4wcffQoHLwSMt7ele2ryTlPiLyHtF5KCIBERk27Tb7heRVhE5KiK3Rxy/UkT2h277mswWrs4DrV3DrK4p4ZK6MiqK8haN9RMIGF5u7cad52BvxwBneiZFx+Pzz5rsBSgMDTSx8gPzxdELQ/yvx/ez+4z95qtkOdUzSt+ojytWBsW/wOXk2jVVPHe0M3ypfrxzZqWPRTjpO8fIbk97P3lO4Z1XBJPNRy/khvVj+f2b6su4EE/k3ztKgcthOzYzEzz/Ue8EX3vmOLd95XlaTvXyhbdt5PE/vJZr11Rz5+Y6nthzLiuukmcj2cj/APAuYGfkQRHZCNwNbALuAP5ZRCx1+ibBYe5rQx93JLmGuDHGcKJzmDVLS3A4hLc0VS6ayP/Q+UH6Rn3cd9MaAJ7cPxn9x5riBZO2T6Q3nkpaO4f4xL/v5o6v7uT7r57hRy0dsR80B3aHdvRevqIifOzG9Utp7x3jZEigjl4YoqGikJIC+9ZVmxvKOHxhaE5lr3vO9HPJsjK2hOyjXPH9LfHf0VRF1/B4zN9de+8YjUsKba3I8sI8xnx+vBMLfxXq8wf4t1dPc8Pf/Zov/+oYt2xYytN/diMfva4JV2g/wvu2LWfQM8FTBy8s+PoWkqTE3xhz2Bhz1Oamu4CHjTHjxpg2oBXYLiLLgDJjzCsmGKZ9D3hHMmtIhAuDHka8flbXFANwdXMVZ/vHwpeomcxLIcvnfVct5/IVFTy593z4No8vEFP8C+fJ82/rHuGPH36T276yk2ePdPKHN61m47KycFlgqnmzvY+SAhdrl05G9TetqwHguSPBks9jF4dYZ2P5WGxuKMc7EQjnf+LFHzDs6+jnsuUVFBe4WF5ZyNGLiZ/nf+09x6O75ufNcS58+VfH+Nh335j1Pie7Rqgrc9NcU4Ix0DU0Puv92/tGbZO9MNniYWgBfX/L17/ty8/z548foLm6mEfvvYZvfuBKlpVPzUvsaK6icUnhvAUwmcJ8ef4NQKRp1hE61hD6evpxW0TkHhFpEZGWrq6Z5XyJ0hqyA1YvDQrD1aurARaF9fNiazfrakuoLXPztq31HDo/yMmQeMUa3g6Tdf6p3OV7fmCMO7+6k6cOXuSeG5p54TM38+nbN3DJsjLbjVepYPfpoPg6HZMR5fLKItYsLeH5Y11M+AOc7BphXd1My8dicqdvYknf1s5hRrx+LlteAQR3Dx+bg+3zzV+f4DOP7mNvKH8wHxhjGBjzxVW18tN953juaOesV4Vt3cM0VRdTVx60cWIlfaPV+EPQ84eF2+VrjAn7+gUuJw99eBs//P0dXBmyDqfjcAjvubKRl05005HFOZ2Y4i8iT4vIAZuPu2Z7mM0xM8txW4wxDxpjthljttXU1MRaakxOhMR/TU1Q/NfVllBZnM+rGW79eHx+3jjVy7Vrgm9Wv7VlGQBP7jsfvj1W5F/gCt6eyoEuzx3pwuML8Oi913D/nZdQVRIUhuaaYi4Ojqe8gdrI+ARHLgxOsXwsbl5fw2snezl8fgivP8C6pdHFv6m6mKJ8Z8JtHva0By0nS/zX1ZZyoms4YfvibP8Y/oDhTx7Zk5IrMWMMD73Yxmd/vI/f+/Zr3PIPv+aSL/yCS//ql3zjudZZH9s34uVE1wgBE7QWo9HWPUJTTTF1ZcEoebaNXgOjPgY9E7Y1/hD0/IEF2+j162NdPH+siz+9bR0/++T13LKhdtbKOID3XNkIwKO7zi7EEtNCTPE3xtxqjNls8/GTWR7WASyP+L4ROBc63mhzfEFo7Rqm1O0KJ6FEhB3Nlbxysiej63p3n+nD4wtwXUj868rdXLVqSbjqxzORnoTv88c6aago5JJlU4W2qTpoq1kVIqliX8cAAUO40ieSm9YvxesP8K+vnAKwrfG3cDqEjcvK5iD+/ZS5XayqCp7f+rpSJgImoauc4fEJBsZ83LCuhpNdI3zpF0cSWoMdrZ3D/O8nD/GrwxcZ8kxwSV0Zv7djJU3VxTxzxL73kcWb7ZOJ+Wi/j74RL32jPpqri1lWHmzSNlvkf7o3+PtYUWUv/uUL2NPfGMM3nm2lvtzNH9y4esoV42w0Lini2tXV/GhXe9aW886X7fMEcLeIFIhIE8HE7uvGmPPAkIjsCFX5fBCY7U0kpZzoHGHN0pIp7/pXN1dxfsATbvmQSvZ19KdkH8FLrd04HcJbmqvCx962tZ5jF4c5dnEoVOq5sAlfnz/Ay6093LCuekYUZYljqq0fq4LIirwj2bZqCcX5Tn6y5ywisGZpdM8fgr7/ofODCb2w3zzTz6XLK3CEBMR6g0lkp6/1//CeKxv50NUr+c5Lp3j5RHeMR83OidDmq+9+5Coev+9avvG7V/D539rInZvr2N8xMOsVWMupPlwOoaIoL2pte1voTbypupiKojzyXY5ZI3/rtbQyivhbtk8itf7GmDnl5l5v66XldB+/f+Nq8mfZBGnHe7c10tE3ljVtYKaTbKnnO0WkA7ga+KmIPAVgjDkIPAIcAn4B3GeMsVTnXuBbBJPAJ4CfJ7OGRLDKPCO5enVQUOfD9//Yv7bwp4/sSfp5Xmzt4fLlFVOqV+7cUodD4Mm95+JK+Ka6zn9Pez9D4xPcsHamHbeqOviiP5Vi8X/zTB/N1cUsCU3miqTA5eSaNdX4/IYVlUXhK51obKovY9TrD1cIxWIk1E/o8og3nuaaYpwOScj3PxsS/4aKQj535yU0VRfz6R/tS2rT08lQct264rLY0VzFRMBEnXkAsOt0H5vqy7hseQX7O6KIf9ek+IsIy8rds0b+Z0IiHd32Sdzz//qzrdz4988l7MH/03OtVJfk8ztXLY9952ncvqmOUrcra2v+k632ecwY02iMKTDG1Bpjbo+47QFjzGpjzHpjzM8jjreEbKPVxphPmAXyWwbGfHQNjc+ICFfXlFBTWpDyd/eB0eDPe/VkL62dcy8HHBj1sb+jP+z3WywtdfOWpiqe3Hc+roSvy+kgzykpS/g+f7QLp0O4Ztq6AIryXSwrd8ctrPEQ3NzVz+U2lo/FzeuDG/dms3wsrJ2+B+Pc7HXgbNByuiwi31DgctJUXZxQ5H+2b1L8C/Od/MP7LuX8wBhf/K9DcT/HdNq6RqgpLaDUnTfl+JUrl+BySNT/bZ8/wN6Ofq5YuYQtDeUc7xyyvTJs6x7B6ZBw9U5tmXvWFg+ne4LrKcq3L7UNR/5xbvQ62TXMPz3bSsDA3vb4rbq97f28cLybj13XHDM4ssOd5+Suy+r5+YELGbUjOVXkzA5fq6xveuQf9P2reOVEan3/ExGljt9/9cycn+eVkz0EDFy3dqbIvu3SZZzsHqFnxBvT8wdwu1I3x3fn8S4uX14xpWtmJKuqilNq+5zpHaVnxMsVKyui3uem9cGrkA2zVPpYrF1aQoHLEbfvb+3svbRx6s9fX1uaUK3/uf4x8pzC0lDe6YoVS7j3ptX8aFcHvzp0Me7niaSte4TmaVE/QHGBi62N5VHF/9C5QTy+ANtWVrK5oTxq0rete4QVlUXhvvzLyt2cH4xuZ57qGWVllKgfgpVn+U5HXIJqjOHzjx2gIC8YvCTSduGff91KmdvFB3asiPsx03nvlcsZnwhEHRq0mMkd8bcqfWy84B3NlXQOjac0Uj0ZulS+tLGcR3d3MOqdW2XDi61dFOc7bX3uOzbVhRNY8UQ27nwn4ylI+PaOeNl/doAb1kWvwGqqKU6p7bM7oplbNOorCvnOR67iI9c2xXw+l9PBhmVlcZd77mnvZ0VlUbiiyWJdbSlnekfj/vue7R+jrtwdzhsAfPKt67hkWRlffHJu0f/J7hGaa2aKPwStn30dA7brs+ygK1ZWhDet2b0ZnuwemWIp1ZW5uTgwHjVYOtMzGjXZC8GAq6zQFVfC9z93n+WVkz187s4NrKstjftK7djFIZ46eJEPX7NqxhVRImxtLGd9bSmPJFHz/8Tec0ld/c8XOSP+rV3D5DsdLF8ys9HU1aFE6my7fQdGfTx96CJ/87PDfOy7b8RM5J7sGsblED575waGPBNzjhxeau1hR3PVlGlIFlUlBVwTylnEsn2s+6Qi4fvC8S6MYVbxb64upm/Ul7JxmW+e6ac438n6GFH9zeuXUmmTE7Bjc30ZB84NxHXFt6e93/YNeH1dcNOTtYckFuf6x2iomPo/mO9y8PZL6znTO5rwxqf+US+9I94Zfr/FW2bx/Xed7qOhopBl5YUsK3dTVZw/I7IOBAynpot/uRuvP0Cvzd/W4/NzYdATTvpHo8ydF9Pz7xvx8sDPDnPFigref9UKtjSUs/9sfH+vb/76BEX5zrgCgdkQEd51RQN72+dWvGGM4VM/2puRnUJzRvxPdA6zqroovIU7kqbqYmrLCngl1Br4+MUhnj1ykX99+RR//vgB7vjHnVz2xV/y8e+18NBLbTxzpJNnY5TQnewKXipf3VzF+tpS/u3V0wn/8Tv6glOqpvv9kbxta7DmP67I3+VMScL3+WNdVBTlhaNFO8IVPykq99x9po9Lp23uSpbNDeUMeSbCCcpoWJ087cQ/XPETZ9L3bN8Y9RUzAxBr17l1xRgv1tVqc7V9ddO2lUtw2vj+xhhaTveGNzqJCJsbymdE/heHPIz5/DMif7Bv8GZV5ESr9LEoK4w90OVvfn6YwTEf/+ddW3A4hE0N5fSP+sJJ82ic6Rnlib3n+G/bV9gWByTKVU2VQLDUOFEGPRN4JwLs7Rhg5/HkqrpSTe6If9fIDL/fQkS4urmKn+0/zyVf+AW3fWUnH/1uC3/xxEEe3d1BTWkBf3rrOn54zw72/+XtlBa4Yr7YT3YP01wTrI74wI4VHDg7yN4E/3lebg2+YO38fos7Ni2jubqYDXVlMZ+vMD/5Ob7GGF443s31a2tmFeKmkJi1JShmdox5/Rw+P2S7uSsZtsTZ3vnN0MSuy2x+/sqqYgpcjrh8/wl/gAuDHhrtxD9kR8Z7BWERrsSJYvtM+v5TR06eG/BwcXB8yi7XrY3lHO8cnpIXiqz0sagL1frbNXizyjyjVfpYxBrl+OrJHh5p6eDj1zeH/7c31wc/x/p7ffP5EzhF+O83NM96v3jZuKwMp0PYf7Y/4cf2DE+2wfjq08cyKvq3T8dnGeMTfk73jISjZDs+fn0zxQXBKpXllUU0LilieWUhNSUFM+rY19WVzir+/oDhVM8oN4WqT95xeQN/+/MjfP/V07bRY/fwOCc6h7lqVeUUL/jF1m5qSgtYO0vNenlRHs9+6qaot0eytLQg6STs4fNDdA2Nc8Msb0gQbOXrdEhKNnrt7ejHHzC2m7uSYW1tCXlO4cC5AX5rlv8Nq5PnxmUz32CdDmFtbUlcPX4uDHoIGGwj/2BCVRLuN3SyexinQ2YV2x3NVfzLzpOMeifCFTgtofnDkeK/uaEcf8Bw6Pxg+HdtXVlEir/VC8cu8j8djvxj2T4uOqJccY1P+Pn8Y/tpXFLIJ9+6Nnz8kpAIHzg7wB2b62wfOzDm49FdHbxnWyO1oSuUZHHnOVlXWzqnyL8nZI39xsZafnnoIi+f6Jn1Sn46h88P8uyRTu67eU3CPzsWORH5n+4ZJWBmVvpEsrmhnAfeuYVP3LKWuy5r4MqVS1ha6rbdBr6+rpQjFwajvouf6x/DOxEIV2CUuvN4x+UN/Nfec/SPTvVJD58f5Le//iK/8+Cr3PD3z/G1Z45zfmCMQMDwUms3162ZuYlqrmxpqOBk90hSDbV2Hg/2WJrN74egj924pDAlSfTHdp/Fnedg26rKpJ8rkgJX8EUdq+JnT3sflywri2qtrYuzx8+5/qBYNtjknfKcDlZWFScs/tMrceyw6v13n+4PH9t9uo+ifOeUyii7pG9b9wjuPEfY6gGoLsnHIfaR/5meEUoLXCwpmj3JWjbLQJd/f+0MJ7pG+OI7Nk/Zr+HOc7J2acmssxheOdGD1x8Iz3dOFVsTyDdEYkX+9960mtqyAr72zPGYjzEm+Nr/vW+/xp1ffYFvPNca99zkRMgJ8W+dpdJnLmyoK2XQMxF1qIX1Am6OeLP5wI6VjE8Epkzh2nmsi/f+31cIGMMX79rEyqoivvyrY1z7t89y97+8Ss+IN6EoIRZbl5djzNynWEGwvn9DXWlcUVVTdXHStk/viJfH95zlnZc3Ri0rTYbN9UGfO9qL2h8w7O8YsL1is1hfW8qFQQ8Do7O/qZ7tD0a6dpE/BH3/E4l6/l32ZZ6RXGnj+7ec7uPyFRVTcmDhpG/HVPFfVVU85YrU5XSwtNRtb/v0Bit9YgUs5YXBgS52v/c3TvWysqoovG8jEisvEe3v9cLxYHVcyi3CxmC+wRpPGS/dw8Fgr6GikD+4cTWvtfVGLb2d8Af4yZ6zvO3rL/K733qNw+eH+PTt63nlc29N2VVMJDkh/laZZ7RyuESxxgMeiRLtnbTxSS9ZVsaVK5fwg9fOEAgYHnmjnY9+9w0alxTy+H3X8ntXr+IHH9/Bzk/fzL03reZU9wj5LgfXx7BXEmFrKLKbi3cJwV2uLad7Y0b9Fk3VxZzqGUnK53z4jTOMTwT4yLWr5vwcs7G5sZy+UV94ROF0pnfytMPqIHosRjlfOPKPKv4lnOoeiXvOQCBgONUzErXSx6KkwMWWhsl6/5HxCQ6fH+TKaTaalfTdPy3yt3vd1Ja77W2fntGYyV4IVvt4/QHGbZriHbkwZDuCE4K+f/ewl4uD9i2lX2ztjlodlwxbG63XTmLWT09I/JcU5/P+7SuoLing68/OjP7P9Y/x7v/7Cp98eA9jPj9/+64tvPjZm7nv5jWUx7iKmis5If6tXcM0VBRG3XGYKFYCKprvf7I72ECuumRqpcEHdqygrXuEe3+wi888uo+rV1fxoz+4eko/8RVVRXz69g28/LlbeOmzt6T0Hb+qpICGisKEE88Wr57swec33JiA+I96/XTG6P0eDZ8/wL+9cppr11TFtWt3LkwmEe1/J6+1BQUzVuQPsSt+OvrGqCrOj2ofrVlawkTAxKw+sjg/6MHjC0RN9kayo7mKvR39jHn97G3vDzbIs2lpHNzpG0z6+vwBzvSO2r65LCubGfn7A4aOvlFWVMZej9XZc3q5p8fn51T3SNSNepsbootwe+8op3tGUxowWayvKyXPKQn7/j0j41QU5ZHndODOc/L7NzTzUmsPu05PJuBfPtHNb3/9RU50DvPVuy/j6T+5kbu3r5jTruREyAnxP9E1HK6mSAXlRXnUlbmji3/XCM01JTMufe/cvIzK4nyeOniRu69azkMfvirqBhSX034EXrJcurycfR39c3rszmNdFOY52bYqvsSrJRqJli9a/PLgRc4PePjINcnVas9GZBLRjh/v6mBDXems0fWycjelBa6YFT/n+sds/X4LKyd1Is6KH2ueQ7Qyz0h2NFfi8xt2n+mj5XQfIti2yrCSvofPD9LeO4o/YGiyef668pnif65/DJ/fxBX5R+vs2do5TMDA+ijVaxvryxCxf7N+IVRKeZ1Nv6lkKXA52VBXlvBVc/fwOFUR5aa/u2MFlcX5fO2ZVowxPLjzBB/41mssKc7n8fuu5a7LGqZYbPNJ1ot/IGA40TkSrqNOFcGkb3TxX20jFu48J19691b+5l1b+Jt3bUn5pWk8bGmooL13bE6br54/1sWO5srwbIBYJNva+TsvtbGisoibN8z0flOFO8/JmpoSWzE5fH6QfR0DvG/b8lk9bBFh3Sz/DxZn+8eoL48u/pa90hpn0teq3IrHzty2qjLs++863ce6paW2OZQtjZNJ3zabSh+LunI3Q+MTDEd0DLWuWGZr7WARrbOnFVCtr7N/QyvKd7G6psR2p++LrV0sK3en/LVusaWxnH0diSV9u4e9U3aFF+W7+Pj1TTx/rIvf/dZr/J+fHeGOzXU8ft+1KctJxkvWi/+5gTHGfP6U/2I31JVyonN4hj87Mh5MBEd7Qd62sZb3b1+RsgqeRLk09OLel6B3eap7hFM9o3H7/QD15YXkuxxzKi/d3zFAy+k+Pnj1ypRu7LJjc0M5B87NTII/0tJOvtPBOy+PXTmyLtTjJ5owGGNiRv6l7jxqywo40Rnf7+tk1wjF+c5wn6DZKClwsbmhnJdP9LD7TB9XRrl6qy93U1mcz76OSfG3Sygvs6n1D9f4x+P5R+nseeziEPkux6w7hLdMy0tA0HJ6qbUnpdVx09ka2hSYSPv3nuHxGfbvB69eRUVRHq+e7OH+Ozfwjf92RdR50/NJ1ou/VT0xW5nnXFhfV4rXH5jRv2YyGlvYd/F42WR5pglYP96JAJ95dB/5Lge3XlIb9+McDmFVVdGcbJ/vvNxGUb6T982hFW+ibG4oo2tonM6IBOb4hJ/H3zzLbRtr49olur62hP5QJ1c7+kd9jHr9USt9LNYsLYm73PNkaLpWvGK3o7mSXaf7GPJMzEj2WkQmfU92j1BRlGd7/lYuaor4946Q73TMmIlrR5k7NM1rWmfPIxeGWFNTYrsT32JTfRkXB8fpHJr82fvPDjAw5pt1Q2SyWPmGRAKnnhEvVcVT35xLClx89yPbeewPr+X3b1ydtkAw68U/1WWeFlaPmemX+icTuBRPB+WFeTRXF8ed9DXG8OePH+D1tl7+/j1bow7ljoZV8ZMIXUPjPLn3PO+5sjFsD8wndknEZw530jfq473bGqM9bApWxU+09s6RffxnY3VNUPzjsRbauofj8vstdkQMA5otb7M1lPQ9cn4waq4jHPlHvGGe6RmlsbIwriu1sOdvY/vE6t9k7Uc4GFGy/GJo/0kqS6Ons662lHyXI+7AyecP0D/qo7pk5pXZZcsruHSWIoKFIOvF/0TXMOWFeVOSLqlgzdISnA6ZkfQ92TWMCDEbW6WTLY3lUQd3TOehl07xw5Z2PnHzGu6aw8aZpuoSTveM4E9gYtZ/vH4Grz/Ah65ZlfDPmwsbl1lJxEkx+eEb7Swrd3N9nMlDqwLskI19BImJ/5BnIuoVhIXH56ejbyxmmWckVp+f6pL8WXcEW0nfN9v7oz7/ZOQ/Wfd+OkYr50hK3TMTvgOjPi4MemKK/0abCq0Xjnezqb7MVmhTRb7LwSXLyuKu+LHyalUlqdWeVJH14t/aOTxjdGMqsAZ5zIj8u0aoLy+c9zKtZNjaWMGFQc8Um8OOXx/t5IGfHuL2TbX86W3r5vSzmqqL8PlNeIhJLLwTAf7t1dPcuK4m5VZdNIoLXDRXF4d3jp7rH2Pn8S7ec2Vj3PmGyuJ8VlUV0RJlapbVEXI2zx8m7clYSd8zvaMYk9gVZqk7j2tWV3Hz+qWzvh6spK8x9n4/BBPlS4rywpG/MYbTPSMx2zpY5LscFOY5p3j+1lVTLPEvdefRFPH3GhmfYPeZvnm1fCy2NpRz8Fx84z+tDV7TPf9MIdkxju8VkYMiEhCRbRHHbxORXSKyP/T5lojbrgwdbxWRr8k8G16lBa4ZAzhSxfq6Uo5enBrpWQ3dMplw0neWCKa1c4g/+vc3WV9Xxpffd9mcy8+sMsF4u3v+/MB5uobG+fA8beqKxuaGcg6GIslHd3VgTHCQRyJctaqSllO9tpbN2b4x3HmOmG0PLHsy1k7fRMo8I/nuR7bzpXdvnfU+VtIXsC3ztKiNqPXvGfEy4vXHbOgWSbCn/6Tnf/RC8LUUbYNXJMGdvsH7v97Wi89vuH5N6ks8p7OlsZzh8Ym4/p97RoJXb9NnQGQKyUb+B4B3ATunHe8GftsYswX4EPBvEbd9E7iH4FD3tcAdSa5hVr794av4wm9vnJfn3lBbSnvvWLjczRhD2yzdQzOFjfVlOISo9f79o14+9q8tFOQ5+daHtlGcRCWCZRu0xZnE/MWBC9SVublxHmq1Z2NzfTnnBjx0D4/zo10dXN1cFVfVSiRXraqkb9RnK9znBoJ9/GPFOrVlBRTnO2PW+lu5JWtecrw4HRLzjdxK+oJ9mafFsohdvrGGtttRPq2/z9GLQ5S6XeF8wmxsri/jbP8YvSNedh7vosDliHv/STKEd/rGYf10h/r6pNpyThXJzvA9bIw5anP8TWOMNb3kIOAWkQIRWQaUGWNeCc3u/R7wjmTWkE6sy1Nrc8/FwXFGvP6Mj/yL8l3BLoVRqhb+4ZfHONc/xv//e1fG9KhjUV2ST0mBK65yT58/wIvHu7lxXc2CbXSxsMTuWy+0caZ3dE4Dvy3xeeNU74zbovXxn46IsDqOip+TXSMstZnbmyquWFFBvtMx65tL5EavM73Bv28i4l/mnib+F4bYUFcal0Ub2YTuxePdbG+qXBCrdU1NCe48R1y+v9XaIVsj/3h4N/CmMWYcaAAi56F1hI7ZIiL3iEiLiLR0dXXN8zITZ3qbh7leiqeDLQ32G1Y6Bz38sKWdd1/ROKXd71wRkWCDtzhqo/e09zM0PsGN6xc26ofJJOJDL7ZR6nZFbRk8G03VxVSX5NuLv80Er2isrimJGfm3dcfu6ZMM99zQzOP3XTtrS5S6skK6h714JwKc7hlFBBqXJGL7TE7zMsZw5MJQ3G08NtUHxf+Zwxc53jnMdfNY5ROJy+lgU315XDt9u4e95DklXNaaacQUfxF5WkQO2HzcFcdjNwFfAn7fOmRzt6iZE2PMg8aYbcaYbTU1Cy8IsWhcUkhRvjMs/icyvMwzkq3LK+gd8c6YivTtF9uY8Af4gxtXp+xnNVUX09Yd2/bZeawLp0PmtVwvGuWFeaysKsLrD3DXZfVziiJFhG0rK2k5NTXp6/H56R72xi3+a5aWcG7Aw8j4RNT7BBuuzV+QUZTvCr8hRsOyZy4OejjTM0pdmTuh35vV2ROCJaNDnomoPX1mPLYojxWVRfywpR0g7qqsVLAllG+IVcHWMzxOVfHMeSCZQkzxN8bcaozZbPPxk9keJyKNwGPAB40xJ0KHO4DIwulGYG7DbTMAh0NYVxvs7Q/BqUeFec4pvc8zFavDZ+Tla/+ol++/epq3ba1nVQqjylXVxXT0jcUcHv/8sS4uX14xL62b42FzKJp837a5byzbtmoJZ3pHp/Rftyp94rF9YHKkYzSrzJrbG6uV83xTGyH+p3tHE0r2QnCjl2X7HAm3dYg9kc5ic0MZHl+A6pL8uN80UsHWxnLGfP6Y1lzPiDdjyzxhnmwfEakAfgrcb4x5yTpujDkPDInIjlCVzweBWd9EMp0NoalexhhOdg/TVF284H71XNiwbGaXwu++fIoRr58/vDl1UT8EywWNCW4Cikb38Dj7OgYSah+Rat6/fQUfvbZp1tnEsbgqNHAm0vqZbYiLHeFyzyjWj910rXRgRf7nBzyhMs8ExT80yjEQMJM9fRLo3mpZP9euqV7Q19zWOKrlwGrtkJl+PyRf6vlOEekArgZ+KiJPhW76BLAG+HMR2RP6sLpz3Qt8C2gFTgA/T2YN6WZ9XSl9oW39J7tG4mqvmwkUuJyhDSv9AAyPT/Cdl05x6yW1cc0DToRwxc8sSd8XQx0Z420XPR9ct7aaL/z2xqQu0zfVl1GU75xi/VhDXOK1fVZWFeN0RB/paLXLSLe9aG30OtE1TPewN+4af4sydx4BAyPeCY5dGKKuzJ1Q73pLhBfK77doqi6hON8ZcwJcsKlb5kb+SWUijDGPEbR2ph//a+CvozymBdiczM/NJKyKn30dA3T0jfKOy+rTvKL42dJQzhN7zxEIGP79tdMMjPm4L8VRPxC2kGYT/53Huqgszk8q6s4EXE4Hl6+omBL5n+33IDI5+DwW+S4HKyuLoop/W/cwLock3Goj1ZS5XRTlO8PnmmjkP9niYSI4wCVB6+ba1dV85Xcu5be2LOxrzukQNtXP3hrdGEPPSBZH/spkxc8vD10gYDK3oZsdlzZWMOSZ4OjFIf7lhTauXVNl2+M9Waz2GtHELBAw7DzexfVrF/byfb7YtrKSw+cHw7OSz/aNUVvqTqiFd3NNSXTbpyv23N6FQESoK3OzK7SreWUcQ1wisQa69I14ae0aTlj8HQ7hnZc3ku9KQ2v0xuBO34koU9dGvH48vkDG1viDin/SVBbnU1NawC8PXQTSfymeCNY2/r/4yUG6hsa57+Y18/aztjdV8vP9F+i1mSNw6Pwg3cPetFo+qWR7UyUBA7vP9AOxh7jYsXppMae6R23FZb7LPBOhrtyNxxdcY6Kb4qymfXs7+vFOBBLy+9PN1sZyxicCHI/yBm0Nbs/UGn9Q8U8JG+pK6Q8N786UF2U8rF0a3LDy+qleLl9RwdURXR9TzZ/9xjpGfX6+9szM+aXPHwvu4VjIcr355LLlFTgdQkvIDjnbH98Gr0hW15Tg9QdmDAwPBEzUubrpwLKyKoryEq7Ssnr6v9EW/D0lGvmnE2ut0cS/ezizm7qBin9KsCKW+dxxOR9YG1YAPnHzmnmtR16ztJS7r1rO9189Hd4MZ/H8sS421ZfNy9jKdFBc4GJTfRlvnOolEDCcH4h/g5fFZI+fqb+rcwNjjE8EZu25s5BYZc3xdvOMxHqzeONUHw5Jfdv1+WRVVTEizPhftrAi/+rizP2fVvFPAVYUkCnRWCK8/dJ6br1kKbfM46hEiz++dR0FLgdf+sWR8LFBj4/dp/uyxvKx2Laykj3t/ZwbCM61bahIbO/H6uqZ5Z5jXj//+HTwymltbWYIpVXuuWIOLcwt2+ds/xirqoszuhPudNx5ThoqCqMOKurJ8HbOoOKfEqyk72JK9lp86JpVfOtDVy3ILsSa0gLuvWk1Tx28yOuhS/2XW3uYCJisE//tTUvw+AI8dTCYC0rU8y8vyqO6pCAc+R+7OMRd33iRH+/q4N6bVrMtBa03UkFtEpF/SUTbg4XcpJUqgjvXo4h/KPKv1IRvdrO2toSlpQVsD23wUaLzseuaqStz88BPDxEIGJ4/1kVJgYsrMkTMUsWVK4P/Cz/ZcxaIf3dvJKtrimntHObh18/w9n96kd4RL9/76HY+e8eGjGkZYJ1XomWeECyZLA29AcTb0yeTWF1TwskoU9e6h72Uul0ZfTWTmR2HFhnuPCevf/7WdC9jUVCY7+RTt6/nUz/ay5P7z7PzWBfXrqlKe9liqqkpLaCpuji8C3Qu3VHXLC3hB6+dYfeZfq5bU82Xf+dSlpZmVuuQTfVl/N27t/K2rXOrtS9z5yXU0yeTaK4pZsTrp3NoPHwFZNEz4s3oGn/QyF9JA++8vIFLlpXxFz85wNn+sbS2dJhPrgq1eC51u+ZUCHDFiuDYxU/fvp7vfXR7xgk/BGv933fVcgrz5xbhWhU/ifT0yRSs7r12+1e6h8YzusYfVPyVNOB0CJ//zUvoC5XH3pAlJZ7T2RayAec6E+FdVzSw5wu3cd/Na7Ji85sdZW4X7jxHwk3hMoHmWRrw9YyMZ3SyF9T2UdLEdWurufWSWi4MjqW9TcF8cVWS4i8ii6p0eC6sXlpCvssR96zkTCLYwtphW/HTM+wNv/lnKir+Str459+9ImZP9MXMqqoimqqL2RSjL34u89d3bcZvkzBdDDgcQlN1yYxaf3/A0DvqpTrDbR8VfyVtpKMny0IiIvzsf1xPnnPxRbULhcMhOGxnPC0OmmuKZ3T37Bv1Ykxmt3YA9fwVZV4pzHfiyrJKJmWS1dXFtPeO4p2Y7MHUswhaO4CKv6IoypxpqikmYCYH2ENEU7cMbu0AKv6KoihzZrLcc1L8u0OtHWpKNfJXFEXJSqxyz8iKn+4hjfwVRVGymlJ3HjWlBbR1T1b89IyM43RIwi2uF5pkZ/i+V0QOikhARLbZ3L5CRIZF5FMRx64Ukf0i0ioiX5NMaVKiKIoyB5qqi6dE/j3DXiqL8zN+Y16ykf8B4F3Azii3f4WZA9q/CdwDrA193JHkGhRFUdLG6ppiTkbs8u0e9mZ8awdIUvyNMYeNMUftbhORdwAngYMRx5YBZcaYV0ywFd73gHckswZFUZR00lxdQu+Il/7RYKI30we3W8yL5y8ixcBngb+adlMD0BHxfUfoWLTnuUdEWkSkpaurK/ULVRRFSZJw0jcU/fcMezO+xh/iEH8ReVpEDth83DXLw/4K+IoxZnq7OzsTLOrebmPMg8aYbcaYbTU12dn8S1GUxY01xMny/XuGxzO+0gfiaO9gjJlLo/q3AO8Rkb8DKoCAiHiAR4HGiPs1Aufm8PyKoigZQeOSQlwO4WTXMGNePyNe/6KI/Oelt48x5nrraxH5S2DYGPNPoe+HRGQH8BrwQeDr87EGRVGUhSDP6WBFVREnu0boDu3urcl2z19E3ikiHcDVwE9F5Kk4HnYv8C2gFTjBzGogRVGURUVzdQlt3SOLYnC7RVKRvzHmMeCxGPf5y2nftwCbk/m5iqIomcTqmmJ2Hu+ic9ADZH5HT9AdvoqiKEnTVF2MdyLA/lB756yv81cURVEmK35eb+sFFofto+KvKIqSJFat/572forynRTlZ/6cLBV/RVGUJKkqzqfM7WJ8IrAoon5Q8VcURUkaEQlbP4thgxeo+CuKoqSE5uqg9VOtkb+iKEruYPn+i6GpG6j4K4qipISw7aORv6IoSu5gRf6V6vkriqLkDmuXlvKJm9dw5+a6dC8lLjK/GFVRFGUR4HQIn7p9fbqXETca+SuKouQgKv6Koig5iIq/oihKDqLiryiKkoOo+CuKouQgKv6Koig5iIq/oihKDqLiryiKkoOIMSbda4gLEekCTs/x4dVAdwqXk26y6Xyy6Vwgu84nm84Fsut8EjmXlcaYmukHF434J4OItBhjtqV7Hakim84nm84Fsut8sulcILvOJxXnoraPoihKDqLiryiKkoPkivg/mO4FpJhsOp9sOhfIrvPJpnOB7DqfpM8lJzx/RVEUZSq5EvkriqIoEaj4K4qi5CBZLf4icoeIHBWRVhH5XLrXkygi8pCIdIrIgYhjlSLyKxE5Hvq8JJ1rjBcRWS4iz4nIYRE5KCKfDB1frOfjFpHXRWRv6Hz+KnR8UZ4PgIg4ReRNEXky9P1iPpdTIrJfRPaISEvo2GI+nwoR+bGIHAm9hq5O9nyyVvxFxAl8A7gT2Ai8X0Q2pndVCfNd4I5pxz4HPGOMWQs8E/p+MTAB/Jkx5hJgB3Bf6O+xWM9nHLjFGHMpcBlwh4jsYPGeD8AngcMR3y/mcwG42RhzWUQ9/GI+n68CvzDGbAAuJfh3Su58jDFZ+QFcDTwV8f39wP3pXtcczmMVcCDi+6PAstDXy4Cj6V7jHM/rJ8Bt2XA+QBGwG3jLYj0foDEkILcAT4aOLcpzCa33FFA97diiPB+gDGgjVKCTqvPJ2sgfaADaI77vCB1b7NQaY84DhD4vTfN6EkZEVgGXA6+xiM8nZJPsATqBXxljFvP5/CPwGSAQcWyxnguAAX4pIrtE5J7QscV6Ps1AF/CdkC33LREpJsnzyWbxF5tjWteaZkSkBHgU+GNjzGC615MMxhi/MeYyglHzdhHZnOYlzQkReRvQaYzZle61pJBrjTFXELR97xORG9K9oCRwAVcA3zTGXA6MkALLKpvFvwNYHvF9I3AuTWtJJRdFZBlA6HNnmtcTNyKSR1D4f2CM+c/Q4UV7PhbGmH7g1wTzM4vxfK4F3i4ip4CHgVtE5PssznMBwBhzLvS5E3gM2M7iPZ8OoCN0ZQnwY4JvBkmdTzaL/xvAWhFpEpF84G7giTSvKRU8AXwo9PWHCHrnGY+ICPBt4LAx5ssRNy3W86kRkYrQ14XArcARFuH5GGPuN8Y0GmNWEXydPGuM+QCL8FwARKRYREqtr4HfAA6wSM/HGHMBaBeR9aFDbwUOkez5pDuZMc+Jkt8EjgEngM+nez1zWP9/AOcBH8F3/48BVQQTc8dDnyvTvc44z+U6grbbPmBP6OM3F/H5bAXeDJ3PAeALoeOL8nwizusmJhO+i/JcCHrke0MfB63X/mI9n9DaLwNaQv9vjwNLkj0fbe+gKIqSg2Sz7aMoiqJEQcVfURQlB1HxVxRFyUFU/BVFUXIQFX9FUZQcRMVfURQlB1HxVxRFyUH+H6iTSmVoTISbAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results_two.total_reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# # make the HABIT ACTION NET\n",
    "# habit_net = HabitualAction(latent_dim, 1, [16, 16], train_epochs=2, show_training=True)\n",
    "# habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "#\n",
    "# daifa.habit_action_model = habit_net\n",
    "#\n",
    "# actor_model = get_actor(latent_dim, 1)\n",
    "# critic_model = get_critic(latent_dim, 1)\n",
    "#\n",
    "# target_actor = get_actor(latent_dim, 1)\n",
    "# target_critic = get_critic(latent_dim, 1)\n",
    "#\n",
    "# # Making the weights equal initially\n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "#\n",
    "# critic_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "# actor_optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "#\n",
    "# habit_net = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau=0.005, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "      reward  timesteps  num_actions\n0  -4.082868       1002          167\n1  -3.421810       1002          167\n2  -4.038446       1002          167\n3  -3.973020       1002          167\n4  -3.670363       1002          167\n5  -4.174999       1002          167\n6  -3.983523       1002          167\n7  -3.491283       1002          167\n8  -3.724104       1002          167\n9  -3.982118       1002          167\n10 -3.570753       1002          167\n11 -3.460083       1002          167\n12 -4.004918       1002          167\n13 -3.862981       1002          167\n14 -4.059007       1002          167\n15 -4.020431       1002          167\n16 -4.021730       1002          167\n17 -4.078449       1002          167\n18 -3.880881       1002          167\n19 -4.025988       1002          167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-4.082868</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-3.421810</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-4.038446</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-3.973020</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-3.670363</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-4.174999</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-3.983523</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-3.491283</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-3.724104</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-3.982118</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-3.570753</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-3.460083</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-4.004918</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-3.862981</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-4.059007</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-4.020431</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-4.021730</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-4.078449</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-3.880881</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-4.025988</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.4251048  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "42 147\n",
      "Success in episode 1 at time step 881\n",
      "Episode 2\n",
      "[-0.4140919  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 53\n",
      "Success in episode 2 at time step 317\n",
      "Episode 3\n",
      "[-0.42047128  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "23 70\n",
      "Success in episode 3 at time step 418\n",
      "Episode 4\n",
      "[-0.49614483  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "32 104\n",
      "Success in episode 4 at time step 623\n",
      "Episode 5\n",
      "[-0.59032875  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "45 145\n",
      "Success in episode 5 at time step 869\n",
      "Episode 6\n",
      "[-0.43056807  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "22 83\n",
      "Success in episode 6 at time step 496\n",
      "Episode 7\n",
      "[-0.508457  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "57 148\n",
      "Success in episode 7 at time step 887\n",
      "Episode 8\n",
      "[-0.4367195  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "24 78\n",
      "Success in episode 8 at time step 468\n",
      "Episode 9\n",
      "[-0.40059456  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "44 164\n",
      "Success in episode 9 at time step 981\n",
      "Episode 10\n",
      "[-0.48095956  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "42 167\n",
      "No Success\n",
      "Episode 11\n",
      "[-0.5639781  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "76 167\n",
      "No Success\n",
      "Episode 12\n",
      "[-0.55992776  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "63 167\n",
      "No Success\n",
      "Episode 13\n",
      "[-0.58924127  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "54 167\n",
      "No Success\n",
      "Episode 14\n",
      "[-0.5742079  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "86 167\n",
      "No Success\n",
      "Episode 15\n",
      "[-0.49093044  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "78 167\n",
      "No Success\n",
      "Episode 16\n",
      "[-0.42902708  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "50 167\n",
      "No Success\n",
      "Episode 17\n",
      "[-0.5009124  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "52 167\n",
      "No Success\n",
      "Episode 18\n",
      "[-0.5562405  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "72 167\n",
      "No Success\n",
      "Episode 19\n",
      "[-0.54843205  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "51 146\n",
      "Success in episode 19 at time step 872\n",
      "Episode 20\n",
      "[-0.41256872  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "59 167\n",
      "No Success\n"
     ]
    }
   ],
   "source": [
    "daifa.habit_action_model.show_training = False\n",
    "daifa.train_habit_net = True\n",
    "daifa.train_after_exploring = True\n",
    "daifa.use_kl_intrinsic = True\n",
    "daifa.use_kl_extrinsic = False\n",
    "daifa.use_fast_thinking = True\n",
    "daifa.uncertainty_tolerance = 0.1\n",
    "\n",
    "# daifa.tran.show_training = False\n",
    "# daifa.prior_model.show_training = False\n",
    "\n",
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results_three = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "      reward  timesteps  num_actions\n0  -4.354593       1002          167\n1  -4.322005       1002          167\n2  -4.579960       1002          167\n3  -4.346885       1002          167\n4  -4.402006       1002          167\n5  -4.537839       1002          167\n6  -4.509408       1002          167\n7  -3.877914       1002          167\n8  -4.507214       1002          167\n9  -4.078587       1002          167\n10 -4.008247       1002          167\n11 -4.329474       1002          167\n12 -4.217757       1002          167\n13 -4.481045       1002          167\n14 -4.073669       1002          167\n15 -3.822468       1002          167\n16 -4.413761       1002          167\n17 -4.462203       1002          167\n18 -3.952928       1002          167\n19 -4.231563       1002          167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-4.354593</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-4.322005</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-4.579960</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-4.346885</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-4.402006</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-4.537839</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-4.509408</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-3.877914</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-4.507214</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-4.078587</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-4.008247</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-4.329474</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-4.217757</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-4.481045</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-4.073669</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-3.822468</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-4.413761</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-4.462203</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-3.952928</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-4.231563</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x17b532190>,\n <matplotlib.lines.Line2D at 0x17b532280>]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(obs_pos)\n",
    "\n",
    "utils = daifa.prior_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x17b7b1430>,\n <matplotlib.lines.Line2D at 0x17b7b1c40>]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(vel_pos)\n",
    "\n",
    "utils = daifa.prior_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x17e681700>,\n <matplotlib.lines.Line2D at 0x17e6817c0>]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(obs_pos)\n",
    "\n",
    "# utils = daifa.habit_action_model.actor_model(latent_mean)\n",
    "utils = daifa.select_fast_thinking_policy(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x17ebd5df0>,\n <matplotlib.lines.Line2D at 0x17ebd5f10>]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(vel_pos)\n",
    "\n",
    "# utils = daifa.habit_action_model.actor_model(latent_mean)\n",
    "utils = daifa.select_fast_thinking_policy(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 5, daifa.agent_time_ratio, show_env=True)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.4574806  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "60 167\n",
      "No Success\n",
      "Episode 2\n",
      "[-0.47844926  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "64 167\n",
      "No Success\n",
      "Episode 3\n",
      "[-0.5311618  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "37 147\n",
      "Success in episode 3 at time step 877\n",
      "Episode 4\n",
      "[-0.46539444  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "60 167\n",
      "No Success\n",
      "Episode 5\n",
      "[-0.43320078  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "70 167\n",
      "No Success\n",
      "Episode 6\n",
      "[-0.56359833  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "54 167\n",
      "No Success\n",
      "Episode 7\n",
      "[-0.41705218  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "51 167\n",
      "No Success\n",
      "Episode 8\n",
      "[-0.49579993  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "90 167\n",
      "No Success\n",
      "Episode 9\n",
      "[-0.40739912  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "94 167\n",
      "No Success\n",
      "Episode 10\n",
      "[-0.57973146  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "54 167\n",
      "No Success\n",
      "Episode 11\n",
      "[-0.47232947  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "62 167\n",
      "No Success\n",
      "Episode 12\n",
      "[-0.49276304  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "72 167\n",
      "No Success\n",
      "Episode 13\n",
      "[-0.5930956  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "114 167\n",
      "No Success\n",
      "Episode 14\n",
      "[-0.5572101  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "56 167\n",
      "No Success\n",
      "Episode 15\n",
      "[-0.4525781  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "76 167\n",
      "No Success\n",
      "Episode 16\n",
      "[-0.49785632  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "64 167\n",
      "No Success\n",
      "Episode 17\n",
      "[-0.5918306  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "38 97\n",
      "Success in episode 17 at time step 582\n",
      "Episode 18\n",
      "[-0.50446576  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "46 119\n",
      "Success in episode 18 at time step 711\n",
      "Episode 19\n",
      "[-0.5515545  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "48 167\n",
      "No Success\n",
      "Episode 20\n",
      "[-0.4675508  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "46 167\n",
      "No Success\n",
      "Episode 21\n",
      "[-0.5987304  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "24 84\n",
      "Success in episode 21 at time step 500\n",
      "Episode 22\n",
      "[-0.46599886  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "14 44\n",
      "Success in episode 22 at time step 259\n",
      "Episode 23\n",
      "[-0.5442812  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 26\n",
      "Success in episode 23 at time step 154\n",
      "Episode 24\n",
      "[-0.56831306  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "65 167\n",
      "No Success\n",
      "Episode 25\n",
      "[-0.4211194  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "35 113\n",
      "Success in episode 25 at time step 676\n",
      "Episode 26\n",
      "[-0.48089644  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 44\n",
      "Success in episode 26 at time step 259\n",
      "Episode 27\n",
      "[-0.49131438  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "13 45\n",
      "Success in episode 27 at time step 265\n",
      "Episode 28\n",
      "[-0.4746755  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 27\n",
      "Success in episode 28 at time step 157\n",
      "Episode 29\n",
      "[-0.59761935  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 57\n",
      "Success in episode 29 at time step 342\n",
      "Episode 30\n",
      "[-0.4673799  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "10 27\n",
      "Success in episode 30 at time step 159\n",
      "Episode 31\n",
      "[-0.5692723  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 26\n",
      "Success in episode 31 at time step 153\n",
      "Episode 32\n",
      "[-0.4338846  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "45 134\n",
      "Success in episode 32 at time step 802\n",
      "Episode 33\n",
      "[-0.43634194  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "31 84\n",
      "Success in episode 33 at time step 503\n",
      "Episode 34\n",
      "[-0.57066995  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "41 107\n",
      "Success in episode 34 at time step 638\n",
      "Episode 35\n",
      "[-0.4396064  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "9 29\n",
      "Success in episode 35 at time step 170\n",
      "Episode 36\n",
      "[-0.4796282  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "9 27\n",
      "Success in episode 36 at time step 159\n",
      "Episode 37\n",
      "[-0.502011  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "13 29\n",
      "Success in episode 37 at time step 172\n",
      "Episode 38\n",
      "[-0.48284528  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "14 39\n",
      "Success in episode 38 at time step 234\n",
      "Episode 39\n",
      "[-0.41663125  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 40\n",
      "Success in episode 39 at time step 238\n",
      "Episode 40\n",
      "[-0.52127314  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 28\n",
      "Success in episode 40 at time step 165\n",
      "Episode 41\n",
      "[-0.5059667  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "10 25\n",
      "Success in episode 41 at time step 150\n",
      "Episode 42\n",
      "[-0.5644291  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "25 55\n",
      "Success in episode 42 at time step 326\n",
      "Episode 43\n",
      "[-0.56273377  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "73 159\n",
      "Success in episode 43 at time step 950\n",
      "Episode 44\n",
      "[-0.41874164  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 45\n",
      "Success in episode 44 at time step 269\n",
      "Episode 45\n",
      "[-0.49548292  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "28 44\n",
      "Success in episode 45 at time step 264\n",
      "Episode 46\n",
      "[-0.5138384  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "30 76\n",
      "Success in episode 46 at time step 452\n",
      "Episode 47\n",
      "[-0.4449977  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 27\n",
      "Success in episode 47 at time step 159\n",
      "Episode 48\n",
      "[-0.51948357  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "56 167\n",
      "No Success\n",
      "Episode 49\n",
      "[-0.58224535  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "66 137\n",
      "Success in episode 49 at time step 819\n",
      "Episode 50\n",
      "[-0.40817586  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 33\n",
      "Success in episode 50 at time step 195\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results_four = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=50, render_env=False, flip_dynamics=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "      reward  timesteps  num_actions\n0  -3.227861       1002          167\n1  -2.494721       1002          167\n2  -3.008866       1002          167\n3  -3.164953       1002          167\n4  -2.522984       1002          167\n5  -3.145121       1002          167\n6  -3.130102       1002          167\n7  -3.264508       1002          167\n8  -2.171139       1002          167\n9  -3.021667       1002          167\n10 -3.096007       1002          167\n11 -2.913626       1002          167\n12 -3.175016       1002          167\n13 -3.037751       1002          167\n14 -2.800117       1002          167\n15 -3.167961       1002          167\n16 -3.235862       1002          167\n17 -3.188346       1002          167\n18 -3.087763       1002          167\n19 -3.198857       1002          167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-3.227861</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-2.494721</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-3.008866</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-3.164953</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-2.522984</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-3.145121</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-3.130102</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-3.264508</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-2.171139</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-3.021667</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-3.096007</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-2.913626</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-3.175016</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-3.037751</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-2.800117</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-3.167961</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-3.235862</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-3.188346</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-3.087763</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-3.198857</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.43750945  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "67 167\n",
      "No Success\n",
      "Episode 2\n",
      "[-0.5206395  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "6 56\n",
      "Success in episode 2 at time step 334\n",
      "Episode 3\n",
      "[-0.53664637  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 53\n",
      "Success in episode 3 at time step 315\n",
      "Episode 4\n",
      "[-0.4584642  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 53\n",
      "Success in episode 4 at time step 314\n",
      "Episode 5\n",
      "[-0.44199425  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "24 62\n",
      "Success in episode 5 at time step 372\n",
      "Episode 6\n",
      "[-0.5820085  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "25 64\n",
      "Success in episode 6 at time step 381\n",
      "Episode 7\n",
      "[-0.43078417  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 24\n",
      "Success in episode 7 at time step 143\n",
      "Episode 8\n",
      "[-0.548233  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "5 25\n",
      "Success in episode 8 at time step 149\n",
      "Episode 9\n",
      "[-0.45224836  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 25\n",
      "Success in episode 9 at time step 149\n",
      "Episode 10\n",
      "[-0.50875205  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "29 62\n",
      "Success in episode 10 at time step 369\n",
      "Episode 11\n",
      "[-0.44876334  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "40 127\n",
      "Success in episode 11 at time step 761\n",
      "Episode 12\n",
      "[-0.5969452  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "56 129\n",
      "Success in episode 12 at time step 771\n",
      "Episode 13\n",
      "[-0.5480532  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "21 52\n",
      "Success in episode 13 at time step 310\n",
      "Episode 14\n",
      "[-0.505934  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "17 71\n",
      "Success in episode 14 at time step 424\n",
      "Episode 15\n",
      "[-0.43395558  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "75 167\n",
      "No Success\n",
      "Episode 16\n",
      "[-0.46172836  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "59 98\n",
      "Success in episode 16 at time step 586\n",
      "Episode 17\n",
      "[-0.58230996  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "60 102\n",
      "Success in episode 17 at time step 608\n",
      "Episode 18\n",
      "[-0.5118248  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "73 167\n",
      "No Success\n",
      "Episode 19\n",
      "[-0.50273025  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "39 103\n",
      "Success in episode 19 at time step 616\n",
      "Episode 20\n",
      "[-0.46269357  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "35 86\n",
      "Success in episode 20 at time step 515\n",
      "Episode 21\n",
      "[-0.57779664  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "20 46\n",
      "Success in episode 21 at time step 274\n",
      "Episode 22\n",
      "[-0.45123324  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "9 38\n",
      "Success in episode 22 at time step 227\n",
      "Episode 23\n",
      "[-0.49308443  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "82 122\n",
      "Success in episode 23 at time step 728\n",
      "Episode 24\n",
      "[-0.551804  0.      ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "55 99\n",
      "Success in episode 24 at time step 593\n",
      "Episode 25\n",
      "[-0.47908628  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "21 50\n",
      "Success in episode 25 at time step 296\n",
      "Episode 26\n",
      "[-0.46834362  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "30 88\n",
      "Success in episode 26 at time step 523\n",
      "Episode 27\n",
      "[-0.5952358  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "100 167\n",
      "No Success\n",
      "Episode 28\n",
      "[-0.43523842  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 25\n",
      "Success in episode 28 at time step 146\n",
      "Episode 29\n",
      "[-0.44274956  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "1 26\n",
      "Success in episode 29 at time step 153\n",
      "Episode 30\n",
      "[-0.4964754  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 66\n",
      "Success in episode 30 at time step 394\n",
      "Episode 31\n",
      "[-0.56049025  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "12 35\n",
      "Success in episode 31 at time step 209\n",
      "Episode 32\n",
      "[-0.49966016  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "110 167\n",
      "No Success\n",
      "Episode 33\n",
      "[-0.49757892  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 25\n",
      "Success in episode 33 at time step 149\n",
      "Episode 34\n",
      "[-0.51177806  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "7 22\n",
      "Success in episode 34 at time step 132\n",
      "Episode 35\n",
      "[-0.47141552  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "6 34\n",
      "Success in episode 35 at time step 203\n",
      "Episode 36\n",
      "[-0.5119726  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "36 81\n",
      "Success in episode 36 at time step 485\n",
      "Episode 37\n",
      "[-0.41683808  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "61 100\n",
      "Success in episode 37 at time step 599\n",
      "Episode 38\n",
      "[-0.40439466  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 23\n",
      "Success in episode 38 at time step 138\n",
      "Episode 39\n",
      "[-0.50185466  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "72 143\n",
      "Success in episode 39 at time step 856\n",
      "Episode 40\n",
      "[-0.40703216  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 25\n",
      "Success in episode 40 at time step 149\n",
      "Episode 41\n",
      "[-0.58549064  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "88 167\n",
      "No Success\n",
      "Episode 42\n",
      "[-0.56644875  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "16 46\n",
      "Success in episode 42 at time step 274\n",
      "Episode 43\n",
      "[-0.49367848  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "19 47\n",
      "Success in episode 43 at time step 280\n",
      "Episode 44\n",
      "[-0.4626446  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "5 25\n",
      "Success in episode 44 at time step 147\n",
      "Episode 45\n",
      "[-0.5570115  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "100 167\n",
      "No Success\n",
      "Episode 46\n",
      "[-0.58739907  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 36\n",
      "Success in episode 46 at time step 215\n",
      "Episode 47\n",
      "[-0.5529454  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "8 26\n",
      "Success in episode 47 at time step 154\n",
      "Episode 48\n",
      "[-0.5576318  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "31 63\n",
      "Success in episode 48 at time step 378\n",
      "Episode 49\n",
      "[-0.52270186  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "47 106\n",
      "Success in episode 49 at time step 634\n",
      "Episode 50\n",
      "[-0.49432787  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "96 167\n",
      "No Success\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results_four = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=50, render_env=False, flip_dynamics=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "      reward  timesteps  num_actions\n0  -6.068081       1002          167\n1  -6.230983       1002          167\n2  -6.431071       1002          167\n3  -6.138415       1002          167\n4  -6.204773       1002          167\n5  -6.439107       1002          167\n6  -6.316643       1002          167\n7  -6.352924       1002          167\n8  -6.375400       1002          167\n9  -6.346141       1002          167\n10 -6.284814       1002          167\n11 -6.313875       1002          167\n12 -6.224095       1002          167\n13 -6.106530       1002          167\n14 -6.240360       1002          167\n15 -6.177990       1002          167\n16 -6.252221       1002          167\n17 -6.372750       1002          167\n18 -6.166451       1002          167\n19 -6.136749       1002          167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-6.068081</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-6.230983</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-6.431071</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-6.138415</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-6.204773</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-6.439107</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-6.316643</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-6.352924</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-6.375400</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-6.346141</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-6.284814</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-6.313875</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-6.224095</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-6.106530</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-6.240360</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-6.177990</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-6.252221</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-6.372750</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-6.166451</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-6.136749</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 20, daifa.agent_time_ratio)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "     reward  timesteps  num_actions\n0 -6.402275       1002          167\n1 -6.180096       1002          167\n2 -6.352802       1002          167\n3 -6.167919       1002          167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-6.402275</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-6.180096</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-6.352802</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-6.167919</td>\n      <td>1002</td>\n      <td>167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = habit_policy(daifa)\n",
    "res = test_policy(env, p, observation_max, observation_min, observation_noise_stddev, 4, daifa.agent_time_ratio, show_env=False)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_results = pd.concat([results_one, results_two, results_three, results_four])\n",
    "full_results.reset_index(drop=True)\n",
    "full_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "T = np.arange(len(full_results))\n",
    "plt.plot(T, full_results.percent_use_fast_thinking)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(T, full_results.success)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(T, full_results.total_reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(T, full_results.sim_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}