{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Habitual DDPG Network\n",
    "\n",
    "Habitual network\n",
    "\n",
    "Assuming generative model is perfect, then action selected would always be the action that maximises chance of observing prior preferences. Hence habitual network can be trained to output maximally rewarding actions, as these actions are the free energy minimising actions.\n",
    "\n",
    "Also has a nice interpretation as long as the generative models keep training. Eventually the generative model is less sure about old things. Why people eventually revisit old states they were previously certain about.\n",
    "\n",
    "As far as an agent knows, if observations are confirming perfectly to expectations then it has a perfect world model. So why would it change it? Itâ€™s only when an uncertain observation comes in that the agent needs to reconsider whether or not it has the best model of the world.\n",
    "\n",
    "\n",
    "I think this network should be performing policy gradient method but instead of minimising the discounted reward sequence it should minimise the discounted external EFE/FEEF component sequence. That way in the end the end the fast and slow thinking methods should be converging as the world model continues to improve\n",
    "\n",
    "\n",
    "What is this network trying to learn?\n",
    "- This network is trying to learn the state action mapping that maximises the probability of being in the preferred states\n",
    "- It is also trying to learn to output actions that minimise the extrinsic part of the EFE/FEEF\n",
    "\n",
    "\n",
    "What does this network take as input?\n",
    "- Current state\n",
    "- Maybe sequence of previous states and actions\n",
    "\n",
    "What should this network output?\n",
    "- The action that leads to the next state that maximally achieves the prior preferences\n",
    "\n",
    "How should this network learn?\n",
    "- It should learn by outputting\n",
    "\n",
    "\n",
    "Okay so new idea! DDPG seems pretty good so far. How about we have the Q function take latent states as input and use the VAEs good latent features as input. Then we'll have\n",
    "- Q(o, a)\n",
    "- p(s|o) and p(o|s)\n",
    "- p(s'|s, a)\n",
    "- V(o) or U(o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations, test_policy\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "from ddpg import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "actor_model = get_actor(3, 1)\n",
    "critic_model = get_critic(3, 1)\n",
    "\n",
    "target_actor = get_actor(3, 1)\n",
    "target_critic = get_critic(3, 1)\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.0001\n",
    "actor_lr = 0.00005\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "# buffer = Buffer(50000, 64, 0.99, 2, 1, critic_optimizer, actor_optimizer)\n",
    "\n",
    "ddpg = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau, observation_dim=3,\n",
    "                 action_dim=1, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1495.2494026596196\n",
      "-0.0688459 -0.39660046\n",
      "-1256.8717758923344\n",
      "-0.10642697 -0.58487535\n",
      "-1214.5810506820167\n",
      "-0.11996522 -0.30506968\n",
      "-1401.7781165901954\n",
      "-0.12607127 -0.7675984\n",
      "-1631.6591951888495\n",
      "-0.14300813 -0.66963863\n",
      "-1069.423748113556\n",
      "-0.21910842 -0.6791527\n",
      "-1144.2693074016706\n",
      "-0.1231804 -0.7378944\n",
      "-1341.6635926206357\n",
      "-0.29725012 -0.5785227\n",
      "-1495.441860676611\n",
      "-0.11625984 -0.7771332\n",
      "-1109.6947476265611\n",
      "-0.16452728 -0.7330205\n",
      "-1537.1589163641968\n",
      "-0.3928673 -0.696255\n",
      "-1116.0084564900485\n",
      "-0.1893647 -0.758295\n",
      "-1333.7874024890866\n",
      "-0.11314931 -0.7425609\n",
      "-1069.3904607859513\n",
      "-0.2038534 -0.8739721\n",
      "-984.5857292149116\n",
      "-0.23613612 -0.7819538\n",
      "-1300.107918606033\n",
      "-0.24947184 -0.9355565\n",
      "-1265.5911715447724\n",
      "-0.51112354 -0.9468732\n",
      "-1452.2163229427795\n",
      "-0.1797495 -0.94677\n",
      "-1152.9031632642486\n",
      "-0.33397523 -0.9612894\n",
      "-1175.3469852472538\n",
      "-0.20201847 -0.9246071\n",
      "-1135.0321511129368\n",
      "-0.21840541 -0.9738228\n",
      "-1474.2933981944057\n",
      "-0.35024738 -0.9621876\n",
      "-934.1147352544053\n",
      "-0.57081854 -0.9781603\n",
      "-1120.8764051019275\n",
      "-0.2572646 -0.97589666\n",
      "-771.639960782688\n",
      "-0.24333608 -0.9927264\n",
      "-1213.8545156144135\n",
      "-0.22636436 -0.9352939\n",
      "-1223.540216114124\n",
      "-0.36424354 -0.9912622\n",
      "-1108.8228921517718\n",
      "-0.26916346 -0.9721746\n",
      "-1203.4557847195076\n",
      "-0.23622645 -0.99233955\n",
      "-1106.117914523737\n",
      "-0.3063302 -0.9888635\n",
      "-1272.1333691479279\n",
      "-0.27048135 -0.9723937\n",
      "-1059.347476714588\n",
      "-0.22054444 -0.9869554\n",
      "-1511.842828131574\n",
      "-0.30216596 -0.99325037\n",
      "-970.9238901416188\n",
      "-0.60720545 -0.9714428\n",
      "-1222.5871592584995\n",
      "-0.16897623 -0.99139774\n",
      "-1092.0806874258749\n",
      "-0.24483219 -0.98182344\n",
      "-864.8768777182032\n",
      "-0.3314021 -0.9963395\n",
      "-1146.6194174346303\n",
      "0.19671313 -0.9991546\n",
      "-1111.1643879308733\n",
      "0.20773682 -0.9878003\n",
      "-1085.65214959744\n",
      "0.42933014 -0.9941222\n",
      "-1207.9579209744545\n",
      "0.64960545 -0.9957332\n",
      "-1251.0758210904817\n",
      "0.6697166 -0.99889106\n",
      "-1318.811904019657\n",
      "0.65865624 -0.9818849\n",
      "-1230.2507996012512\n",
      "0.6304696 -0.9965871\n",
      "-951.8233157541764\n",
      "0.61885047 -0.9871261\n",
      "-1160.1816546435475\n",
      "0.86208683 -0.99391097\n",
      "-1599.6834636329977\n",
      "0.8129661 -0.99833024\n",
      "-1690.7154537795172\n",
      "0.82492226 -0.7505651\n",
      "-1137.5460981194458\n",
      "0.9063747 -0.99787384\n",
      "-1305.028054899683\n",
      "0.8978761 -0.9976571\n"
     ]
    }
   ],
   "source": [
    "t_max = 1000\n",
    "num_episodes = 50\n",
    "\n",
    "min_reward_cutoff = -1000\n",
    "min_reward_set = -0.5\n",
    "\n",
    "reward_increase = 0\n",
    "\n",
    "# observation_max = np.array([0.6, 0.07])\n",
    "# observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    all_observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    o, a, r = random_observation_sequence(env, t_max, epsilon=0.2)\n",
    "    # o = transform_observations(o, observation_max, observation_min, [0, 0, 0])\n",
    "    # o = transform_observations(o, observation_max, observation_min, [0.05, 0.05])\n",
    "    # print(o)\n",
    "    # print(o)\n",
    "    for i in range(len(a)):\n",
    "\n",
    "        prev_state = o[i]\n",
    "        state = o[i+1]\n",
    "        action = a[i]\n",
    "        reward = r[i] + reward_increase\n",
    "\n",
    "        # if reward < 0:\n",
    "        #     print(\"yes\")\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        # ddpg.buffer.record((prev_state, action, reward, state))\n",
    "        # # episodic_reward += reward\n",
    "        #\n",
    "        # ddpg.buffer.learn()\n",
    "\n",
    "        ddpg.record((prev_state, action, reward, state))\n",
    "        # episodic_reward += reward\n",
    "\n",
    "        ddpg.train([], [], [], [])\n",
    "\n",
    "    print(total_reward)\n",
    "\n",
    "    acts = ddpg.actor_model((np.random.random(size=(10, 3))*2 - 1))\n",
    "    print(np.max(acts), np.min(acts))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\narray([[-189.68011],\n       [-270.22662],\n       [-196.07523],\n       [-215.2425 ],\n       [-279.59113],\n       [-172.63388],\n       [-197.31567],\n       [-207.98175],\n       [-267.731  ],\n       [-236.1659 ]], dtype=float32)>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg.critic_model([(np.random.random(size=(10, 3))*2 - 1), (np.random.random(size=(10, 1))*2 - 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\narray([[-0.9985531 ],\n       [-0.499748  ],\n       [-0.63756365],\n       [ 0.7546994 ],\n       [-0.99078894],\n       [-0.9978237 ],\n       [ 0.5274505 ],\n       [ 0.31478363],\n       [ 0.61984825],\n       [ 0.89840513]], dtype=float32)>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg.actor_model((np.random.random(size=(10, 3))*2 - 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"model_16\" is incompatible with the layer: expected shape=(None, 3), found shape=(100, 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [28]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m obs_pos \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack([np\u001B[38;5;241m.\u001B[39mlinspace(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m100\u001B[39m), np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m100\u001B[39m)])\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m----> 2\u001B[0m actions_pred \u001B[38;5;241m=\u001B[39m \u001B[43mddpg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactor_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_pos\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m actions_pred\n\u001B[1;32m      7\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(obs_pos, actions_pred)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif_LL/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif_LL/lib/python3.8/site-packages/keras/engine/input_spec.py:264\u001B[0m, in \u001B[0;36massert_input_compatibility\u001B[0;34m(input_spec, inputs, layer_name)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec_dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m spec_dim \u001B[38;5;241m!=\u001B[39m dim:\n\u001B[0;32m--> 264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    265\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mincompatible with the layer: \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    266\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexpected shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspec\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    267\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfound shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdisplay_shape(x\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Input 0 of layer \"model_16\" is incompatible with the layer: expected shape=(None, 3), found shape=(100, 2)"
     ]
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "actions_pred = ddpg.actor_model(obs_pos)\n",
    "\n",
    "actions_pred\n",
    "\n",
    "\n",
    "plt.plot(obs_pos, actions_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"model_16\" is incompatible with the layer: expected shape=(None, 3), found shape=(100, 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [23]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m vel_pos \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack([np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m100\u001B[39m), np\u001B[38;5;241m.\u001B[39mlinspace(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m100\u001B[39m)])\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m----> 2\u001B[0m actions_pred \u001B[38;5;241m=\u001B[39m \u001B[43mddpg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactor_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvel_pos\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(actions_pred)\n\u001B[1;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(obs_pos, actions_pred)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif_LL/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif_LL/lib/python3.8/site-packages/keras/engine/input_spec.py:264\u001B[0m, in \u001B[0;36massert_input_compatibility\u001B[0;34m(input_spec, inputs, layer_name)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec_dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m spec_dim \u001B[38;5;241m!=\u001B[39m dim:\n\u001B[0;32m--> 264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    265\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mincompatible with the layer: \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    266\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexpected shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspec\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    267\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfound shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdisplay_shape(x\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Input 0 of layer \"model_16\" is incompatible with the layer: expected shape=(None, 3), found shape=(100, 2)"
     ]
    }
   ],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "actions_pred = ddpg.actor_model(vel_pos)\n",
    "print(actions_pred)\n",
    "\n",
    "plt.plot(obs_pos, actions_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Can it solve the environment?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "       reward  timesteps  num_actions\n0 -911.249023        200          200\n1 -982.500122        200          200\n2 -879.075562        200          200\n3 -991.384033        200          200\n4 -901.615967        200          200",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward</th>\n      <th>timesteps</th>\n      <th>num_actions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-911.249023</td>\n      <td>200</td>\n      <td>200</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-982.500122</td>\n      <td>200</td>\n      <td>200</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-879.075562</td>\n      <td>200</td>\n      <td>200</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-991.384033</td>\n      <td>200</td>\n      <td>200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-901.615967</td>\n      <td>200</td>\n      <td>200</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_policy(env, ddpg.actor_model, None, None, None, 5, 1, show_env=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_policy(env, ddpg.actor_model, None, None, None, 5, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m both \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(both)\n\u001B[1;32m      4\u001B[0m both\n\u001B[0;32m----> 6\u001B[0m both_acts \u001B[38;5;241m=\u001B[39m \u001B[43mact_net\u001B[49m(both)\n\u001B[1;32m      8\u001B[0m both_acts\n",
      "\u001B[0;31mNameError\u001B[0m: name 'act_net' is not defined"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "both = [[i/n, j/n] for i in range(-1*n, n) for j in range(-1*n, n)]\n",
    "both = np.array(both)\n",
    "both\n",
    "\n",
    "both_acts = act_net(both)\n",
    "\n",
    "both_acts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = 50\n",
    "coords = [[i/n, j/n] for i in range(-1*n, n) for j in range(-1*n, n)]\n",
    "coords = np.array(coords)\n",
    "coords\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5.1, 0.5)\n",
    "y = np.arange(-5, 5.1, 0.5)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}