{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run The Agent on Car Racing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from train_agent import train_single_agent_car_racing\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "import gym\n",
    "\n",
    "from conv_vae import ConvVAE, create_conv_encoder, create_conv_decoder\n",
    "from transition_gru import TransitionGRU\n",
    "from recurrent_agent import DAIFAgentRecurrent\n",
    "from prior_model import PriorModelBellman\n",
    "from ddpg import *\n",
    "\n",
    "latent_dim = 32\n",
    "pln_hrzn = 2\n",
    "action_dim = 3\n",
    "\n",
    "e = create_conv_encoder(input_dim=(64, 64, 3), latent_dim=latent_dim, num_filters=[16, 32, 32], dense_units=[32])\n",
    "d = create_conv_decoder(latent_dim=latent_dim, output_shape=(64, 64, 3), deconv_shapes=[8, 16, 32],\n",
    "                        num_filters=[16, 16, 3], dense_units=[8 * 8 * 16])\n",
    "\n",
    "cvae = ConvVAE(e, d, latent_dim=latent_dim, reg_mean=[0] * latent_dim, reg_stddev=[1] * latent_dim, show_training=True)\n",
    "cvae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "tran = TransitionGRU(latent_dim, action_dim=action_dim, hidden_units=2 * latent_dim * pln_hrzn, output_dim=latent_dim)\n",
    "tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(latent_dim)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           cvae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           agent_time_ratio=50,\n",
    "                           planning_horizon=pln_hrzn,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           train_habit_net=False,\n",
    "                           train_prior_model=True,\n",
    "                           train_tran=True,\n",
    "                           train_after_exploring=True,\n",
    "                           train_with_replay=True,\n",
    "                           use_fast_thinking=False,\n",
    "                           n_policies=100,\n",
    "                           n_policy_candidates=10)\n",
    "\n",
    "# train the agent on the env\n",
    "env = gym.make(\"CarRacing-v2\", new_step_api=True)\n",
    "agent, results = train_single_agent_car_racing(env, daifa, num_episodes=10, render_env=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_best_policies = tf.zeros((5))\n",
    "std_best_policies = tf.ones((5))\n",
    "\n",
    "policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "\n",
    "policy_distr.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([10, 5])"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_distr.sample([10]).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import skimage as ski\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "from conv_vae import ConvVAE, create_conv_encoder, create_conv_decoder\n",
    "from transition_gru import TransitionGRU\n",
    "from recurrent_agent import DAIFAgentRecurrent\n",
    "from prior_model import PriorModelBellman\n",
    "\n",
    "from train_agent import train_single_agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from identity_vae import IdentityVAE, identity_encoder, identity_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# Hide GPU from visible devices\n",
    "# tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with no prior model FEEF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_stddev) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_stddev = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "\n",
    "        return z_mean + z_stddev * epsilon\n",
    "\n",
    "\n",
    "def create_conv_encoder(input_dim, latent_dim, conv_shapes=[3, 3, 3], num_filters=[32, 64, 64], dense_units=[16]):\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=input_dim)\n",
    "\n",
    "    x = encoder_inputs\n",
    "    for n in range(len(conv_shapes)):\n",
    "        filter_shape = conv_shapes[n]\n",
    "        num = num_filters[n]\n",
    "        x = layers.Conv2D(num, filter_shape, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    for d in dense_units:\n",
    "        x = layers.Dense(d, activation=\"relu\")(x)\n",
    "\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_std = layers.Dense(latent_dim, name=\"z_stddev\")(x)  # output log of sd\n",
    "    z_stddev = tf.exp(z_log_std)  # exponentiate to get sd\n",
    "    z = Sampling()([z_mean, z_stddev])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_stddev, z], name=\"encoder\")\n",
    "\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def create_conv_decoder(latent_dim, output_shape, deconv_shapes=[3, 3, 3], num_filters=[64, 64, 32], dense_units=[16], rgb=True):\n",
    "\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "    x = latent_inputs\n",
    "\n",
    "    for d in dense_units:\n",
    "        x = layers.Dense(d, activation=\"relu\")(x)\n",
    "\n",
    "    first_shape = (deconv_shapes[0], deconv_shapes[0], num_filters[0])\n",
    "    x = layers.Reshape(first_shape)(x)\n",
    "\n",
    "    for n in range(len(deconv_shapes)):\n",
    "        filter_shape = deconv_shapes[n]\n",
    "        num = num_filters[n]\n",
    "        x = layers.Conv2DTranspose(num, filter_shape, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "\n",
    "    if rgb:\n",
    "        decoder_outputs = x\n",
    "    else:\n",
    "        decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    return decoder\n",
    "\n",
    "\n",
    "class ConvVAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, latent_dim, reg_mean, reg_stddev, recon_stddev=0.05, llik_scaling=1, kl_scaling=1, **kwargs):\n",
    "        super(ConvVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.reg_mean = reg_mean\n",
    "        self.reg_stddev = reg_stddev\n",
    "\n",
    "        self.reconstruction_stddev = recon_stddev\n",
    "\n",
    "        self.llik_scaling = llik_scaling\n",
    "        self.kl_scaling = kl_scaling\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        _, _, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        # unpack data\n",
    "        # x, reg_vals = data\n",
    "        x = data\n",
    "        # reg_mean, reg_stddev = reg_vals\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_stddev, z = self.encoder(x)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            # TODO Fix this to be a real loss function\n",
    "            reconstruction_loss = nll_gaussian(reconstruction, x, self.reconstruction_stddev, use_consts=False)\n",
    "\n",
    "            posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=z_mean, scale_diag=z_stddev)\n",
    "            reg_dist = tfp.distributions.MultivariateNormalDiag(loc=self.reg_mean, scale_diag=self.reg_stddev)\n",
    "            kl_loss = tfp.distributions.kl_divergence(posterior_dist, reg_dist) * self.kl_scaling\n",
    "\n",
    "            # kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),  # TODO should this be total_loss not loss\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "\n",
    "def nll_gaussian(pred, target, variance, use_consts=True):\n",
    "\n",
    "    neg_log_prob = ((pred - target)**2/(2*variance))\n",
    "\n",
    "    if use_consts:\n",
    "        const = 0.5*np.log(2*np.pi*variance)\n",
    "        neg_log_prob += const\n",
    "\n",
    "    return tf.reduce_sum(neg_log_prob)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[[0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         ...,\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n \n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         ...,\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n \n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         ...,\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n \n        ...,\n \n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         ...,\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n \n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         ...,\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n \n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         ...,\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0]]], dtype=uint8),\n 8.374576271186442,\n False,\n False,\n {})"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v2\", new_step_api=True)\n",
    "\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "res = env.step(action)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "253"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(res[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "(1001, 64, 64, 3)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seqs = 1\n",
    "seq_length = 1000\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make(\"CarRacing-v2\", new_step_api=True)\n",
    "\n",
    "observation_max = np.ones((96))\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.2, render_env=False)\n",
    "    print(f\"Episode {num_seqs + 1} finished\")\n",
    "\n",
    "    # o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    # train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    # test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    # ob_seqs.append(train)\n",
    "    # next_obs.append(test)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "observations = observations/255\n",
    "\n",
    "\n",
    "x_min = 16\n",
    "x_max = 80\n",
    "y_min = 16\n",
    "y_max = 80\n",
    "\n",
    "observations = observations[:, x_min:x_max, y_min:y_max, :]\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# observations_scaled = ski.transform.resize(observations, (observations.shape[0], 64, 64, 3))\n",
    "observations_scaled = observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x28f2d9790>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(observations_scaled[0], cmap=\"gray\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   448         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 32)   4640        ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 8, 8, 32)     9248        ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 2048)         0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           65568       ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " z_stddev (Dense)               (None, 32)           1056        ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 32)           1056        ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.exp_2 (TFOpLambda)     (None, 32)           0           ['z_stddev[0][0]']               \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 32)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'tf.math.exp_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 82,016\n",
      "Trainable params: 82,016\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 32)]              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1024)              33792     \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 8, 8, 16)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  (None, 16, 16, 16)       16400     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 32, 32, 16)       65552     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_8 (Conv2DT  (None, 64, 64, 3)        49155     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,899\n",
      "Trainable params: 164,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 32\n",
    "e = create_conv_encoder(input_dim=(64, 64, 3), latent_dim=latent_dim, num_filters=[16, 32, 32], dense_units=[32])\n",
    "e.summary()\n",
    "d = create_conv_decoder(latent_dim=latent_dim, output_shape=(64, 64, 3), deconv_shapes=[8, 16, 32], num_filters=[16, 16, 3], dense_units=[8 * 8 * 16])\n",
    "d.summary()\n",
    "cvae = ConvVAE(e, d, latent_dim=latent_dim, reg_mean=[0] * latent_dim, reg_stddev=[1] * latent_dim)\n",
    "cvae.compile(optimizer=tf.keras.optimizers.Adam())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1082174.1250 - reconstruction_loss: 650040.8125 - kl_loss: 0.2254\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 233137.6471 - reconstruction_loss: 185578.1562 - kl_loss: 0.5911\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 108994.6043 - reconstruction_loss: 99174.8750 - kl_loss: 0.8801\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 82273.3571 - reconstruction_loss: 73127.8906 - kl_loss: 0.3652\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 58735.3775 - reconstruction_loss: 55830.3008 - kl_loss: 0.3704\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 47594.6397 - reconstruction_loss: 43311.1719 - kl_loss: 0.4965\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 38171.7776 - reconstruction_loss: 37746.0117 - kl_loss: 0.8632\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 33808.0404 - reconstruction_loss: 35230.8477 - kl_loss: 1.8953\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 37243.9846 - reconstruction_loss: 33851.5039 - kl_loss: 3.8596\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 33369.9472 - reconstruction_loss: 32384.3086 - kl_loss: 5.8786\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 29470.4864 - reconstruction_loss: 30644.3691 - kl_loss: 8.4594\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 31054.9809 - reconstruction_loss: 29357.6133 - kl_loss: 12.8726\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 26781.7895 - reconstruction_loss: 27668.8789 - kl_loss: 18.3754\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 28543.8995 - reconstruction_loss: 26329.4316 - kl_loss: 26.4112\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 24863.5804 - reconstruction_loss: 24585.1211 - kl_loss: 34.1317\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 23665.5773 - reconstruction_loss: 23056.8223 - kl_loss: 48.2965\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 20953.3509 - reconstruction_loss: 21753.1094 - kl_loss: 62.3711\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 18967.8346 - reconstruction_loss: 19891.8906 - kl_loss: 78.3418\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 26781.5649 - reconstruction_loss: 27269.8418 - kl_loss: 76.5824\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 24563.0449 - reconstruction_loss: 21088.3535 - kl_loss: 61.4190\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x28f5fcc10>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvae.fit(observations_scaled, epochs=20, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1000, 64, 64, 3), dtype=float32, numpy=\narray([[[[0.279991  , 0.56479   , 0.2785984 ],\n         [0.28569606, 0.57181066, 0.28495952],\n         [0.2922292 , 0.59424573, 0.29742694],\n         ...,\n         [0.255032  , 0.53628397, 0.2692313 ],\n         [0.24979982, 0.5222344 , 0.25466087],\n         [0.243483  , 0.5030785 , 0.25824374]],\n\n        [[0.27436408, 0.5862918 , 0.28863138],\n         [0.28496602, 0.5839717 , 0.2842786 ],\n         [0.29415962, 0.60919696, 0.29141432],\n         ...,\n         [0.25435668, 0.54543483, 0.27260244],\n         [0.25082386, 0.52860224, 0.25809196],\n         [0.2437247 , 0.5138388 , 0.25128773]],\n\n        [[0.28560364, 0.59762484, 0.28284666],\n         [0.29449692, 0.6056116 , 0.2875592 ],\n         [0.30936065, 0.61951715, 0.29499885],\n         ...,\n         [0.27025455, 0.5684311 , 0.28557587],\n         [0.26142898, 0.54060286, 0.2681469 ],\n         [0.24915236, 0.5243183 , 0.2676764 ]],\n\n        ...,\n\n        [[0.27347806, 0.5881136 , 0.29352143],\n         [0.2796944 , 0.569492  , 0.2916316 ],\n         [0.29013315, 0.58548886, 0.28091112],\n         ...,\n         [0.2469558 , 0.468375  , 0.23852946],\n         [0.23922893, 0.42553025, 0.2285142 ],\n         [0.2470632 , 0.42908058, 0.22306342]],\n\n        [[0.2644755 , 0.55893666, 0.2753943 ],\n         [0.27335054, 0.56121135, 0.27408415],\n         [0.2869475 , 0.5777875 , 0.27560654],\n         ...,\n         [0.2261274 , 0.4562444 , 0.23756547],\n         [0.22904944, 0.41371205, 0.22499195],\n         [0.22091544, 0.41806585, 0.2195251 ]],\n\n        [[0.26696494, 0.5467612 , 0.26379818],\n         [0.27411708, 0.55124044, 0.27836692],\n         [0.28022432, 0.56319636, 0.27754104],\n         ...,\n         [0.2355005 , 0.43727335, 0.23247288],\n         [0.22142777, 0.4120836 , 0.21889219],\n         [0.22295836, 0.40025502, 0.20775105]]],\n\n\n       [[[0.29971036, 0.61708695, 0.3051548 ],\n         [0.30187795, 0.62767357, 0.30723634],\n         [0.3246887 , 0.6539837 , 0.30737936],\n         ...,\n         [0.23846583, 0.50484264, 0.25211957],\n         [0.23471577, 0.49438453, 0.2338095 ],\n         [0.21689007, 0.4758645 , 0.23983873]],\n\n        [[0.29294845, 0.6618752 , 0.31930766],\n         [0.31549376, 0.6496143 , 0.30563623],\n         [0.3122845 , 0.6677106 , 0.30908144],\n         ...,\n         [0.24327359, 0.51963395, 0.239777  ],\n         [0.23514369, 0.49430272, 0.24976774],\n         [0.23361221, 0.49228993, 0.23352979]],\n\n        [[0.29978204, 0.6555416 , 0.3100615 ],\n         [0.3106709 , 0.6629596 , 0.31376773],\n         [0.325698  , 0.68966264, 0.31844437],\n         ...,\n         [0.25342512, 0.5232659 , 0.2678957 ],\n         [0.25179565, 0.5072774 , 0.25228274],\n         [0.24055237, 0.49423724, 0.24595097]],\n\n        ...,\n\n        [[0.31666487, 0.68318444, 0.3449323 ],\n         [0.32483515, 0.6704724 , 0.32283425],\n         [0.32172176, 0.68117887, 0.33021107],\n         ...,\n         [0.2570776 , 0.5171039 , 0.25890234],\n         [0.25651827, 0.4861638 , 0.25138274],\n         [0.2618871 , 0.4800805 , 0.24060719]],\n\n        [[0.2986774 , 0.6386555 , 0.3181699 ],\n         [0.3129799 , 0.65044934, 0.31268814],\n         [0.32264656, 0.6509155 , 0.32412383],\n         ...,\n         [0.24041544, 0.507095  , 0.2532064 ],\n         [0.2391536 , 0.47372052, 0.25007474],\n         [0.23421209, 0.4681233 , 0.2525673 ]],\n\n        [[0.30854222, 0.6498733 , 0.31356105],\n         [0.31162438, 0.63901085, 0.3144643 ],\n         [0.32165274, 0.65419483, 0.3169573 ],\n         ...,\n         [0.26201892, 0.4779992 , 0.25018737],\n         [0.24167193, 0.46235973, 0.2496034 ],\n         [0.24834126, 0.45295718, 0.2388393 ]]],\n\n\n       [[[0.27551177, 0.5779881 , 0.2849118 ],\n         [0.2944058 , 0.5848376 , 0.27979454],\n         [0.30522603, 0.61366504, 0.28614244],\n         ...,\n         [0.23621172, 0.49928594, 0.2506617 ],\n         [0.23092785, 0.5041307 , 0.24172656],\n         [0.2170071 , 0.4777834 , 0.24475086]],\n\n        [[0.27344963, 0.6184687 , 0.29134616],\n         [0.3008538 , 0.6047901 , 0.29186836],\n         [0.30078638, 0.6276906 , 0.29545435],\n         ...,\n         [0.24390487, 0.51664186, 0.23841727],\n         [0.23702559, 0.4939452 , 0.24299726],\n         [0.22688568, 0.48842147, 0.23370142]],\n\n        [[0.2991329 , 0.6061907 , 0.29050598],\n         [0.3052384 , 0.6217215 , 0.29469764],\n         [0.31616902, 0.64265364, 0.30508462],\n         ...,\n         [0.2503542 , 0.5133305 , 0.26374677],\n         [0.24261494, 0.49887466, 0.2506002 ],\n         [0.2297729 , 0.4904715 , 0.2462242 ]],\n\n        ...,\n\n        [[0.31687322, 0.6722068 , 0.32732087],\n         [0.3179992 , 0.66076165, 0.3136787 ],\n         [0.32496825, 0.6658386 , 0.3187252 ],\n         ...,\n         [0.23455547, 0.46811038, 0.2393023 ],\n         [0.24124788, 0.42895296, 0.22999123],\n         [0.2485777 , 0.41727033, 0.21418157]],\n\n        [[0.3079413 , 0.6252608 , 0.3032703 ],\n         [0.3041077 , 0.64735   , 0.31194097],\n         [0.3144997 , 0.6520684 , 0.31980863],\n         ...,\n         [0.22940026, 0.45482326, 0.24189271],\n         [0.22409505, 0.41289932, 0.2326229 ],\n         [0.22129703, 0.41087553, 0.22132641]],\n\n        [[0.31240004, 0.63917077, 0.3072943 ],\n         [0.31059855, 0.63648015, 0.31131998],\n         [0.31861812, 0.64372545, 0.32242507],\n         ...,\n         [0.24263166, 0.44377476, 0.23647588],\n         [0.22213049, 0.40737712, 0.22822784],\n         [0.23557551, 0.40114096, 0.20823008]]],\n\n\n       ...,\n\n\n       [[[0.34165156, 0.7283156 , 0.35923982],\n         [0.35963833, 0.749544  , 0.35981277],\n         [0.37740168, 0.7792804 , 0.37236738],\n         ...,\n         [0.36794436, 0.77591074, 0.37715307],\n         [0.36912054, 0.77740854, 0.36743805],\n         [0.34650183, 0.7634237 , 0.363174  ]],\n\n        [[0.34239408, 0.76736253, 0.36107612],\n         [0.3596762 , 0.77274114, 0.36713174],\n         [0.359256  , 0.7925982 , 0.3769646 ],\n         ...,\n         [0.37133226, 0.78519875, 0.38103306],\n         [0.3746668 , 0.7748778 , 0.3766407 ],\n         [0.35844213, 0.77453375, 0.36772895]],\n\n        [[0.35319883, 0.7692823 , 0.36238644],\n         [0.36828643, 0.78306973, 0.37228382],\n         [0.3827137 , 0.81040287, 0.36876324],\n         ...,\n         [0.3878558 , 0.8047966 , 0.39240626],\n         [0.38000277, 0.79655623, 0.39127442],\n         [0.35709316, 0.7620281 , 0.3859359 ]],\n\n        ...,\n\n        [[0.37863162, 0.81854516, 0.41339073],\n         [0.36680093, 0.80216926, 0.3869495 ],\n         [0.38042787, 0.7902223 , 0.380364  ],\n         ...,\n         [0.37803745, 0.7740593 , 0.39346817],\n         [0.36507478, 0.7625998 , 0.3769381 ],\n         [0.37251994, 0.7573971 , 0.37973374]],\n\n        [[0.36968252, 0.7680068 , 0.3762011 ],\n         [0.37822774, 0.7787304 , 0.37450844],\n         [0.3760082 , 0.76954913, 0.37337857],\n         ...,\n         [0.3696007 , 0.7581687 , 0.37030157],\n         [0.3603028 , 0.73804873, 0.36653143],\n         [0.35129803, 0.7353649 , 0.38549235]],\n\n        [[0.362974  , 0.766799  , 0.37245876],\n         [0.3825208 , 0.7720851 , 0.37590435],\n         [0.3823154 , 0.76866704, 0.3828638 ],\n         ...,\n         [0.37807283, 0.72195697, 0.36885998],\n         [0.34747273, 0.7234127 , 0.37369   ],\n         [0.35641116, 0.7226629 , 0.37281483]]],\n\n\n       [[[0.33943576, 0.72518945, 0.35675466],\n         [0.35815978, 0.7423768 , 0.35516497],\n         [0.37605858, 0.7720086 , 0.36902955],\n         ...,\n         [0.36031213, 0.7570724 , 0.36576644],\n         [0.36037695, 0.7570952 , 0.35898945],\n         [0.33832356, 0.74369025, 0.35471126]],\n\n        [[0.34164715, 0.7609678 , 0.35752833],\n         [0.35629204, 0.76511025, 0.36376545],\n         [0.35621974, 0.78719616, 0.37623462],\n         ...,\n         [0.36436635, 0.7675225 , 0.37112945],\n         [0.36638105, 0.75286686, 0.36653873],\n         [0.3500429 , 0.7567641 , 0.36094415]],\n\n        [[0.350962  , 0.76351154, 0.3607508 ],\n         [0.36508617, 0.7759022 , 0.36849943],\n         [0.37992305, 0.8050235 , 0.36807376],\n         ...,\n         [0.37803957, 0.785036  , 0.38493758],\n         [0.37316024, 0.7769063 , 0.38373482],\n         [0.35154098, 0.7460149 , 0.37703383]],\n\n        ...,\n\n        [[0.38128936, 0.82303756, 0.41654637],\n         [0.36956814, 0.8076221 , 0.38962838],\n         [0.3840425 , 0.7951897 , 0.3832807 ],\n         ...,\n         [0.37743008, 0.77756643, 0.39516813],\n         [0.36600626, 0.7668789 , 0.38023514],\n         [0.37477612, 0.76170385, 0.38210022]],\n\n        [[0.37190127, 0.7735594 , 0.37795871],\n         [0.38300064, 0.78094405, 0.37732312],\n         [0.37982342, 0.7724752 , 0.37215862],\n         ...,\n         [0.3676485 , 0.7643058 , 0.37132764],\n         [0.36057776, 0.74355364, 0.3688568 ],\n         [0.35111085, 0.7422368 , 0.38363442]],\n\n        [[0.3668106 , 0.77142054, 0.37405884],\n         [0.38307446, 0.775848  , 0.3797789 ],\n         [0.38567457, 0.77110845, 0.3805631 ],\n         ...,\n         [0.37829378, 0.72288126, 0.36955005],\n         [0.34877267, 0.7257825 , 0.3747344 ],\n         [0.35873738, 0.72213745, 0.37603077]]],\n\n\n       [[[0.34130794, 0.7293357 , 0.35884175],\n         [0.3614186 , 0.7445188 , 0.35561153],\n         [0.37678185, 0.7774203 , 0.3740318 ],\n         ...,\n         [0.36375654, 0.77087873, 0.37361303],\n         [0.36743078, 0.7751447 , 0.3650982 ],\n         [0.34517443, 0.76159126, 0.3630333 ]],\n\n        [[0.3428565 , 0.7632487 , 0.3642391 ],\n         [0.35840416, 0.7671317 , 0.36516267],\n         [0.3591332 , 0.7916    , 0.3793021 ],\n         ...,\n         [0.36861214, 0.7861414 , 0.38014856],\n         [0.37128824, 0.77423996, 0.3760704 ],\n         [0.3581277 , 0.7800545 , 0.37299883]],\n\n        [[0.35454053, 0.7703726 , 0.36466515],\n         [0.36853254, 0.7823419 , 0.37271723],\n         [0.38125446, 0.8129263 , 0.36886168],\n         ...,\n         [0.38486964, 0.8033446 , 0.39208022],\n         [0.37957668, 0.7934067 , 0.39061567],\n         [0.3588809 , 0.76111853, 0.38159394]],\n\n        ...,\n\n        [[0.3925407 , 0.85206896, 0.42885357],\n         [0.38067394, 0.83688754, 0.4003519 ],\n         [0.3967381 , 0.8246378 , 0.39789718],\n         ...,\n         [0.38254562, 0.7912076 , 0.397393  ],\n         [0.3699228 , 0.78016925, 0.3879245 ],\n         [0.38251612, 0.7744583 , 0.38657838]],\n\n        [[0.38629216, 0.8022578 , 0.39364278],\n         [0.39521682, 0.80883616, 0.38725394],\n         [0.3956723 , 0.79951924, 0.38343424],\n         ...,\n         [0.3743373 , 0.7757182 , 0.3744791 ],\n         [0.36868683, 0.7566633 , 0.3793785 ],\n         [0.35634845, 0.7519583 , 0.38811383]],\n\n        [[0.37690213, 0.7961806 , 0.38866842],\n         [0.39713562, 0.8071711 , 0.39309785],\n         [0.40183452, 0.8018276 , 0.39528304],\n         ...,\n         [0.38369685, 0.7346626 , 0.3763096 ],\n         [0.3537191 , 0.73897684, 0.37847278],\n         [0.36363015, 0.7371427 , 0.38344613]]]], dtype=float32)>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = cvae(observations_scaled[0:1000])\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x2d92a46d0>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(observations_scaled[10], cmap=\"gray\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"conv_vae_2\" (type ConvVAE).\n\nInput 0 of layer \"encoder\" is incompatible with the layer: expected shape=(None, 64, 64, 3), found shape=(64, 64, 3)\n\nCall arguments received:\n  ‚Ä¢ inputs=tf.Tensor(shape=(64, 64, 3), dtype=float32)\n  ‚Ä¢ training=None\n  ‚Ä¢ mask=None",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [57]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcvae\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservations_scaled\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif_car_race/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "Input \u001B[0;32mIn [21]\u001B[0m, in \u001B[0;36mConvVAE.call\u001B[0;34m(self, inputs, training, mask)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 101\u001B[0m     _, _, z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    102\u001B[0m     reconstruction \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(z)\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m reconstruction\n",
      "\u001B[0;31mValueError\u001B[0m: Exception encountered when calling layer \"conv_vae_2\" (type ConvVAE).\n\nInput 0 of layer \"encoder\" is incompatible with the layer: expected shape=(None, 64, 64, 3), found shape=(64, 64, 3)\n\nCall arguments received:\n  ‚Ä¢ inputs=tf.Tensor(shape=(64, 64, 3), dtype=float32)\n  ‚Ä¢ training=None\n  ‚Ä¢ mask=None"
     ]
    }
   ],
   "source": [
    "cvae(observations_scaled[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x2d3cf0970>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(res[0], cmap=\"gray\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 64, 64, 3)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].numpy().reshape([1] + res[0].shape).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with the prior model FEEF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def transform_image(img, x_min, x_max, y_min, y_max):\n",
    "    img_out = img/255\n",
    "\n",
    "    img_out = img_out[x_min:x_max, y_min:y_max, :]\n",
    "\n",
    "    return img_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def train_single_agent_car_racing(cr_env,\n",
    "                                   agent,\n",
    "                                   num_episodes=100,\n",
    "                                   render_env=False):\n",
    "\n",
    "    # Set up to store results in pandas frame\n",
    "    cols = [\"episode\", \"success\", \"sim_steps\", \"VFE_post_run\", \"noise_stddev\"]\n",
    "    rows = []\n",
    "\n",
    "    for n in range(num_episodes):\n",
    "\n",
    "        print(\"Episode\", n+1)\n",
    "\n",
    "        # reset the agent\n",
    "        agent.reset_all_states()\n",
    "\n",
    "        # get the first observation from the environment\n",
    "        first_observation = cr_env.reset()\n",
    "        print(first_observation)\n",
    "        print(first_observation.shape)\n",
    "\n",
    "        # apply noise to and scaling to first observation\n",
    "        observation = transform_image(first_observation, 16, 80, 16, 80)\n",
    "        observation = observation.reshape([1] + list(observation.shape))\n",
    "        print(observation.shape)\n",
    "\n",
    "        # loop until episode ends or the agent succeeds\n",
    "        t = 0\n",
    "        reward = None\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            if render_env:\n",
    "                cr_env.render()\n",
    "\n",
    "            action = agent.perceive_and_act(observation, reward=reward, done=done)\n",
    "            # print(action)\n",
    "            observation, reward, done, info = cr_env.step(action)  # action should be array to satisfy gym requirements\n",
    "            # print(observation)\n",
    "            observation = transform_image(observation, 16, 84, 16, 84)\n",
    "            observation = observation.reshape([1] + list(observation.shape))\n",
    "\n",
    "            t += 1\n",
    "\n",
    "        # final training when the episode is done\n",
    "        agent.perceive_and_act(observation, reward=reward, done=done)\n",
    "\n",
    "        success = t < 999\n",
    "\n",
    "        # get the VFE of the model for the run\n",
    "        # VFE = float(tf.reduce_mean(agent.model_vae.compute_loss(all_post_observations)))\n",
    "        VFE = 0\n",
    "\n",
    "        rows.append(dict(zip(cols, [n, success, t, VFE, observation_noise_stddev])))\n",
    "\n",
    "        if success:\n",
    "            print(\"Success in episode\", n+1, \"at time step\", t)\n",
    "        else:\n",
    "            print(\"No Success\")\n",
    "\n",
    "    results = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "    cr_env.close()\n",
    "\n",
    "    return agent, results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "pln_hrzn = 5\n",
    "action_dim = 3\n",
    "\n",
    "e = create_conv_encoder(input_dim=(64, 64, 3), latent_dim=latent_dim, num_filters=[16, 32, 32], dense_units=[32])\n",
    "d = create_conv_decoder(latent_dim=latent_dim, output_shape=(64, 64, 3), deconv_shapes=[8, 16, 32], num_filters=[16, 16, 3], dense_units=[8 * 8 * 16])\n",
    "\n",
    "cvae = ConvVAE(e, d, latent_dim=latent_dim, reg_mean=[0] * latent_dim, reg_stddev=[1] * latent_dim)\n",
    "cvae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "tran = TransitionGRU(latent_dim, action_dim=action_dim, hidden_units=2*latent_dim*pln_hrzn, output_dim=latent_dim)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(latent_dim)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           cvae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           planning_horizon=pln_hrzn,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           train_habit_net=False,\n",
    "                           train_prior_model=True,\n",
    "                           train_tran=True,\n",
    "                           train_after_exploring=True,\n",
    "                           train_with_replay=True,\n",
    "                           use_fast_thinking=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n",
      "(96, 96, 3)\n",
      "(1, 64, 64, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (1,3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [81]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# train the agent on the env\u001B[39;00m\n\u001B[1;32m      2\u001B[0m env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCarRacing-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m, new_step_api\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 3\u001B[0m agent, results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_single_agent_car_racing\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdaifa\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrender_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [79]\u001B[0m, in \u001B[0;36mtrain_single_agent_car_racing\u001B[0;34m(cr_env, agent, num_episodes, render_env)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m render_env:\n\u001B[1;32m     34\u001B[0m     cr_env\u001B[38;5;241m.\u001B[39mrender()\n\u001B[0;32m---> 36\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperceive_and_act\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreward\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdone\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# print(action)\u001B[39;00m\n\u001B[1;32m     38\u001B[0m observation, reward, done, info \u001B[38;5;241m=\u001B[39m cr_env\u001B[38;5;241m.\u001B[39mstep(action)  \u001B[38;5;66;03m# action should be array to satisfy gym requirements\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Masters/Thesis/DAIF_Agents/recurrent_agent.py:162\u001B[0m, in \u001B[0;36mDAIFAgentRecurrent.perceive_and_act\u001B[0;34m(self, observation, reward, done)\u001B[0m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;66;03m# Now we select our action. If we aren't exploring then either we act out of habit or we might need to explore\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;66;03m# I think I can check this based on whether or not there are actions left to execute in the current policy\u001B[39;00m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexploring:\n\u001B[0;32m--> 162\u001B[0m     action_as_array \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_being_executed\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtran\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_dim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# pred_next_observation, next_tran_hidden_state = self.predict_next_observation(self.previous_observation, action_as_array)\u001B[39;00m\n\u001B[1;32m    164\u001B[0m     pred_next_observation, next_tran_hidden_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: cannot reshape array of size 1 into shape (1,3)"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make(\"CarRacing-v2\", new_step_api=True)\n",
    "agent, results = train_single_agent_car_racing(env, daifa, num_episodes=1, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the models produced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.model_vae(observations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Test the models produced\n",
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape\n",
    "observations\n",
    "agent.model_vae(observations)\n",
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "pl_hoz = 5\n",
    "latent_dim = 2\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 2*pl_hoz*latent_dim, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(2)\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "# without prior model\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "# with prior model\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           None,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, action_repeats=10, num_actions_to_execute=2, train_on_full_data=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Test the models produced\n",
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.model_vae(observations)\n",
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Identity VAE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = agent.tran((ob_seqs[0:1], None))\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = ob_seqs[0:1, -1].reshape(1,1,3)\n",
    "h = out[3]\n",
    "h = h[0, -2, :]\n",
    "h = h.numpy().reshape(1,30)\n",
    "h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((t, h))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ob_seqs[0:1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test to see how the agent trains on standard observation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, vae_train_epochs=1, tran_train_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "success, agent, t, pre_obs, post_obs, acts = run_episode(env, daifa, observation_max, observation_min, observation_noise_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_np = np.array(pre_obs)\n",
    "a = np.array(acts)\n",
    "a.shape\n",
    "pre_a = np.concatenate([pre_np, a], axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(a.max(), a.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict = np.array(post_obs)[:, 14, :]\n",
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((pre_a, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine training the model on the observation data\n",
    "\n",
    "Does it eventually converge to a good model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_runs = 1\n",
    "for i in range(num_train_runs):\n",
    "\n",
    "    for j in range(len(pre)):\n",
    "        pre = pre_obs[j]\n",
    "        post = post_obs[j]\n",
    "        actions = acts[j]\n",
    "\n",
    "        daifa.train(pre, post, actions, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation(np.array([0.5, 0.1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the FEEF computations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, planning_horizon=15, n_policy_candidates=70, n_policies=1500, n_cem_policy_iterations=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_policy(agent, env, policy, action_repeats):\n",
    "\n",
    "    observation = env.reset()\n",
    "    obs = transform_observations(observation, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    z_t_minus_1 = obs\n",
    "    p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "    p\n",
    "    print(obs)\n",
    "    print(p)\n",
    "\n",
    "    for action in p:\n",
    "        for t in range(action_repeats):\n",
    "            res = env.step(np.array([action]))\n",
    "            print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([0, 0])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p\n",
    "\n",
    "agent.forward_policies(p, z_t_minus_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "test_policy(agent, env, p.numpy(), 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([-0.27691475,  0.01688306])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 given_prior_mean,\n",
    "                 given_prior_stddev,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 tran_train_epochs=1,\n",
    "                 vae_train_epochs=1,\n",
    "                 agent_time_ratio=6,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        self.prior_model = prior_model\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        self.vae_train_epochs = vae_train_epochs\n",
    "        self.tran_train_epochs = tran_train_epochs\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.model_vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        # transition\n",
    "        # takes action plus last state and outputs next latent state\n",
    "        self.tran = tran\n",
    "        self.tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        self.hidden_state = None\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(observation)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    def train(self, pre_observations_raw, post_observations_raw, actions_complete, rewards, verbose=0):\n",
    "\n",
    "        # compress the observations based on the agents time compression factor\n",
    "        # pre_observations = pre_observations_raw[::self.agent_time_ratio]  # for example take every 6th element\n",
    "        # post_observations = np.array([post_observations_raw[i] for i in range(len(post_observations_raw)) if i % self.agent_time_ratio == self.agent_time_ratio - 1])\n",
    "        #\n",
    "        # print(pre_observations_raw)\n",
    "        # print(pre_observations)\n",
    "        # print(post_observations_raw)\n",
    "        # print(post_observations)\n",
    "\n",
    "        pre_observations = pre_observations_raw\n",
    "        post_observations = post_observations_raw\n",
    "\n",
    "        # only look at the first n actions that we took\n",
    "        actions = actions_complete[0: len(pre_observations)]\n",
    "\n",
    "        num_observations = pre_observations.shape[0]\n",
    "        observation_dim = pre_observations.shape[1]\n",
    "        action_dim = actions.shape[1]\n",
    "        # action_dim = 1  # TODO fix this to allow different actions\n",
    "\n",
    "        # find the actual observed latent states using the vae\n",
    "        pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "        post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "        # set up the input training data that we use to train the transition model\n",
    "        z_train = np.concatenate([np.array(pre_latent_mean), np.array(actions)], axis=1)\n",
    "\n",
    "        # we use the sequence to find the right hidden states to use as input\n",
    "        z_train_seq = z_train.reshape((1, num_observations, observation_dim + action_dim))\n",
    "        z_train_singles = z_train.reshape(num_observations, 1, observation_dim + action_dim)\n",
    "\n",
    "        # the previous hidden state is the memory after observing some sequences but it might be None\n",
    "        if self.hidden_state is None:\n",
    "            self.hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "        if self.train_tran:\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.hidden_state))\n",
    "\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran_train_epochs, verbose=verbose)\n",
    "\n",
    "        # now find the new predicted hidden state that we will use for finding the policy\n",
    "        # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "        _, _, final_hidden_state, _ = self.tran((z_train_seq, self.hidden_state))\n",
    "        # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "        self.hidden_state = final_hidden_state\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            self.model_vae.fit(post_observations, epochs=self.vae_train_epochs, verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by one to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        print(z_t_minus_one)\n",
    "        print(mean_best_policies)\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape [self.n_policies, len(z_t_minus_one)]\n",
    "        prev_latent_mean = np.stack([z_t_minus_one]*self.n_policies)\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "            likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "            if self.prior_model is None:\n",
    "\n",
    "                # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                # create the prior distribution\n",
    "                prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "            # TODO Fix the learned prior model\n",
    "            else:\n",
    "                prior_dist = self.prior_model()\n",
    "\n",
    "            kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "            predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "            kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    def EFE(self, policy_posteriors, predicted_likelihood, predicted_posterior):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing with a pretrained transition model\n",
    "\n",
    "This works well! So the problem can't lie with the transition model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0.07]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, train_vae=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 200\n",
    "seq_length = 500\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "next_obs_stddev = []\n",
    "actions = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.2)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    # train = np.concatenate([o[:-1], a], axis=1)\n",
    "    train = o[:-1]\n",
    "    test = o[1:]\n",
    "\n",
    "    actions.append(a)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "    ob_seqs_stddev = np.ones_like(train)\n",
    "    next_stddev = np.ones_like(test)\n",
    "\n",
    "    next_obs_stddev.append(next_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(num_seqs):\n",
    "\n",
    "    pre = ob_seqs[i]\n",
    "    next = next_obs[i]\n",
    "    acts = actions[i]\n",
    "\n",
    "    next_sd = next_obs_stddev[i]\n",
    "\n",
    "    daifa.train(pre, next, acts, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 150\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    test = o[-1]\n",
    "\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "ob_seqs = np.array(ob_seqs)[:, -5:, :]\n",
    "next_obs = np.array(next_obs)\n",
    "ob_seqs.shape\n",
    "\n",
    "ob_seqs_stddev = np.ones_like(ob_seqs)\n",
    "next_obs_stddev = np.ones_like(next_obs)\n",
    "\n",
    "ob_seqs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.tran((ob_seqs, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_obs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks fantastic!!! With enough data the transition model is training very well"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.hidden_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([0.4, 0.5])\n",
    "daifa.hidden_state = None\n",
    "p, s = daifa.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.tran((np.array([[[0.4, 0.5, 1]]]), None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "daifa.train_tran = False\n",
    "daifa.train_vae = False\n",
    "\n",
    "daifa.hidden_state = None\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}