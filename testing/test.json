{
  "experiment_name": "PG_train_from_start",
  "agent": {
    "given_prior_mean":None,
    "given_prior_stddev":None,
    agent_time_ratio=6,
    actions_to_execute_when_exploring=2,
    planning_horizon=5,
    n_policies=1500,
    n_cem_policy_iterations=2,
    n_policy_candidates=70,
    train_vae=True,
    train_tran=True,
    train_prior_model=True,
    train_habit_net=True,
    train_with_replay=True,
    train_after_exploring=True,
    use_kl_extrinsic=True,
    use_kl_intrinsic=True,
    use_FEEF=False,
    use_fast_thinking=True,
    uncertainty_tolerance=0.1,
    habit_model_type="PG"
  },

  "vae": {
    "observation_dim": 2,
    "latent_dim": 2,
    "observation_noise_std": null
  },

  "prior": {
    "Normal": {
      "loc": [
        0.9,
        0.0
      ],
      "std": [
        1.0,
        1.0
      ]
    },
  },

  "simulation": {
    VAE_RUNS=6,
    TRAN_RUNS=2,
    HABIT_RUNS=2,
    FLIP_DYNAMICS_RUNS=5,
    EPISODES_BETWEEN_HABIT_TESTS=10,
  }
}