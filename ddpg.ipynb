{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Habitual DDPG Network\n",
    "\n",
    "Habitual network\n",
    "\n",
    "Assuming generative model is perfect, then action selected would always be the action that maximises chance of observing prior preferences. Hence habitual network can be trained to output maximally rewarding actions, as these actions are the free energy minimising actions.\n",
    "\n",
    "Also has a nice interpretation as long as the generative models keep training. Eventually the generative model is less sure about old things. Why people eventually revisit old states they were previously certain about.\n",
    "\n",
    "As far as an agent knows, if observations are confirming perfectly to expectations then it has a perfect world model. So why would it change it? Itâ€™s only when an uncertain observation comes in that the agent needs to reconsider whether or not it has the best model of the world.\n",
    "\n",
    "\n",
    "I think this network should be performing policy gradient method but instead of minimising the discounted reward sequence it should minimise the discounted external EFE/FEEF component sequence. That way in the end the end the fast and slow thinking methods should be converging as the world model continues to improve\n",
    "\n",
    "\n",
    "What is this network trying to learn?\n",
    "- This network is trying to learn the state action mapping that maximises the probability of being in the preferred states\n",
    "- It is also trying to learn to output actions that minimise the extrinsic part of the EFE/FEEF\n",
    "\n",
    "\n",
    "What does this network take as input?\n",
    "- Current state\n",
    "- Maybe sequence of previous states and actions\n",
    "\n",
    "What should this network output?\n",
    "- The action that leads to the next state that maximally achieves the prior preferences\n",
    "\n",
    "How should this network learn?\n",
    "- It should learn by outputting\n",
    "\n",
    "\n",
    "Okay so new idea! DDPG seems pretty good so far. How about we have the Q function take latent states as input and use the VAEs good latent features as input. Then we'll have\n",
    "- Q(o, a)\n",
    "- p(s|o) and p(o|s)\n",
    "- p(s'|s, a)\n",
    "- V(o) or U(o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "from habitual_action_network import HabitualAction, compute_discounted_cumulative_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  2\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self,\n",
    "                 buffer_capacity=100000,\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 observation_dim=2,\n",
    "                 action_dim=1,\n",
    "                 critic_optimizer=\"adam\",\n",
    "                 actor_optimizer=\"adam\"):\n",
    "\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, observation_dim))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, action_dim))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, observation_dim))\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.critic_optimizer = critic_optimizer\n",
    "        self.actor_optimizer = actor_optimizer\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # clears the buffer\n",
    "    def clear(self):\n",
    "        self.state_buffer= []\n",
    "        self.action_buffer = []\n",
    "        self.reward_buffer = []\n",
    "        self.next_state_buffer = []\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "            self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + self.gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_actor(observation_dim, action_max):\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(observation_dim,))\n",
    "    out = layers.Dense(16, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(32, activation=\"relu\")(out)\n",
    "    out = layers.Dense(16, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * action_max\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic(observation_dim, action_dim):\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=observation_dim)\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=action_dim)\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    # was 256\n",
    "    out = layers.Dense(128, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(128, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class BasicDDPG:\n",
    "\n",
    "    def __init__(self, actor, critic, target_actor, target_critic, buffer, tau):\n",
    "\n",
    "        self.actor_model = actor\n",
    "        self.critic_model = critic\n",
    "\n",
    "        self.target_actor = target_actor\n",
    "        self.target_critic = target_critic\n",
    "\n",
    "        self.buffer = buffer\n",
    "        self.tau = tau\n",
    "\n",
    "    def update_actor_target(self):\n",
    "        update_target(self.target_actor.variables, self.actor_model.variables, self.tau)\n",
    "\n",
    "    def update_critic_target(self):\n",
    "        update_target(self.target_critic.variables, self.critic_model.variables, self.tau)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "actor_model = get_actor(2, 1)\n",
    "critic_model = get_critic(2, 1)\n",
    "\n",
    "target_actor = get_actor(2, 1)\n",
    "target_critic = get_critic(2, 1)\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.0001\n",
    "actor_lr = 0.00005\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64, 0.99, 2, 1, critic_optimizer, actor_optimizer)\n",
    "\n",
    "ddpg = BasicDDPG(actor_model, critic_model, target_actor, target_critic, buffer, tau)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 17:30:34.298859: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-32.963745824437254\n",
      "0.0035701806 -0.03100249\n",
      "86.68404216376675\n",
      "-0.005363233 -0.043388568\n",
      "92.1443371592248\n",
      "0.061502587 0.02080527\n",
      "-38.34286132328569\n",
      "0.43083033 0.2759526\n",
      "86.42506779463984\n",
      "0.42712528 0.19821759\n",
      "65.09329994858183\n",
      "0.48775825 0.32344788\n",
      "70.08626133059292\n",
      "0.816972 0.50715816\n",
      "81.57467205559598\n",
      "0.85989547 0.41579473\n",
      "73.3166470604361\n",
      "0.86640894 0.29630744\n",
      "69.98795232498819\n",
      "0.9851574 0.121006206\n",
      "-32.456244955254434\n",
      "0.9982692 -0.13641658\n",
      "-30.08506976348149\n",
      "0.9999931 0.1497526\n",
      "-27.42503418034901\n",
      "0.99999976 -0.120499395\n",
      "-35.41489023422968\n",
      "0.99999976 -0.07794662\n",
      "70.86375380387494\n",
      "0.99999976 -0.14111122\n",
      "94.52054392650672\n",
      "0.99999976 -0.07773594\n",
      "81.6220225772017\n",
      "1.0 0.64832085\n",
      "76.1405439089829\n",
      "1.0 -0.0047395765\n",
      "-34.11562576232058\n",
      "0.99999976 -0.13441344\n",
      "-30.5486287114235\n",
      "0.99999976 -0.18020196\n",
      "-33.23004983689618\n",
      "1.0 -0.20430213\n",
      "-29.15379126666621\n",
      "1.0 -0.31424105\n",
      "-36.87250747700676\n",
      "0.99999976 -0.45255142\n",
      "82.3017664596413\n",
      "1.0 -0.77813107\n",
      "91.09502325132146\n",
      "1.0 -0.7645173\n",
      "-34.95882308941864\n",
      "1.0 -0.6947016\n",
      "83.71042167205167\n",
      "1.0 -0.785864\n",
      "82.6007398793354\n",
      "1.0 -0.79948384\n",
      "-38.60719357893341\n",
      "1.0 -0.849496\n",
      "87.53192958372085\n",
      "0.99999976 -0.8651841\n",
      "83.43987278993367\n",
      "1.0 -0.8441161\n",
      "77.50573446903547\n",
      "0.99999976 -0.90256864\n",
      "78.81013884380717\n",
      "0.99999976 -0.9379528\n",
      "69.96210177338946\n",
      "1.0 -0.9773499\n",
      "-30.034388181474725\n",
      "1.0 -0.9956743\n",
      "-31.82518578510301\n",
      "1.0 -0.998539\n",
      "67.4663272305817\n",
      "1.0 -0.99944174\n",
      "80.04455452942457\n",
      "0.99999976 -0.9995461\n",
      "81.68887578989309\n",
      "1.0 -0.9993501\n",
      "78.66580108321611\n",
      "1.0 -0.99988735\n",
      "-30.880360174833502\n",
      "1.0 -0.99986225\n",
      "80.37905892306402\n",
      "0.99999976 -0.9999601\n",
      "-30.81974193556577\n",
      "0.99999976 -0.99983513\n",
      "-32.51234449132956\n",
      "1.0 -0.99998516\n",
      "84.87746826574352\n",
      "1.0 -0.9999917\n",
      "89.13720294235215\n",
      "1.0 -0.9999812\n",
      "-28.47974865903717\n",
      "0.99999976 -0.99999523\n",
      "-31.908809793940502\n",
      "1.0 -0.9999801\n",
      "-36.54720263913287\n",
      "0.99999976 -0.999997\n",
      "64.45462927257422\n",
      "0.99999976 -0.99999595\n"
     ]
    }
   ],
   "source": [
    "t_max = 1000\n",
    "num_episodes = 50\n",
    "\n",
    "min_reward_cutoff = -1000\n",
    "min_reward_set = -0.5\n",
    "\n",
    "reward_increase = 0\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    all_observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    o, a, r = random_observation_sequence(env, t_max, epsilon=0.2)\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "\n",
    "    for i in range(len(a)):\n",
    "\n",
    "        prev_state = o[i]\n",
    "        state = o[i+1]\n",
    "        action = a[i]\n",
    "        reward = r[i] + reward_increase\n",
    "\n",
    "        # if reward < 0:\n",
    "        #     print(\"yes\")\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        ddpg.buffer.record((prev_state, action, reward, state))\n",
    "        # episodic_reward += reward\n",
    "        #\n",
    "        ddpg.buffer.learn()\n",
    "        ddpg.update_actor_target()\n",
    "        ddpg.update_critic_target()\n",
    "\n",
    "        # buffer.record((prev_state, action, reward, state))\n",
    "        # # episodic_reward += reward\n",
    "        # #\n",
    "        # buffer.learn()\n",
    "        # update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        # update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "    print(total_reward)\n",
    "\n",
    "    acts = ddpg.actor_model((np.random.random(size=(10, 2))*2 - 1))\n",
    "    print(np.max(acts), np.min(acts))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\narray([[5811.5957 ],\n       [2591.8093 ],\n       [1217.9972 ],\n       [2252.3525 ],\n       [2650.7405 ],\n       [1022.43146],\n       [2465.303  ],\n       [2555.3264 ],\n       [2447.9504 ],\n       [ 935.72943]], dtype=float32)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg.critic_model([(np.random.random(size=(10, 2))*2 - 1), (np.random.random(size=(10, 1))*2 - 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Can it solve the environment?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\narray([[ 0.99999976],\n       [ 0.99999976],\n       [ 0.99999976],\n       [-0.9993669 ],\n       [ 0.99999976],\n       [ 0.99999976],\n       [ 0.99999976],\n       [ 0.99999976],\n       [-0.99695086],\n       [ 1.        ]], dtype=float32)>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg.actor_model((np.random.random(size=(10, 2))*2 - 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "success\n",
      "93.36774634396815\n",
      "68\n",
      "success\n",
      "93.3693611066235\n",
      "68\n",
      "success\n",
      "93.36889756146266\n",
      "69\n",
      "success\n",
      "93.29928721775096\n",
      "68\n",
      "success\n",
      "93.39075685396485\n",
      "68\n",
      "success\n",
      "93.38905150359172\n",
      "68\n",
      "success\n",
      "93.37940230180583\n",
      "68\n",
      "success\n",
      "93.36928243179368\n",
      "68\n",
      "success\n",
      "93.39846199351936\n",
      "69\n",
      "success\n",
      "93.30938248436043\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# obs_stddev = [0.05, 0.05]\n",
    "obs_stddev = [0, 0]\n",
    "\n",
    "\n",
    "t_max = 1000\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    done = False\n",
    "    rewards = []\n",
    "\n",
    "    t = 0\n",
    "    while not done:\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        obs = obs.reshape(1, obs.shape[0])\n",
    "        obs = transform_observations(obs, observation_max, observation_min, obs_stddev)\n",
    "\n",
    "        # print(obs)\n",
    "\n",
    "        # action = act_net(obs) * 10\n",
    "        # action = np.clip(action.numpy(), -1, 1)\n",
    "\n",
    "        action = actor_model(obs)\n",
    "        action = action.numpy()\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # print(obs)\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "        t += 1\n",
    "\n",
    "        if t == t_max:\n",
    "            done = True\n",
    "\n",
    "    print(t)\n",
    "    if t < t_max:\n",
    "        print(\"success\")\n",
    "    else:\n",
    "        print(\"Failure\")\n",
    "        print(\"max obs\", obs)\n",
    "\n",
    "    print(np.sum(rewards))\n",
    "    # print(rewards)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m both \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(both)\n\u001B[1;32m      4\u001B[0m both\n\u001B[0;32m----> 6\u001B[0m both_acts \u001B[38;5;241m=\u001B[39m \u001B[43mact_net\u001B[49m(both)\n\u001B[1;32m      8\u001B[0m both_acts\n",
      "\u001B[0;31mNameError\u001B[0m: name 'act_net' is not defined"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "both = [[i/n, j/n] for i in range(-1*n, n) for j in range(-1*n, n)]\n",
    "both = np.array(both)\n",
    "both\n",
    "\n",
    "both_acts = act_net(both)\n",
    "\n",
    "both_acts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = 50\n",
    "coords = [[i/n, j/n] for i in range(-1*n, n) for j in range(-1*n, n)]\n",
    "coords = np.array(coords)\n",
    "coords\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5.1, 0.5)\n",
    "y = np.arange(-5, 5.1, 0.5)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}