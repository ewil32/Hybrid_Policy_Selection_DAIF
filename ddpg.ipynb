{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Habitual DDPG Network\n",
    "\n",
    "Habitual network\n",
    "\n",
    "Assuming generative model is perfect, then action selected would always be the action that maximises chance of observing prior preferences. Hence habitual network can be trained to output maximally rewarding actions, as these actions are the free energy minimising actions.\n",
    "\n",
    "Also has a nice interpretation as long as the generative models keep training. Eventually the generative model is less sure about old things. Why people eventually revisit old states they were previously certain about.\n",
    "\n",
    "As far as an agent knows, if observations are confirming perfectly to expectations then it has a perfect world model. So why would it change it? Itâ€™s only when an uncertain observation comes in that the agent needs to reconsider whether or not it has the best model of the world.\n",
    "\n",
    "\n",
    "I think this network should be performing policy gradient method but instead of minimising the discounted reward sequence it should minimise the discounted external EFE/FEEF component sequence. That way in the end the end the fast and slow thinking methods should be converging as the world model continues to improve\n",
    "\n",
    "\n",
    "What is this network trying to learn?\n",
    "- This network is trying to learn the state action mapping that maximises the probability of being in the preferred states\n",
    "- It is also trying to learn to output actions that minimise the extrinsic part of the EFE/FEEF\n",
    "\n",
    "\n",
    "What does this network take as input?\n",
    "- Current state\n",
    "- Maybe sequence of previous states and actions\n",
    "\n",
    "What should this network output?\n",
    "- The action that leads to the next state that maximally achieves the prior preferences\n",
    "\n",
    "How should this network learn?\n",
    "- It should learn by outputting\n",
    "\n",
    "\n",
    "Okay so new idea! DDPG seems pretty good so far. How about we have the Q function take latent states as input and use the VAEs good latent features as input. Then we'll have\n",
    "- Q(o, a)\n",
    "- p(s|o) and p(o|s)\n",
    "- p(s'|s, a)\n",
    "- V(o) or U(o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "from ddpg import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  2\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# class Buffer:\n",
    "#     def __init__(self,\n",
    "#                  buffer_capacity=100000,\n",
    "#                  batch_size=64,\n",
    "#                  gamma=0.99,\n",
    "#                  observation_dim=2,\n",
    "#                  action_dim=1,\n",
    "#                  critic_optimizer=\"adam\",\n",
    "#                  actor_optimizer=\"adam\"):\n",
    "#\n",
    "#         # Number of \"experiences\" to store at max\n",
    "#         self.buffer_capacity = buffer_capacity\n",
    "#         # Num of tuples to train on.\n",
    "#         self.batch_size = batch_size\n",
    "#\n",
    "#         # Its tells us num of times record() was called.\n",
    "#         self.buffer_counter = 0\n",
    "#\n",
    "#         # Instead of list of tuples as the exp.replay concept go\n",
    "#         # We use different np.arrays for each tuple element\n",
    "#         self.state_buffer = np.zeros((self.buffer_capacity, observation_dim))\n",
    "#         self.action_buffer = np.zeros((self.buffer_capacity, action_dim))\n",
    "#         self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "#         self.next_state_buffer = np.zeros((self.buffer_capacity, observation_dim))\n",
    "#\n",
    "#         self.gamma = gamma\n",
    "#\n",
    "#         self.critic_optimizer = critic_optimizer\n",
    "#         self.actor_optimizer = actor_optimizer\n",
    "#\n",
    "#     # Takes (s,a,r,s') obervation tuple as input\n",
    "#     def record(self, obs_tuple):\n",
    "#         # Set index to zero if buffer_capacity is exceeded,\n",
    "#         # replacing old records\n",
    "#         index = self.buffer_counter % self.buffer_capacity\n",
    "#\n",
    "#         self.state_buffer[index] = obs_tuple[0]\n",
    "#         self.action_buffer[index] = obs_tuple[1]\n",
    "#         self.reward_buffer[index] = obs_tuple[2]\n",
    "#         self.next_state_buffer[index] = obs_tuple[3]\n",
    "#\n",
    "#         self.buffer_counter += 1\n",
    "#\n",
    "#     # clears the buffer\n",
    "#     def clear(self):\n",
    "#         self.state_buffer= []\n",
    "#         self.action_buffer = []\n",
    "#         self.reward_buffer = []\n",
    "#         self.next_state_buffer = []\n",
    "#\n",
    "#     # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "#     # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "#     # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "#     @tf.function\n",
    "#     def update(\n",
    "#             self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "#     ):\n",
    "#         # Training and updating Actor & Critic networks.\n",
    "#         # See Pseudo Code.\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             target_actions = target_actor(next_state_batch, training=True)\n",
    "#             y = reward_batch + self.gamma * target_critic(\n",
    "#                 [next_state_batch, target_actions], training=True\n",
    "#             )\n",
    "#             critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "#             critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "#\n",
    "#         critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "#         self.critic_optimizer.apply_gradients(\n",
    "#             zip(critic_grad, critic_model.trainable_variables)\n",
    "#         )\n",
    "#\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             actions = actor_model(state_batch, training=True)\n",
    "#             critic_value = critic_model([state_batch, actions], training=True)\n",
    "#             # Used `-value` as we want to maximize the value given\n",
    "#             # by the critic for our actions\n",
    "#             actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "#\n",
    "#         actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "#         self.actor_optimizer.apply_gradients(\n",
    "#             zip(actor_grad, actor_model.trainable_variables)\n",
    "#         )\n",
    "#\n",
    "#     # We compute the loss and update parameters\n",
    "#     def learn(self):\n",
    "#         # Get sampling range\n",
    "#         record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "#         # Randomly sample indices\n",
    "#         batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "#\n",
    "#         # Convert to tensors\n",
    "#         state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "#         action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "#         reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "#         reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "#         next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "#\n",
    "#         self.update(state_batch, action_batch, reward_batch, next_state_batch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# def get_actor(observation_dim, action_max):\n",
    "#     # Initialize weights between -3e-3 and 3-e3\n",
    "#     last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "#\n",
    "#     inputs = layers.Input(shape=(observation_dim,))\n",
    "#     out = layers.Dense(16, activation=\"relu\")(inputs)\n",
    "#     out = layers.Dense(32, activation=\"relu\")(out)\n",
    "#     out = layers.Dense(16, activation=\"relu\")(out)\n",
    "#     outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "#\n",
    "#     # Our upper bound is 2.0 for Pendulum.\n",
    "#     outputs = outputs * action_max\n",
    "#     model = tf.keras.Model(inputs, outputs)\n",
    "#     return model\n",
    "#\n",
    "#\n",
    "# def get_critic(observation_dim, action_dim):\n",
    "#     # State as input\n",
    "#     state_input = layers.Input(shape=(observation_dim, ))\n",
    "#     state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#\n",
    "#     # Action as input\n",
    "#     action_input = layers.Input(shape=action_dim)\n",
    "#     action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#\n",
    "#     # Both are passed through seperate layer before concatenating\n",
    "#     concat = layers.Concatenate()([state_out, action_out])\n",
    "#\n",
    "#     # was 256\n",
    "#     out = layers.Dense(128, activation=\"relu\")(concat)\n",
    "#     out = layers.Dense(128, activation=\"relu\")(out)\n",
    "#     outputs = layers.Dense(1)(out)\n",
    "#\n",
    "#     # Outputs single value for give state-action\n",
    "#     model = tf.keras.Model([state_input, action_input], outputs)\n",
    "#\n",
    "#     return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# class BasicDDPG:\n",
    "#\n",
    "#     def __init__(self, actor, critic, target_actor, target_critic, tau,\n",
    "#                  buffer_capacity=100000,\n",
    "#                  batch_size=64,\n",
    "#                  gamma=0.99,\n",
    "#                  observation_dim=2,\n",
    "#                  action_dim=1,\n",
    "#                  critic_optimizer=\"adam\",\n",
    "#                  actor_optimizer=\"adam\"):\n",
    "#\n",
    "#         self.actor_model = actor\n",
    "#         self.critic_model = critic\n",
    "#\n",
    "#         self.target_actor = target_actor\n",
    "#         self.target_critic = target_critic\n",
    "#\n",
    "#         # self.buffer = buffer\n",
    "#         self.tau = tau\n",
    "#\n",
    "#         # BUFFER\n",
    "#\n",
    "#         # Number of \"experiences\" to store at max\n",
    "#         self.buffer_capacity = buffer_capacity\n",
    "#         # Num of tuples to train on.\n",
    "#         self.batch_size = batch_size\n",
    "#\n",
    "#         # Its tells us num of times record() was called.\n",
    "#         self.buffer_counter = 0\n",
    "#\n",
    "#         # Instead of list of tuples as the exp.replay concept go\n",
    "#         # We use different np.arrays for each tuple element\n",
    "#         self.state_buffer = np.zeros((self.buffer_capacity, observation_dim))\n",
    "#         self.action_buffer = np.zeros((self.buffer_capacity, action_dim))\n",
    "#         self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "#         self.next_state_buffer = np.zeros((self.buffer_capacity, observation_dim))\n",
    "#\n",
    "#         self.gamma = gamma\n",
    "#\n",
    "#         self.critic_optimizer = critic_optimizer\n",
    "#         self.actor_optimizer = actor_optimizer\n",
    "#\n",
    "#     def update_actor_target(self):\n",
    "#         update_target(self.target_actor.variables, self.actor_model.variables, self.tau)\n",
    "#\n",
    "#     def update_critic_target(self):\n",
    "#         update_target(self.target_critic.variables, self.critic_model.variables, self.tau)\n",
    "#\n",
    "#     # Takes (s,a,r,s') obervation tuple as input\n",
    "#     def record(self, obs_tuple):\n",
    "#         # Set index to zero if buffer_capacity is exceeded,\n",
    "#         # replacing old records\n",
    "#         index = self.buffer_counter % self.buffer_capacity\n",
    "#\n",
    "#         self.state_buffer[index] = obs_tuple[0]\n",
    "#         self.action_buffer[index] = obs_tuple[1]\n",
    "#         self.reward_buffer[index] = obs_tuple[2]\n",
    "#         self.next_state_buffer[index] = obs_tuple[3]\n",
    "#\n",
    "#         self.buffer_counter += 1\n",
    "#\n",
    "#     # clears the buffer\n",
    "#     def clear(self):\n",
    "#         self.state_buffer= []\n",
    "#         self.action_buffer = []\n",
    "#         self.reward_buffer = []\n",
    "#         self.next_state_buffer = []\n",
    "#\n",
    "#     # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "#     # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "#     # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "#     @tf.function\n",
    "#     def update(\n",
    "#             self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "#     ):\n",
    "#         # Training and updating Actor & Critic networks.\n",
    "#         # See Pseudo Code.\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             target_actions = self.target_actor(next_state_batch, training=True)\n",
    "#             y = reward_batch + self.gamma * self.target_critic(\n",
    "#                 [next_state_batch, target_actions], training=True\n",
    "#             )\n",
    "#             critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "#             critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "#\n",
    "#         critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "#         self.critic_optimizer.apply_gradients(\n",
    "#             zip(critic_grad, self.critic_model.trainable_variables)\n",
    "#         )\n",
    "#\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             actions = self.actor_model(state_batch, training=True)\n",
    "#             critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "#             # Used `-value` as we want to maximize the value given\n",
    "#             # by the critic for our actions\n",
    "#             actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "#\n",
    "#         actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "#         self.actor_optimizer.apply_gradients(\n",
    "#             zip(actor_grad, self.actor_model.trainable_variables)\n",
    "#         )\n",
    "#\n",
    "#     # We compute the loss and update parameters\n",
    "#     def learn(self):\n",
    "#         # Get sampling range\n",
    "#         record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "#         # Randomly sample indices\n",
    "#         batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "#\n",
    "#         # Convert to tensors\n",
    "#         state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "#         action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "#         reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "#         reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "#         next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "#\n",
    "#         self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "#\n",
    "#     def train(self):\n",
    "#         self.learn()\n",
    "#         self.update_actor_target()\n",
    "#         self.update_critic_target()\n",
    "#\n",
    "#\n",
    "# # This update target parameters slowly\n",
    "# # Based on rate `tau`, which is much less than one.\n",
    "# @tf.function\n",
    "# def update_target(target_weights, weights, tau):\n",
    "#     for (a, b) in zip(target_weights, weights):\n",
    "#         a.assign(b * tau + a * (1 - tau))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "actor_model = get_actor(2, 1)\n",
    "critic_model = get_critic(2, 1)\n",
    "\n",
    "target_actor = get_actor(2, 1)\n",
    "target_critic = get_critic(2, 1)\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.0001\n",
    "actor_lr = 0.00005\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "# buffer = Buffer(50000, 64, 0.99, 2, 1, critic_optimizer, actor_optimizer)\n",
    "\n",
    "ddpg = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 21:52:17.536133: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-33.424322627462075\n",
      "0.06587567 0.014397912\n",
      "-35.35045031681202\n",
      "0.051338397 0.022356827\n",
      "72.27156689120763\n",
      "0.07101009 0.017263131\n",
      "-32.70369907036973\n",
      "0.6897657 -0.024086904\n",
      "90.77576154085813\n",
      "0.2173323 -0.031274978\n",
      "85.04573819991703\n",
      "0.963663 -0.0074115032\n",
      "74.0665016351666\n",
      "0.9999845 -0.085101694\n",
      "-32.47587974133253\n",
      "0.9999813 -0.14758924\n",
      "-28.873203949061825\n",
      "0.99999917 -0.15841354\n",
      "88.82792863606231\n",
      "1.0 -0.13598327\n",
      "-30.032951099197184\n",
      "0.99999976 -0.17659165\n",
      "87.90769479308469\n",
      "0.9999994 -0.2000603\n",
      "-32.16579916405158\n",
      "1.0 -0.35338718\n",
      "-35.31154210149196\n",
      "0.99999976 -0.35647464\n",
      "-31.158652382460517\n",
      "1.0 0.99999976\n",
      "77.2398679400852\n",
      "1.0 -0.3716555\n",
      "80.49772082162337\n",
      "1.0 -0.38257816\n",
      "-33.38279204495365\n",
      "1.0 -0.41619313\n",
      "86.00887659693429\n",
      "1.0 -0.4270042\n",
      "81.26184905768638\n",
      "1.0 -0.4468089\n",
      "78.30079055965817\n",
      "1.0 -0.41408217\n",
      "78.45110869198396\n",
      "1.0 -0.3927032\n",
      "84.58975929648372\n",
      "0.99999976 -0.420844\n",
      "75.01979424038088\n",
      "1.0 -0.38102672\n",
      "89.728579082291\n",
      "0.99999976 -0.28652906\n",
      "-32.68142277430651\n",
      "1.0 -0.43215403\n",
      "81.58584703213596\n",
      "1.0 -0.3459804\n",
      "-32.2742940924572\n",
      "0.99999976 -0.40663308\n",
      "80.97189163037902\n",
      "0.99999934 -0.47327358\n",
      "-32.13134167703842\n",
      "0.99999976 -0.84295535\n",
      "74.87787482728933\n",
      "0.99999976 -0.9351387\n",
      "86.0180487540367\n",
      "1.0 -0.9296269\n",
      "87.83573259246995\n",
      "0.99999976 -0.978566\n",
      "87.27840533192783\n",
      "1.0 -0.9877689\n",
      "74.01810377154543\n",
      "1.0 -0.99410117\n",
      "-27.943393389303377\n",
      "1.0 -0.9976636\n",
      "-33.97530705803488\n",
      "1.0 -0.998461\n",
      "82.8511885504449\n",
      "1.0 -0.999056\n",
      "78.53508077051438\n",
      "0.99999976 -0.9987094\n",
      "-35.60183150069747\n",
      "1.0 -0.9990527\n",
      "66.71873662804191\n",
      "1.0 -0.999787\n",
      "92.2463193797939\n",
      "1.0 -0.9997788\n",
      "83.22541879307383\n",
      "1.0 -0.9994128\n",
      "77.37521847096586\n",
      "0.99999976 -0.9995921\n",
      "-28.286333850768788\n",
      "1.0 -0.9999467\n",
      "-34.045877224544434\n",
      "1.0 -0.9999655\n",
      "-37.97938884779438\n",
      "0.8848054 -0.9999842\n",
      "80.97970755368007\n",
      "1.0 -0.9999242\n",
      "79.63487077922306\n",
      "0.99999976 -0.9999204\n",
      "84.28786386519087\n",
      "0.99999976 -0.99998057\n"
     ]
    }
   ],
   "source": [
    "t_max = 1000\n",
    "num_episodes = 50\n",
    "\n",
    "min_reward_cutoff = -1000\n",
    "min_reward_set = -0.5\n",
    "\n",
    "reward_increase = 0\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    all_observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    o, a, r = random_observation_sequence(env, t_max, epsilon=0.2)\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "\n",
    "    for i in range(len(a)):\n",
    "\n",
    "        prev_state = o[i]\n",
    "        state = o[i+1]\n",
    "        action = a[i]\n",
    "        reward = r[i] + reward_increase\n",
    "\n",
    "        # if reward < 0:\n",
    "        #     print(\"yes\")\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        # ddpg.buffer.record((prev_state, action, reward, state))\n",
    "        # # episodic_reward += reward\n",
    "        #\n",
    "        # ddpg.buffer.learn()\n",
    "\n",
    "        ddpg.record((prev_state, action, reward, state))\n",
    "        # episodic_reward += reward\n",
    "\n",
    "        ddpg.train()\n",
    "\n",
    "        # ddpg.learn()\n",
    "        #\n",
    "        # ddpg.update_actor_target()\n",
    "        # ddpg.update_critic_target()\n",
    "\n",
    "        # buffer.record((prev_state, action, reward, state))\n",
    "        # # episodic_reward += reward\n",
    "        # #\n",
    "        # buffer.learn()\n",
    "        # update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        # update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "    print(total_reward)\n",
    "\n",
    "    acts = ddpg.actor_model((np.random.random(size=(10, 2))*2 - 1))\n",
    "    print(np.max(acts), np.min(acts))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\narray([[5626.16  ],\n       [1278.3018],\n       [2108.9463],\n       [1838.2856],\n       [1254.0297],\n       [1424.8224],\n       [1419.4712],\n       [1090.5853],\n       [4454.496 ],\n       [8095.8843]], dtype=float32)>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg.critic_model([(np.random.random(size=(10, 2))*2 - 1), (np.random.random(size=(10, 1))*2 - 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\narray([[-0.9993459 ],\n       [-0.99995506],\n       [ 0.99999976],\n       [ 0.99999976],\n       [ 0.99999976],\n       [ 0.99999976],\n       [-0.9428255 ],\n       [ 0.99999976],\n       [ 1.        ],\n       [-0.99981874]], dtype=float32)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg.actor_model((np.random.random(size=(10, 2))*2 - 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x16b2c01c0>,\n <matplotlib.lines.Line2D at 0x16b2c0280>]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "actions_pred = ddpg.actor_model(obs_pos)\n",
    "\n",
    "actions_pred\n",
    "\n",
    "\n",
    "plt.plot(obs_pos, actions_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.9999821 ]\n",
      " [-0.99998087]\n",
      " [-0.9999791 ]\n",
      " [-0.99997747]\n",
      " [-0.9999757 ]\n",
      " [-0.9999737 ]\n",
      " [-0.9999715 ]\n",
      " [-0.99996924]\n",
      " [-0.99996674]\n",
      " [-0.9999642 ]\n",
      " [-0.9999611 ]\n",
      " [-0.99995804]\n",
      " [-0.9999546 ]\n",
      " [-0.99995077]\n",
      " [-0.99994683]\n",
      " [-0.99994254]\n",
      " [-0.9999379 ]\n",
      " [-0.99993277]\n",
      " [-0.9999274 ]\n",
      " [-0.99992144]\n",
      " [-0.9999152 ]\n",
      " [-0.99990815]\n",
      " [-0.99990064]\n",
      " [-0.9998926 ]\n",
      " [-0.99988383]\n",
      " [-0.99987435]\n",
      " [-0.9998642 ]\n",
      " [-0.99985343]\n",
      " [-0.99984163]\n",
      " [-0.9998289 ]\n",
      " [-0.999815  ]\n",
      " [-0.9997995 ]\n",
      " [-0.99978274]\n",
      " [-0.99976444]\n",
      " [-0.9997448 ]\n",
      " [-0.999724  ]\n",
      " [-0.9997021 ]\n",
      " [-0.99967486]\n",
      " [-0.9996449 ]\n",
      " [-0.99961585]\n",
      " [-0.99958503]\n",
      " [-0.9995521 ]\n",
      " [-0.9995174 ]\n",
      " [-0.99947995]\n",
      " [-0.99944746]\n",
      " [-0.99941444]\n",
      " [-0.9993793 ]\n",
      " [-0.9993373 ]\n",
      " [-0.99929243]\n",
      " [-0.99924445]\n",
      " [-0.99919343]\n",
      " [-0.9991352 ]\n",
      " [-0.9990668 ]\n",
      " [-0.99896604]\n",
      " [-0.9988227 ]\n",
      " [-0.9986512 ]\n",
      " [-0.99841416]\n",
      " [-0.9979071 ]\n",
      " [-0.9787494 ]\n",
      " [-0.40187365]\n",
      " [ 0.88991016]\n",
      " [ 0.9971087 ]\n",
      " [ 0.9999282 ]\n",
      " [ 0.99999833]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]\n",
      " [ 0.99999976]], shape=(100, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x16b3bb670>,\n <matplotlib.lines.Line2D at 0x16b3bb790>]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "actions_pred = ddpg.actor_model(vel_pos)\n",
    "print(actions_pred)\n",
    "\n",
    "plt.plot(obs_pos, actions_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Can it solve the environment?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "success\n",
      "93.4595678463096\n",
      "67\n",
      "success\n",
      "93.45142535320213\n",
      "66\n",
      "success\n",
      "93.52785287271396\n",
      "68\n",
      "success\n",
      "93.39642693701599\n",
      "68\n",
      "success\n",
      "93.402857427759\n",
      "66\n",
      "success\n",
      "93.5422590178614\n",
      "67\n",
      "success\n",
      "93.44712708842327\n",
      "67\n",
      "success\n",
      "93.45972519340326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# obs_stddev = [0.05, 0.05]\n",
    "obs_stddev = [0, 0]\n",
    "\n",
    "\n",
    "t_max = 1000\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    done = False\n",
    "    rewards = []\n",
    "\n",
    "    t = 0\n",
    "    while not done:\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        obs = obs.reshape(1, obs.shape[0])\n",
    "        obs = transform_observations(obs, observation_max, observation_min, obs_stddev)\n",
    "\n",
    "        # print(obs)\n",
    "\n",
    "        # action = act_net(obs) * 10\n",
    "        # action = np.clip(action.numpy(), -1, 1)\n",
    "\n",
    "        action = ddpg.actor_model(obs)\n",
    "        action = action.numpy()\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # print(obs)\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "        t += 1\n",
    "\n",
    "        if t == t_max:\n",
    "            done = True\n",
    "\n",
    "    print(t)\n",
    "    if t < t_max:\n",
    "        print(\"success\")\n",
    "    else:\n",
    "        print(\"Failure\")\n",
    "        print(\"max obs\", obs)\n",
    "\n",
    "    print(np.sum(rewards))\n",
    "    # print(rewards)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m both \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(both)\n\u001B[1;32m      4\u001B[0m both\n\u001B[0;32m----> 6\u001B[0m both_acts \u001B[38;5;241m=\u001B[39m \u001B[43mact_net\u001B[49m(both)\n\u001B[1;32m      8\u001B[0m both_acts\n",
      "\u001B[0;31mNameError\u001B[0m: name 'act_net' is not defined"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "both = [[i/n, j/n] for i in range(-1*n, n) for j in range(-1*n, n)]\n",
    "both = np.array(both)\n",
    "both\n",
    "\n",
    "both_acts = act_net(both)\n",
    "\n",
    "both_acts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = 50\n",
    "coords = [[i/n, j/n] for i in range(-1*n, n) for j in range(-1*n, n)]\n",
    "coords = np.array(coords)\n",
    "coords\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5.1, 0.5)\n",
    "y = np.arange(-5, 5.1, 0.5)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}