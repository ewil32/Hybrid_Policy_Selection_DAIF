{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test the prior model to see how well it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# from prior_model import PriorModelBellman"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PriorModelBellman(keras.Model):\n",
    "\n",
    "\n",
    "    def __init__(self, observation_dim, iterate_train=1, discount_factor=0.99):\n",
    "        super(PriorModelBellman, self).__init__()\n",
    "        self.observation_dim = observation_dim\n",
    "        self.iterate_train = iterate_train\n",
    "        self.discount_factor = discount_factor\n",
    "        self.train_epochs = 1\n",
    "\n",
    "        # make the model\n",
    "        transition_inputs = layers.Input(observation_dim)\n",
    "        h = layers.Dense(observation_dim * 20, activation=\"silu\")(transition_inputs)\n",
    "        h = layers.Dense(observation_dim, activation=\"tanh\")(h)\n",
    "\n",
    "        self.prior_model = keras.Model(transition_inputs, h, name=\"prior_model\")\n",
    "        self.prior_model.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.prior_model(inputs)\n",
    "\n",
    "    def extrinsic_kl(self, y):\n",
    "        return 1.0 - self.forward(y) # map from [-1, 1] to [2, 0]\n",
    "\n",
    "\n",
    "    def train(self, observations, rewards):\n",
    "        \"\"\"\n",
    "\n",
    "        :param observations: o_0, o_1, ... , o_n\n",
    "        :param rewards: list with r_0, r_1, ... , r_n\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        num_observations = len(observations)\n",
    "\n",
    "        # expand rewards to have the same dimension as observation dimension and transpose to give [num_observations, observation_dimension\n",
    "        rewards_stacked = np.stack([rewards]*self.observation_dim).T\n",
    "\n",
    "        for i in range(self.iterate_train):\n",
    "\n",
    "            # reducing discount factors through time\n",
    "            discount_factors = np.power([self.discount_factor]*num_observations, np.arange(num_observations)).reshape(observations.shape[0], 1)\n",
    "            discount_factors = np.flip(discount_factors)\n",
    "\n",
    "            # print(discount_factors)\n",
    "\n",
    "            # TODO Still seems a little strange that we add 0 to the end and discount the way we do but I think it makes sense. Check what predicted utilities are in practice\n",
    "            utility_t = self.prior_model(observations)\n",
    "            utility_t_plus_one = tf.concat([utility_t[1:], tf.zeros((1, self.observation_dim), dtype=utility_t.dtype)], axis=0)\n",
    "\n",
    "            # print(predicted_utility, pred_next_v)\n",
    "\n",
    "            expected_utility = rewards_stacked + discount_factors * utility_t_plus_one\n",
    "\n",
    "            # print(rewards_stacked)\n",
    "            # print(discount_factors * utility_t_plus_one)\n",
    "\n",
    "            self.prior_model.fit(observations, expected_utility, epochs=self.train_epochs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.flip(np.arange(10))\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations\n",
    "import gym"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "p = PriorModelBellman(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.05975893, -0.04120084, -0.04120084, -0.01470336, -0.01470336,\n        -0.01092644, -0.01165737, -0.01165737, -0.01165737, -0.01165737]])"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "num_seqs = 1\n",
    "seq_length = 10\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "rewards = []\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length)\n",
    "\n",
    "    train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    rewards.append(r)\n",
    "\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "ob_seqs = np.array(ob_seqs)\n",
    "next_obs = np.array(next_obs)\n",
    "rewards = np.array(rewards)\n",
    "ob_seqs.shape\n",
    "\n",
    "ob_seqs_stddev = np.ones_like(ob_seqs)\n",
    "next_obs_stddev = np.ones_like(next_obs)\n",
    "\n",
    "next_obs\n",
    "rewards"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "p.train(ob_seqs[0], rewards)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-5.53517997e-01,  0.00000000e+00],\n       [-5.53142369e-01,  3.75652395e-04],\n       [-5.51775575e-01,  1.36682065e-03],\n       [-5.51400542e-01,  3.75043048e-04],\n       [-5.51913500e-01, -5.12973871e-04],\n       [-5.50954700e-01,  9.58819001e-04],\n       [-5.49935400e-01,  1.01928855e-03],\n       [-5.48296630e-01,  1.63875264e-03],\n       [-5.46126664e-01,  2.16995459e-03],\n       [-5.42930782e-01,  3.19589116e-03],\n       [-5.40861130e-01,  2.06966931e-03],\n       [-5.38667977e-01,  2.19315826e-03]])"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob_seqs[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.00000000e+00, -0.00000000e+00],\n       [-2.63429839e-02, -2.63429839e-02],\n       [-1.28609152e-01, -1.28609152e-01],\n       [-1.60246455e-01, -1.60246455e-01],\n       [-2.82100130e-01, -2.82100130e-01],\n       [-4.63810342e-03, -4.63810342e-03],\n       [-4.75216788e-02, -4.75216788e-02],\n       [-3.72745830e-02, -3.72745830e-02],\n       [-2.61202061e-01, -2.61202061e-01],\n       [-6.46309192e-01, -6.46309192e-01],\n       [-1.55811912e-05, -1.55811912e-05],\n       [-2.80436117e-01, -2.80436117e-01]])"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([rewards[0]]*2).T * np.arange(12).reshape(12, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "pm = PriorModelBellman(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 114ms/step - loss: 4.1720e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 19:21:28.791784: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "pm.train(ob_seqs[0], rewards[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}