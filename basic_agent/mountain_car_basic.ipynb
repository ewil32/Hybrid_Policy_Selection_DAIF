{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run The Agent on Mountain Car"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from vae import VAE, create_decoder, create_encoder\n",
    "from transition_basic import TransitionModel\n",
    "from basic_agent import DAIFAgent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def run_episode(mcc_env, agent, observation_noise_stddev=[0.1, 0.05], episode_length=1000):\n",
    "\n",
    "    # first environment observation\n",
    "    policy_observation, info = mcc_env.reset()\n",
    "\n",
    "    policy_observation = np.array([policy_observation, 0])\n",
    "\n",
    "    # apply noise to and scaling to first observation\n",
    "    policy_observation_noisy = transform_observations(policy_observation, observation_noise_stddev)\n",
    "\n",
    "    # clear the sequences\n",
    "    observation_sequence = []\n",
    "    reward_sequence = []\n",
    "\n",
    "    # loop until completion\n",
    "    t = 0\n",
    "    while True:\n",
    "\n",
    "        # agent selects policy\n",
    "        policy_mean, policy_stddev = agent.cem_policy_optimisation(policy_observation_noisy)\n",
    "\n",
    "        # execute the first two actions of the policy for 6 time steps each\n",
    "        action0 = policy_mean[0]\n",
    "        action1 = policy_mean[1]\n",
    "\n",
    "        # sequence of actions to execute\n",
    "        action_sequence = [action0]*6 + [action1]*6\n",
    "\n",
    "        for action in action_sequence:\n",
    "\n",
    "            action_as_array = np.array([action])  # need to this to satisfy gym requirements\n",
    "            observation, reward, done, info = mcc_env.step(action_as_array)\n",
    "\n",
    "            observation_sequence.append(observation)\n",
    "            reward_sequence.append(reward)\n",
    "\n",
    "            t += 1\n",
    "            if done:\n",
    "                if t < episode_length - 1:\n",
    "                    return agent, True, t\n",
    "                else:\n",
    "                    return agent, False, t\n",
    "\n",
    "        # assemble the training data\n",
    "        observation_sequence = np.array(observation_sequence)\n",
    "        action_sequence = np.array(action_sequence).reshape(12, 1)  # reshape to concatenate when training\n",
    "\n",
    "        observation_sequence_noisy = transform_observations(observation_sequence, observation_noise_stddev)  # OBS_1:t+1\n",
    "\n",
    "        # add the noisy observation from last time to the start of the observation sequence\n",
    "        observation_sequence_noisy = np.vstack([policy_observation_noisy, observation_sequence_noisy])\n",
    "\n",
    "        pre_observation_sequence_noisy = observation_sequence_noisy[:-1]  # OBS_0:t\n",
    "        post_observation_sequence_noisy = observation_sequence_noisy[1:]  # OBS_1:t+1\n",
    "\n",
    "        # train perception\n",
    "        agent.train_vae(observation_sequence_noisy)\n",
    "\n",
    "        # train transition\n",
    "        agent.train_transition(pre_observation_sequence_noisy, post_observation_sequence_noisy, action_sequence)\n",
    "\n",
    "        # get the next policy_observation_noisy which is the last item in the post observation list\n",
    "        policy_observation_noisy = post_observation_sequence_noisy[-1]\n",
    "\n",
    "        # clear all the observation sequences then return to the start of the loop to use policy_observation_noisy to start a new policy\n",
    "        observation_sequence = []\n",
    "        reward_sequence = []\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def train_agent(mcc_env, agent, num_episodes=100, observation_noise_stddev=[0.1, 0.05], episode_length=1000):\n",
    "\n",
    "    for n in range(num_episodes):\n",
    "        print(\"Episode\", n+1)\n",
    "        agent, success, t = run_episode(mcc_env, agent, observation_noise_stddev=[0.1, 0.05], episode_length=1000)\n",
    "\n",
    "        if success:\n",
    "            print(\"Success in episode\", n+1, \"at time step\", t)\n",
    "        else:\n",
    "            print(\"No Success\")\n",
    "\n",
    "    return agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vae import VAE\n",
    "\n",
    "\n",
    "class DAIFAgent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 enc,\n",
    "                 dec,\n",
    "                 tran,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70):\n",
    "\n",
    "        super(DAIFAgent, self).__init__()\n",
    "\n",
    "        self.prior_model = prior_model\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        # encoder\n",
    "        self.enc = enc\n",
    "\n",
    "        # decoder\n",
    "        # takes latent state and outputs observation\n",
    "        self.dec = dec\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = VAE(enc, dec)\n",
    "        self.model_vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        # transition\n",
    "        # takes action plus last state and outputs next latent state\n",
    "        self.tran = tran\n",
    "        self.tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "\n",
    "    def select_action(self, observation):\n",
    "\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(observation)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    def train_vae(self, observation, verbose=0):\n",
    "        self.model_vae.fit(observation, verbose=verbose)\n",
    "\n",
    "\n",
    "    def train_transition(self, o_t_minus_one, o_t, action_t_minus_one, verbose=0):\n",
    "\n",
    "        # find the latent reps with the decoder\n",
    "        z_t_minus_1_mean, z_t_minus_1_stddev, z_t_minus = self.enc(o_t_minus_one)\n",
    "        z_t_mean, z_t_stddev, z_t = self.enc(o_t)\n",
    "\n",
    "        # concatenate action and observation for input into transition\n",
    "        z_train = np.concatenate([np.array(z_t_minus_1_mean), np.array(action_t_minus_one)], axis=1)\n",
    "\n",
    "        # train the transition model\n",
    "        self.tran.fit(z_train, (z_t_mean, z_t_stddev), epochs=1, verbose=verbose)\n",
    "\n",
    "\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project into the future and calculate FEEF\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by one to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape [self.n_policies, len(z_t_minus_one)]\n",
    "        prev_latent_mean = np.stack([z_t_minus_one]*self.n_policies)\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            tran_input = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            next_latent_mean, next_latent_sd = self.tran(tran_input)  # shape = [num policies, latent dim\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.dec(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.enc(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "            likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "            if self.prior_model is None:\n",
    "\n",
    "                # TODO how exactly is the prior defined\n",
    "                # create the prior distribution\n",
    "                prior_preferences = tf.convert_to_tensor(np.stack([[0.5, 100]]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences, scale_diag=np.ones_like(prior_preferences))\n",
    "\n",
    "            # TODO Fix the learned prior model\n",
    "            else:\n",
    "                prior_dist = self.prior_model()\n",
    "\n",
    "\n",
    "            kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "            predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "            kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    def EFE(self, policy_posteriors, predicted_likelihood, predicted_posterior):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 10:44:10.356569: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-07 10:44:10.802549: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Success\n",
      "Episode 2\n",
      "No Success\n",
      "Episode 3\n",
      "No Success\n",
      "Episode 4\n",
      "Success in episode 4 at time step 716\n",
      "Episode 5\n",
      "Success in episode 5 at time step 766\n",
      "Episode 6\n",
      "Success in episode 6 at time step 431\n",
      "Episode 7\n",
      "No Success\n",
      "Episode 8\n",
      "Success in episode 8 at time step 902\n",
      "Episode 9\n",
      "Success in episode 9 at time step 343\n",
      "Episode 10\n",
      "Success in episode 10 at time step 406\n",
      "Episode 11\n",
      "No Success\n",
      "Episode 12\n",
      "No Success\n",
      "Episode 13\n",
      "No Success\n",
      "Episode 14\n",
      "Success in episode 14 at time step 828\n",
      "Episode 15\n",
      "Success in episode 15 at time step 735\n",
      "Episode 16\n",
      "No Success\n",
      "Episode 17\n",
      "No Success\n",
      "Episode 18\n",
      "Success in episode 18 at time step 331\n",
      "Episode 19\n",
      "Success in episode 19 at time step 456\n",
      "Episode 20\n",
      "Success in episode 20 at time step 620\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.DAIFAgent at 0x2f88c4c10>"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "tran = TransitionModel(2, 1)\n",
    "\n",
    "daifa = DAIFAgent(None, enc, dec, tran)\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "train_agent(env, daifa, num_episodes=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 06:46:30.583715: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-07 06:46:30.936236: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Success\n",
      "Episode 2\n",
      "No Success\n",
      "Episode 3\n",
      "No Success\n",
      "Episode 4\n",
      "Success in episode 4 at time step 753\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.DAIFAgent at 0x2b4140910>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "tran = TransitionModel(2, 1)\n",
    "\n",
    "daifa = DAIFAgent(None, enc, dec, tran)\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "train_agent(env, daifa)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 06:50:21.611683: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-07 06:50:21.986528: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Success\n",
      "Episode 2\n",
      "No Success\n",
      "Episode 3\n",
      "No Success\n",
      "Episode 4\n",
      "No Success\n",
      "Episode 5\n",
      "Success in episode 5 at time step 953\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.DAIFAgent at 0x2c63c9490>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "tran = TransitionModel(2, 1)\n",
    "\n",
    "daifa = DAIFAgent(None, enc, dec, tran)\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "train_agent(env, daifa)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 06:55:21.234757: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-07 06:55:21.584529: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Success\n",
      "Episode 2\n",
      "No Success\n",
      "Episode 3\n",
      "No Success\n",
      "Episode 4\n",
      "No Success\n",
      "Episode 5\n",
      "No Success\n",
      "Episode 6\n",
      "No Success\n",
      "Episode 7\n",
      "Success in episode 7 at time step 683\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.DAIFAgent at 0x2bdb494c0>"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "tran = TransitionModel(2, 1)\n",
    "\n",
    "daifa = DAIFAgent(None, enc, dec, tran)\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "train_agent(env, daifa)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 07:02:03.874260: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-07 07:02:04.264139: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Success\n",
      "Episode 2\n",
      "Success in episode 2 at time step 605\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.DAIFAgent at 0x29c9b6580>"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "tran = TransitionModel(2, 1)\n",
    "\n",
    "daifa = DAIFAgent(None, enc, dec, tran)\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "train_agent(env, daifa)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 07:05:31.535007: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-07 07:05:31.940020: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Success\n",
      "Episode 2\n",
      "No Success\n",
      "Episode 3\n",
      "No Success\n",
      "Episode 4\n",
      "Success in episode 4 at time step 411\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.DAIFAgent at 0x177ac2c40>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "tran = TransitionModel(2, 1)\n",
    "\n",
    "daifa = DAIFAgent(None, enc, dec, tran)\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "train_agent(env, daifa)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 07:03:41.624143: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-07 07:03:42.026746: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Success\n",
      "Episode 2\n",
      "Success in episode 2 at time step 813\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.DAIFAgent at 0x2c635d880>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "tran = TransitionModel(2, 1)\n",
    "\n",
    "daifa = DAIFAgent(None, enc, dec, tran)\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "train_agent(env, daifa)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}