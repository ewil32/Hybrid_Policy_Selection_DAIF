{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run The Agent on Mountain Car"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from vae_recurrent import VAE, create_decoder, create_encoder\n",
    "from transition_gru import TransitionGRU\n",
    "from recurrent_agent import DAIFAgentRecurrent\n",
    "from prior_model import PriorModelBellman\n",
    "from habitual_action_network import HabitualAction, compute_discounted_cumulative_reward\n",
    "from ddpg import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations\n",
    "from train_agent import train_single_agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from identity_vae import IdentityVAE, identity_encoder, identity_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with no prior model FEEF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "# from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 habitual_action_net,\n",
    "                 given_prior_mean=None,\n",
    "                 given_prior_stddev=None,\n",
    "                 agent_time_ratio=6,\n",
    "                 actions_to_execute_when_exploring=2,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True,\n",
    "                 train_prior_model=True,\n",
    "                 train_habit_net=True,\n",
    "                 train_with_replay=True,\n",
    "                 train_after_exploring=True,\n",
    "                 use_kl_extrinsic=True,\n",
    "                 use_kl_intrinsic=True,\n",
    "                 use_FEEF=True,\n",
    "                 use_fast_thinking=False):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        # parameters for slow policy planning\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        # flags for whether or not we are training models or using pretrained models and when we should train\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "        self.train_habit_net = train_habit_net\n",
    "        self.train_prior = train_prior_model\n",
    "        self.train_with_replay = train_with_replay\n",
    "        self.train_after_exploring = train_after_exploring\n",
    "\n",
    "        # do we use the kl divergence for extrinsic vs intrinsic\n",
    "        self.use_kl_intrinsic = use_kl_intrinsic\n",
    "        self.use_kl_extrinsic = use_kl_extrinsic\n",
    "\n",
    "        # do we use the FEEF or EFE?\n",
    "        self.use_FEEF = use_FEEF\n",
    "\n",
    "        # given prior values\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.tran = tran\n",
    "        self.prior_model = prior_model\n",
    "        self.habit_action_model = habitual_action_net\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "        self.actions_to_execute_when_exploring = actions_to_execute_when_exploring\n",
    "        self.time_step = 0\n",
    "        self.exploring = False\n",
    "\n",
    "        # track the hidden state of the transition gru model so we can use it to train\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "        # store the full observations for the episode so we can train using replay\n",
    "        self.full_observation_sequence = []\n",
    "        self.full_action_sequence = []\n",
    "        self.full_reward_sequence = []\n",
    "\n",
    "        # store the observations while the agent is in exploration mode\n",
    "        self.exploring_observation_sequence = []\n",
    "        self.exploring_action_sequence = []\n",
    "        self.exploring_reward_sequence = []\n",
    "\n",
    "        # store the observations at the world time scale\n",
    "        self.env_time_scale_observations = []\n",
    "\n",
    "        self.policy_left_to_execute = [None]\n",
    "        self.previous_observation = None\n",
    "        self.action_being_executed = None\n",
    "\n",
    "        self.use_fast_thinking = use_fast_thinking\n",
    "\n",
    "\n",
    "    def perceive_and_act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        The function called to have the agent interact with the environment\n",
    "        We assume the agent gets a transformed/noisy observation from the environment and then returns an action\n",
    "\n",
    "        TODO: possibly the agent returns some other information for logging and showing experiments\n",
    "\n",
    "        :param observation:\n",
    "        :param reward:\n",
    "        :param done:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # track the world time scale observation sequence\n",
    "        self.env_time_scale_observations.append(observation)\n",
    "\n",
    "        # if the episode is finished, then do any training on the full data set\n",
    "        if done:\n",
    "\n",
    "            if self.train_with_replay:\n",
    "                print(\"training on full data\")\n",
    "\n",
    "                # add the final observation and reward we observed to the sequences\n",
    "                self.full_observation_sequence.append(observation)\n",
    "                self.full_reward_sequence.append(reward)\n",
    "\n",
    "                # Call the training function on the observation sequences to train everything we need to train\n",
    "                self.train_models(np.vstack(self.full_observation_sequence),\n",
    "                                  np.vstack(self.full_action_sequence),\n",
    "                                  np.array(self.full_reward_sequence))\n",
    "\n",
    "\n",
    "        # Otherwise are we at a point where we can reconsider our policy and maybe train the world model\n",
    "        elif self.time_step % self.agent_time_ratio == 0:\n",
    "\n",
    "            # add the reward only if it's not the first observation\n",
    "            if self.time_step != 0:\n",
    "                self.full_reward_sequence.append(reward)\n",
    "\n",
    "            # add the observation to the sequence\n",
    "            self.full_observation_sequence.append(observation)\n",
    "\n",
    "            # We only update the model during the episode when we were exploring using the planning method and we have executed all of the actions in the policy\n",
    "            if self.exploring and len(self.policy_left_to_execute) == 0:\n",
    "\n",
    "                # print(\"f\", self.full_observation_sequence)\n",
    "                # print(\"e\", self.full_observation_sequence[-1*(self.actions_to_execute_when_exploring + 1):])\n",
    "\n",
    "                if self.train_after_exploring:\n",
    "                    # the actions done while exploring were the last self.actions_to_execute_when_exploring\n",
    "                    self.exploring_action_sequence = self.full_action_sequence[-1*self.actions_to_execute_when_exploring:]\n",
    "                    self.exploring_reward_sequence = self.full_reward_sequence[-1*self.actions_to_execute_when_exploring:]\n",
    "                    self.exploring_observation_sequence = self.full_observation_sequence[-1*(self.actions_to_execute_when_exploring + 1):]\n",
    "\n",
    "                    # Call the training function on the observation sequences to train everything we need to train\n",
    "                    self.train_models(np.vstack(self.exploring_observation_sequence),\n",
    "                                      np.vstack(self.exploring_action_sequence),\n",
    "                                      np.array(self.exploring_reward_sequence))\n",
    "\n",
    "                self.exploring = False\n",
    "\n",
    "\n",
    "            # Now we select our action. If we aren't exploring then either we act out of habit or we might need to explore\n",
    "            # I think I can check this based on whether or not there are actions left to execute in the current policy\n",
    "            if not self.exploring:\n",
    "\n",
    "                action_as_array = np.array(self.action_being_executed).reshape(1, self.tran.action_dim)\n",
    "                # pred_next_observation, next_tran_hidden_state = self.predict_next_observation(self.previous_observation, action_as_array)\n",
    "                pred_next_observation, next_tran_hidden_state = None, None\n",
    "\n",
    "                if self.previous_observation is None:\n",
    "                    # self.policy_left_to_execute = self.habit_action_model(observation)\n",
    "                    self.policy_left_to_execute = self.habit_action_model.actor_model(observation)\n",
    "                    self.policy_left_to_execute = self.policy_left_to_execute.numpy().tolist()  # tf tensor to list\n",
    "\n",
    "                # TDOD Fix this to work however it needs to\n",
    "                # we need to see what the generative model now thinks about what the expected current observation is\n",
    "                elif self.use_fast_thinking and np.allclose(observation, pred_next_observation, atol=self.model_vae.reconstruction_stddev):  # within some tolerance\n",
    "                    # self.policy_left_to_execute = self.habit_action_model(observation)\n",
    "                    self.policy_left_to_execute = self.habit_action_model.actor_model(observation)\n",
    "                    # self.policy_left_to_execute = self.policy_left_to_execute + np.random.normal(0, scale=self.habit_action_model.action_std_dev)\n",
    "                    self.policy_left_to_execute = self.policy_left_to_execute.numpy().tolist()\n",
    "\n",
    "                    self.tran_hidden_state = next_tran_hidden_state\n",
    "\n",
    "                    print(\"fast thinking\")\n",
    "\n",
    "                # the generative model is surprised so we should use the slow deliberation for planning out a policy that balances exploration and exploitation\n",
    "                else:\n",
    "                    # print(\"slow thinking\")\n",
    "                    policy = self.select_policy(observation)\n",
    "                    # print(policy.mean())\n",
    "                    policy = policy.mean().numpy()\n",
    "                    policy = policy.reshape(policy.shape[0], self.tran.action_dim).tolist()\n",
    "                    self.policy_left_to_execute = policy[0: self.actions_to_execute_when_exploring]\n",
    "\n",
    "                    self.exploring = True\n",
    "\n",
    "                # print(observation)\n",
    "                # print(pred_next_observation)\n",
    "\n",
    "            # finally update the previous observation and action to be the one we just had/did\n",
    "            self.previous_observation = observation\n",
    "            self.action_being_executed = self.policy_left_to_execute[0]\n",
    "            self.full_action_sequence.append(self.action_being_executed)\n",
    "            self.policy_left_to_execute.pop(0)\n",
    "\n",
    "        # final updates increment the current timestep and return the action specified by the policy\n",
    "        self.time_step += 1\n",
    "\n",
    "        return self.action_being_executed\n",
    "\n",
    "\n",
    "    def predict_next_observation(self, obs, action):\n",
    "\n",
    "        # TODO: Fix this with the transition hidden states\n",
    "        if obs is None:\n",
    "            return None, None\n",
    "        else:\n",
    "            # print(obs.shape)\n",
    "            z_mean, z_std, z = self.model_vae.encoder(obs)\n",
    "            # print(z_mean.shape)\n",
    "            # print(action.shape)\n",
    "            z_mean = z_mean.numpy()\n",
    "            z_plus_action = np.concatenate([z_mean, action], axis=1)\n",
    "            # print(z_mean)\n",
    "            # print(action)\n",
    "            # print(z_plus_action)\n",
    "\n",
    "            z_plus_action = z_plus_action.reshape(1, 1, z_plus_action.shape[1])\n",
    "            # print(z_plus_action)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((z_plus_action, self.tran_hidden_state))\n",
    "\n",
    "            next_observation = self.model_vae.decoder(next_latent_mean)\n",
    "            # print(next_observation)\n",
    "            return next_observation.numpy(), next_hidden_state\n",
    "\n",
    "\n",
    "    # We use this function to reset the hidden state of the transition model when we want to train on the full data set\n",
    "    def reset_tran_hidden_state(self):\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "\n",
    "    def reset_all_states(self):\n",
    "        self.time_step = 0\n",
    "        self.exploring = False\n",
    "\n",
    "        # track the hidden state of the transition gru model so we can use it to train\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "        # store the full observations for the episode so we can train using replay\n",
    "        self.complete_observation_sequence = []\n",
    "        self.full_observation_sequence = []\n",
    "        self.full_action_sequence = []\n",
    "        self.full_reward_sequence = []\n",
    "\n",
    "        # store the observations while the agent is in exploration mode\n",
    "        self.exploring_observation_sequence = []\n",
    "        self.exploring_action_sequence = []\n",
    "        self.exploring_reward_sequence = []\n",
    "\n",
    "        self.policy_left_to_execute = []\n",
    "        self.previous_observation = None\n",
    "        self.previous_action_executed = None\n",
    "\n",
    "\n",
    "    def train_models(self, observations_full, actions, rewards):\n",
    "\n",
    "        pre_observations = observations_full[:-1]\n",
    "        post_observations = observations_full[1:]\n",
    "\n",
    "        #### TRAIN THE TRANSITION MODEL ####\n",
    "        if self.train_tran:\n",
    "\n",
    "            num_observations = pre_observations.shape[0]\n",
    "            # observation_dim = pre_observations.shape[1]\n",
    "            action_dim = actions.shape[1]\n",
    "            latent_dim = self.model_vae.latent_dim\n",
    "\n",
    "            # find the actual observed latent states using the vae\n",
    "            pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "            post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "            # set up the input training data that we use to train the transition model\n",
    "            z_train = np.concatenate([np.array(pre_latent_mean), actions], axis=1)\n",
    "\n",
    "            # we use the sequence to find the right hidden states to use as input\n",
    "            z_train_seq = z_train.reshape((1, num_observations, latent_dim + action_dim))\n",
    "            z_train_singles = z_train.reshape(num_observations, 1, latent_dim + action_dim)\n",
    "\n",
    "            # the previous hidden state is the memory after observing some sequences but it might be None if we're just starting\n",
    "            if self.tran_hidden_state is None:\n",
    "                self.tran_hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.tran_hidden_state))\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.tran_hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran.train_epochs, verbose=self.tran.show_training, batch_size=z_train_singles.shape[0])\n",
    "\n",
    "            # now find the new predicted hidden state that we will use for finding the policy\n",
    "            # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "            _, _, final_hidden_state, _ = self.tran((z_train_seq, self.tran_hidden_state))\n",
    "            # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "            z_pred, _, _, _ = self.tran((z_train_singles, h_states_for_training))\n",
    "\n",
    "            self.tran_hidden_state = final_hidden_state\n",
    "\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            # self.model_vae.fit(pre_observations_raw, epochs=self.vae_train_epochs, verbose=self.show_vae_training)\n",
    "            self.model_vae.fit(pre_observations, epochs=self.model_vae.train_epochs, verbose=self.model_vae.show_training, batch_size=pre_observations.shape[0])\n",
    "\n",
    "\n",
    "        #### TRAIN THE PRIOR MODEL ####\n",
    "        # TODO fix how this part should work\n",
    "        if self.train_prior:\n",
    "            # self.prior_model.train(post_observations, rewards, verbose=self.show_prior_training)\n",
    "            if max(rewards) > 0:\n",
    "                self.prior_model.train(post_observations, rewards)\n",
    "\n",
    "\n",
    "        #### TRAIN THE HABIT ACTION NET ####\n",
    "        if self.train_habit_net:\n",
    "\n",
    "            # TODO: Be careful with how the reward compares to EFE in terms of sign. Is the habit net maximising or minimising?\n",
    "\n",
    "            # prior_preferences_mean = tf.convert_to_tensor(self.given_prior_mean, dtype=\"float32\")\n",
    "            # prior_preferences_stddev = tf.convert_to_tensor(self.given_prior_stddev, dtype=\"float32\")\n",
    "            #\n",
    "            # prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "            #\n",
    "            # external_efe = -1 * tf.math.log(prior_dist.prob(post_observations))\n",
    "            # external_efe = external_efe.numpy().reshape(external_efe.shape[0], 1)\n",
    "            #\n",
    "            # one_over_external_efe = 1/external_efe\n",
    "            #\n",
    "            # ten_minus_external_efe = -1*external_efe + 10\n",
    "\n",
    "            # ten_minus_external_efe = ten_minus_external_efe.numpy().reshape(ten_minus_external_efe.shape[0], 1)\n",
    "\n",
    "            # one_over_external_efe = one_over_external_efe.numpy().reshape(one_over_external_efe.shape[0], 1)\n",
    "            # print(one_over_external_efe.shape)\n",
    "\n",
    "            # print(post_observations)\n",
    "            # print(one_over_external_efe)\n",
    "\n",
    "            # obs_utilities = self.prior_model(pre_observations)\n",
    "            # obs_utilities = tf.reduce_sum(obs_utilities, axis=-1)\n",
    "            # obs_utilities = obs_utilities.numpy().reshape(obs_utilities.shape[0], 1)\n",
    "            # # print(obs_utilities)\n",
    "            #\n",
    "            # cum_rewards = compute_discounted_cumulative_reward(obs_utilities, self.habit_action_model.discount_factor)\n",
    "\n",
    "            # rewards = rewards.reshape(rewards.shape[0], 1)\n",
    "            #\n",
    "            # cum_rewards = compute_discounted_cumulative_reward(rewards, self.habit_action_model.discount_factor)\n",
    "\n",
    "            # print(cum_rewards)\n",
    "            # print(cum_rewards.sum())\n",
    "\n",
    "            # self.habit_action_model.fit(pre_observations, (actions, cum_rewards), epochs=self.habit_action_model.train_epochs, verbose=self.habit_action_model.show_training, batch_size=pre_observations.shape[0])\n",
    "\n",
    "            # DDPG stuff\n",
    "            self.habit_action_model.record_from_lists(pre_observations, actions, rewards, post_observations)\n",
    "            self.habit_action_model.train()\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "        \"\"\"\n",
    "        :param observation: needs to be [n, observation_dim] shape np array or tf tensor\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO do you take the mean or that latent here?\n",
    "        # get the latent state from this observation\n",
    "        _,  _, latent_state = self.model_vae.encoder(observation)\n",
    "        # latent_state = latent_state.numpy().reshape((1, latent_state.shape[0]))\n",
    "\n",
    "        # print(latent_state)\n",
    "        # select the policy\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(latent_state)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    # TODO Fix this so we can use different action dimensions\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "            # policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            # print(\"POLICIES\", policies)\n",
    "            # print(\"FEEFS\", FEEFs)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by -1 to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape (self.n_policies, latent_dim) when z_t_minus is tensor with shape (1, latent_dim\n",
    "        prev_latent_mean = tf.squeeze(tf.stack([z_t_minus_one]*self.n_policies, axis=1))\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.tran_hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.tran_hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # print(prev_latent_mean)\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            # print(tran_input)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        if self.use_FEEF:\n",
    "            return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "        else:\n",
    "            return self.EFE(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "                # Compute the extrinisc approximation with the prior model\n",
    "                else:\n",
    "                    kl_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    kl_extrinsic = tf.reduce_sum(kl_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                kl_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"Extrinsic\", kl_extrinsic)\n",
    "            # print(\"Intrinsic\", kl_intrinsic)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    # TODO Find out how this works with the log probability extrinsic term\n",
    "    def EFE(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        EFEs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack(self.given_prior_mean), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack(self.given_prior_stddev), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    # compute extrinsic prior preferences term\n",
    "                    efe_extrinsic = -1 * tf.math.log(prior_dist.prob(predicted_likelihood))\n",
    "\n",
    "                # TODO Can I use the learned prior model here?\n",
    "                else:\n",
    "                    efe_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    efe_extrinsic = tf.reduce_sum(efe_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                efe_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"EX\", efe_extrinsic)\n",
    "            # print(\"IN\", kl_intrinsic)\n",
    "\n",
    "            EFE = efe_extrinsic - kl_intrinsic\n",
    "\n",
    "            EFEs.append(EFE)\n",
    "\n",
    "        return EFEs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "pln_hrzn = 5\n",
    "latent_dim = 2\n",
    "obs_dim = 2\n",
    "\n",
    "# make the VAE\n",
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2,  [0, 0], [0.3, 0.3], train_epochs=1, show_training=True)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the TRANSITION\n",
    "tran = TransitionGRU(2, 1, 2*pln_hrzn*latent_dim, 2, train_epochs=1, show_training=True)\n",
    "tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the HABIT ACTION NET\n",
    "habit_net = HabitualAction(latent_dim, 1, [20, 20], train_epochs=2, show_training=True)\n",
    "habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "actor_model = get_actor(2, 1)\n",
    "critic_model = get_critic(2, 1)\n",
    "\n",
    "target_actor = get_actor(2, 1)\n",
    "target_critic = get_critic(2, 1)\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "\n",
    "habit_net = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau=0.005, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)\n",
    "\n",
    "# make the PRIOR NET\n",
    "prior_model = PriorModelBellman(2, show_training=True)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "# prior_stddev = [0.1, 0.1]\n",
    "# prior_stddev = [3, 3]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\n",
    "# daifa = DAIFAgentRecurrent(None,\n",
    "#                            vae,\n",
    "#                            tran,\n",
    "#                            habit_net,\n",
    "#                            scaled_prior_mean,\n",
    "#                            prior_stddev,\n",
    "#                            planning_horizon=pln_hrzn,\n",
    "#                            use_kl_extrinsic=True,\n",
    "#                            use_kl_intrinsic=True,\n",
    "#                            use_FEEF=False,\n",
    "#                            train_habit_net=True,\n",
    "#                            train_prior_model=False,\n",
    "#                            train_with_replay=True,\n",
    "#                            train_after_exploring=True,\n",
    "#                            use_fast_thinking=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.83333333, 0.        ])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           habit_net,\n",
    "                           planning_horizon=pln_hrzn,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           train_habit_net=False,\n",
    "                           train_prior_model=True,\n",
    "                           train_tran=True,\n",
    "                           train_after_exploring=True,\n",
    "                           train_with_replay=True,\n",
    "                           use_fast_thinking=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "daifa.train_vae = False\n",
    "daifa.model_vae.show_training = False\n",
    "\n",
    "daifa.train_tran = False\n",
    "daifa.tran.show_training = False\n",
    "daifa.train_prior = False\n",
    "daifa.prior_model.show_training = False\n",
    "# # # #\n",
    "# daifa.habit_action_model.show_training = True\n",
    "# daifa.train_habit_net = True\n",
    "# # # #\n",
    "# # daifa.use_fast_thinking = True\n",
    "# # daifa.habit_action_model.action_stddev = 0\n",
    "# # #\n",
    "# # daifa.model_vae.reconstruction_stddev = 0.1\n",
    "#\n",
    "# actor_model = get_actor(2, 1)\n",
    "# critic_model = get_critic(2, 1)\n",
    "#\n",
    "# target_actor = get_actor(2, 1)\n",
    "# target_critic = get_critic(2, 1)\n",
    "#\n",
    "# # Making the weights equal initially\n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "#\n",
    "# critic_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "# actor_optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "#\n",
    "# habit_net = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau=0.005, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)\n",
    "#\n",
    "# daifa.habit_action_model = habit_net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/core.py:57: DeprecationWarning: \u001B[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.4127254  0.       ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [23]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# train the agent on the env\u001B[39;00m\n\u001B[1;32m      2\u001B[0m env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMountainCarContinuous-v0\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m daifa, results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_single_agent\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdaifa\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservation_max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservation_min\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservation_noise_stddev\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrender_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Masters/Thesis/DAIF_Agents/train_agent.py:42\u001B[0m, in \u001B[0;36mtrain_single_agent\u001B[0;34m(mcc_env, agent, obs_max, obs_min, observation_noise_stddev, num_episodes, render_env)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m render_env:\n\u001B[0;32m---> 42\u001B[0m         \u001B[43mmcc_env\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     action \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mperceive_and_act(observation_noisy, reward\u001B[38;5;241m=\u001B[39mreward, done\u001B[38;5;241m=\u001B[39mdone)\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;66;03m# print(action)\u001B[39;00m\n",
      "File \u001B[0;32m~/python_repos/gym/gym/core.py:66\u001B[0m, in \u001B[0;36m_EnvDecorator._deprecate_mode.<locals>.render\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrender_mode\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspec\u001B[38;5;241m.\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mkeys():  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     57\u001B[0m     deprecation(\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are calling render method, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut you didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt specified the argument render_mode at environment initialization. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee here for more information: https://www.gymlibrary.ml/content/api/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m     )\n\u001B[0;32m---> 66\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrender_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/python_repos/gym/gym/core.py:434\u001B[0m, in \u001B[0;36mWrapper.render\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\n\u001B[1;32m    431\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    432\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 434\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/python_repos/gym/gym/core.py:66\u001B[0m, in \u001B[0;36m_EnvDecorator._deprecate_mode.<locals>.render\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrender_mode\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspec\u001B[38;5;241m.\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mkeys():  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     57\u001B[0m     deprecation(\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are calling render method, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut you didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt specified the argument render_mode at environment initialization. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee here for more information: https://www.gymlibrary.ml/content/api/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m     )\n\u001B[0;32m---> 66\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrender_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/python_repos/gym/gym/wrappers/order_enforcing.py:51\u001B[0m, in \u001B[0;36mOrderEnforcing.render\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_disable_render_order_enforcing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\n\u001B[1;32m     48\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     49\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     50\u001B[0m     )\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/python_repos/gym/gym/core.py:66\u001B[0m, in \u001B[0;36m_EnvDecorator._deprecate_mode.<locals>.render\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrender_mode\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspec\u001B[38;5;241m.\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mkeys():  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     57\u001B[0m     deprecation(\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are calling render method, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut you didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt specified the argument render_mode at environment initialization. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee here for more information: https://www.gymlibrary.ml/content/api/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m     )\n\u001B[0;32m---> 66\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrender_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/python_repos/gym/gym/core.py:434\u001B[0m, in \u001B[0;36mWrapper.render\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\n\u001B[1;32m    431\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    432\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 434\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/python_repos/gym/gym/core.py:66\u001B[0m, in \u001B[0;36m_EnvDecorator._deprecate_mode.<locals>.render\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrender_mode\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspec\u001B[38;5;241m.\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mkeys():  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     57\u001B[0m     deprecation(\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are calling render method, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut you didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt specified the argument render_mode at environment initialization. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee here for more information: https://www.gymlibrary.ml/content/api/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m     )\n\u001B[0;32m---> 66\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrender_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/python_repos/gym/gym/wrappers/env_checker.py:55\u001B[0m, in \u001B[0;36mPassiveEnvChecker.render\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_render_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/python_repos/gym/gym/core.py:66\u001B[0m, in \u001B[0;36m_EnvDecorator._deprecate_mode.<locals>.render\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrender_mode\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspec\u001B[38;5;241m.\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mkeys():  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     57\u001B[0m     deprecation(\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are calling render method, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut you didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt specified the argument render_mode at environment initialization. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee here for more information: https://www.gymlibrary.ml/content/api/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m     )\n\u001B[0;32m---> 66\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrender_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/python_repos/gym/gym/envs/classic_control/continuous_mountain_car.py:203\u001B[0m, in \u001B[0;36mContinuous_MountainCarEnv.render\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrenderer\u001B[38;5;241m.\u001B[39mget_renders()\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_render\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/python_repos/gym/gym/envs/classic_control/continuous_mountain_car.py:295\u001B[0m, in \u001B[0;36mContinuous_MountainCarEnv._render\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m    293\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclock\u001B[38;5;241m.\u001B[39mtick(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrender_fps\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m--> 295\u001B[0m     \u001B[43mpygame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplay\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msingle_rgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m}:\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mtranspose(\n\u001B[1;32m    299\u001B[0m         np\u001B[38;5;241m.\u001B[39marray(pygame\u001B[38;5;241m.\u001B[39msurfarray\u001B[38;5;241m.\u001B[39mpixels3d(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscreen)), axes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    300\u001B[0m     )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=10, render_env=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.1687512  0.61769015]\n",
      " [0.18971458 0.6298453 ]\n",
      " [0.21053267 0.6416735 ]\n",
      " [0.23118788 0.6531788 ]\n",
      " [0.2516626  0.66436523]\n",
      " [0.27194023 0.6752374 ]\n",
      " [0.29200488 0.68580025]\n",
      " [0.3118409  0.6960588 ]\n",
      " [0.33143353 0.7060182 ]\n",
      " [0.3507692  0.7156841 ]\n",
      " [0.36983448 0.725062  ]\n",
      " [0.38861722 0.7341577 ]\n",
      " [0.4071059  0.74297696]\n",
      " [0.42529    0.75152594]\n",
      " [0.44315976 0.7598104 ]\n",
      " [0.46070644 0.76783663]\n",
      " [0.47792196 0.7756107 ]\n",
      " [0.49479932 0.78313875]\n",
      " [0.51133245 0.7904268 ]\n",
      " [0.5275158  0.79748106]\n",
      " [0.5433451  0.8043077 ]\n",
      " [0.5588167  0.81091255]\n",
      " [0.57392764 0.8173017 ]\n",
      " [0.5886761  0.8234811 ]\n",
      " [0.60306054 0.82945675]\n",
      " [0.61708075 0.8352344 ]\n",
      " [0.6307366  0.84081966]\n",
      " [0.64402896 0.8462182 ]\n",
      " [0.65695953 0.8514357 ]\n",
      " [0.66953003 0.8564775 ]\n",
      " [0.6817434  0.86134887]\n",
      " [0.69360244 0.86605513]\n",
      " [0.705111   0.8706014 ]\n",
      " [0.7162732  0.8749926 ]\n",
      " [0.7270934  0.8792339 ]\n",
      " [0.7375764  0.88332975]\n",
      " [0.7477277  0.88728493]\n",
      " [0.7575526  0.89110404]\n",
      " [0.76705694 0.8947916 ]\n",
      " [0.7762469  0.8983519 ]\n",
      " [0.78512865 0.90178895]\n",
      " [0.79370856 0.90510714]\n",
      " [0.8019933  0.9083101 ]\n",
      " [0.80998975 0.9114019 ]\n",
      " [0.81770456 0.91438645]\n",
      " [0.8251446  0.91726714]\n",
      " [0.832317   0.9200475 ]\n",
      " [0.83922875 0.9227311 ]\n",
      " [0.84588677 0.92532134]\n",
      " [0.85229814 0.92782116]\n",
      " [0.85846984 0.93023396]\n",
      " [0.864409   0.93256253]\n",
      " [0.8701222  0.93481004]\n",
      " [0.87561655 0.936979  ]\n",
      " [0.8808987  0.93907255]\n",
      " [0.8859753  0.9410929 ]\n",
      " [0.89085305 0.94304305]\n",
      " [0.89553845 0.9449252 ]\n",
      " [0.9000378  0.94674176]\n",
      " [0.9043574  0.9484952 ]\n",
      " [0.90850335 0.9501876 ]\n",
      " [0.91248167 0.9518212 ]\n",
      " [0.91629815 0.9533981 ]\n",
      " [0.91995865 0.9549203 ]\n",
      " [0.92346877 0.95638955]\n",
      " [0.92683387 0.9578081 ]\n",
      " [0.9300593  0.9591774 ]\n",
      " [0.93315023 0.9604994 ]\n",
      " [0.9361116  0.9617758 ]\n",
      " [0.9389485  0.96300805]\n",
      " [0.9416656  0.96419793]\n",
      " [0.9442674  0.9653469 ]\n",
      " [0.9467584  0.96645635]\n",
      " [0.949143   0.9675277 ]\n",
      " [0.9514252  0.9685624 ]\n",
      " [0.9536092  0.96956164]\n",
      " [0.95569897 0.97052675]\n",
      " [0.9576981  0.97145885]\n",
      " [0.9596103  0.9723593 ]\n",
      " [0.96143913 0.9732291 ]\n",
      " [0.96318805 0.9740695 ]\n",
      " [0.96486026 0.97488123]\n",
      " [0.9664589  0.97566557]\n",
      " [0.9679871  0.9764233 ]\n",
      " [0.9694477  0.9771557 ]\n",
      " [0.9708437  0.97786325]\n",
      " [0.97217757 0.978547  ]\n",
      " [0.9734521  0.97920793]\n",
      " [0.97466975 0.9798467 ]\n",
      " [0.9758329  0.9804641 ]\n",
      " [0.976944   0.981061  ]\n",
      " [0.9780052  0.98163795]\n",
      " [0.9790186  0.98219585]\n",
      " [0.97998637 0.98273516]\n",
      " [0.98091036 0.9832568 ]\n",
      " [0.9817926  0.98376095]\n",
      " [0.9826349  0.98424876]\n",
      " [0.98343897 0.98472035]\n",
      " [0.98420656 0.9851766 ]\n",
      " [0.984939   0.98561776]], shape=(100, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x2fe27d370>,\n <matplotlib.lines.Line2D at 0x2fe27d100>]"
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "utils = daifa.prior_model(obs_pos)\n",
    "print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.57241553 0.86080855]\n",
      " [0.58096063 0.86276644]\n",
      " [0.5893747  0.8646942 ]\n",
      " [0.5976582  0.8665923 ]\n",
      " [0.6058113  0.8684612 ]\n",
      " [0.6138345  0.87030137]\n",
      " [0.6217284  0.8721133 ]\n",
      " [0.6294936  0.8738973 ]\n",
      " [0.6371306  0.875654  ]\n",
      " [0.6446403  0.8773835 ]\n",
      " [0.6520234  0.8790865 ]\n",
      " [0.6592808  0.8807633 ]\n",
      " [0.66641325 0.8824142 ]\n",
      " [0.67342174 0.8840397 ]\n",
      " [0.6803074  0.88564044]\n",
      " [0.6870712  0.8872163 ]\n",
      " [0.69371414 0.8887681 ]\n",
      " [0.70023733 0.89029586]\n",
      " [0.7066421  0.89180034]\n",
      " [0.7129294  0.89328164]\n",
      " [0.7191006  0.8947401 ]\n",
      " [0.725157   0.8961761 ]\n",
      " [0.73109984 0.8975903 ]\n",
      " [0.73693043 0.89898247]\n",
      " [0.74265003 0.90035343]\n",
      " [0.74826    0.90170336]\n",
      " [0.75376177 0.9030326 ]\n",
      " [0.7591567  0.90434146]\n",
      " [0.76444644 0.90563023]\n",
      " [0.7696319  0.9068992 ]\n",
      " [0.7747149  0.90814877]\n",
      " [0.77969676 0.9093792 ]\n",
      " [0.784579   0.9105908 ]\n",
      " [0.789363   0.91178393]\n",
      " [0.7940502  0.9129587 ]\n",
      " [0.7986421  0.91411567]\n",
      " [0.80314016 0.9152547 ]\n",
      " [0.80754584 0.91637653]\n",
      " [0.8118606  0.91748124]\n",
      " [0.8160858  0.918569  ]\n",
      " [0.82022315 0.9196403 ]\n",
      " [0.8242739  0.92069525]\n",
      " [0.82823956 0.9217341 ]\n",
      " [0.8321215  0.9227571 ]\n",
      " [0.8359213  0.92376465]\n",
      " [0.83964026 0.92475677]\n",
      " [0.8432799  0.9257338 ]\n",
      " [0.8468415  0.926696  ]\n",
      " [0.8503266  0.9276437 ]\n",
      " [0.8537364  0.9285769 ]\n",
      " [0.8570724  0.92949605]\n",
      " [0.86033595 0.93040127]\n",
      " [0.86352843 0.9312927 ]\n",
      " [0.8666511  0.93217075]\n",
      " [0.8697053  0.93303543]\n",
      " [0.8726924  0.93388706]\n",
      " [0.8756137  0.9347259 ]\n",
      " [0.8784701  0.93555194]\n",
      " [0.88126355 0.93636566]\n",
      " [0.8839947  0.9371671 ]\n",
      " [0.8866651  0.9379564 ]\n",
      " [0.8892757  0.93873394]\n",
      " [0.891828   0.9394997 ]\n",
      " [0.89432305 0.940254  ]\n",
      " [0.8967619  0.94099694]\n",
      " [0.8991457  0.9417287 ]\n",
      " [0.90147585 0.9424495 ]\n",
      " [0.9037533  0.94315964]\n",
      " [0.905979   0.9438591 ]\n",
      " [0.9081541  0.9445479 ]\n",
      " [0.9102797  0.9452266 ]\n",
      " [0.912357   0.9458952 ]\n",
      " [0.9143867  0.9465537 ]\n",
      " [0.91637003 0.94720244]\n",
      " [0.91830796 0.9478415 ]\n",
      " [0.92020136 0.948471  ]\n",
      " [0.92205125 0.94909126]\n",
      " [0.92385876 0.9497022 ]\n",
      " [0.9256245  0.9503041 ]\n",
      " [0.92734945 0.9508971 ]\n",
      " [0.9290346  0.9514813 ]\n",
      " [0.9306807  0.9520568 ]\n",
      " [0.93228877 0.95262384]\n",
      " [0.93385965 0.9531825 ]\n",
      " [0.93539387 0.95373285]\n",
      " [0.9368925  0.9542752 ]\n",
      " [0.9383564  0.9548095 ]\n",
      " [0.93978614 0.95533586]\n",
      " [0.9411826  0.9558545 ]\n",
      " [0.94254655 0.9563655 ]\n",
      " [0.94387865 0.956869  ]\n",
      " [0.9451796  0.95736516]\n",
      " [0.9464501  0.95785403]\n",
      " [0.9476909  0.9583357 ]\n",
      " [0.94890267 0.9588103 ]\n",
      " [0.9500862  0.9592781 ]\n",
      " [0.95124173 0.95973897]\n",
      " [0.9523704  0.9601931 ]\n",
      " [0.95347255 0.9606406 ]\n",
      " [0.9545488  0.9610817 ]], shape=(100, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x31ca00c10>,\n <matplotlib.lines.Line2D at 0x31ca00550>]"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "utils = daifa.prior_model(vel_pos)\n",
    "print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.9758997 ]\n",
      " [ 0.9729304 ]\n",
      " [ 0.96955293]\n",
      " [ 0.9654565 ]\n",
      " [ 0.960627  ]\n",
      " [ 0.9548891 ]\n",
      " [ 0.94786257]\n",
      " [ 0.93969876]\n",
      " [ 0.93030244]\n",
      " [ 0.9195027 ]\n",
      " [ 0.90711004]\n",
      " [ 0.8929159 ]\n",
      " [ 0.87669325]\n",
      " [ 0.85819674]\n",
      " [ 0.83716667]\n",
      " [ 0.813331  ]\n",
      " [ 0.7864117 ]\n",
      " [ 0.7561327 ]\n",
      " [ 0.72222865]\n",
      " [ 0.68445796]\n",
      " [ 0.64261705]\n",
      " [ 0.5965568 ]\n",
      " [ 0.5462003 ]\n",
      " [ 0.49156   ]\n",
      " [ 0.43275428]\n",
      " [ 0.36940077]\n",
      " [ 0.28745127]\n",
      " [ 0.2010652 ]\n",
      " [ 0.11143417]\n",
      " [ 0.01995304]\n",
      " [-0.07186358]\n",
      " [-0.16228153]\n",
      " [-0.24817589]\n",
      " [-0.32937416]\n",
      " [-0.39173776]\n",
      " [-0.43524262]\n",
      " [-0.46414724]\n",
      " [-0.4868523 ]\n",
      " [-0.50891775]\n",
      " [-0.5207025 ]\n",
      " [-0.5281645 ]\n",
      " [-0.5330335 ]\n",
      " [-0.5378673 ]\n",
      " [-0.54181117]\n",
      " [-0.54631764]\n",
      " [-0.55008405]\n",
      " [-0.55286306]\n",
      " [-0.554485  ]\n",
      " [-0.5556544 ]\n",
      " [-0.5568216 ]\n",
      " [-0.558042  ]\n",
      " [-0.5593047 ]\n",
      " [-0.56056494]\n",
      " [-0.56182253]\n",
      " [-0.5628206 ]\n",
      " [-0.5645143 ]\n",
      " [-0.5661451 ]\n",
      " [-0.56725985]\n",
      " [-0.5682604 ]\n",
      " [-0.56688756]\n",
      " [-0.5655116 ]\n",
      " [-0.5641301 ]\n",
      " [-0.5623451 ]\n",
      " [-0.55357784]\n",
      " [-0.54270047]\n",
      " [-0.53163797]\n",
      " [-0.52039105]\n",
      " [-0.5089606 ]\n",
      " [-0.49734783]\n",
      " [-0.485554  ]\n",
      " [-0.46986   ]\n",
      " [-0.45076293]\n",
      " [-0.42911422]\n",
      " [-0.40406162]\n",
      " [-0.3740413 ]\n",
      " [-0.34198213]\n",
      " [-0.30698577]\n",
      " [-0.26604214]\n",
      " [-0.2241156 ]\n",
      " [-0.18134275]\n",
      " [-0.13548093]\n",
      " [-0.09344777]\n",
      " [-0.05221801]\n",
      " [-0.01080938]\n",
      " [ 0.03063635]\n",
      " [ 0.07198736]\n",
      " [ 0.11433788]\n",
      " [ 0.15627703]\n",
      " [ 0.19766015]\n",
      " [ 0.2383505 ]\n",
      " [ 0.27822086]\n",
      " [ 0.31715494]\n",
      " [ 0.35504848]\n",
      " [ 0.39181033]\n",
      " [ 0.42736226]\n",
      " [ 0.46163958]\n",
      " [ 0.4945919 ]\n",
      " [ 0.5261811 ]\n",
      " [ 0.5563818 ]\n",
      " [ 0.5851807 ]], shape=(100, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x30f47deb0>,\n <matplotlib.lines.Line2D at 0x30f47dfa0>]"
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "utils = daifa.habit_action_model.actor_model(obs_pos)\n",
    "print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.8576372 ]\n",
      " [-0.8542569 ]\n",
      " [-0.8508029 ]\n",
      " [-0.84727365]\n",
      " [-0.84366804]\n",
      " [-0.8399847 ]\n",
      " [-0.8362223 ]\n",
      " [-0.83237946]\n",
      " [-0.82845485]\n",
      " [-0.82444715]\n",
      " [-0.8203551 ]\n",
      " [-0.81617725]\n",
      " [-0.8119123 ]\n",
      " [-0.8075588 ]\n",
      " [-0.8031155 ]\n",
      " [-0.798581  ]\n",
      " [-0.7939541 ]\n",
      " [-0.78927755]\n",
      " [-0.78451073]\n",
      " [-0.7796493 ]\n",
      " [-0.774692  ]\n",
      " [-0.76963776]\n",
      " [-0.76448506]\n",
      " [-0.7592328 ]\n",
      " [-0.7538797 ]\n",
      " [-0.7484247 ]\n",
      " [-0.74286646]\n",
      " [-0.73720396]\n",
      " [-0.73143584]\n",
      " [-0.7255612 ]\n",
      " [-0.7195957 ]\n",
      " [-0.7135727 ]\n",
      " [-0.70723736]\n",
      " [-0.7006943 ]\n",
      " [-0.6939357 ]\n",
      " [-0.6870525 ]\n",
      " [-0.68004376]\n",
      " [-0.67290837]\n",
      " [-0.66564566]\n",
      " [-0.6582546 ]\n",
      " [-0.6507345 ]\n",
      " [-0.642718  ]\n",
      " [-0.6345275 ]\n",
      " [-0.626192  ]\n",
      " [-0.6175271 ]\n",
      " [-0.6083816 ]\n",
      " [-0.59869015]\n",
      " [-0.5874988 ]\n",
      " [-0.57596004]\n",
      " [-0.56384134]\n",
      " [-0.5509313 ]\n",
      " [-0.5372121 ]\n",
      " [-0.51727587]\n",
      " [-0.4899417 ]\n",
      " [-0.4462234 ]\n",
      " [-0.34226513]\n",
      " [-0.16687334]\n",
      " [ 0.04021694]\n",
      " [ 0.24430099]\n",
      " [ 0.4288182 ]\n",
      " [ 0.5833674 ]\n",
      " [ 0.7047405 ]\n",
      " [ 0.7953258 ]\n",
      " [ 0.86039567]\n",
      " [ 0.900967  ]\n",
      " [ 0.92637634]\n",
      " [ 0.94511163]\n",
      " [ 0.9582586 ]\n",
      " [ 0.9673831 ]\n",
      " [ 0.97426796]\n",
      " [ 0.9795599 ]\n",
      " [ 0.9837165 ]\n",
      " [ 0.9870333 ]\n",
      " [ 0.9896628 ]\n",
      " [ 0.9917284 ]\n",
      " [ 0.99330354]\n",
      " [ 0.99454933]\n",
      " [ 0.9955638 ]\n",
      " [ 0.996375  ]\n",
      " [ 0.9970335 ]\n",
      " [ 0.99757254]\n",
      " [ 0.99801373]\n",
      " [ 0.9983744 ]\n",
      " [ 0.9986669 ]\n",
      " [ 0.998906  ]\n",
      " [ 0.99910223]\n",
      " [ 0.9992569 ]\n",
      " [ 0.9993846 ]\n",
      " [ 0.9994902 ]\n",
      " [ 0.99957764]\n",
      " [ 0.99965024]\n",
      " [ 0.9997102 ]\n",
      " [ 0.99975985]\n",
      " [ 0.99980074]\n",
      " [ 0.9998348 ]\n",
      " [ 0.99986297]\n",
      " [ 0.9998859 ]\n",
      " [ 0.99990493]\n",
      " [ 0.99992096]\n",
      " [ 0.999934  ]], shape=(100, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x2f0acb970>,\n <matplotlib.lines.Line2D at 0x2f0acbc10>]"
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "utils = daifa.habit_action_model.actor_model(vel_pos)\n",
    "print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "success\n",
      "91.63224167837254\n",
      "146\n",
      "success\n",
      "91.52211609551956\n",
      "75\n",
      "success\n",
      "95.31403658650211\n",
      "75\n",
      "success\n",
      "95.2333161488404\n",
      "160\n",
      "success\n",
      "92.4824130691937\n",
      "76\n",
      "success\n",
      "95.41642231346867\n",
      "83\n",
      "success\n",
      "94.93200555577017\n",
      "80\n",
      "success\n",
      "95.12500576847329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Context leak detected, msgtracer returned -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "success\n",
      "95.35163766944933\n",
      "79\n",
      "success\n",
      "95.33123536297711\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "obs_stddev = [0.05, 0.05]\n",
    "# obs_stddev = [0, 0]\n",
    "\n",
    "\n",
    "t_max = 999\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    done = False\n",
    "    rewards = []\n",
    "\n",
    "    t = 0\n",
    "    while not done:\n",
    "\n",
    "        obs = obs.reshape(1, obs.shape[0])\n",
    "        obs = transform_observations(obs, observation_max, observation_min, obs_stddev)\n",
    "\n",
    "        # print(obs)\n",
    "\n",
    "        action = daifa.habit_action_model.actor_model(obs)\n",
    "        action = action.numpy()\n",
    "\n",
    "        for k in range(daifa.agent_time_ratio):\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            env.render()\n",
    "\n",
    "            # print(obs)\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "            t += 1\n",
    "\n",
    "            if t == t_max:\n",
    "                done = True\n",
    "                break\n",
    "            elif done:\n",
    "                break\n",
    "\n",
    "    print(t)\n",
    "    if t < t_max:\n",
    "        print(\"success\")\n",
    "    else:\n",
    "        print(\"Failure\")\n",
    "        print(\"max obs\", obs)\n",
    "\n",
    "    print(np.sum(rewards))\n",
    "    # print(rewards)\n",
    "\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8813, 2) (8813, 1)\n"
     ]
    }
   ],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 500\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "actions = []\n",
    "\n",
    "pre_obs = []\n",
    "post_obs = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "    actions.append(a)\n",
    "\n",
    "    pre_obs.append(o[:-1])\n",
    "    post_obs.append(o[1:])\n",
    "\n",
    "\n",
    "    observations.append(o)\n",
    "    # ob_seqs.append(train)\n",
    "    # next_obs.append(test)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "actions = np.vstack(actions)\n",
    "\n",
    "pre_obs = np.vstack(pre_obs)\n",
    "post_obs = np.vstack(post_obs)\n",
    "\n",
    "\n",
    "print(pre_obs.shape, actions.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.habit_action_model(pre_obs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = agent.model_vae(observations)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m, s, z_pre = agent.model_vae.encoder(pre_obs)\n",
    "m, s, z_post = agent.model_vae.encoder(post_obs)\n",
    "z_pre"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_plus_a = np.concatenate([z_pre, actions], axis=1)\n",
    "z_plus_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((z, np.zeros_like(z)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test without the replay training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2,  [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 20, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=5,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=40, action_repeats=6, num_actions_to_execute=2, train_on_full_data=False, show_replay_training=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = agent.model_vae(observations)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with the prior model FEEF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2, [0, 0], [0.3, 0.3], llik_scaling=1, recon_stddev=0.05)\n",
    "\n",
    "pl_hoz = 5\n",
    "latent_dim = 2\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 2*latent_dim*pl_hoz, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(2)\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           None,\n",
    "                           train_prior_model=True,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=40, action_repeats=6, num_actions_to_execute=5, train_on_full_data=False, show_replay_training=True, train_during_episode=True, train_vae=True, train_tran=True, train_prior=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the models produced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.model_vae(observations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.reduce_mean(agent.model_vae.compute_loss(observations))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.prior_model(observations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.prior_model.extrinsic_kl(observations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "pl_hoz = 5\n",
    "latent_dim = 2\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 2*pl_hoz*latent_dim, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(2)\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "# without prior model\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "# with prior model\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           None,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, action_repeats=10, num_actions_to_execute=2, train_on_full_data=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Test the models produced\n",
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.model_vae(observations)\n",
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Identity VAE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = agent.tran((ob_seqs[0:1], None))\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = ob_seqs[0:1, -1].reshape(1,1,3)\n",
    "h = out[3]\n",
    "h = h[0, -2, :]\n",
    "h = h.numpy().reshape(1,30)\n",
    "h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((t, h))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ob_seqs[0:1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test to see how the agent trains on standard observation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, vae_train_epochs=1, tran_train_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "success, agent, t, pre_obs, post_obs, acts = run_episode(env, daifa, observation_max, observation_min, observation_noise_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_np = np.array(pre_obs)\n",
    "a = np.array(acts)\n",
    "a.shape\n",
    "pre_a = np.concatenate([pre_np, a], axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(a.max(), a.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict = np.array(post_obs)[:, 14, :]\n",
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((pre_a, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine training the model on the observation data\n",
    "\n",
    "Does it eventually converge to a good model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_runs = 1\n",
    "for i in range(num_train_runs):\n",
    "\n",
    "    for j in range(len(pre)):\n",
    "        pre = pre_obs[j]\n",
    "        post = post_obs[j]\n",
    "        actions = acts[j]\n",
    "\n",
    "        daifa.train(pre, post, actions, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation(np.array([0.5, 0.1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the FEEF computations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, planning_horizon=15, n_policy_candidates=70, n_policies=1500, n_cem_policy_iterations=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_policy(agent, env, policy, action_repeats):\n",
    "\n",
    "    observation = env.reset()\n",
    "    obs = transform_observations(observation, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    z_t_minus_1 = obs\n",
    "    p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "    p\n",
    "    print(obs)\n",
    "    print(p)\n",
    "\n",
    "    for action in p:\n",
    "        for t in range(action_repeats):\n",
    "            res = env.step(np.array([action]))\n",
    "            print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([0, 0])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p\n",
    "\n",
    "agent.forward_policies(p, z_t_minus_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "test_policy(agent, env, p.numpy(), 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([-0.27691475,  0.01688306])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 given_prior_mean,\n",
    "                 given_prior_stddev,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 tran_train_epochs=1,\n",
    "                 vae_train_epochs=1,\n",
    "                 agent_time_ratio=6,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        self.prior_model = prior_model\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        self.vae_train_epochs = vae_train_epochs\n",
    "        self.tran_train_epochs = tran_train_epochs\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.model_vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        # transition\n",
    "        # takes action plus last state and outputs next latent state\n",
    "        self.tran = tran\n",
    "        self.tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        self.hidden_state = None\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(observation)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    def train(self, pre_observations_raw, post_observations_raw, actions_complete, rewards, verbose=0):\n",
    "\n",
    "        # compress the observations based on the agents time compression factor\n",
    "        # pre_observations = pre_observations_raw[::self.agent_time_ratio]  # for example take every 6th element\n",
    "        # post_observations = np.array([post_observations_raw[i] for i in range(len(post_observations_raw)) if i % self.agent_time_ratio == self.agent_time_ratio - 1])\n",
    "        #\n",
    "        # print(pre_observations_raw)\n",
    "        # print(pre_observations)\n",
    "        # print(post_observations_raw)\n",
    "        # print(post_observations)\n",
    "\n",
    "        pre_observations = pre_observations_raw\n",
    "        post_observations = post_observations_raw\n",
    "\n",
    "        # only look at the first n actions that we took\n",
    "        actions = actions_complete[0: len(pre_observations)]\n",
    "\n",
    "        num_observations = pre_observations.shape[0]\n",
    "        observation_dim = pre_observations.shape[1]\n",
    "        action_dim = actions.shape[1]\n",
    "        # action_dim = 1  # TODO fix this to allow different actions\n",
    "\n",
    "        # find the actual observed latent states using the vae\n",
    "        pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "        post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "        # set up the input training data that we use to train the transition model\n",
    "        z_train = np.concatenate([np.array(pre_latent_mean), np.array(actions)], axis=1)\n",
    "\n",
    "        # we use the sequence to find the right hidden states to use as input\n",
    "        z_train_seq = z_train.reshape((1, num_observations, observation_dim + action_dim))\n",
    "        z_train_singles = z_train.reshape(num_observations, 1, observation_dim + action_dim)\n",
    "\n",
    "        # the previous hidden state is the memory after observing some sequences but it might be None\n",
    "        if self.hidden_state is None:\n",
    "            self.hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "        if self.train_tran:\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.hidden_state))\n",
    "\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran_train_epochs, verbose=verbose)\n",
    "\n",
    "        # now find the new predicted hidden state that we will use for finding the policy\n",
    "        # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "        _, _, final_hidden_state, _ = self.tran((z_train_seq, self.hidden_state))\n",
    "        # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "        self.hidden_state = final_hidden_state\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            self.model_vae.fit(post_observations, epochs=self.vae_train_epochs, verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by one to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        print(z_t_minus_one)\n",
    "        print(mean_best_policies)\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape [self.n_policies, len(z_t_minus_one)]\n",
    "        prev_latent_mean = np.stack([z_t_minus_one]*self.n_policies)\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "            likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "            if self.prior_model is None:\n",
    "\n",
    "                # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                # create the prior distribution\n",
    "                prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "            # TODO Fix the learned prior model\n",
    "            else:\n",
    "                prior_dist = self.prior_model()\n",
    "\n",
    "            kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "            predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "            kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    def EFE(self, policy_posteriors, predicted_likelihood, predicted_posterior):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing with a pretrained transition model\n",
    "\n",
    "This works well! So the problem can't lie with the transition model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0.07]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, train_vae=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 200\n",
    "seq_length = 500\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "next_obs_stddev = []\n",
    "actions = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.2)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    # train = np.concatenate([o[:-1], a], axis=1)\n",
    "    train = o[:-1]\n",
    "    test = o[1:]\n",
    "\n",
    "    actions.append(a)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "    ob_seqs_stddev = np.ones_like(train)\n",
    "    next_stddev = np.ones_like(test)\n",
    "\n",
    "    next_obs_stddev.append(next_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(num_seqs):\n",
    "\n",
    "    pre = ob_seqs[i]\n",
    "    next = next_obs[i]\n",
    "    acts = actions[i]\n",
    "\n",
    "    next_sd = next_obs_stddev[i]\n",
    "\n",
    "    daifa.train(pre, next, acts, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 150\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    test = o[-1]\n",
    "\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "ob_seqs = np.array(ob_seqs)[:, -5:, :]\n",
    "next_obs = np.array(next_obs)\n",
    "ob_seqs.shape\n",
    "\n",
    "ob_seqs_stddev = np.ones_like(ob_seqs)\n",
    "next_obs_stddev = np.ones_like(next_obs)\n",
    "\n",
    "ob_seqs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.tran((ob_seqs, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_obs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks fantastic!!! With enough data the transition model is training very well"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.hidden_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([0.4, 0.5])\n",
    "daifa.hidden_state = None\n",
    "p, s = daifa.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.tran((np.array([[[0.4, 0.5, 1]]]), None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "daifa.train_tran = False\n",
    "daifa.train_vae = False\n",
    "\n",
    "daifa.hidden_state = None\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}