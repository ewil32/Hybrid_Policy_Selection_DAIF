{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run The Agent on Mountain Car"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from vae_recurrent import VAE, create_decoder, create_encoder\n",
    "from transition_gru import TransitionGRU\n",
    "from recurrent_agent import DAIFAgentRecurrent\n",
    "from prior_model import PriorModelBellman\n",
    "from habitual_action_network import HabitualAction, compute_discounted_cumulative_reward\n",
    "from ddpg import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations\n",
    "from train_agent import train_single_agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from identity_vae import IdentityVAE, identity_encoder, identity_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with no prior model FEEF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PriorModelBellman(keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 observation_dim,\n",
    "                 output_dim=1,\n",
    "                 iterate_train=1,\n",
    "                 discount_factor=0.99,\n",
    "                 training_epochs=1,\n",
    "                 show_training=True,\n",
    "                 use_tanh_on_output=True):\n",
    "\n",
    "        super(PriorModelBellman, self).__init__()\n",
    "        self.observation_dim = observation_dim\n",
    "        self.iterate_train = iterate_train\n",
    "        self.discount_factor = discount_factor\n",
    "        self.train_epochs = 1\n",
    "\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.train_epochs = training_epochs\n",
    "        self.show_training = show_training\n",
    "\n",
    "        # make the model\n",
    "        transition_inputs = layers.Input(observation_dim)\n",
    "        h = layers.Dense(observation_dim * 20, activation=\"silu\")(transition_inputs)\n",
    "        if use_tanh_on_output:\n",
    "            h = layers.Dense(output_dim, activation=\"tanh\")(h)\n",
    "        else:\n",
    "            h = layers.Dense(output_dim)(h)\n",
    "\n",
    "        self.prior_model = keras.Model(transition_inputs, h, name=\"prior_model\")\n",
    "        self.prior_model.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "\n",
    "    def call(self, observations):\n",
    "        return self.prior_model(observations)\n",
    "\n",
    "\n",
    "    def extrinsic_kl(self, observations):\n",
    "        return 1.0 - self(observations)  # map from [-1, 1] to [2, 0]\n",
    "\n",
    "\n",
    "    def train(self, observations, rewards):\n",
    "        \"\"\"\n",
    "        :param observations: o_0, o_1, ... , o_n\n",
    "        :param rewards: list with r_0, r_1, ... , r_n\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        num_observations = len(observations)\n",
    "\n",
    "        # print(rewards)\n",
    "\n",
    "        for i in range(self.iterate_train):\n",
    "\n",
    "            # TODO Still seems a little strange that we add 0 to the end and discount the way we do but I think it makes sense. Check what predicted utilities are in practice\n",
    "            utility_t = self.prior_model(observations)\n",
    "            # utility_t_plus_one = tf.concat([utility_t[1:], tf.zeros((1, self.output_dim), dtype=utility_t.dtype)], axis=0)\n",
    "            utility_t_plus_one = tf.concat([utility_t[1:], np.zeros((1,1))], axis=0)\n",
    "\n",
    "            # just have constant gamma\n",
    "            discount_factors = np.ones_like(utility_t_plus_one) * self.discount_factor\n",
    "\n",
    "\n",
    "            # OR reducing discount factors through time\n",
    "            # discount_factors = np.power([self.discount_factor]*num_observations, np.arange(num_observations)).reshape(observations.shape[0], 1)\n",
    "            # discount_factors = np.flip(discount_factors)\n",
    "\n",
    "            # print(discount_factors)\n",
    "\n",
    "            # print(predicted_utility, pred_next_v)\n",
    "\n",
    "            expected_utility = rewards + discount_factors * utility_t_plus_one\n",
    "\n",
    "            # print(rewards_stacked)\n",
    "            # print(discount_factors * utility_t_plus_one)\n",
    "\n",
    "            self.prior_model.fit(observations, expected_utility, epochs=self.train_epochs, verbose=self.show_training)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "# from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 habitual_action_net,\n",
    "                 given_prior_mean=None,\n",
    "                 given_prior_stddev=None,\n",
    "                 agent_time_ratio=6,\n",
    "                 actions_to_execute_when_exploring=2,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True,\n",
    "                 train_prior_model=True,\n",
    "                 train_habit_net=True,\n",
    "                 train_with_replay=True,\n",
    "                 train_after_exploring=True,\n",
    "                 use_kl_extrinsic=True,\n",
    "                 use_kl_intrinsic=True,\n",
    "                 use_FEEF=True,\n",
    "                 use_fast_thinking=False,\n",
    "                 uncertainty_tolerance=0.05,\n",
    "                 habit_model_type=\"name_of_model\"):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        # parameters for slow policy planning\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        # flags for whether or not we are training models or using pretrained models and when we should train\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "        self.train_habit_net = train_habit_net\n",
    "        self.train_prior = train_prior_model\n",
    "        self.train_with_replay = train_with_replay\n",
    "        self.train_after_exploring = train_after_exploring\n",
    "\n",
    "        # do we use the kl divergence for extrinsic vs intrinsic\n",
    "        self.use_kl_intrinsic = use_kl_intrinsic\n",
    "        self.use_kl_extrinsic = use_kl_extrinsic\n",
    "\n",
    "        # do we use the FEEF or EFE?\n",
    "        self.use_FEEF = use_FEEF\n",
    "\n",
    "        # given prior values\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.tran = tran\n",
    "        self.prior_model = prior_model\n",
    "        self.habit_action_model = habitual_action_net\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "        self.actions_to_execute_when_exploring = actions_to_execute_when_exploring\n",
    "        self.time_step = 0\n",
    "        self.exploring = False\n",
    "\n",
    "        # track the hidden state of the transition gru model so we can use it to train\n",
    "        self.tran_hidden_state = None\n",
    "        self.tran_hidden_state_pre_exploring = None\n",
    "        self.prev_tran_hidden_state = None\n",
    "\n",
    "        # store the full observations for the episode so we can train using replay\n",
    "        self.full_observation_sequence = []\n",
    "        self.full_action_sequence = []\n",
    "        self.full_reward_sequence = []\n",
    "\n",
    "        # store the observations while the agent is in exploration mode\n",
    "        self.exploring_observation_sequence = []\n",
    "        self.exploring_action_sequence = []\n",
    "        self.exploring_reward_sequence = []\n",
    "\n",
    "        # store the observations at the world time scale\n",
    "        self.env_time_scale_observations = []\n",
    "\n",
    "        self.policy_left_to_execute = [None]\n",
    "        self.previous_observation = None\n",
    "        self.action_being_executed = None\n",
    "        self.action_being_executed = 0\n",
    "\n",
    "        self.use_fast_thinking = use_fast_thinking\n",
    "        self.habit_model_type = habit_model_type\n",
    "        self.uncertainty_tolerance = uncertainty_tolerance\n",
    "\n",
    "\n",
    "    def perceive_and_act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        The function called to have the agent interact with the environment\n",
    "        We assume the agent gets a transformed/noisy observation from the environment and then returns an action\n",
    "\n",
    "        TODO: possibly the agent returns some other information for logging and showing experiments\n",
    "\n",
    "        :param observation:\n",
    "        :param reward:\n",
    "        :param done:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # track the world time scale observation sequence\n",
    "        self.env_time_scale_observations.append(observation)\n",
    "\n",
    "        # if the episode is finished, then do any training on the full data set\n",
    "        if done:\n",
    "\n",
    "            if self.train_with_replay:\n",
    "                print(\"training on full data\")\n",
    "\n",
    "                # add the final observation and reward we observed to the sequences\n",
    "                self.full_observation_sequence.append(observation)\n",
    "                self.full_reward_sequence.append(reward)\n",
    "\n",
    "                # Call the training function on the observation sequences to train everything we need to train\n",
    "                self.train_models(np.vstack(self.full_observation_sequence),\n",
    "                                  np.vstack(self.full_action_sequence),\n",
    "                                  np.array(self.full_reward_sequence),\n",
    "                                  None)\n",
    "\n",
    "\n",
    "        # Otherwise are we at a point where we can reconsider our policy and maybe train the world model\n",
    "        elif self.time_step % self.agent_time_ratio == 0:\n",
    "\n",
    "            # add the reward only if it's not the first observation\n",
    "            if self.time_step != 0:\n",
    "                self.full_reward_sequence.append(reward)\n",
    "\n",
    "            # add the observation to the sequence\n",
    "            self.full_observation_sequence.append(observation)\n",
    "\n",
    "            # We only update the model during the episode when we were exploring using the planning method and we have executed all of the actions in the policy\n",
    "            if self.exploring and len(self.policy_left_to_execute) == 0:\n",
    "\n",
    "                # print(\"f\", self.full_observation_sequence)\n",
    "                # print(\"e\", self.full_observation_sequence[-1*(self.actions_to_execute_when_exploring + 1):])\n",
    "\n",
    "                if self.train_after_exploring:\n",
    "\n",
    "                    # the actions done while exploring were the last self.actions_to_execute_when_exploring\n",
    "                    self.exploring_action_sequence = self.full_action_sequence[-1*self.actions_to_execute_when_exploring:]\n",
    "                    self.exploring_reward_sequence = self.full_reward_sequence[-1*self.actions_to_execute_when_exploring:]\n",
    "                    self.exploring_observation_sequence = self.full_observation_sequence[-1*(self.actions_to_execute_when_exploring + 1):]\n",
    "\n",
    "                    # Call the training function on the observation sequences to train everything we need to train\n",
    "                    self.train_models(np.vstack(self.exploring_observation_sequence),\n",
    "                                                np.vstack(self.exploring_action_sequence),\n",
    "                                                np.array(self.exploring_reward_sequence),\n",
    "                                                self.tran_hidden_state_pre_exploring)\n",
    "\n",
    "                self.exploring = False\n",
    "\n",
    "\n",
    "            # Predict the expected observation\n",
    "            action_as_array = np.array(self.action_being_executed).reshape(1, self.tran.action_dim)\n",
    "            expected_observation, self.tran_hidden_state = self.predict_next_observation(self.previous_observation, action_as_array, self.prev_tran_hidden_state)\n",
    "            # pred_next_observation, next_tran_hidden_state = None, None\n",
    "            # print(self.previous_observation, expected_observation, observation)\n",
    "            # Now we select our action. If we aren't exploring then either we act out of habit or we might need to explore\n",
    "            # I think I can check this based on whether or not there are actions left to execute in the current policy\n",
    "            if not self.exploring:\n",
    "\n",
    "                if self.use_fast_thinking and self.previous_observation is None:\n",
    "                    # self.policy_left_to_execute = self.habit_action_model(observation)\n",
    "                    self.policy_left_to_execute = self.select_fast_thinking_policy(observation)\n",
    "                    self.policy_left_to_execute = self.policy_left_to_execute.numpy().tolist()  # tf tensor to list\n",
    "                    print(\"fast thinking\")\n",
    "\n",
    "                # TDOD Fix this to work however it needs to\n",
    "                # we need to see what the generative model now thinks about what the expected current observation is\n",
    "                elif self.use_fast_thinking and np.allclose(observation, expected_observation, atol=self.uncertainty_tolerance):  # within some tolerance\n",
    "\n",
    "                    self.policy_left_to_execute = self.select_fast_thinking_policy(observation)\n",
    "                    # self.policy_left_to_execute = self.policy_left_to_execute + np.random.normal(0, scale=self.habit_action_model.action_std_dev)\n",
    "                    self.policy_left_to_execute = self.policy_left_to_execute.numpy().tolist()\n",
    "\n",
    "                    # self.tran_hidden_state = next_tran_hidden_state\n",
    "\n",
    "                    print(\"fast thinking\")\n",
    "\n",
    "                # the generative model is surprised so we should use the slow deliberation for planning out a policy that balances exploration and exploitation\n",
    "                else:\n",
    "                    # print(\"slow thinking\")\n",
    "                    policy = self.select_policy(observation)\n",
    "                    # print(policy.mean())\n",
    "                    # TODO should we actually sample here?\n",
    "                    policy = policy.mean().numpy()\n",
    "                    policy = policy.reshape(policy.shape[0], self.tran.action_dim).tolist()\n",
    "                    self.policy_left_to_execute = policy[0: self.actions_to_execute_when_exploring]\n",
    "\n",
    "                    self.tran_hidden_state_pre_exploring = self.tran_hidden_state\n",
    "\n",
    "                    self.exploring = True\n",
    "\n",
    "                # print(observation)\n",
    "                # print(pred_next_observation)\n",
    "\n",
    "            # finally update the previous observation and action to be the one we just had/did\n",
    "            self.previous_observation = observation\n",
    "            self.prev_tran_hidden_state = self.tran_hidden_state\n",
    "            self.action_being_executed = self.policy_left_to_execute[0]\n",
    "            self.full_action_sequence.append(self.action_being_executed)\n",
    "            self.policy_left_to_execute.pop(0)\n",
    "\n",
    "        # final updates increment the current timestep and return the action specified by the policy\n",
    "        self.time_step += 1\n",
    "\n",
    "        return self.action_being_executed\n",
    "\n",
    "\n",
    "    def predict_next_observation(self, obs, action, tran_hidden_state):\n",
    "\n",
    "        # TODO: Fix this with the transition hidden states\n",
    "        if obs is None:\n",
    "            return None, None\n",
    "        else:\n",
    "            z_mean, z_std, z = self.model_vae.encoder(obs)\n",
    "            # print(z_mean.shape)\n",
    "            # print(action.shape)\n",
    "            z_mean = z_mean.numpy()\n",
    "            z_plus_action = np.concatenate([z_mean, action], axis=1)\n",
    "            # print(z_mean)\n",
    "            # print(action)\n",
    "            # print(z_plus_action)\n",
    "\n",
    "            z_plus_action = z_plus_action.reshape(1, 1, z_plus_action.shape[1])\n",
    "            # print(z_plus_action)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((z_plus_action, tran_hidden_state))\n",
    "\n",
    "            next_observation = self.model_vae.decoder(next_latent_mean)\n",
    "            # print(next_observation)\n",
    "            return next_observation.numpy(), next_hidden_state\n",
    "\n",
    "\n",
    "    # We use this function to reset the hidden state of the transition model when we want to train on the full data set\n",
    "    def reset_tran_hidden_state(self):\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "\n",
    "    def reset_all_states(self):\n",
    "        self.time_step = 0\n",
    "        self.exploring = False\n",
    "\n",
    "        # track the hidden state of the transition gru model so we can use it to train\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "        # store the full observations for the episode so we can train using replay\n",
    "        self.complete_observation_sequence = []\n",
    "        self.full_observation_sequence = []\n",
    "        self.full_action_sequence = []\n",
    "        self.full_reward_sequence = []\n",
    "\n",
    "        # store the observations while the agent is in exploration mode\n",
    "        self.exploring_observation_sequence = []\n",
    "        self.exploring_action_sequence = []\n",
    "        self.exploring_reward_sequence = []\n",
    "\n",
    "        self.policy_left_to_execute = []\n",
    "        self.previous_observation = None\n",
    "        self.previous_action_executed = None\n",
    "\n",
    "\n",
    "    def train_models(self, observations_full, actions, rewards, tran_hidden_state_pre_obs):\n",
    "\n",
    "        pre_observations = observations_full[:-1]\n",
    "        post_observations = observations_full[1:]\n",
    "\n",
    "        # find the actual observed latent states using the vae\n",
    "        pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "        post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "        #### TRAIN THE TRANSITION MODEL ####\n",
    "        if self.train_tran:\n",
    "\n",
    "            num_observations = pre_observations.shape[0]\n",
    "            # observation_dim = pre_observations.shape[1]\n",
    "            action_dim = actions.shape[1]\n",
    "            latent_dim = self.model_vae.latent_dim\n",
    "\n",
    "            # set up the input training data that we use to train the transition model\n",
    "            z_train = np.concatenate([np.array(pre_latent_mean), actions], axis=1)\n",
    "\n",
    "            # we use the sequence to find the right hidden states to use as input\n",
    "            z_train_seq = z_train.reshape((1, num_observations, latent_dim + action_dim))\n",
    "            z_train_singles = z_train.reshape(num_observations, 1, latent_dim + action_dim)\n",
    "\n",
    "            # the previous hidden state is the memory after observing some sequences but it might be None if we're just starting\n",
    "            if tran_hidden_state_pre_obs is None:\n",
    "                tran_hidden_state_pre_obs = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, tran_hidden_state_pre_obs))\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([tran_hidden_state_pre_obs, h_states_for_training], axis=0)\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran.train_epochs, verbose=self.tran.show_training, batch_size=z_train_singles.shape[0])\n",
    "\n",
    "            # now find the new predicted hidden state that we will use for finding the policy\n",
    "            # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "            _, _, final_hidden_state, h_states = self.tran((z_train_seq, tran_hidden_state_pre_obs))\n",
    "            # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "            z_pred, _, _, _ = self.tran((z_train_singles, h_states_for_training))\n",
    "            # print(h_states)\n",
    "            # print(final_hidden_state)\n",
    "            # print(h_states[:, -2, :])\n",
    "            self.prev_tran_hidden_state = h_states[:, -2, :]\n",
    "            self.tran_hidden_state = final_hidden_state\n",
    "\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            # self.model_vae.fit(pre_observations_raw, epochs=self.vae_train_epochs, verbose=self.show_vae_training)\n",
    "            self.model_vae.fit(pre_observations, epochs=self.model_vae.train_epochs, verbose=self.model_vae.show_training, batch_size=pre_observations.shape[0])\n",
    "\n",
    "\n",
    "        #### TRAIN THE PRIOR MODEL ####\n",
    "        # TODO fix how this part should work\n",
    "        if self.train_prior:\n",
    "            # self.prior_model.train(post_observations, rewards, verbose=self.show_prior_training)\n",
    "            if max(rewards) > 0:\n",
    "                # self.prior_model.train(post_observations, rewards)\n",
    "                self.prior_model.train(post_latent_mean, rewards)\n",
    "\n",
    "\n",
    "        #### TRAIN THE HABIT ACTION NET ####\n",
    "        if self.train_habit_net:\n",
    "\n",
    "            # prior_preferences_mean = tf.convert_to_tensor(self.given_prior_mean, dtype=\"float32\")\n",
    "            # prior_preferences_stddev = tf.convert_to_tensor(self.given_prior_stddev, dtype=\"float32\")\n",
    "            #\n",
    "            # prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "            #\n",
    "            # external_efe = -1 * tf.math.log(prior_dist.prob(post_observations))\n",
    "            # external_efe = external_efe.numpy().reshape(external_efe.shape[0], 1)\n",
    "            #\n",
    "            # one_over_external_efe = 1/external_efe\n",
    "            #\n",
    "            # ten_minus_external_efe = -1*external_efe + 10\n",
    "\n",
    "            # ten_minus_external_efe = ten_minus_external_efe.numpy().reshape(ten_minus_external_efe.shape[0], 1)\n",
    "\n",
    "            # one_over_external_efe = one_over_external_efe.numpy().reshape(one_over_external_efe.shape[0], 1)\n",
    "            # print(one_over_external_efe.shape)\n",
    "\n",
    "            # print(post_observations)\n",
    "            # print(one_over_external_efe)\n",
    "\n",
    "            # obs_utilities = self.prior_model(pre_observations)\n",
    "            # obs_utilities = tf.reduce_sum(obs_utilities, axis=-1)\n",
    "            # obs_utilities = obs_utilities.numpy().reshape(obs_utilities.shape[0], 1)\n",
    "            # # print(obs_utilities)\n",
    "            #\n",
    "            # cum_rewards = compute_discounted_cumulative_reward(obs_utilities, self.habit_action_model.discount_factor)\n",
    "\n",
    "            if self.habit_model_type == \"PG\":\n",
    "                rewards = rewards.reshape(rewards.shape[0], 1)\n",
    "                cum_rewards = compute_discounted_cumulative_reward(rewards, self.habit_action_model.discount_factor)\n",
    "                rewards_to_train_on = cum_rewards\n",
    "\n",
    "                # TODO I think for the final state the V(s_t+1) should be set to 0\n",
    "                # ADVANTAGE\n",
    "                v_state = self.prior_model(pre_latent_mean)\n",
    "                v_plus_one_state = self.prior_model(post_latent_mean)\n",
    "                advantage = rewards + self.prior_model.discount_factor * v_plus_one_state - v_state\n",
    "\n",
    "                # print(advantage)\n",
    "\n",
    "                # DDPG and policy gradient interface with same function\n",
    "                # self.habit_action_model.train(pre_latent_mean, actions, rewards_to_train_on, post_latent_mean)\n",
    "                self.habit_action_model.train(pre_latent_mean, actions, advantage, post_latent_mean)\n",
    "\n",
    "            if self.habit_model_type == \"DDPG\":\n",
    "                self.habit_action_model.train(pre_latent_mean, actions, rewards, post_latent_mean)\n",
    "\n",
    "\n",
    "    def select_fast_thinking_policy(self, observation):\n",
    "\n",
    "        # TODO should you select the mean here?\n",
    "        # _,  _, latent_state = self.model_vae.encoder(observation)\n",
    "        latent_state,  _, _ = self.model_vae.encoder(observation)\n",
    "        if self.habit_model_type == \"DDPG\":\n",
    "            action = self.habit_action_model.actor_model(latent_state)\n",
    "        elif self.habit_model_type == \"PG\":\n",
    "            action = self.habit_action_model(latent_state)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "        \"\"\"\n",
    "        :param observation: needs to be [n, observation_dim] shape np array or tf tensor\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO do you take the mean or that latent here?\n",
    "        # get the latent state from this observation\n",
    "        # TODO should I use the mean here?\n",
    "        _,  _, latent_state = self.model_vae.encoder(observation)\n",
    "        # latent_state,  _, _ = self.model_vae.encoder(observation)\n",
    "        # latent_state = latent_state.numpy().reshape((1, latent_state.shape[0]))\n",
    "        # print(latent_state)\n",
    "        # print(latent_state)\n",
    "        # select the policy\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(latent_state)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    # TODO Fix this so we can use different action dimensions\n",
    "    def cem_policy_optimisation(self, latent_z):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros((self.planning_horizon, self.tran.action_dim))\n",
    "        std_best_policies = tf.ones((self.planning_horizon, self.tran.action_dim))\n",
    "\n",
    "        # print(mean_best_policies)\n",
    "        # print(mean_best_policies.shape)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            # print(\"p\", policies.shape)\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "            # policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), latent_z)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            # print(\"POLICIES\", policies)\n",
    "            # print(\"FEEFS\", FEEFs)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by -1 to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape (self.n_policies, latent_dim) when z_t_minus is tensor with shape (1, latent_dim)\n",
    "        prev_latent_mean = tf.squeeze(tf.stack([z_t_minus_one]*self.n_policies, axis=1))\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.tran_hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.tran_hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # print(prev_latent_mean)\n",
    "            # print(policies[:, t, :].shape)\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t, :]], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            # print(tran_input.shape)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        if self.use_FEEF:\n",
    "            return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "        else:\n",
    "            return self.EFE(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "                # Compute the extrinisc approximation with the prior model\n",
    "                else:\n",
    "                    kl_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    kl_extrinsic = tf.reduce_sum(kl_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                kl_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"Extrinsic\", kl_extrinsic)\n",
    "            # print(\"Intrinsic\", kl_intrinsic)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    # TODO Find out how this works with the log probability extrinsic term\n",
    "    def EFE(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        EFEs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack(self.given_prior_mean), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack(self.given_prior_stddev), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    # compute extrinsic prior preferences term\n",
    "                    efe_extrinsic = -1 * tf.math.log(prior_dist.prob(predicted_likelihood))\n",
    "\n",
    "                # TODO Can I use the learned prior model here?\n",
    "                else:\n",
    "                    # efe_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    efe_extrinsic = self.prior_model.extrinsic_kl(predicted_posterior)\n",
    "                    efe_extrinsic = tf.reduce_sum(efe_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                efe_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"EX\", efe_extrinsic)\n",
    "            # print(\"IN\", kl_intrinsic)\n",
    "\n",
    "            EFE = efe_extrinsic - kl_intrinsic\n",
    "\n",
    "            EFEs.append(EFE)\n",
    "\n",
    "        return EFEs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "pln_hrzn = 5\n",
    "latent_dim = 2\n",
    "obs_dim = 2\n",
    "\n",
    "# make the VAE\n",
    "enc = create_encoder(2, latent_dim, [20])\n",
    "dec = create_decoder(latent_dim, 2, [20])\n",
    "vae = VAE(enc, dec, latent_dim,  [0]*latent_dim, [0.3]*latent_dim, train_epochs=2, show_training=True)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the TRANSITION\n",
    "tran = TransitionGRU(latent_dim, 1, 2*pln_hrzn*latent_dim, 2, train_epochs=2, show_training=True)\n",
    "tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the HABIT ACTION NET\n",
    "habit_net = HabitualAction(latent_dim, 1, [16, 16], train_epochs=2, show_training=True)\n",
    "habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# actor_model = get_actor(latent_dim, 1)\n",
    "# critic_model = get_critic(latent_dim, 1)\n",
    "#\n",
    "# target_actor = get_actor(latent_dim, 1)\n",
    "# target_critic = get_critic(latent_dim, 1)\n",
    "#\n",
    "# # Making the weights equal initially\n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "#\n",
    "# critic_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "# actor_optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "#\n",
    "# habit_net = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau=0.005, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)\n",
    "\n",
    "# make the PRIOR NET\n",
    "prior_model = PriorModelBellman(latent_dim, output_dim=1, show_training=True, use_tanh_on_output=False)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "# daifa = DAIFAgentRecurrent(None,\n",
    "#                            vae,\n",
    "#                            tran,\n",
    "#                            habit_net,\n",
    "#                            scaled_prior_mean,\n",
    "#                            prior_stddev,\n",
    "#                            planning_horizon=pln_hrzn,\n",
    "#                            use_kl_extrinsic=True,\n",
    "#                            use_kl_intrinsic=True,\n",
    "#                            use_FEEF=False,\n",
    "#                            train_habit_net=True,\n",
    "#                            train_prior_model=False,\n",
    "#                            train_with_replay=True,\n",
    "#                            train_after_exploring=True,\n",
    "#                            use_fast_thinking=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           habit_net,\n",
    "                           planning_horizon=pln_hrzn,\n",
    "                           use_kl_extrinsic=False,  # maybe this works\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           train_habit_net=False,\n",
    "                           train_prior_model=True,\n",
    "                           train_tran=True,\n",
    "                           train_after_exploring=True,\n",
    "                           train_with_replay=True,\n",
    "                           use_fast_thinking=False,\n",
    "                           habit_model_type=\"PG\",\n",
    "                           uncertainty_tolerance=0.1)\n",
    "\n",
    "\n",
    "\n",
    "daifa.train_prior = True\n",
    "daifa.prior_model.show_training = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.56052524  0.        ]\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "2022-09-14 08:44:39.845236: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Ethan/miniconda3/envs/tf_daif_car_race/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n",
      "1/1 [==============================] - 0s 482ms/step - kl_loss: 0.0078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 217.8476 - reconstruction_loss: 210.1774 - kl_loss: 7.6702\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 103.0458 - reconstruction_loss: 95.4400 - kl_loss: 7.6058\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0063\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0058\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 55.4771 - reconstruction_loss: 48.2232 - kl_loss: 7.2539\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86.6642 - reconstruction_loss: 79.4557 - kl_loss: 7.2085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0253\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.0162 - reconstruction_loss: 59.8075 - kl_loss: 9.2087\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.0306 - reconstruction_loss: 37.8789 - kl_loss: 9.1516\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 147.9760 - reconstruction_loss: 136.1939 - kl_loss: 11.7821\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 166.1218 - reconstruction_loss: 154.4565 - kl_loss: 11.6653\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0126\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0124\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 127.7111 - reconstruction_loss: 117.6649 - kl_loss: 10.0462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 248.2380 - reconstruction_loss: 238.2991 - kl_loss: 9.9389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.1509\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 155.7089 - reconstruction_loss: 147.8076 - kl_loss: 7.9014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 226.5493 - reconstruction_loss: 218.7273 - kl_loss: 7.8220\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2134\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 95.2713 - reconstruction_loss: 88.8886 - kl_loss: 6.3827\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 197.0454 - reconstruction_loss: 190.6983 - kl_loss: 6.3471\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2574\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2413\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 174.1075 - reconstruction_loss: 167.9386 - kl_loss: 6.1690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.7363 - reconstruction_loss: 48.6052 - kl_loss: 6.1311\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1064\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0972\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 97.5755 - reconstruction_loss: 91.7206 - kl_loss: 5.8549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.8310 - reconstruction_loss: 10.9970 - kl_loss: 5.8340\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0091\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.1188 - reconstruction_loss: 21.2230 - kl_loss: 6.8958\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.9739 - reconstruction_loss: 17.1020 - kl_loss: 6.8719\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0243\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0259\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 128.6155 - reconstruction_loss: 120.2775 - kl_loss: 8.3381\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.6422 - reconstruction_loss: 64.3439 - kl_loss: 8.2983\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0130\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.9767 - reconstruction_loss: 28.7029 - kl_loss: 9.2738\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.3145 - reconstruction_loss: 44.0994 - kl_loss: 9.2151\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0339\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0332\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 146.4692 - reconstruction_loss: 138.8773 - kl_loss: 7.5919\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 113.9343 - reconstruction_loss: 106.3900 - kl_loss: 7.5442\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54.6733 - reconstruction_loss: 48.9924 - kl_loss: 5.6809\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 127.3118 - reconstruction_loss: 121.6400 - kl_loss: 5.6717\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0571\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0551\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.5125 - reconstruction_loss: 114.2959 - kl_loss: 5.2166\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.7540 - reconstruction_loss: 114.5408 - kl_loss: 5.2132\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 283ms/step - kl_loss: 0.0438\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0426\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 98.0485 - reconstruction_loss: 91.1119 - kl_loss: 6.9365\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.5189 - reconstruction_loss: 87.6177 - kl_loss: 6.9012\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 311.9339\n",
      "Success in episode 1 at time step 192\n",
      "Episode 2\n",
      "[-0.5643517  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0043\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0041\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.9594 - reconstruction_loss: 53.2254 - kl_loss: 6.7340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.2119 - reconstruction_loss: 50.5216 - kl_loss: 6.6903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0103\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0097\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.7689 - reconstruction_loss: 37.8419 - kl_loss: 6.9270\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.7466 - reconstruction_loss: 36.8626 - kl_loss: 6.8840\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0111\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0102\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.7956 - reconstruction_loss: 35.1690 - kl_loss: 5.6266\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.7226 - reconstruction_loss: 21.1188 - kl_loss: 5.6038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0102\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.1966 - reconstruction_loss: 22.7318 - kl_loss: 5.4649\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.4313 - reconstruction_loss: 47.9811 - kl_loss: 5.4502\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0386\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51.7758 - reconstruction_loss: 44.6751 - kl_loss: 7.1007\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.1472 - reconstruction_loss: 48.0607 - kl_loss: 7.0865\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1185\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1179\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 169.1159 - reconstruction_loss: 158.5614 - kl_loss: 10.5545\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.0825 - reconstruction_loss: 58.5675 - kl_loss: 10.5150\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0897\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0893\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 201.5397 - reconstruction_loss: 188.4918 - kl_loss: 13.0479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 300.2755 - reconstruction_loss: 287.3145 - kl_loss: 12.9610\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0503\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0492\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 105.6637 - reconstruction_loss: 99.9072 - kl_loss: 5.7565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 89.6189 - reconstruction_loss: 83.8886 - kl_loss: 5.7303\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2107\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62.8989 - reconstruction_loss: 57.4230 - kl_loss: 5.4759\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.7785 - reconstruction_loss: 123.3012 - kl_loss: 5.4772\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0364\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0348\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 330.9020 - reconstruction_loss: 325.8539 - kl_loss: 5.0481\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.7219 - reconstruction_loss: 109.6773 - kl_loss: 5.0447\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 441ms/step - kl_loss: 0.0501\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0489\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 102.2713 - reconstruction_loss: 95.4699 - kl_loss: 6.8014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.3473 - reconstruction_loss: 105.5711 - kl_loss: 6.7762\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 475.2655\n",
      "Success in episode 2 at time step 123\n",
      "Episode 3\n",
      "[-0.4948751  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.2980 - reconstruction_loss: 18.3578 - kl_loss: 5.9402\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.9960 - reconstruction_loss: 12.0832 - kl_loss: 5.9127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0149\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.1303 - reconstruction_loss: 23.7418 - kl_loss: 5.3886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.5571 - reconstruction_loss: 24.1899 - kl_loss: 5.3672\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0501\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0510\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 50.9997 - reconstruction_loss: 45.2462 - kl_loss: 5.7536\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.1534 - reconstruction_loss: 31.4222 - kl_loss: 5.7312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0642\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.2088 - reconstruction_loss: 21.0388 - kl_loss: 6.1700\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.2147 - reconstruction_loss: 27.0660 - kl_loss: 6.1487\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0769\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0760\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 74.2202 - reconstruction_loss: 67.4351 - kl_loss: 6.7850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.7024 - reconstruction_loss: 50.9396 - kl_loss: 6.7628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0764\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0753\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.4341 - reconstruction_loss: 30.4911 - kl_loss: 7.9431\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.1540 - reconstruction_loss: 24.2354 - kl_loss: 7.9186\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0316\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 140.7113 - reconstruction_loss: 131.1833 - kl_loss: 9.5279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122.6306 - reconstruction_loss: 113.1332 - kl_loss: 9.4974\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0541\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 148.4793 - reconstruction_loss: 141.5680 - kl_loss: 6.9112\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 137.4313 - reconstruction_loss: 130.5398 - kl_loss: 6.8914\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0334\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0323\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 137.6882 - reconstruction_loss: 132.4372 - kl_loss: 5.2509\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 131.7598 - reconstruction_loss: 126.5170 - kl_loss: 5.2428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0378\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 149.2571 - reconstruction_loss: 144.1505 - kl_loss: 5.1066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.5501 - reconstruction_loss: 101.4518 - kl_loss: 5.0984\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0230\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0225\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.4845 - reconstruction_loss: 87.7719 - kl_loss: 4.7126\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 157.7511 - reconstruction_loss: 153.0423 - kl_loss: 4.7089\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0316\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0307\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 82.3453 - reconstruction_loss: 76.2828 - kl_loss: 6.0626\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.3386 - reconstruction_loss: 77.2927 - kl_loss: 6.0459\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 433.8657\n",
      "Success in episode 3 at time step 137\n",
      "Episode 4\n",
      "[-0.5550872  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0059\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0057\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.9624 - reconstruction_loss: 7.2136 - kl_loss: 5.7488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.9475 - reconstruction_loss: 9.2235 - kl_loss: 5.7240\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0115\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21.5988 - reconstruction_loss: 15.3929 - kl_loss: 6.2059\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.7488 - reconstruction_loss: 24.5681 - kl_loss: 6.1807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.2051 - reconstruction_loss: 29.2513 - kl_loss: 5.9538\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.3943 - reconstruction_loss: 38.4616 - kl_loss: 5.9327\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0105\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 44.1571 - reconstruction_loss: 38.9454 - kl_loss: 5.2117\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.0879 - reconstruction_loss: 39.8892 - kl_loss: 5.1987\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0128\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.4985 - reconstruction_loss: 46.7378 - kl_loss: 4.7607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.1897 - reconstruction_loss: 66.4356 - kl_loss: 4.7541\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0043\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0041\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 184.2473 - reconstruction_loss: 179.6333 - kl_loss: 4.6140\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.1066 - reconstruction_loss: 83.5049 - kl_loss: 4.6016\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0090\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0088\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65.3932 - reconstruction_loss: 60.0918 - kl_loss: 5.3015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.1516 - reconstruction_loss: 58.8627 - kl_loss: 5.2889\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 767.3593\n",
      "Success in episode 4 at time step 76\n",
      "Episode 5\n",
      "[-0.57727164  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0024\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0022\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.9876 - reconstruction_loss: 18.4215 - kl_loss: 5.5661\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57.2271 - reconstruction_loss: 51.6767 - kl_loss: 5.5503\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0091\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0088\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.3628 - reconstruction_loss: 55.2007 - kl_loss: 6.1620\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.4746 - reconstruction_loss: 35.3314 - kl_loss: 6.1433\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0025\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.7437 - reconstruction_loss: 63.8983 - kl_loss: 5.8454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.4542 - reconstruction_loss: 65.6264 - kl_loss: 5.8278\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0038\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0039\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 74.3896 - reconstruction_loss: 69.1854 - kl_loss: 5.2042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 96.4216 - reconstruction_loss: 91.2326 - kl_loss: 5.1890\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0053\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0050\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.0920 - reconstruction_loss: 34.2821 - kl_loss: 4.8099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.3485 - reconstruction_loss: 84.5497 - kl_loss: 4.7988\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0077\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0076\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.4829 - reconstruction_loss: 31.8757 - kl_loss: 4.6072\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.3302 - reconstruction_loss: 27.7309 - kl_loss: 4.5993\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 46.5226 - reconstruction_loss: 41.2913 - kl_loss: 5.2313\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.6390 - reconstruction_loss: 59.4176 - kl_loss: 5.2214\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0467\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0465\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 32.5417 - reconstruction_loss: 26.5040 - kl_loss: 6.0376\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.7685 - reconstruction_loss: 18.7430 - kl_loss: 6.0255\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0603\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0598\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.6486 - reconstruction_loss: 84.9318 - kl_loss: 8.7168\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 74.8060 - reconstruction_loss: 66.1054 - kl_loss: 8.7006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0103\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 130.5144 - reconstruction_loss: 124.0407 - kl_loss: 6.4738\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.7338 - reconstruction_loss: 77.2834 - kl_loss: 6.4504\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0420\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0416\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.9419 - reconstruction_loss: 79.5795 - kl_loss: 5.3624\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 134.9620 - reconstruction_loss: 129.6139 - kl_loss: 5.3482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2988\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2860\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.2368 - reconstruction_loss: 116.1568 - kl_loss: 5.0800\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 977us/step - loss: 92.4437 - reconstruction_loss: 87.3690 - kl_loss: 5.0747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0268\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 220.8206 - reconstruction_loss: 216.1324 - kl_loss: 4.6882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 192.4197 - reconstruction_loss: 187.7318 - kl_loss: 4.6879\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0334\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0318\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 79.0956 - reconstruction_loss: 73.6258 - kl_loss: 5.4698\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 76.0868 - reconstruction_loss: 70.6252 - kl_loss: 5.4616\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 369.7274\n",
      "Success in episode 5 at time step 161\n",
      "Episode 6\n",
      "[-0.43242854  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0110\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.5393 - reconstruction_loss: 11.2823 - kl_loss: 5.2571\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 937us/step - loss: 9.6407 - reconstruction_loss: 4.4010 - kl_loss: 5.2397\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.8901 - reconstruction_loss: 4.8578 - kl_loss: 5.0323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.0918 - reconstruction_loss: 9.0737 - kl_loss: 5.0181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0085\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0081\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.3273 - reconstruction_loss: 29.6314 - kl_loss: 6.6959\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.1476 - reconstruction_loss: 33.4671 - kl_loss: 6.6805\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.4878 - reconstruction_loss: 62.3866 - kl_loss: 6.1012\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.8841 - reconstruction_loss: 86.7931 - kl_loss: 6.0910\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0048\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 140.5869 - reconstruction_loss: 135.3297 - kl_loss: 5.2573\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 128.9626 - reconstruction_loss: 123.7141 - kl_loss: 5.2485\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0247\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.6935 - reconstruction_loss: 78.7840 - kl_loss: 4.9095\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.5696 - reconstruction_loss: 72.6596 - kl_loss: 4.9100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1107\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1102\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 190.1912 - reconstruction_loss: 185.8868 - kl_loss: 4.3044\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 138.2963 - reconstruction_loss: 133.9970 - kl_loss: 4.2994\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.0262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0249\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 107.9891 - reconstruction_loss: 102.7624 - kl_loss: 5.2267\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 93.9434 - reconstruction_loss: 88.7310 - kl_loss: 5.2124\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 664.7854\n",
      "Success in episode 6 at time step 87\n",
      "Episode 7\n",
      "[-0.453947  0.      ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0236\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0227\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23.1814 - reconstruction_loss: 18.1753 - kl_loss: 5.0061\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.8205 - reconstruction_loss: 1.8336 - kl_loss: 4.9870\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0033\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.5490 - reconstruction_loss: 8.4172 - kl_loss: 5.1318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.4644 - reconstruction_loss: 12.3485 - kl_loss: 5.1159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6174e-04\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4601e-04\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.7382 - reconstruction_loss: 21.4281 - kl_loss: 5.3101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 961us/step - loss: 35.6176 - reconstruction_loss: 30.3204 - kl_loss: 5.2972\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0021\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0020\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 47.4498 - reconstruction_loss: 42.4920 - kl_loss: 4.9578\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.9762 - reconstruction_loss: 55.0298 - kl_loss: 4.9464\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38.3848 - reconstruction_loss: 34.0374 - kl_loss: 4.3474\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.8313 - reconstruction_loss: 32.4926 - kl_loss: 4.3388\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0043\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54.4150 - reconstruction_loss: 50.2584 - kl_loss: 4.1567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.1616 - reconstruction_loss: 70.0131 - kl_loss: 4.1484\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0259\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0256\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63.3108 - reconstruction_loss: 57.2158 - kl_loss: 6.0950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.6953 - reconstruction_loss: 33.6092 - kl_loss: 6.0860\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0762\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 94.6108 - reconstruction_loss: 86.4878 - kl_loss: 8.1230\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.9489 - reconstruction_loss: 44.8351 - kl_loss: 8.1138\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0022\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0022\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 88.2831 - reconstruction_loss: 82.3874 - kl_loss: 5.8957\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 127.3725 - reconstruction_loss: 121.4853 - kl_loss: 5.8872\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0608\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0608\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 119.8219 - reconstruction_loss: 114.7382 - kl_loss: 5.0837\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.3606 - reconstruction_loss: 103.2891 - kl_loss: 5.0715\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0399\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 115.8616 - reconstruction_loss: 111.2722 - kl_loss: 4.5893\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.6935 - reconstruction_loss: 70.1101 - kl_loss: 4.5834\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1191\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1134\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75.8722 - reconstruction_loss: 71.3952 - kl_loss: 4.4770\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.1679 - reconstruction_loss: 50.6882 - kl_loss: 4.4797\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0367\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 91.2302 - reconstruction_loss: 86.4897 - kl_loss: 4.7406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.0130 - reconstruction_loss: 74.2653 - kl_loss: 4.7477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0571\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0573\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 144.1252 - reconstruction_loss: 139.9374 - kl_loss: 4.1878\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 200.1913 - reconstruction_loss: 195.9992 - kl_loss: 4.1921\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0244\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 72.2593 - reconstruction_loss: 67.2231 - kl_loss: 5.0362\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.1697 - reconstruction_loss: 72.1331 - kl_loss: 5.0365\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 344.0837\n",
      "Success in episode 7 at time step 170\n",
      "Episode 8\n",
      "[-0.49837318  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0114\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.1982 - reconstruction_loss: 6.5336 - kl_loss: 4.6646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6456 - reconstruction_loss: 1.9908 - kl_loss: 4.6548\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.6548 - reconstruction_loss: 23.0534 - kl_loss: 5.6015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.2899 - reconstruction_loss: 30.6958 - kl_loss: 5.5942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0072\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.0898 - reconstruction_loss: 106.8305 - kl_loss: 5.2594\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.6509 - reconstruction_loss: 53.4025 - kl_loss: 5.2484\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0072\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64.5943 - reconstruction_loss: 59.9857 - kl_loss: 4.6086\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.3111 - reconstruction_loss: 26.7080 - kl_loss: 4.6031\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0038\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.8538 - reconstruction_loss: 65.2689 - kl_loss: 4.5848\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.8717 - reconstruction_loss: 79.2842 - kl_loss: 4.5875\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0079\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 113.1698 - reconstruction_loss: 108.6306 - kl_loss: 4.5392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 105.9786 - reconstruction_loss: 101.4335 - kl_loss: 4.5451\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0094\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0092\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 71.8499 - reconstruction_loss: 67.1351 - kl_loss: 4.7148\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 88.3258 - reconstruction_loss: 83.6138 - kl_loss: 4.7120\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 713.0142\n",
      "Success in episode 8 at time step 82\n",
      "Episode 9\n",
      "[-0.53649664  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0195\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.8127 - reconstruction_loss: 1.2223 - kl_loss: 4.5904\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.4419 - reconstruction_loss: 9.8639 - kl_loss: 4.5780\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0048\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.9422 - reconstruction_loss: 17.7122 - kl_loss: 5.2299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.5305 - reconstruction_loss: 23.3134 - kl_loss: 5.2171\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0130\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.4064 - reconstruction_loss: 43.7002 - kl_loss: 4.7062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.4938 - reconstruction_loss: 55.7967 - kl_loss: 4.6970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0064\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 31.7699 - reconstruction_loss: 27.5088 - kl_loss: 4.2610\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.8483 - reconstruction_loss: 8.5945 - kl_loss: 4.2538\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0048\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0047\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0700 - reconstruction_loss: 2.9874 - kl_loss: 4.0825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 917us/step - loss: 12.3317 - reconstruction_loss: 8.2550 - kl_loss: 4.0767\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0367\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0357\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.9950 - reconstruction_loss: 66.0621 - kl_loss: 3.9329\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.3820 - reconstruction_loss: 68.4536 - kl_loss: 3.9284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0119\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 88.7917 - reconstruction_loss: 84.6693 - kl_loss: 4.1224\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.1996 - reconstruction_loss: 88.0837 - kl_loss: 4.1159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0144\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 196.0705 - reconstruction_loss: 190.9060 - kl_loss: 5.1645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 152.9259 - reconstruction_loss: 147.7695 - kl_loss: 5.1565\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0532\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0520\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 253.9135 - reconstruction_loss: 246.6672 - kl_loss: 7.2463\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 186.8202 - reconstruction_loss: 179.5925 - kl_loss: 7.2277\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1751\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1728\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 190.9221 - reconstruction_loss: 179.5781 - kl_loss: 11.3439\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 320.3254 - reconstruction_loss: 308.9982 - kl_loss: 11.3273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1357\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 222.9648 - reconstruction_loss: 211.1192 - kl_loss: 11.8455\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 251.1265 - reconstruction_loss: 239.3189 - kl_loss: 11.8076\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0775\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0784\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 88.5845 - reconstruction_loss: 83.0722 - kl_loss: 5.5123\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.3239 - reconstruction_loss: 88.8154 - kl_loss: 5.5085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0860\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.0715 - reconstruction_loss: 69.6081 - kl_loss: 4.4634\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.1395 - reconstruction_loss: 36.6719 - kl_loss: 4.4676\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0486\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0444\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68.4106 - reconstruction_loss: 63.5042 - kl_loss: 4.9064\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.3521 - reconstruction_loss: 50.4415 - kl_loss: 4.9106\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0137\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0140\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 209.2319 - reconstruction_loss: 205.2035 - kl_loss: 4.0285\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 199.7673 - reconstruction_loss: 195.7419 - kl_loss: 4.0254\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0358\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 118.8916 - reconstruction_loss: 113.3931 - kl_loss: 5.4985\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118.7578 - reconstruction_loss: 113.2819 - kl_loss: 5.4759\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 322.2954\n",
      "Success in episode 9 at time step 181\n",
      "Episode 10\n",
      "[-0.46031436  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0186\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.4328 - reconstruction_loss: 3.0467 - kl_loss: 4.3861\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.0623 - reconstruction_loss: 8.7030 - kl_loss: 4.3593\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0264\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0264\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.1280 - reconstruction_loss: 74.5926 - kl_loss: 5.5354\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.5464 - reconstruction_loss: 49.0369 - kl_loss: 5.5095\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0084\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0082\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.3637 - reconstruction_loss: 54.2361 - kl_loss: 5.1276\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.3705 - reconstruction_loss: 47.2568 - kl_loss: 5.1137\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0136\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.2064 - reconstruction_loss: 32.1254 - kl_loss: 4.0810\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.9131 - reconstruction_loss: 19.8411 - kl_loss: 4.0720\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0202\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.7907 - reconstruction_loss: 14.9449 - kl_loss: 3.8458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.6138 - reconstruction_loss: 25.7765 - kl_loss: 3.8373\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0153\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.7393 - reconstruction_loss: 66.1017 - kl_loss: 3.6375\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 81.4547 - reconstruction_loss: 77.8244 - kl_loss: 3.6302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0144\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0133\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 150.2516 - reconstruction_loss: 146.5194 - kl_loss: 3.7322\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 167.1265 - reconstruction_loss: 163.4051 - kl_loss: 3.7214\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0151\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73.0767 - reconstruction_loss: 68.8388 - kl_loss: 4.2379\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.5800 - reconstruction_loss: 55.3548 - kl_loss: 4.2253\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 664.1551\n",
      "Success in episode 10 at time step 85\n",
      "Episode 11\n",
      "[-0.5360863  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0275\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.8368 - reconstruction_loss: 6.0731 - kl_loss: 3.7637\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6031 - reconstruction_loss: 2.8557 - kl_loss: 3.7474\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0111\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.7378 - reconstruction_loss: 27.2435 - kl_loss: 3.4942\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 940us/step - loss: 19.1537 - reconstruction_loss: 15.6738 - kl_loss: 3.4799\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.2755 - reconstruction_loss: 18.7110 - kl_loss: 3.5645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.4385 - reconstruction_loss: 28.8878 - kl_loss: 3.5507\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0202\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.9494 - reconstruction_loss: 43.0991 - kl_loss: 3.8503\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.4212 - reconstruction_loss: 35.5841 - kl_loss: 3.8371\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0307\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.2826 - reconstruction_loss: 57.5256 - kl_loss: 4.7570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 107.2808 - reconstruction_loss: 102.5364 - kl_loss: 4.7445\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0248\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.9008 - reconstruction_loss: 71.2211 - kl_loss: 4.6798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 941us/step - loss: 30.0496 - reconstruction_loss: 25.3843 - kl_loss: 4.6653\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0277\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21.3149 - reconstruction_loss: 16.9632 - kl_loss: 4.3517\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1530 - reconstruction_loss: 2.8138 - kl_loss: 4.3392\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0293\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0282\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.9177 - reconstruction_loss: 15.0149 - kl_loss: 3.9028\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.7532 - reconstruction_loss: 3.8615 - kl_loss: 3.8917\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.1851 - reconstruction_loss: 36.8048 - kl_loss: 4.3803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.6989 - reconstruction_loss: 25.3280 - kl_loss: 4.3709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5270e-04\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.5648e-04\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.0529 - reconstruction_loss: 16.7758 - kl_loss: 4.2771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.1120 - reconstruction_loss: 61.8411 - kl_loss: 4.2709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0013\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0012\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.6718 - reconstruction_loss: 51.6312 - kl_loss: 4.0406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54.8573 - reconstruction_loss: 50.8223 - kl_loss: 4.0350\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0100\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0099\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.7901 - reconstruction_loss: 38.9549 - kl_loss: 3.8352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.2454 - reconstruction_loss: 34.4144 - kl_loss: 3.8310\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0290\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23.7245 - reconstruction_loss: 20.1011 - kl_loss: 3.6234\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.5226 - reconstruction_loss: 24.9031 - kl_loss: 3.6195\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0281\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0275\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.1815 - reconstruction_loss: 20.7902 - kl_loss: 3.3913\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.2703 - reconstruction_loss: 38.8832 - kl_loss: 3.3871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0833\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.2627 - reconstruction_loss: 45.4144 - kl_loss: 4.8483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.8265 - reconstruction_loss: 23.9837 - kl_loss: 4.8428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0640\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0622\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.1197 - reconstruction_loss: 75.3493 - kl_loss: 7.7703\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.2480 - reconstruction_loss: 69.4792 - kl_loss: 7.7688\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1102\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68.8336 - reconstruction_loss: 63.3319 - kl_loss: 5.5017\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 930us/step - loss: 94.8823 - reconstruction_loss: 89.3735 - kl_loss: 5.5088\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0312\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0289\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 135.0609 - reconstruction_loss: 130.6734 - kl_loss: 4.3875\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.0189 - reconstruction_loss: 107.6270 - kl_loss: 4.3920\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0540\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 114.4665 - reconstruction_loss: 109.6756 - kl_loss: 4.7909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100.9001 - reconstruction_loss: 96.1062 - kl_loss: 4.7940\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0330\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0307\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 189.7386 - reconstruction_loss: 185.8564 - kl_loss: 3.8823\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 176.8998 - reconstruction_loss: 173.0105 - kl_loss: 3.8893\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0245\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0234\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.4365 - reconstruction_loss: 55.1860 - kl_loss: 4.2504\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.9644 - reconstruction_loss: 52.7206 - kl_loss: 4.2438\n",
      "2/2 [==============================] - 0s 962us/step - loss: 243.1666\n",
      "Success in episode 11 at time step 243\n",
      "Episode 12\n",
      "[-0.4484389  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0230\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.5963 - reconstruction_loss: 7.9765 - kl_loss: 3.6198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.2869 - reconstruction_loss: 8.6836 - kl_loss: 3.6033\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0091\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.0982 - reconstruction_loss: 40.8173 - kl_loss: 4.2809\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.3844 - reconstruction_loss: 17.1155 - kl_loss: 4.2689\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.4990 - reconstruction_loss: 23.1396 - kl_loss: 3.3594\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.9771 - reconstruction_loss: 11.6268 - kl_loss: 3.3503\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.0988 - reconstruction_loss: 40.8716 - kl_loss: 3.2272\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 64.0284 - reconstruction_loss: 60.8062 - kl_loss: 3.2223\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89.0433 - reconstruction_loss: 85.7774 - kl_loss: 3.2659\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.2901 - reconstruction_loss: 40.0359 - kl_loss: 3.2542\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0031\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 103.1494 - reconstruction_loss: 99.3958 - kl_loss: 3.7536\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.2410 - reconstruction_loss: 113.5009 - kl_loss: 3.7402\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0247\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0243\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 148.0338 - reconstruction_loss: 142.1086 - kl_loss: 5.9252\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 113.3106 - reconstruction_loss: 107.4054 - kl_loss: 5.9052\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1045\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.9769 - reconstruction_loss: 162.5454 - kl_loss: 11.4314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 179.7450 - reconstruction_loss: 168.3202 - kl_loss: 11.4248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3761\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.8297 - reconstruction_loss: 117.5218 - kl_loss: 8.3078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 146.0733 - reconstruction_loss: 137.7710 - kl_loss: 8.3023\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0092\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102.2214 - reconstruction_loss: 97.7901 - kl_loss: 4.4314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 125.3117 - reconstruction_loss: 120.8745 - kl_loss: 4.4372\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.0187\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.0181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 83.4462 - reconstruction_loss: 78.5760 - kl_loss: 4.8702\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 88.5001 - reconstruction_loss: 83.6180 - kl_loss: 4.8821\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0460\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0446\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 97.3713 - reconstruction_loss: 92.4413 - kl_loss: 4.9300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.1460 - reconstruction_loss: 78.2161 - kl_loss: 4.9299\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 415.3657\n",
      "Success in episode 12 at time step 143\n",
      "Episode 13\n",
      "[-0.49207196  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0337\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0332\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.7799 - reconstruction_loss: 15.2967 - kl_loss: 3.4833\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.4476 - reconstruction_loss: 6.9802 - kl_loss: 3.4673\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0188\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0193\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.5881 - reconstruction_loss: 20.2628 - kl_loss: 4.3252\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.6998 - reconstruction_loss: 30.3860 - kl_loss: 4.3138\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0113\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0110\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.0502 - reconstruction_loss: 30.6661 - kl_loss: 3.3841\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.1212 - reconstruction_loss: 25.7455 - kl_loss: 3.3757\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0083\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0087\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.8156 - reconstruction_loss: 17.4767 - kl_loss: 3.3390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.3025 - reconstruction_loss: 9.9698 - kl_loss: 3.3327\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0061\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.4297 - reconstruction_loss: 41.1569 - kl_loss: 3.2728\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.0571 - reconstruction_loss: 33.7836 - kl_loss: 3.2735\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0163\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0165\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 140.1824 - reconstruction_loss: 137.1213 - kl_loss: 3.0611\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 112.2938 - reconstruction_loss: 109.2377 - kl_loss: 3.0560\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0786\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 196.8901 - reconstruction_loss: 192.9879 - kl_loss: 3.9022\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 194.9990 - reconstruction_loss: 191.1142 - kl_loss: 3.8847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1023\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0999\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 200.2661 - reconstruction_loss: 195.7487 - kl_loss: 4.5174\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 159.8145 - reconstruction_loss: 155.3296 - kl_loss: 4.4849\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0766\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0753\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 136.3348 - reconstruction_loss: 130.5146 - kl_loss: 5.8203\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: 104.2730 - reconstruction_loss: 98.4808 - kl_loss: 5.7922\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2337\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2301\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 270.4492 - reconstruction_loss: 259.0867 - kl_loss: 11.3626\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 110.7447 - reconstruction_loss: 99.4017 - kl_loss: 11.3431\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1666\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1684\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 187.4597 - reconstruction_loss: 177.9778 - kl_loss: 9.4819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 194.0547 - reconstruction_loss: 184.5522 - kl_loss: 9.5025\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0344\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.3438 - reconstruction_loss: 127.8630 - kl_loss: 4.4808\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 972us/step - loss: 104.7174 - reconstruction_loss: 100.2328 - kl_loss: 4.4847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0201\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 76.7839 - reconstruction_loss: 72.6053 - kl_loss: 4.1785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.5676 - reconstruction_loss: 56.3846 - kl_loss: 4.1830\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0171\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.3528 - reconstruction_loss: 21.6988 - kl_loss: 3.6539\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.3035 - reconstruction_loss: 39.6464 - kl_loss: 3.6571\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0148\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0142\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69.5887 - reconstruction_loss: 66.3226 - kl_loss: 3.2661\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77.5038 - reconstruction_loss: 74.2340 - kl_loss: 3.2698\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0571\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0557\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 87.5592 - reconstruction_loss: 82.9537 - kl_loss: 4.6055\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.3208 - reconstruction_loss: 87.7118 - kl_loss: 4.6090\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 311.6843\n",
      "Success in episode 13 at time step 187\n",
      "Episode 14\n",
      "[-0.45417857  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0493\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0497\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7201 - reconstruction_loss: 0.7787 - kl_loss: 2.9414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.9776 - reconstruction_loss: 7.0490 - kl_loss: 2.9286\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0109\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.7527 - reconstruction_loss: 5.9525 - kl_loss: 2.8001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 963us/step - loss: 10.4663 - reconstruction_loss: 7.6786 - kl_loss: 2.7877\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0291\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.0905 - reconstruction_loss: 34.3832 - kl_loss: 2.7073\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.7286 - reconstruction_loss: 15.0319 - kl_loss: 2.6967\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0578\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0572\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 47.9285 - reconstruction_loss: 43.5356 - kl_loss: 4.3929\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.8899 - reconstruction_loss: 33.5058 - kl_loss: 4.3841\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0843\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 46.9899 - reconstruction_loss: 40.3999 - kl_loss: 6.5900\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.1488 - reconstruction_loss: 39.5499 - kl_loss: 6.5989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0687\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0698\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 116.5680 - reconstruction_loss: 110.3958 - kl_loss: 6.1722\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 127.7790 - reconstruction_loss: 121.5898 - kl_loss: 6.1892\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0236\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0215\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 116.7391 - reconstruction_loss: 112.0387 - kl_loss: 4.7005\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 99.9711 - reconstruction_loss: 95.2626 - kl_loss: 4.7085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.0264 - reconstruction_loss: 113.0262 - kl_loss: 6.0001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106.6904 - reconstruction_loss: 100.6643 - kl_loss: 6.0261\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.0360\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75.1308 - reconstruction_loss: 70.6242 - kl_loss: 4.5066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.7738 - reconstruction_loss: 81.2561 - kl_loss: 4.5177\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 553.2104\n",
      "Success in episode 14 at time step 107\n",
      "Episode 15\n",
      "[-0.42083767  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0295\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.4096 - reconstruction_loss: 7.5385 - kl_loss: 2.8711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0216 - reconstruction_loss: 1.1642 - kl_loss: 2.8573\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0201\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.4947 - reconstruction_loss: 31.8970 - kl_loss: 4.5977\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 900us/step - loss: 47.7541 - reconstruction_loss: 43.1583 - kl_loss: 4.5958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0816\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.1894 - reconstruction_loss: 31.6867 - kl_loss: 4.5028\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.2323 - reconstruction_loss: 37.7256 - kl_loss: 4.5068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0684\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0610\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 132.3255 - reconstruction_loss: 127.2046 - kl_loss: 5.1209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94.0444 - reconstruction_loss: 88.9082 - kl_loss: 5.1363\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0202\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 91.4344 - reconstruction_loss: 86.8836 - kl_loss: 4.5508\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112.6182 - reconstruction_loss: 108.0532 - kl_loss: 4.5650\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0047\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.0672 - reconstruction_loss: 74.5078 - kl_loss: 5.5593\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83.8010 - reconstruction_loss: 78.2256 - kl_loss: 5.5754\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0225\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0209\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.2803 - reconstruction_loss: 76.7232 - kl_loss: 4.5571\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.1747 - reconstruction_loss: 66.6035 - kl_loss: 4.5711\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 710.6672\n",
      "Success in episode 15 at time step 84\n",
      "Episode 16\n",
      "[-0.47743148  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0150\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0149\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.1597 - reconstruction_loss: 6.0389 - kl_loss: 3.1208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.4698 - reconstruction_loss: 13.3523 - kl_loss: 3.1176\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0037\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.5719 - reconstruction_loss: 44.1739 - kl_loss: 4.3980\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.2555 - reconstruction_loss: 68.8520 - kl_loss: 4.4034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.7786 - reconstruction_loss: 65.1587 - kl_loss: 4.6199\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.7339 - reconstruction_loss: 27.1018 - kl_loss: 4.6322\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.5081 - reconstruction_loss: 55.2791 - kl_loss: 4.2290\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.8589 - reconstruction_loss: 63.6178 - kl_loss: 4.2411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0382\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.4047 - reconstruction_loss: 34.2805 - kl_loss: 5.1243\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.3465 - reconstruction_loss: 54.2126 - kl_loss: 5.1339\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0222\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.2989 - reconstruction_loss: 116.2846 - kl_loss: 5.0143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108.0055 - reconstruction_loss: 102.9686 - kl_loss: 5.0369\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0176\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0165\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 70.0965 - reconstruction_loss: 65.7041 - kl_loss: 4.3924\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.7677 - reconstruction_loss: 62.3631 - kl_loss: 4.4046\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 765.1698\n",
      "Success in episode 16 at time step 78\n",
      "Episode 17\n",
      "[-0.43173233  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0237\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.6160 - reconstruction_loss: 18.7832 - kl_loss: 2.8328\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.5944 - reconstruction_loss: 16.7719 - kl_loss: 2.8224\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0135\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.9049 - reconstruction_loss: 20.1932 - kl_loss: 4.7116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.8323 - reconstruction_loss: 51.1177 - kl_loss: 4.7146\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0422\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.4509 - reconstruction_loss: 38.2936 - kl_loss: 5.1573\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 815us/step - loss: 83.5070 - reconstruction_loss: 78.3343 - kl_loss: 5.1727\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0103\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.4964 - reconstruction_loss: 51.7952 - kl_loss: 4.7013\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.4206 - reconstruction_loss: 50.7073 - kl_loss: 4.7133\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0337\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58.7474 - reconstruction_loss: 53.2830 - kl_loss: 5.4644\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.6876 - reconstruction_loss: 49.2127 - kl_loss: 5.4749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 79.7964 - reconstruction_loss: 74.8508 - kl_loss: 4.9455\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.1302 - reconstruction_loss: 55.1584 - kl_loss: 4.9718\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0225\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63.2044 - reconstruction_loss: 58.5762 - kl_loss: 4.6282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.0861 - reconstruction_loss: 57.4455 - kl_loss: 4.6406\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 764.4345\n",
      "Success in episode 17 at time step 76\n",
      "Episode 18\n",
      "[-0.47168946  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0272\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.8922 - reconstruction_loss: 23.1575 - kl_loss: 2.7347\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.5918 - reconstruction_loss: 5.8663 - kl_loss: 2.7255\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0109\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.4099 - reconstruction_loss: 17.0472 - kl_loss: 3.3627\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.8426 - reconstruction_loss: 11.4838 - kl_loss: 3.3587\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0570\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.9196 - reconstruction_loss: 12.5661 - kl_loss: 5.3535\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.2031 - reconstruction_loss: 47.8361 - kl_loss: 5.3670\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0123\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0124\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 43.1023 - reconstruction_loss: 38.7251 - kl_loss: 4.3773\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.7218 - reconstruction_loss: 48.3385 - kl_loss: 4.3834\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0576\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.6806 - reconstruction_loss: 33.4655 - kl_loss: 5.2151\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.9610 - reconstruction_loss: 45.7405 - kl_loss: 5.2206\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0473\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0433\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 123.7664 - reconstruction_loss: 119.1770 - kl_loss: 4.5894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 999us/step - loss: 91.9544 - reconstruction_loss: 87.3397 - kl_loss: 4.6147\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0631\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61.2665 - reconstruction_loss: 56.9629 - kl_loss: 4.3036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52.6546 - reconstruction_loss: 48.3455 - kl_loss: 4.3091\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 765.3609\n",
      "Success in episode 18 at time step 74\n",
      "Episode 19\n",
      "[-0.47502458  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0670\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.6023 - reconstruction_loss: 9.1721 - kl_loss: 2.4302\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.5576 - reconstruction_loss: 5.1395 - kl_loss: 2.4181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.9198 - reconstruction_loss: 70.4255 - kl_loss: 3.4944\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.2873 - reconstruction_loss: 10.8015 - kl_loss: 3.4858\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0053\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0056\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 101.8828 - reconstruction_loss: 96.7502 - kl_loss: 5.1327\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.6989 - reconstruction_loss: 27.5652 - kl_loss: 5.1337\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0399\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0394\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.3727 - reconstruction_loss: 30.5768 - kl_loss: 4.7959\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 58.9434 - reconstruction_loss: 54.1431 - kl_loss: 4.8004\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0172\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0187\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.4579 - reconstruction_loss: 57.4304 - kl_loss: 5.0275\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.4802 - reconstruction_loss: 37.4581 - kl_loss: 5.0221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0815\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.0800 - reconstruction_loss: 73.1169 - kl_loss: 5.9631\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.8479 - reconstruction_loss: 42.8619 - kl_loss: 5.9860\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0300\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.1198 - reconstruction_loss: 46.6523 - kl_loss: 4.4675\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.3676 - reconstruction_loss: 62.8946 - kl_loss: 4.4730\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 763.4880\n",
      "Success in episode 19 at time step 77\n",
      "Episode 20\n",
      "[-0.5945555  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1375\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1361\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.6847 - reconstruction_loss: 6.1965 - kl_loss: 2.4882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.9469 - reconstruction_loss: 4.4730 - kl_loss: 2.4739\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0383\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0343\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.5532 - reconstruction_loss: 30.7433 - kl_loss: 2.8099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.5489 - reconstruction_loss: 37.7435 - kl_loss: 2.8054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0464\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.7828 - reconstruction_loss: 33.2858 - kl_loss: 2.4970\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51.9821 - reconstruction_loss: 49.4872 - kl_loss: 2.4949\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0290\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80.8426 - reconstruction_loss: 78.4240 - kl_loss: 2.4186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.9227 - reconstruction_loss: 79.5103 - kl_loss: 2.4124\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1710\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1688\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 92.1182 - reconstruction_loss: 88.3426 - kl_loss: 3.7756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.8826 - reconstruction_loss: 45.1156 - kl_loss: 3.7670\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1726\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1690\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 81.5558 - reconstruction_loss: 68.8612 - kl_loss: 12.6947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 144.3232 - reconstruction_loss: 131.6304 - kl_loss: 12.6929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6799\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 106.3167 - reconstruction_loss: 94.5131 - kl_loss: 11.8035\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.2194 - reconstruction_loss: 59.3938 - kl_loss: 11.8257\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1491\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 55.3003 - reconstruction_loss: 50.5971 - kl_loss: 4.7032\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58.2859 - reconstruction_loss: 53.5864 - kl_loss: 4.6995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1808\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.2470 - reconstruction_loss: 82.1441 - kl_loss: 8.1030\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.9752 - reconstruction_loss: 31.8501 - kl_loss: 8.1251\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.1765\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1688\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77.8471 - reconstruction_loss: 72.1369 - kl_loss: 5.7102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73.7241 - reconstruction_loss: 68.0055 - kl_loss: 5.7186\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 497.2556\n",
      "Success in episode 20 at time step 118\n",
      "Episode 21\n",
      "[-0.48261636  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0694\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0689\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1244 - reconstruction_loss: 6.1248 - kl_loss: 1.9996\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.6504 - reconstruction_loss: 13.6657 - kl_loss: 1.9847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0962\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1026\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.7539 - reconstruction_loss: 5.3866 - kl_loss: 2.3673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.1607 - reconstruction_loss: 13.8108 - kl_loss: 2.3499\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2804\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.8904 - reconstruction_loss: 6.6345 - kl_loss: 4.2558\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.6658 - reconstruction_loss: 9.4200 - kl_loss: 4.2458\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4459\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.0713 - reconstruction_loss: 27.2667 - kl_loss: 5.8046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.2378 - reconstruction_loss: 19.4339 - kl_loss: 5.8039\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6004\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 61.6540 - reconstruction_loss: 57.4652 - kl_loss: 4.1887\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.3880 - reconstruction_loss: 30.1978 - kl_loss: 4.1903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1382\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 32.6115 - reconstruction_loss: 27.2086 - kl_loss: 5.4029\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.7500 - reconstruction_loss: 40.3443 - kl_loss: 5.4056\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0390\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58.6778 - reconstruction_loss: 51.6341 - kl_loss: 7.0437\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 69.8213 - reconstruction_loss: 62.7513 - kl_loss: 7.0701\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0846\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0740\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 55.5623 - reconstruction_loss: 51.0667 - kl_loss: 4.4956\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.9287 - reconstruction_loss: 56.4255 - kl_loss: 4.5032\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 661.6633\n",
      "Success in episode 21 at time step 88\n",
      "Episode 22\n",
      "[-0.47530818  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0782\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0785\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5311 - reconstruction_loss: 1.5253 - kl_loss: 2.0058\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1130 - reconstruction_loss: 4.1238 - kl_loss: 1.9892\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0053\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.6037 - reconstruction_loss: 46.9918 - kl_loss: 3.6119\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.3591 - reconstruction_loss: 21.7560 - kl_loss: 3.6030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0215\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0243\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.0454 - reconstruction_loss: 13.3176 - kl_loss: 4.7278\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.5555 - reconstruction_loss: 66.8269 - kl_loss: 4.7287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0896\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0982\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.9183 - reconstruction_loss: 20.7115 - kl_loss: 4.2068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44.0054 - reconstruction_loss: 39.7927 - kl_loss: 4.2127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4199\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 56.7593 - reconstruction_loss: 53.0872 - kl_loss: 3.6721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74.4131 - reconstruction_loss: 70.7336 - kl_loss: 3.6795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0041\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9212\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.8191 - reconstruction_loss: 8.5910 - kl_loss: 3.2281\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.5990 - reconstruction_loss: 22.3688 - kl_loss: 3.2301\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2808\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.3777 - reconstruction_loss: 19.6358 - kl_loss: 2.7418\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.2971 - reconstruction_loss: 6.5577 - kl_loss: 2.7394\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0536\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.1923 - reconstruction_loss: 8.4135 - kl_loss: 1.7788\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8099 - reconstruction_loss: 3.0362 - kl_loss: 1.7737\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0568\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.9191 - reconstruction_loss: 8.6803 - kl_loss: 2.2387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 33.2102 - reconstruction_loss: 30.9755 - kl_loss: 2.2347\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.4164 - reconstruction_loss: 8.4600 - kl_loss: 1.9563\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7541 - reconstruction_loss: 1.8028 - kl_loss: 1.9512\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0158\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.5191 - reconstruction_loss: 14.1551 - kl_loss: 2.3641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.4698 - reconstruction_loss: 16.1099 - kl_loss: 2.3599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0168\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.1144 - reconstruction_loss: 31.7929 - kl_loss: 2.3215\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.0195 - reconstruction_loss: 25.7034 - kl_loss: 2.3160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0638\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66.7297 - reconstruction_loss: 64.7057 - kl_loss: 2.0239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.4421 - reconstruction_loss: 39.4228 - kl_loss: 2.0193\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0743\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0718\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 72.3646 - reconstruction_loss: 70.1322 - kl_loss: 2.2323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 831us/step - loss: 78.6217 - reconstruction_loss: 76.3925 - kl_loss: 2.2292\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3727\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.4132 - reconstruction_loss: 38.3058 - kl_loss: 4.1073\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.1750 - reconstruction_loss: 58.0688 - kl_loss: 4.1063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0216\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9941\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 55.1986 - reconstruction_loss: 42.2049 - kl_loss: 12.9937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 865us/step - loss: 64.2805 - reconstruction_loss: 51.2609 - kl_loss: 13.0197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1977\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1946\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 55.9624 - reconstruction_loss: 45.0819 - kl_loss: 10.8806\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 104.6510 - reconstruction_loss: 93.7332 - kl_loss: 10.9178\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5476\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.6650 - reconstruction_loss: 46.0565 - kl_loss: 4.6085\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 70.0487 - reconstruction_loss: 65.4283 - kl_loss: 4.6203\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0789\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0820\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.4793 - reconstruction_loss: 25.7183 - kl_loss: 7.7609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.4046 - reconstruction_loss: 28.6394 - kl_loss: 7.7652\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1542\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1420\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 45.1761 - reconstruction_loss: 40.8108 - kl_loss: 4.3654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.6232 - reconstruction_loss: 32.2460 - kl_loss: 4.3772\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 249.1445\n",
      "Success in episode 22 at time step 239\n",
      "Episode 23\n",
      "[-0.47731942  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1032\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1004\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.6771 - reconstruction_loss: 7.2008 - kl_loss: 1.4762\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.2294 - reconstruction_loss: 10.7679 - kl_loss: 1.4615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0145\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.2729 - reconstruction_loss: 10.8240 - kl_loss: 1.4489\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 927us/step - loss: 22.5624 - reconstruction_loss: 21.1282 - kl_loss: 1.4343\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1039\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.4408 - reconstruction_loss: 45.4647 - kl_loss: 1.9761\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41.6321 - reconstruction_loss: 39.6642 - kl_loss: 1.9680\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0394\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0391\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.5511 - reconstruction_loss: 52.0663 - kl_loss: 1.4848\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.9170 - reconstruction_loss: 41.4429 - kl_loss: 1.4741\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0174\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0150\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116.9973 - reconstruction_loss: 114.7588 - kl_loss: 2.2385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.1144 - reconstruction_loss: 19.8874 - kl_loss: 2.2271\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2254\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58.3456 - reconstruction_loss: 47.5810 - kl_loss: 10.7646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.2167 - reconstruction_loss: 13.4271 - kl_loss: 10.7895\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7344\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.5879\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.2818 - reconstruction_loss: 26.3800 - kl_loss: 12.9019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80.6124 - reconstruction_loss: 67.6789 - kl_loss: 12.9334\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2306\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0729\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 74.5390 - reconstruction_loss: 69.2549 - kl_loss: 5.2841\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 904us/step - loss: 80.7132 - reconstruction_loss: 75.4161 - kl_loss: 5.2971\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1877\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 45.8367 - reconstruction_loss: 36.7807 - kl_loss: 9.0560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71.4766 - reconstruction_loss: 62.3969 - kl_loss: 9.0797\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2773\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.4129 - reconstruction_loss: 45.9802 - kl_loss: 5.4327\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.4408 - reconstruction_loss: 43.9917 - kl_loss: 5.4490\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 497.1105\n",
      "Success in episode 23 at time step 116\n",
      "Episode 24\n",
      "[-0.5223106  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1137\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1076\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6815 - reconstruction_loss: 7.8617 - kl_loss: 1.8198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.8468 - reconstruction_loss: 4.0469 - kl_loss: 1.8000\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0983\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1185\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3441 - reconstruction_loss: 5.5962 - kl_loss: 1.7479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.3348 - reconstruction_loss: 9.6018 - kl_loss: 1.7331\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0958\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1038\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7257 - reconstruction_loss: 1.3265 - kl_loss: 2.3992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1635 - reconstruction_loss: 2.7614 - kl_loss: 2.4021\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1412\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1393\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.3709 - reconstruction_loss: 39.1067 - kl_loss: 3.2642\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 21.9688 - reconstruction_loss: 18.6859 - kl_loss: 3.2829\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0438\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47.3168 - reconstruction_loss: 45.1216 - kl_loss: 2.1953\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.6194 - reconstruction_loss: 119.4193 - kl_loss: 2.2001\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0964\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 138.5959 - reconstruction_loss: 135.7944 - kl_loss: 2.8015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 76.8394 - reconstruction_loss: 74.0425 - kl_loss: 2.7969\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 91.8992 - reconstruction_loss: 80.6533 - kl_loss: 11.2458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.7863 - reconstruction_loss: 49.5164 - kl_loss: 11.2700\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2838\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2743\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.9412 - reconstruction_loss: 32.5175 - kl_loss: 27.4236\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.9455 - reconstruction_loss: 27.4677 - kl_loss: 27.4778\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8892\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6545\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 52.0766 - reconstruction_loss: 46.2453 - kl_loss: 5.8314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84.4637 - reconstruction_loss: 78.6371 - kl_loss: 5.8266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2151\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.0377 - reconstruction_loss: 43.5793 - kl_loss: 7.4584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.9746 - reconstruction_loss: 34.4890 - kl_loss: 7.4856\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0884\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0703\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.3850 - reconstruction_loss: 38.1527 - kl_loss: 10.2323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 854us/step - loss: 63.2558 - reconstruction_loss: 52.9887 - kl_loss: 10.2671\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2824\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2703\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59.9022 - reconstruction_loss: 52.8920 - kl_loss: 7.0102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.0793 - reconstruction_loss: 46.0415 - kl_loss: 7.0378\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 432.6171\n",
      "Success in episode 24 at time step 133\n",
      "Episode 25\n",
      "[-0.49148723  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1585\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1516\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2776 - reconstruction_loss: 7.0705 - kl_loss: 1.2071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.7815 - reconstruction_loss: 6.5871 - kl_loss: 1.1944\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0707\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0598\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.8478 - reconstruction_loss: 20.3065 - kl_loss: 4.5414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7092 - reconstruction_loss: 2.1769 - kl_loss: 4.5323\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0343\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.0198 - reconstruction_loss: 18.2793 - kl_loss: 5.7404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.7569 - reconstruction_loss: 14.0175 - kl_loss: 5.7394\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0729\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0665\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.8577 - reconstruction_loss: 23.8474 - kl_loss: 5.0103\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.6844 - reconstruction_loss: 18.6642 - kl_loss: 5.0202\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4082\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4331\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.7040 - reconstruction_loss: 36.9173 - kl_loss: 3.7867\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.2036 - reconstruction_loss: 19.4012 - kl_loss: 3.8024\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6893\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6793\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59.4461 - reconstruction_loss: 55.4865 - kl_loss: 3.9595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.7203 - reconstruction_loss: 24.7394 - kl_loss: 3.9809\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1485\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.7272 - reconstruction_loss: 35.3704 - kl_loss: 7.3567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.1701 - reconstruction_loss: 14.7828 - kl_loss: 7.3873\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2182\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.8272 - reconstruction_loss: 29.9523 - kl_loss: 4.8749\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.4806 - reconstruction_loss: 37.5799 - kl_loss: 4.9007\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 619.8025\n",
      "Success in episode 25 at time step 92\n",
      "Episode 26\n",
      "[-0.52258474  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2730\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2538\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.4028 - reconstruction_loss: 1.3562 - kl_loss: 1.0466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 833us/step - loss: 4.4739 - reconstruction_loss: 3.4260 - kl_loss: 1.0479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1044\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1086\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5865 - reconstruction_loss: 2.4790 - kl_loss: 2.1076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.3214 - reconstruction_loss: 9.2031 - kl_loss: 2.1183\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0383\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.9725 - reconstruction_loss: 25.4031 - kl_loss: 1.5694\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.6912 - reconstruction_loss: 11.1232 - kl_loss: 1.5680\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1694\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1759\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 50.9282 - reconstruction_loss: 49.6104 - kl_loss: 1.3178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49.3518 - reconstruction_loss: 48.0381 - kl_loss: 1.3137\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1414\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1422\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 76.1977 - reconstruction_loss: 73.5984 - kl_loss: 2.5993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 920us/step - loss: 29.8843 - reconstruction_loss: 27.2883 - kl_loss: 2.5960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4021\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.6351 - reconstruction_loss: 39.8614 - kl_loss: 11.7737\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.6996 - reconstruction_loss: 28.9228 - kl_loss: 11.7768\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8853\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8151\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.6590 - reconstruction_loss: 14.1787 - kl_loss: 12.4803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34.8818 - reconstruction_loss: 22.4174 - kl_loss: 12.4644\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1866\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1827\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.8473 - reconstruction_loss: 86.0439 - kl_loss: 6.8034\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79.9069 - reconstruction_loss: 73.0803 - kl_loss: 6.8266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0913\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.8895 - reconstruction_loss: 44.5903 - kl_loss: 11.2992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 54.7595 - reconstruction_loss: 43.4242 - kl_loss: 11.3353\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3097\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2983\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54.2836 - reconstruction_loss: 48.0738 - kl_loss: 6.2098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.2316 - reconstruction_loss: 32.0080 - kl_loss: 6.2236\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 497.0621\n",
      "Success in episode 26 at time step 116\n",
      "Episode 27\n",
      "[-0.4279657  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1716\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1648\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9223 - reconstruction_loss: 3.1205 - kl_loss: 0.8018\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.1757 - reconstruction_loss: 13.3819 - kl_loss: 0.7938\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1003\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.1390 - reconstruction_loss: 16.0626 - kl_loss: 1.0763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 975us/step - loss: 31.0466 - reconstruction_loss: 29.9656 - kl_loss: 1.0809\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3175\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.4949 - reconstruction_loss: 14.2404 - kl_loss: 1.2545\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.9109 - reconstruction_loss: 25.6655 - kl_loss: 1.2454\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4678\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4732\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40.4747 - reconstruction_loss: 33.5542 - kl_loss: 6.9205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.1529 - reconstruction_loss: 24.2668 - kl_loss: 6.8861\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1117\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1108\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48.5655 - reconstruction_loss: 35.2197 - kl_loss: 13.3457\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.2522 - reconstruction_loss: 6.9133 - kl_loss: 13.3389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1541\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86.4941 - reconstruction_loss: 79.4558 - kl_loss: 7.0383\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75.0137 - reconstruction_loss: 67.9645 - kl_loss: 7.0492\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.2284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.2671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 57.9442 - reconstruction_loss: 50.3238 - kl_loss: 7.6203\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.5108 - reconstruction_loss: 42.8570 - kl_loss: 7.6538\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2087\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2032\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.0268 - reconstruction_loss: 6.1806 - kl_loss: 10.8462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.0383 - reconstruction_loss: 11.1708 - kl_loss: 10.8675\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3616\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3478\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.3818 - reconstruction_loss: 30.9910 - kl_loss: 6.3907\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.2707 - reconstruction_loss: 26.8632 - kl_loss: 6.4075\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 583.4853\n",
      "Success in episode 27 at time step 102\n",
      "Episode 28\n",
      "[-0.5497547  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3339\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3433\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.3699 - reconstruction_loss: 17.3137 - kl_loss: 1.0562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.1729 - reconstruction_loss: 16.1183 - kl_loss: 1.0546\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0480\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0436\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.1604 - reconstruction_loss: 7.8841 - kl_loss: 2.2763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: 14.2998 - reconstruction_loss: 12.0239 - kl_loss: 2.2759\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1058\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1019\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.9448 - reconstruction_loss: 6.5589 - kl_loss: 4.3859\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.7807 - reconstruction_loss: 41.3843 - kl_loss: 4.3964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1878\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.2862 - reconstruction_loss: 7.9971 - kl_loss: 4.2891\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.7689 - reconstruction_loss: 15.4601 - kl_loss: 4.3088\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0317\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0252\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.5038 - reconstruction_loss: 21.4981 - kl_loss: 4.0056\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.4213 - reconstruction_loss: 34.4042 - kl_loss: 4.0170\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0563\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.3848 - reconstruction_loss: 6.4786 - kl_loss: 7.9062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.3274 - reconstruction_loss: 23.4303 - kl_loss: 7.8971\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.1224\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20.4044 - reconstruction_loss: 15.6376 - kl_loss: 4.7668\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.8914 - reconstruction_loss: 16.1105 - kl_loss: 4.7809\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 705.3232\n",
      "Success in episode 28 at time step 83\n",
      "Episode 29\n",
      "[-0.42521894  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.7458 - reconstruction_loss: 17.2632 - kl_loss: 1.4826\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.9081 - reconstruction_loss: 14.4286 - kl_loss: 1.4794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1513\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.1994 - reconstruction_loss: 23.8124 - kl_loss: 3.3870\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.2330 - reconstruction_loss: 3.8378 - kl_loss: 3.3952\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1083\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.4897 - reconstruction_loss: 25.8018 - kl_loss: 8.6879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.9258 - reconstruction_loss: 27.1982 - kl_loss: 8.7275\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3989\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3797\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.2512 - reconstruction_loss: 18.0260 - kl_loss: 6.2252\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: 39.9550 - reconstruction_loss: 33.6944 - kl_loss: 6.2606\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3456\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56.5154 - reconstruction_loss: 46.6045 - kl_loss: 9.9109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32.6245 - reconstruction_loss: 22.7037 - kl_loss: 9.9208\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2846\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2816\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.1423 - reconstruction_loss: 29.5866 - kl_loss: 11.5557\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.9126 - reconstruction_loss: 26.3514 - kl_loss: 11.5612\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2268\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2158\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.1992 - reconstruction_loss: 21.9146 - kl_loss: 7.2846\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.6596 - reconstruction_loss: 19.3515 - kl_loss: 7.3081\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 760.3511\n",
      "Success in episode 29 at time step 74\n",
      "Episode 30\n",
      "[-0.5338159  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.7254 - reconstruction_loss: 8.9803 - kl_loss: 0.7450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 983us/step - loss: 8.6599 - reconstruction_loss: 7.9167 - kl_loss: 0.7432\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1157\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.4905 - reconstruction_loss: 13.6278 - kl_loss: 1.8627\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.2012 - reconstruction_loss: 18.3482 - kl_loss: 1.8530\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1531\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1465\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.7396 - reconstruction_loss: 20.8720 - kl_loss: 1.8676\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 977us/step - loss: 10.4049 - reconstruction_loss: 8.5456 - kl_loss: 1.8593\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0361\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49.7533 - reconstruction_loss: 48.2061 - kl_loss: 1.5472\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.3137 - reconstruction_loss: 35.7689 - kl_loss: 1.5448\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92.7256 - reconstruction_loss: 91.3635 - kl_loss: 1.3622\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.1877 - reconstruction_loss: 43.8249 - kl_loss: 1.3628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2840\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2779\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55.0817 - reconstruction_loss: 51.3893 - kl_loss: 3.6924\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.9737 - reconstruction_loss: 34.2909 - kl_loss: 3.6827\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9276\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.3118 - reconstruction_loss: 27.3671 - kl_loss: 17.9446\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.6346 - reconstruction_loss: 28.7724 - kl_loss: 17.8621\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8537\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.6739\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45.9668 - reconstruction_loss: 29.1565 - kl_loss: 16.8103\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.3666 - reconstruction_loss: 13.5806 - kl_loss: 16.7860\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1875\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.6817 - reconstruction_loss: 27.0197 - kl_loss: 6.6620\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.2584 - reconstruction_loss: 23.5891 - kl_loss: 6.6694\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2438\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2179\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.0606 - reconstruction_loss: 11.4528 - kl_loss: 5.6079\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 925us/step - loss: 23.1825 - reconstruction_loss: 17.5628 - kl_loss: 5.6197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1895\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1727\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.1594 - reconstruction_loss: 21.3336 - kl_loss: 9.8257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.1507 - reconstruction_loss: 13.3073 - kl_loss: 9.8434\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 0.4195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 0.4139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.0753 - reconstruction_loss: 23.7053 - kl_loss: 6.3700\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30.1804 - reconstruction_loss: 23.8070 - kl_loss: 6.3734\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 434.0849\n",
      "Success in episode 30 at time step 137\n",
      "Episode 31\n",
      "[-0.49070036  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1604\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1586\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6158 - reconstruction_loss: 8.6628 - kl_loss: 0.9530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.4938 - reconstruction_loss: 7.5516 - kl_loss: 0.9423\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.1306 - reconstruction_loss: 8.0558 - kl_loss: 4.0748\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.8082 - reconstruction_loss: 10.7500 - kl_loss: 4.0582\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2626\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2520\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.8740 - reconstruction_loss: 12.5019 - kl_loss: 6.3720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.0067 - reconstruction_loss: 2.6559 - kl_loss: 6.3508\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0243\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 32.4142 - reconstruction_loss: 26.0549 - kl_loss: 6.3593\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.4523 - reconstruction_loss: 17.0911 - kl_loss: 6.3611\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2519\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 22.7360 - reconstruction_loss: 14.6388 - kl_loss: 8.0972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 24.4821 - reconstruction_loss: 16.3742 - kl_loss: 8.1079\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1772\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1688\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.7047 - reconstruction_loss: 39.1748 - kl_loss: 9.5299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 60.1733 - reconstruction_loss: 50.6180 - kl_loss: 9.5553\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1658\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.1530\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.7505 - reconstruction_loss: 23.3477 - kl_loss: 6.4028\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.4035 - reconstruction_loss: 22.9784 - kl_loss: 6.4251\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 756.7457\n",
      "Success in episode 31 at time step 74\n",
      "Episode 32\n",
      "[-0.51356375  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3903\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3855\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.6817 - reconstruction_loss: 14.6702 - kl_loss: 1.0115\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.9949 - reconstruction_loss: 8.9834 - kl_loss: 1.0115\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2286\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7385 - reconstruction_loss: 2.4250 - kl_loss: 3.3135\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.5771 - reconstruction_loss: 10.2573 - kl_loss: 3.3198\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0657\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0660\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.9075 - reconstruction_loss: 14.9799 - kl_loss: 5.9275\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.1846 - reconstruction_loss: 6.2395 - kl_loss: 5.9452\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1849\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1802\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.8607 - reconstruction_loss: 17.0587 - kl_loss: 5.8020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 899us/step - loss: 16.8570 - reconstruction_loss: 11.0295 - kl_loss: 5.8276\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0661\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0649\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.0705 - reconstruction_loss: 7.9811 - kl_loss: 8.0894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.6014 - reconstruction_loss: 21.4951 - kl_loss: 8.1063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2542\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2483\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.5948 - reconstruction_loss: 19.4996 - kl_loss: 10.0952\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.3307 - reconstruction_loss: 17.2253 - kl_loss: 10.1054\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.1841\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1751\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.4692 - reconstruction_loss: 18.2255 - kl_loss: 6.2438\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.6770 - reconstruction_loss: 28.4129 - kl_loss: 6.2640\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 755.2291\n",
      "Success in episode 32 at time step 73\n",
      "Episode 33\n",
      "[-0.49232078  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.3803 - reconstruction_loss: 3.8536 - kl_loss: 0.5267\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.8627 - reconstruction_loss: 9.3389 - kl_loss: 0.5238\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1153\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.7520 - reconstruction_loss: 8.5391 - kl_loss: 3.2129\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 834us/step - loss: 6.3780 - reconstruction_loss: 3.1635 - kl_loss: 3.2145\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4169\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4213\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 31.7868 - reconstruction_loss: 24.8192 - kl_loss: 6.9676\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.9910 - reconstruction_loss: 4.0057 - kl_loss: 6.9854\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2149\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2153\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.5031 - reconstruction_loss: 16.2648 - kl_loss: 7.2383\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.7051 - reconstruction_loss: 16.4612 - kl_loss: 7.2439\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6508\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6186\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.9597 - reconstruction_loss: 17.8363 - kl_loss: 7.1234\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.1578 - reconstruction_loss: 26.0066 - kl_loss: 7.1512\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0367\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0422\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.4438 - reconstruction_loss: 14.0212 - kl_loss: 11.4226\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.4057 - reconstruction_loss: 4.9635 - kl_loss: 11.4422\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2857\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.7127 - reconstruction_loss: 19.6389 - kl_loss: 7.0739\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.1099 - reconstruction_loss: 24.0177 - kl_loss: 7.0921\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 708.4335\n",
      "Success in episode 33 at time step 79\n",
      "Episode 34\n",
      "[-0.46294674  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3346\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3304\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.5144 - reconstruction_loss: 24.7119 - kl_loss: 0.8025\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.5163 - reconstruction_loss: 5.7154 - kl_loss: 0.8009\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0854\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.1643 - reconstruction_loss: 14.0896 - kl_loss: 3.0747\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.9322 - reconstruction_loss: 26.8530 - kl_loss: 3.0792\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2330\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2262\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.2590 - reconstruction_loss: 3.2409 - kl_loss: 7.0181\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.8165 - reconstruction_loss: 17.7835 - kl_loss: 7.0331\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.0521 - reconstruction_loss: 12.5135 - kl_loss: 6.5387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.2873 - reconstruction_loss: 8.7238 - kl_loss: 6.5635\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.7819 - reconstruction_loss: 19.1694 - kl_loss: 9.6124\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.0192 - reconstruction_loss: 7.4047 - kl_loss: 9.6145\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1499\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.8290 - reconstruction_loss: 15.4726 - kl_loss: 10.3564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.3207 - reconstruction_loss: 11.9589 - kl_loss: 10.3618\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2439\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2369\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21.2871 - reconstruction_loss: 14.3679 - kl_loss: 6.9192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.2814 - reconstruction_loss: 18.3542 - kl_loss: 6.9271\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 756.2717\n",
      "Success in episode 34 at time step 73\n",
      "Episode 35\n",
      "[-0.4569664  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1148\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.2529 - reconstruction_loss: 8.7019 - kl_loss: 0.5510\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6128 - reconstruction_loss: 4.0620 - kl_loss: 0.5508\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0663\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.7777 - reconstruction_loss: 11.5629 - kl_loss: 2.2148\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.3920 - reconstruction_loss: 10.1748 - kl_loss: 2.2172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0907\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.0599 - reconstruction_loss: 12.9070 - kl_loss: 5.1529\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4944 - reconstruction_loss: 1.3361 - kl_loss: 5.1583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0296\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0253\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.3275 - reconstruction_loss: 17.3924 - kl_loss: 4.9351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.9763 - reconstruction_loss: 14.0385 - kl_loss: 4.9377\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1579\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1643\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.8152 - reconstruction_loss: 12.9159 - kl_loss: 6.8993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.9514 - reconstruction_loss: 4.0731 - kl_loss: 6.8783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0657\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.9237 - reconstruction_loss: 19.4255 - kl_loss: 9.4982\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.1622 - reconstruction_loss: 19.6691 - kl_loss: 9.4932\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1013\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0959\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.8133 - reconstruction_loss: 10.4937 - kl_loss: 5.3195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.8917 - reconstruction_loss: 17.5697 - kl_loss: 5.3220\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 757.7737\n",
      "Success in episode 35 at time step 75\n",
      "Episode 36\n",
      "[-0.49886352  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2499\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.3581 - reconstruction_loss: 10.9338 - kl_loss: 0.4243\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.9012 - reconstruction_loss: 17.4825 - kl_loss: 0.4187\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1669\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1732\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.4339 - reconstruction_loss: 15.3755 - kl_loss: 1.0585\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0252 - reconstruction_loss: 4.9744 - kl_loss: 1.0508\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2604\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.9550 - reconstruction_loss: 7.9331 - kl_loss: 1.0219\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.5097 - reconstruction_loss: 15.4915 - kl_loss: 1.0182\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1849\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6830 - reconstruction_loss: 4.8669 - kl_loss: 0.8161\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38.9597 - reconstruction_loss: 38.1417 - kl_loss: 0.8180\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4301\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4219\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20.1606 - reconstruction_loss: 19.3734 - kl_loss: 0.7872\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.7160 - reconstruction_loss: 32.9257 - kl_loss: 0.7902\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4354\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.1366 - reconstruction_loss: 48.7174 - kl_loss: 4.4192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.7417 - reconstruction_loss: 10.3362 - kl_loss: 4.4055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6590\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.3234 - reconstruction_loss: 4.3157 - kl_loss: 17.0077\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 806us/step - loss: 57.4711 - reconstruction_loss: 40.5472 - kl_loss: 16.9239\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8383\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8366\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30.7082 - reconstruction_loss: 16.2458 - kl_loss: 14.4625\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.7537 - reconstruction_loss: 6.4151 - kl_loss: 14.3387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4364\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4178\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.8789 - reconstruction_loss: 16.5942 - kl_loss: 11.2847\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 918us/step - loss: 38.4085 - reconstruction_loss: 27.1245 - kl_loss: 11.2840\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0506\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0486\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.9052 - reconstruction_loss: 12.7296 - kl_loss: 14.1756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.7711 - reconstruction_loss: 10.5887 - kl_loss: 14.1824\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2992\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 31.1061 - reconstruction_loss: 24.2066 - kl_loss: 6.8995\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.7833 - reconstruction_loss: 15.8948 - kl_loss: 6.8884\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 526.0706\n",
      "Success in episode 36 at time step 123\n",
      "Episode 37\n",
      "[-0.44863534  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1895\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1930\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.3365 - reconstruction_loss: 9.8136 - kl_loss: 0.5229\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.0408 - reconstruction_loss: 7.5304 - kl_loss: 0.5104\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1271\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6196 - reconstruction_loss: 3.2062 - kl_loss: 4.4134\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7162 - reconstruction_loss: 0.3354 - kl_loss: 4.3808\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1152\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.2636 - reconstruction_loss: 5.6090 - kl_loss: 8.6545\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.3765 - reconstruction_loss: 7.7600 - kl_loss: 8.6165\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.7880 - reconstruction_loss: 8.9387 - kl_loss: 7.8493\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.5647 - reconstruction_loss: 11.7328 - kl_loss: 7.8319\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4838\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.0347 - reconstruction_loss: 19.5413 - kl_loss: 7.4934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39.0533 - reconstruction_loss: 31.5309 - kl_loss: 7.5224\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0960\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.6182 - reconstruction_loss: 3.6342 - kl_loss: 12.9840\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.3824 - reconstruction_loss: 9.3834 - kl_loss: 12.9991\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2975\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.6435 - reconstruction_loss: 15.0897 - kl_loss: 7.5538\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.9436 - reconstruction_loss: 19.3705 - kl_loss: 7.5731\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 801.9055\n",
      "Success in episode 37 at time step 76\n",
      "Episode 38\n",
      "[-0.4748382  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6288\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5969\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.8858 - reconstruction_loss: 13.3215 - kl_loss: 0.5644\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.5151 - reconstruction_loss: 4.9476 - kl_loss: 0.5675\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3427\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3050\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.8311 - reconstruction_loss: 8.7659 - kl_loss: 3.0652\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2052 - reconstruction_loss: 3.1243 - kl_loss: 3.0809\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1712\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1560\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.5330 - reconstruction_loss: 8.8534 - kl_loss: 6.6796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.9968 - reconstruction_loss: 1.2753 - kl_loss: 6.7215\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2131\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.1598 - reconstruction_loss: 7.5534 - kl_loss: 9.6064\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.7058 - reconstruction_loss: 19.0617 - kl_loss: 9.6441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0637\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0589\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.8026 - reconstruction_loss: 9.5863 - kl_loss: 7.2164\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.3340 - reconstruction_loss: 20.0812 - kl_loss: 7.2528\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2554\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2522\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.0493 - reconstruction_loss: 5.1540 - kl_loss: 11.8953\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.9237 - reconstruction_loss: 4.0200 - kl_loss: 11.9036\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2986\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2856\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20.8558 - reconstruction_loss: 13.1688 - kl_loss: 7.6869\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.3337 - reconstruction_loss: 16.6131 - kl_loss: 7.7207\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 771.2997\n",
      "Success in episode 38 at time step 81\n",
      "Episode 39\n",
      "[-0.48796996  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5183\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5128\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.8920 - reconstruction_loss: 5.3871 - kl_loss: 0.5049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.9349 - reconstruction_loss: 8.4212 - kl_loss: 0.5137\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1033\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2209 - reconstruction_loss: 2.6663 - kl_loss: 3.5546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.6389 - reconstruction_loss: 6.0559 - kl_loss: 3.5830\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1830\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1629\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.2937 - reconstruction_loss: 0.3500 - kl_loss: 7.9437\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: 9.4788 - reconstruction_loss: 1.4995 - kl_loss: 7.9793\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3056\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3084\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.2850 - reconstruction_loss: 1.9670 - kl_loss: 8.3180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.8339 - reconstruction_loss: 4.4918 - kl_loss: 8.3420\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2672\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0648 - reconstruction_loss: 6.6064 - kl_loss: 7.4584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.4150 - reconstruction_loss: 5.9292 - kl_loss: 7.4858\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4898\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4791\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.3708 - reconstruction_loss: 3.5603 - kl_loss: 10.8105\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.1193 - reconstruction_loss: 5.2987 - kl_loss: 10.8206\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4168\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4040\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.1522 - reconstruction_loss: 6.7270 - kl_loss: 7.4253\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.3083 - reconstruction_loss: 12.8828 - kl_loss: 7.4256\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 848.1312\n",
      "Success in episode 39 at time step 82\n",
      "Episode 40\n",
      "[-0.4942272  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4130\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0076 - reconstruction_loss: 4.4354 - kl_loss: 0.5723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2465 - reconstruction_loss: 3.6790 - kl_loss: 0.5674\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2120\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2223\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.5879 - reconstruction_loss: 12.5333 - kl_loss: 1.0546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7383 - reconstruction_loss: 0.6956 - kl_loss: 1.0427\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0394\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0415\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.6277 - reconstruction_loss: 22.3846 - kl_loss: 1.2431\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.8511 - reconstruction_loss: 13.6139 - kl_loss: 1.2372\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0915\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0912\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.3642 - reconstruction_loss: 9.4074 - kl_loss: 1.9569\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.5809 - reconstruction_loss: 29.6207 - kl_loss: 1.9602\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2805\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53.9230 - reconstruction_loss: 51.8161 - kl_loss: 2.1069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.0679 - reconstruction_loss: 40.9556 - kl_loss: 2.1123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5629\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5466\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46.6936 - reconstruction_loss: 30.3533 - kl_loss: 16.3403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31.4870 - reconstruction_loss: 15.2078 - kl_loss: 16.2792\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.8215\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.8513 - reconstruction_loss: 10.0255 - kl_loss: 18.8259\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28.8139 - reconstruction_loss: 10.0911 - kl_loss: 18.7228\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7499\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8475\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.1489 - reconstruction_loss: 16.4522 - kl_loss: 10.6967\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.1085 - reconstruction_loss: 13.4320 - kl_loss: 10.6766\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8034\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6761\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.3725 - reconstruction_loss: 6.9187 - kl_loss: 14.4538\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.7668 - reconstruction_loss: 4.3074 - kl_loss: 14.4594\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.0179\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9701\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.5683 - reconstruction_loss: 16.4413 - kl_loss: 8.1270\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.7367 - reconstruction_loss: 10.6267 - kl_loss: 8.1100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 848.1136\n",
      "Success in episode 40 at time step 117\n",
      "Episode 41\n",
      "[-0.43964344  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6614 - reconstruction_loss: 2.1475 - kl_loss: 0.5138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 966us/step - loss: 5.0702 - reconstruction_loss: 4.5631 - kl_loss: 0.5071\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2724\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2630\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6147 - reconstruction_loss: 2.4939 - kl_loss: 4.1208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.0252 - reconstruction_loss: 3.9346 - kl_loss: 4.0906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6627\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.4994 - reconstruction_loss: 2.5715 - kl_loss: 8.9279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.1223 - reconstruction_loss: 0.2379 - kl_loss: 8.8844\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.6189 - reconstruction_loss: 7.8043 - kl_loss: 9.8146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.2648 - reconstruction_loss: 3.4790 - kl_loss: 9.7858\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6280\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5878\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.7135 - reconstruction_loss: 6.5084 - kl_loss: 9.2050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.1044 - reconstruction_loss: 16.9049 - kl_loss: 9.1995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7982\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.9160\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 15.1765 - reconstruction_loss: 8.4354 - kl_loss: 6.7411\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21.6186 - reconstruction_loss: 14.8538 - kl_loss: 6.7648\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6141\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0560 - reconstruction_loss: 1.1797 - kl_loss: 4.8763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 989us/step - loss: 13.5980 - reconstruction_loss: 8.7029 - kl_loss: 4.8952\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5650 - reconstruction_loss: 5.6874 - kl_loss: 1.8776\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.6564 - reconstruction_loss: 7.7767 - kl_loss: 1.8797\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.3553 - reconstruction_loss: 23.9943 - kl_loss: 1.3609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.4360 - reconstruction_loss: 8.0756 - kl_loss: 1.3604\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1915\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.3666 - reconstruction_loss: 6.6839 - kl_loss: 8.6827\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.4891 - reconstruction_loss: 24.8271 - kl_loss: 8.6620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4548\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4473\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.3065 - reconstruction_loss: 4.1416 - kl_loss: 11.1649\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.4798 - reconstruction_loss: 5.3394 - kl_loss: 11.1404\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9382\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6980\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.0947 - reconstruction_loss: 6.8109 - kl_loss: 9.2838\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.5423 - reconstruction_loss: 6.2550 - kl_loss: 9.2873\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1820\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0687\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20.0244 - reconstruction_loss: 7.5250 - kl_loss: 12.4994\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.3778 - reconstruction_loss: 5.8684 - kl_loss: 12.5094\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.7263\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6581\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.7087 - reconstruction_loss: 9.3707 - kl_loss: 7.3380\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.3005 - reconstruction_loss: 11.9400 - kl_loss: 7.3605\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 544.1164\n",
      "Success in episode 41 at time step 163\n",
      "Episode 42\n",
      "[-0.40096855  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3637\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3457\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5725 - reconstruction_loss: 5.2504 - kl_loss: 0.3221\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6644 - reconstruction_loss: 4.3390 - kl_loss: 0.3254\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0772\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.0215 - reconstruction_loss: 6.6461 - kl_loss: 5.3754\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33.9567 - reconstruction_loss: 28.5667 - kl_loss: 5.3900\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0988\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0336\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.5945 - reconstruction_loss: 1.4123 - kl_loss: 9.1822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.1102 - reconstruction_loss: 2.8970 - kl_loss: 9.2132\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1610\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1623\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21.4157 - reconstruction_loss: 9.9951 - kl_loss: 11.4206\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.5064 - reconstruction_loss: 7.0553 - kl_loss: 11.4510\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3676\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3211\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.4549 - reconstruction_loss: 10.8334 - kl_loss: 10.6215\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.4884 - reconstruction_loss: 7.8489 - kl_loss: 10.6395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5838\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.8630 - reconstruction_loss: 6.7013 - kl_loss: 13.1618\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.8803 - reconstruction_loss: 11.7251 - kl_loss: 13.1552\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3988\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3872\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.5175 - reconstruction_loss: 7.7642 - kl_loss: 8.7533\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.3129 - reconstruction_loss: 6.5518 - kl_loss: 8.7611\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 930.4764\n",
      "Success in episode 42 at time step 75\n",
      "Episode 43\n",
      "[-0.41117394  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4589\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4556\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0704 - reconstruction_loss: 1.9088 - kl_loss: 0.1616\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3094 - reconstruction_loss: 5.1454 - kl_loss: 0.1640\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0531\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0514\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.3356 - reconstruction_loss: 10.5148 - kl_loss: 0.8208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.2142 - reconstruction_loss: 13.3900 - kl_loss: 0.8242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1161\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.7564 - reconstruction_loss: 7.3042 - kl_loss: 0.4523\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 961us/step - loss: 4.4526 - reconstruction_loss: 3.9981 - kl_loss: 0.4545\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2268\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.4547 - reconstruction_loss: 2.1325 - kl_loss: 5.3222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.9349 - reconstruction_loss: 5.6122 - kl_loss: 5.3227\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8542\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.1237\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.5480 - reconstruction_loss: 11.8292 - kl_loss: 14.7188\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.3374 - reconstruction_loss: 2.6205 - kl_loss: 14.7170\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4647\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4954\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.9154 - reconstruction_loss: 9.2683 - kl_loss: 11.6471\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.5554 - reconstruction_loss: 14.9201 - kl_loss: 11.6353\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1793\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.5801\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.2984 - reconstruction_loss: 15.5243 - kl_loss: 9.7742\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.8913 - reconstruction_loss: 9.0829 - kl_loss: 9.8084\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4442\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.7114 - reconstruction_loss: 7.2773 - kl_loss: 14.4340\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.5236 - reconstruction_loss: 2.0644 - kl_loss: 14.4592\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9538\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.5890 - reconstruction_loss: 10.6291 - kl_loss: 7.9600\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.2906 - reconstruction_loss: 6.3133 - kl_loss: 7.9774\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 790.7809\n",
      "Success in episode 43 at time step 103\n",
      "Episode 44\n",
      "[-0.5970127  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3089\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3144\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2781 - reconstruction_loss: 4.0659 - kl_loss: 1.2123\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 909us/step - loss: 5.2144 - reconstruction_loss: 4.0013 - kl_loss: 1.2132\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7773\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7894\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.9558 - reconstruction_loss: 16.3874 - kl_loss: 1.5684\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.3558 - reconstruction_loss: 5.7836 - kl_loss: 1.5721\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4452\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4433\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7650 - reconstruction_loss: 1.6020 - kl_loss: 3.1630\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.5509 - reconstruction_loss: 5.3888 - kl_loss: 3.1621\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2958\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2826\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9564 - reconstruction_loss: 1.9253 - kl_loss: 5.0312\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.2431 - reconstruction_loss: 3.2058 - kl_loss: 5.0373\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1755\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1396\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.3174 - reconstruction_loss: 5.2991 - kl_loss: 5.0183\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2612 - reconstruction_loss: 0.2339 - kl_loss: 5.0273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5827\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2484 - reconstruction_loss: 2.7207 - kl_loss: 5.5277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.1192 - reconstruction_loss: 12.5934 - kl_loss: 5.5258\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7006\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6763\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41.6001 - reconstruction_loss: 33.7779 - kl_loss: 7.8222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.4128 - reconstruction_loss: 7.5771 - kl_loss: 7.8358\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.7883\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7455\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.9297 - reconstruction_loss: 8.0789 - kl_loss: 4.8508\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.5134 - reconstruction_loss: 9.6590 - kl_loss: 4.8544\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 781.3815\n",
      "Success in episode 44 at time step 86\n",
      "Episode 45\n",
      "[-0.45531878  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.4843 - reconstruction_loss: 22.0453 - kl_loss: 0.4390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 996us/step - loss: 5.0485 - reconstruction_loss: 4.6098 - kl_loss: 0.4387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0916\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0938\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0902 - reconstruction_loss: 1.5471 - kl_loss: 0.5431\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0360 - reconstruction_loss: 3.4900 - kl_loss: 0.5461\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4006\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.4742 - reconstruction_loss: 11.2044 - kl_loss: 1.2698\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4870 - reconstruction_loss: 1.2144 - kl_loss: 1.2726\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1297\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.5892 - reconstruction_loss: 7.3423 - kl_loss: 1.2470\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.8391 - reconstruction_loss: 7.5887 - kl_loss: 1.2504\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1775\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.8383 - reconstruction_loss: 10.3627 - kl_loss: 0.4755\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.7006 - reconstruction_loss: 14.2195 - kl_loss: 0.4811\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5375\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.5324 - reconstruction_loss: 9.1680 - kl_loss: 1.3644\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6301 - reconstruction_loss: 3.2559 - kl_loss: 1.3742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.5775\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.4123 - reconstruction_loss: 8.4392 - kl_loss: 8.9731\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.5838 - reconstruction_loss: 2.5965 - kl_loss: 8.9873\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8607\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.7126\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.2887 - reconstruction_loss: 6.0137 - kl_loss: 14.2751\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.4453 - reconstruction_loss: 1.1909 - kl_loss: 14.2545\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9679\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.2977\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.4029 - reconstruction_loss: 6.6884 - kl_loss: 11.7145\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.1075 - reconstruction_loss: 8.4300 - kl_loss: 11.6775\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.5976 - reconstruction_loss: 7.7094 - kl_loss: 13.8882\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 947us/step - loss: 16.0633 - reconstruction_loss: 2.1484 - kl_loss: 13.9149\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.6103\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.5770 - reconstruction_loss: 6.2042 - kl_loss: 6.3728\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.7100 - reconstruction_loss: 5.3435 - kl_loss: 6.3665\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 741.9613\n",
      "Success in episode 45 at time step 130\n",
      "Episode 46\n",
      "[-0.5700514  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6790\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6592\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.3284 - reconstruction_loss: 8.2201 - kl_loss: 1.1083\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.2411 - reconstruction_loss: 9.1377 - kl_loss: 1.1034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0959\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.4281 - reconstruction_loss: 11.8738 - kl_loss: 2.5542\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.8389 - reconstruction_loss: 5.2929 - kl_loss: 2.5460\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3386\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3757\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.8133 - reconstruction_loss: 3.3177 - kl_loss: 4.4956\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1859 - reconstruction_loss: 1.7001 - kl_loss: 4.4858\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 0.4763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4914\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.4454 - reconstruction_loss: 2.9560 - kl_loss: 5.4893\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.0780 - reconstruction_loss: 1.5914 - kl_loss: 5.4866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8679\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8955\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9276 - reconstruction_loss: 1.0110 - kl_loss: 4.9166\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.1921 - reconstruction_loss: 5.2717 - kl_loss: 4.9203\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2339\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1985\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20.1538 - reconstruction_loss: 12.3464 - kl_loss: 7.8074\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.5977 - reconstruction_loss: 0.7882 - kl_loss: 7.8096\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.3747\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.8439 - reconstruction_loss: 6.8075 - kl_loss: 10.0364\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.2209 - reconstruction_loss: 8.1961 - kl_loss: 10.0248\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6960\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6564\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.1752 - reconstruction_loss: 9.1517 - kl_loss: 6.0235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.5771 - reconstruction_loss: 6.5410 - kl_loss: 6.0361\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 847.4520\n",
      "Success in episode 46 at time step 85\n",
      "Episode 47\n",
      "[-0.58498424  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3255\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6264 - reconstruction_loss: 4.0587 - kl_loss: 0.5676\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6365 - reconstruction_loss: 3.0676 - kl_loss: 0.5689\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5616\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5323\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3983 - reconstruction_loss: 1.4905 - kl_loss: 1.9078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.7964 - reconstruction_loss: 10.8863 - kl_loss: 1.9101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4347 - reconstruction_loss: 0.1510 - kl_loss: 4.2837\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.8759 - reconstruction_loss: 3.5813 - kl_loss: 4.2946\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3751\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.0762 - reconstruction_loss: 4.7560 - kl_loss: 5.3203\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.2177 - reconstruction_loss: 2.8800 - kl_loss: 5.3377\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4955\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4780\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.1440 - reconstruction_loss: 5.4168 - kl_loss: 3.7272\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 995us/step - loss: 5.2639 - reconstruction_loss: 1.5200 - kl_loss: 3.7439\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6307\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6212\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.7066 - reconstruction_loss: 9.0328 - kl_loss: 2.6738\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4602 - reconstruction_loss: 3.7797 - kl_loss: 2.6804\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1565\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3198 - reconstruction_loss: 1.2540 - kl_loss: 1.0658\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0813 - reconstruction_loss: 3.0168 - kl_loss: 1.0645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3166\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.2792 - reconstruction_loss: 16.9927 - kl_loss: 0.2865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.8703 - reconstruction_loss: 12.5809 - kl_loss: 0.2894\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3155\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3162\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2318 - reconstruction_loss: 3.1018 - kl_loss: 1.1301\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.9397 - reconstruction_loss: 5.8033 - kl_loss: 1.1365\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9108\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8784\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.8485 - reconstruction_loss: 9.5910 - kl_loss: 2.2575\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9085 - reconstruction_loss: 2.6371 - kl_loss: 2.2714\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3840\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.3886 - reconstruction_loss: 8.3234 - kl_loss: 2.0652\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6281 - reconstruction_loss: 4.5655 - kl_loss: 2.0626\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5939\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9758 - reconstruction_loss: 1.7186 - kl_loss: 1.2572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9727 - reconstruction_loss: 1.7203 - kl_loss: 1.2524\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3798 - reconstruction_loss: 3.9292 - kl_loss: 0.4506\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.4063 - reconstruction_loss: 17.9547 - kl_loss: 0.4516\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6373\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6245\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7896 - reconstruction_loss: 2.9791 - kl_loss: 1.8105\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.1444 - reconstruction_loss: 13.3213 - kl_loss: 1.8231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.6120\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38.8043 - reconstruction_loss: 27.7583 - kl_loss: 11.0459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.0070 - reconstruction_loss: 15.9458 - kl_loss: 11.0612\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3845\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.3256 - reconstruction_loss: 2.2459 - kl_loss: 14.0798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.6285 - reconstruction_loss: 3.5810 - kl_loss: 14.0475\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1032\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3019\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.9134 - reconstruction_loss: 2.3485 - kl_loss: 11.5649\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 948us/step - loss: 17.8703 - reconstruction_loss: 6.3263 - kl_loss: 11.5440\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2541\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0766\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.8199 - reconstruction_loss: 1.3706 - kl_loss: 13.4492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.4786 - reconstruction_loss: 7.0388 - kl_loss: 13.4399\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6604\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5979\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.6948 - reconstruction_loss: 3.9484 - kl_loss: 4.7464\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.9892 - reconstruction_loss: 9.2529 - kl_loss: 4.7363\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 439.7287\n",
      "Success in episode 47 at time step 221\n",
      "Episode 48\n",
      "[-0.51853454  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4436\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4263\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0595 - reconstruction_loss: 3.5972 - kl_loss: 0.4623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.9415 - reconstruction_loss: 6.4804 - kl_loss: 0.4611\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1329\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1330\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5710 - reconstruction_loss: 3.8903 - kl_loss: 0.6806\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.6402 - reconstruction_loss: 8.9564 - kl_loss: 0.6838\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7305\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7468\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.7356 - reconstruction_loss: 10.0658 - kl_loss: 0.6697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6082 - reconstruction_loss: 2.9320 - kl_loss: 0.6762\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3827\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.1579 - reconstruction_loss: 4.7618 - kl_loss: 8.3961\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.6036 - reconstruction_loss: 11.2330 - kl_loss: 8.3706\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1503\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.7539\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.4312 - reconstruction_loss: 2.8956 - kl_loss: 12.5356\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.4233 - reconstruction_loss: 6.9920 - kl_loss: 12.4313\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.7168\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.5900\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.0123 - reconstruction_loss: 3.8632 - kl_loss: 12.1492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.2636 - reconstruction_loss: 5.2032 - kl_loss: 12.0603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6932\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2106\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.0047 - reconstruction_loss: 7.1519 - kl_loss: 11.8527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.4716 - reconstruction_loss: 7.6094 - kl_loss: 11.8622\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2339\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.3271 - reconstruction_loss: 3.6267 - kl_loss: 13.7004\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.7575 - reconstruction_loss: 8.0506 - kl_loss: 13.7069\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2559\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4316\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.9918 - reconstruction_loss: 6.2213 - kl_loss: 7.7704\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.0461 - reconstruction_loss: 9.2882 - kl_loss: 7.7579\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 843.9811\n",
      "Success in episode 48 at time step 100\n",
      "Episode 49\n",
      "[-0.46244308  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7336\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4126 - reconstruction_loss: 1.0892 - kl_loss: 0.3234\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.3303 - reconstruction_loss: 4.0078 - kl_loss: 0.3225\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3650\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6026 - reconstruction_loss: 3.3745 - kl_loss: 3.2280\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.3941 - reconstruction_loss: 4.1841 - kl_loss: 3.2100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2411\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2600\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.1732 - reconstruction_loss: 4.7653 - kl_loss: 7.4079\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.9976 - reconstruction_loss: 5.6128 - kl_loss: 7.3847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4023\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3587\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.3264 - reconstruction_loss: 3.8055 - kl_loss: 10.5209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.3994 - reconstruction_loss: 4.8810 - kl_loss: 10.5185\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0658\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.3602 - reconstruction_loss: 10.5789 - kl_loss: 8.7814\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.5412 - reconstruction_loss: 3.7249 - kl_loss: 8.8163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5853\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.0581\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.3124 - reconstruction_loss: 1.3101 - kl_loss: 6.0023\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.1999 - reconstruction_loss: 1.1666 - kl_loss: 6.0332\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2175\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9937\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.0800 - reconstruction_loss: 2.2844 - kl_loss: 4.7956\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4457 - reconstruction_loss: 0.6336 - kl_loss: 4.8121\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6735\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7496\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9332 - reconstruction_loss: 1.8895 - kl_loss: 1.0437\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 895us/step - loss: 5.3747 - reconstruction_loss: 4.3289 - kl_loss: 1.0458\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27.2109 - reconstruction_loss: 25.3734 - kl_loss: 1.8376\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8250 - reconstruction_loss: 1.9775 - kl_loss: 1.8475\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9318\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0821 - reconstruction_loss: 3.8903 - kl_loss: 3.1918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6440 - reconstruction_loss: 3.4375 - kl_loss: 3.2065\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3212\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.3127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.7243 - reconstruction_loss: 5.0141 - kl_loss: 8.7102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.1705 - reconstruction_loss: 2.4354 - kl_loss: 8.7351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1053\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.6577 - reconstruction_loss: 6.0531 - kl_loss: 9.6046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 937us/step - loss: 11.8967 - reconstruction_loss: 2.2514 - kl_loss: 9.6453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1724\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.9147\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.2203 - reconstruction_loss: 3.5173 - kl_loss: 9.7030\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.1535 - reconstruction_loss: 1.4194 - kl_loss: 9.7341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0649\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0729\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.2434 - reconstruction_loss: 2.6234 - kl_loss: 11.6201\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.1370 - reconstruction_loss: 0.5317 - kl_loss: 11.6053\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6831\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.8569 - reconstruction_loss: 2.9317 - kl_loss: 6.9252\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.0597 - reconstruction_loss: 4.1323 - kl_loss: 6.9274\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 559.5663\n",
      "Success in episode 49 at time step 176\n",
      "Episode 50\n",
      "[-0.5561621  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6832\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6112 - reconstruction_loss: 1.5380 - kl_loss: 1.0732\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1146 - reconstruction_loss: 1.0363 - kl_loss: 1.0783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.6727\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.8133 - reconstruction_loss: 8.5265 - kl_loss: 2.2868\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 982us/step - loss: 6.4481 - reconstruction_loss: 4.1600 - kl_loss: 2.2881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0280\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.5027 - reconstruction_loss: 5.0480 - kl_loss: 6.4547\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 938us/step - loss: 11.5617 - reconstruction_loss: 5.1103 - kl_loss: 6.4514\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8304\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.9759\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.8121 - reconstruction_loss: 2.5447 - kl_loss: 6.2674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6215 - reconstruction_loss: 0.3568 - kl_loss: 6.2646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0065\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9147\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4642 - reconstruction_loss: 1.3710 - kl_loss: 4.0933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1508 - reconstruction_loss: 2.0646 - kl_loss: 4.0862\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0586\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.2889 - reconstruction_loss: 5.6819 - kl_loss: 3.6071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.1361 - reconstruction_loss: 0.5331 - kl_loss: 3.6030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6272\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1993 - reconstruction_loss: 5.4154 - kl_loss: 0.7839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1684 - reconstruction_loss: 1.3867 - kl_loss: 0.7817\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7484\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7311\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7077 - reconstruction_loss: 5.0722 - kl_loss: 0.6356\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9377 - reconstruction_loss: 2.3013 - kl_loss: 0.6363\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0806\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5659 - reconstruction_loss: 1.2912 - kl_loss: 3.2747\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8168 - reconstruction_loss: 1.5400 - kl_loss: 3.2768\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4888\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1208 - reconstruction_loss: 1.9621 - kl_loss: 4.1587\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.5117 - reconstruction_loss: 0.3559 - kl_loss: 4.1558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5095\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.4088\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.8699 - reconstruction_loss: 7.9229 - kl_loss: 5.9469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.7950 - reconstruction_loss: 1.8529 - kl_loss: 5.9421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.5794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6870 - reconstruction_loss: 0.8083 - kl_loss: 4.8787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 959us/step - loss: 6.3200 - reconstruction_loss: 1.4422 - kl_loss: 4.8778\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9907\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0323\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.6651 - reconstruction_loss: 4.2834 - kl_loss: 7.3816\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.4446 - reconstruction_loss: 3.0671 - kl_loss: 7.3775\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1452\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1019\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.6135 - reconstruction_loss: 4.2531 - kl_loss: 4.3604\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.0877 - reconstruction_loss: 4.7345 - kl_loss: 4.3532\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 641.8196\n",
      "Success in episode 50 at time step 167\n",
      "Episode 51\n",
      "[-0.47070056  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7152\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7382\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4926 - reconstruction_loss: 5.0597 - kl_loss: 0.4329\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.1195 - reconstruction_loss: 9.6847 - kl_loss: 0.4348\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2286\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2627\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2736 - reconstruction_loss: 1.9573 - kl_loss: 1.3163\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.3081 - reconstruction_loss: 2.9812 - kl_loss: 1.3268\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6399\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1490 - reconstruction_loss: 3.3000 - kl_loss: 4.8491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.5880 - reconstruction_loss: 0.7284 - kl_loss: 4.8596\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2504\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.2257\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.3447 - reconstruction_loss: 3.3645 - kl_loss: 8.9802\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.8871 - reconstruction_loss: 2.9018 - kl_loss: 8.9853\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8117\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7711\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.0766 - reconstruction_loss: 1.2049 - kl_loss: 6.8717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.5272 - reconstruction_loss: 6.6713 - kl_loss: 6.8559\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1486\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0423\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.0156 - reconstruction_loss: 1.2037 - kl_loss: 10.8120\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.6246 - reconstruction_loss: 1.8344 - kl_loss: 10.7902\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1461\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.8155 - reconstruction_loss: 4.0624 - kl_loss: 6.7531\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.1389 - reconstruction_loss: 6.3899 - kl_loss: 6.7490\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1021.8087\n",
      "Success in episode 51 at time step 80\n",
      "Episode 52\n",
      "[-0.4990316  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5087\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4922\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2794 - reconstruction_loss: 0.6880 - kl_loss: 0.5914\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5809 - reconstruction_loss: 1.9858 - kl_loss: 0.5951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5401\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5245\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9678 - reconstruction_loss: 2.5997 - kl_loss: 3.3681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.4725 - reconstruction_loss: 7.1046 - kl_loss: 3.3679\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2417\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2126\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.0438 - reconstruction_loss: 1.7953 - kl_loss: 5.2485\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.1127 - reconstruction_loss: 2.8656 - kl_loss: 5.2471\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2838\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.4838 - reconstruction_loss: 2.9425 - kl_loss: 6.5413\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 996us/step - loss: 7.3733 - reconstruction_loss: 0.8280 - kl_loss: 6.5453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8629\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9535\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.7042 - reconstruction_loss: 2.1887 - kl_loss: 6.5155\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.1297 - reconstruction_loss: 1.6057 - kl_loss: 6.5240\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3107\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2909\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.5319 - reconstruction_loss: 3.4279 - kl_loss: 10.1039\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.3733 - reconstruction_loss: 1.2581 - kl_loss: 10.1152\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5681\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.0783 - reconstruction_loss: 3.4548 - kl_loss: 6.6234\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.3855 - reconstruction_loss: 4.7549 - kl_loss: 6.6306\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 896.2099\n",
      "Success in episode 52 at time step 82\n",
      "Episode 53\n",
      "[-0.5453861  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6843\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0102 - reconstruction_loss: 0.7306 - kl_loss: 1.2796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7945 - reconstruction_loss: 1.5213 - kl_loss: 1.2732\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0840\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0870\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7523 - reconstruction_loss: 2.4874 - kl_loss: 2.2649\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6031 - reconstruction_loss: 3.3485 - kl_loss: 2.2546\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1628\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1631\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.5556 - reconstruction_loss: 2.3429 - kl_loss: 5.2127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.4437 - reconstruction_loss: 3.2411 - kl_loss: 5.2026\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2537\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.1181 - reconstruction_loss: 3.0569 - kl_loss: 5.0612\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.5801 - reconstruction_loss: 0.5324 - kl_loss: 5.0476\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4706\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3967\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.2597 - reconstruction_loss: 1.7813 - kl_loss: 5.4784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.1960 - reconstruction_loss: 1.7174 - kl_loss: 5.4786\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4548\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.6549 - reconstruction_loss: 0.4053 - kl_loss: 8.2496\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 922us/step - loss: 10.1246 - reconstruction_loss: 1.8651 - kl_loss: 8.2595\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4202\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.4042 - reconstruction_loss: 2.8377 - kl_loss: 5.5665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.8576 - reconstruction_loss: 3.2953 - kl_loss: 5.5623\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 883.8853\n",
      "Success in episode 53 at time step 83\n",
      "Episode 54\n",
      "[-0.5286013  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5251\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0737 - reconstruction_loss: 3.2585 - kl_loss: 0.8151\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6009 - reconstruction_loss: 0.7887 - kl_loss: 0.8122\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6105\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9266 - reconstruction_loss: 3.0080 - kl_loss: 0.9186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 935us/step - loss: 5.5957 - reconstruction_loss: 4.6779 - kl_loss: 0.9178\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.8570 - reconstruction_loss: 1.5348 - kl_loss: 4.3223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1081 - reconstruction_loss: 1.7984 - kl_loss: 4.3097\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0372\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9941\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.7803 - reconstruction_loss: 4.8257 - kl_loss: 6.9545\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 960us/step - loss: 9.4959 - reconstruction_loss: 2.5478 - kl_loss: 6.9481\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0329\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9977\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.5479 - reconstruction_loss: 1.7530 - kl_loss: 5.7949\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2827 - reconstruction_loss: 0.4888 - kl_loss: 5.7939\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5098\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.5787 - reconstruction_loss: 2.6641 - kl_loss: 5.9145\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.1878 - reconstruction_loss: 1.2808 - kl_loss: 5.9070\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3485\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3397\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.4283 - reconstruction_loss: 0.9966 - kl_loss: 8.4317\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.2963 - reconstruction_loss: 0.8709 - kl_loss: 8.4254\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4202\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4118\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.5975 - reconstruction_loss: 2.9375 - kl_loss: 5.6600\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.3836 - reconstruction_loss: 3.7342 - kl_loss: 5.6494\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 796.2805\n",
      "Success in episode 54 at time step 94\n",
      "Episode 55\n",
      "[-0.57470745  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6194\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2281 - reconstruction_loss: 1.3612 - kl_loss: 0.8669\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1102 - reconstruction_loss: 1.2449 - kl_loss: 0.8654\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8589\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8097 - reconstruction_loss: 2.6620 - kl_loss: 2.1477\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.5862 - reconstruction_loss: 2.4323 - kl_loss: 2.1538\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8760\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8665\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7254 - reconstruction_loss: 0.8081 - kl_loss: 3.9173\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4745 - reconstruction_loss: 2.5593 - kl_loss: 3.9153\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9913\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1067 - reconstruction_loss: 1.1499 - kl_loss: 3.9568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 950us/step - loss: 6.8529 - reconstruction_loss: 2.8998 - kl_loss: 3.9531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3391\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8030 - reconstruction_loss: 3.4506 - kl_loss: 4.3524\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4720 - reconstruction_loss: 2.1231 - kl_loss: 4.3490\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5846\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8293 - reconstruction_loss: 4.1428 - kl_loss: 2.6865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1630 - reconstruction_loss: 3.4793 - kl_loss: 2.6837\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0084\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0096\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.5401 - reconstruction_loss: 1.6913 - kl_loss: 6.8488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.5624 - reconstruction_loss: 1.7248 - kl_loss: 6.8376\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7722\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7674\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.7743 - reconstruction_loss: 10.0452 - kl_loss: 9.7291\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.0144 - reconstruction_loss: 16.3020 - kl_loss: 9.7123\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9016\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8916\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.2842 - reconstruction_loss: 5.0614 - kl_loss: 5.2228\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.4339 - reconstruction_loss: 5.2026 - kl_loss: 5.2313\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 782.7678\n",
      "Success in episode 55 at time step 98\n",
      "Episode 56\n",
      "[-0.50975317  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7202\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7074\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4286 - reconstruction_loss: 2.8314 - kl_loss: 0.5972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2592 - reconstruction_loss: 3.6632 - kl_loss: 0.5961\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.8294 - reconstruction_loss: 2.4516 - kl_loss: 3.3778\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 983us/step - loss: 3.9667 - reconstruction_loss: 0.5794 - kl_loss: 3.3874\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4039\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4120\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8465 - reconstruction_loss: 0.6649 - kl_loss: 5.1816\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.5948 - reconstruction_loss: 0.3960 - kl_loss: 5.1989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0225\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0176\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6803 - reconstruction_loss: 0.8227 - kl_loss: 4.8575\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9164 - reconstruction_loss: 1.0462 - kl_loss: 4.8701\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7008\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.6078 - reconstruction_loss: 1.9370 - kl_loss: 8.6707\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.1966 - reconstruction_loss: 0.5039 - kl_loss: 8.6927\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1144\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1136\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.8208 - reconstruction_loss: 0.5723 - kl_loss: 11.2485\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.5769 - reconstruction_loss: 6.3261 - kl_loss: 11.2508\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9266\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.7563 - reconstruction_loss: 4.2356 - kl_loss: 6.5206\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.7393 - reconstruction_loss: 4.2185 - kl_loss: 6.5208\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 898.0124\n",
      "Success in episode 56 at time step 74\n",
      "Episode 57\n",
      "[-0.49623865  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3864\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3858\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3905 - reconstruction_loss: 0.7303 - kl_loss: 0.6602\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1061 - reconstruction_loss: 0.4450 - kl_loss: 0.6612\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.9589\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.9460\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5218 - reconstruction_loss: 4.4203 - kl_loss: 1.1015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.7302 - reconstruction_loss: 7.6291 - kl_loss: 1.1010\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4484\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4394\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0667 - reconstruction_loss: 5.9759 - kl_loss: 1.0908\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5804 - reconstruction_loss: 1.4845 - kl_loss: 1.0959\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9305\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3845 - reconstruction_loss: 1.4759 - kl_loss: 1.9085\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.6181 - reconstruction_loss: 9.7072 - kl_loss: 1.9109\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1791\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1728\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.4920 - reconstruction_loss: 3.2370 - kl_loss: 9.2550\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.4436 - reconstruction_loss: 2.2163 - kl_loss: 9.2273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.8504 - reconstruction_loss: 2.0443 - kl_loss: 8.8061\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.9016 - reconstruction_loss: 1.1429 - kl_loss: 8.7587\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0808\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.5703 - reconstruction_loss: 3.9505 - kl_loss: 10.6197\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.4336 - reconstruction_loss: 2.8703 - kl_loss: 10.5633\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.8479 - reconstruction_loss: 3.4382 - kl_loss: 9.4096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.5255 - reconstruction_loss: 1.1193 - kl_loss: 9.4063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5935\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5184\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.6637 - reconstruction_loss: 1.7212 - kl_loss: 10.9425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.2432 - reconstruction_loss: 0.2752 - kl_loss: 10.9680\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0064\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.8162 - reconstruction_loss: 4.0052 - kl_loss: 6.8111\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.1192 - reconstruction_loss: 2.3261 - kl_loss: 6.7931\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 654.5365\n",
      "Success in episode 57 at time step 116\n",
      "Episode 58\n",
      "[-0.45330012  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8260\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2241 - reconstruction_loss: 5.7111 - kl_loss: 0.5130\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9857 - reconstruction_loss: 2.4706 - kl_loss: 0.5151\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8511\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14.8052 - reconstruction_loss: 13.2318 - kl_loss: 1.5734\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9261 - reconstruction_loss: 2.3497 - kl_loss: 1.5764\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5079\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4835\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7639 - reconstruction_loss: 1.7478 - kl_loss: 2.0162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 947us/step - loss: 5.1045 - reconstruction_loss: 3.0883 - kl_loss: 2.0162\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4512\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.3211 - reconstruction_loss: 4.6012 - kl_loss: 4.7198\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.7525 - reconstruction_loss: 5.0438 - kl_loss: 4.7087\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2772\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.5150 - reconstruction_loss: 5.8670 - kl_loss: 6.6480\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.8272 - reconstruction_loss: 0.1937 - kl_loss: 6.6335\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5358\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.4329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6486 - reconstruction_loss: 0.2466 - kl_loss: 5.4020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.3360 - reconstruction_loss: 4.9473 - kl_loss: 5.3887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9368\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8484\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3190 - reconstruction_loss: 2.9272 - kl_loss: 3.3918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 972us/step - loss: 4.9694 - reconstruction_loss: 1.5799 - kl_loss: 3.3895\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1756\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7847 - reconstruction_loss: 4.0988 - kl_loss: 2.6859\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6545 - reconstruction_loss: 0.9805 - kl_loss: 2.6740\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3537\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3581\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1232 - reconstruction_loss: 1.0452 - kl_loss: 4.0780\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8511 - reconstruction_loss: 0.7877 - kl_loss: 4.0634\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1007\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0991\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.1643 - reconstruction_loss: 5.2050 - kl_loss: 4.9593\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.9853 - reconstruction_loss: 3.0329 - kl_loss: 4.9524\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4938\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4998\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.4261 - reconstruction_loss: 32.8962 - kl_loss: 6.5299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37.8537 - reconstruction_loss: 31.3090 - kl_loss: 6.5446\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7817\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.6840\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24.3769 - reconstruction_loss: 4.4253 - kl_loss: 19.9516\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.0087 - reconstruction_loss: 7.1026 - kl_loss: 19.9061\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 71.4633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 64.0902\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.2496 - reconstruction_loss: 0.7319 - kl_loss: 18.5177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 933us/step - loss: 21.0137 - reconstruction_loss: 2.5507 - kl_loss: 18.4630\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2419\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.6330 - reconstruction_loss: 3.0964 - kl_loss: 8.5366\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.6417 - reconstruction_loss: 0.0976 - kl_loss: 8.5441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4869\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.3133 - reconstruction_loss: 0.6231 - kl_loss: 11.6902\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.2899 - reconstruction_loss: 1.6107 - kl_loss: 11.6791\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9860\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9475\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0028 - reconstruction_loss: 7.0479 - kl_loss: 6.9549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.4500 - reconstruction_loss: 5.5042 - kl_loss: 6.9458\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 763.0106\n",
      "Success in episode 58 at time step 185\n",
      "Episode 59\n",
      "[-0.578998  0.      ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0817\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5599 - reconstruction_loss: 2.0457 - kl_loss: 1.5141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 921us/step - loss: 6.0491 - reconstruction_loss: 4.5247 - kl_loss: 1.5245\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5523\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0584 - reconstruction_loss: 2.1548 - kl_loss: 1.9036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4381 - reconstruction_loss: 1.5254 - kl_loss: 1.9127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2158\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.8722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0833 - reconstruction_loss: 2.4996 - kl_loss: 3.5837\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.9293 - reconstruction_loss: 3.3389 - kl_loss: 3.5905\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5017\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3363\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2019 - reconstruction_loss: 1.7235 - kl_loss: 4.4784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 881us/step - loss: 6.3468 - reconstruction_loss: 1.8622 - kl_loss: 4.4846\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4427\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4304\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7563 - reconstruction_loss: 0.3568 - kl_loss: 4.3994\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8050 - reconstruction_loss: 0.4141 - kl_loss: 4.3909\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4303\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4465\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.1878 - reconstruction_loss: 1.5448 - kl_loss: 7.6430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.6049 - reconstruction_loss: 7.9778 - kl_loss: 7.6271\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2644\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.4445 - reconstruction_loss: 4.6094 - kl_loss: 4.8351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.2211 - reconstruction_loss: 3.3858 - kl_loss: 4.8353\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1103.2891\n",
      "Success in episode 59 at time step 81\n",
      "Episode 60\n",
      "[-0.4850393  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5971\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5889\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4019 - reconstruction_loss: 0.3707 - kl_loss: 1.0312\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3882 - reconstruction_loss: 6.3514 - kl_loss: 1.0368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6206\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9705 - reconstruction_loss: 0.9317 - kl_loss: 3.0388\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8381 - reconstruction_loss: 0.7931 - kl_loss: 3.0449\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2533\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0740\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.9670 - reconstruction_loss: 4.0828 - kl_loss: 4.8842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7881 - reconstruction_loss: 0.8917 - kl_loss: 4.8965\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7318\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.4444\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.4078 - reconstruction_loss: 2.6461 - kl_loss: 6.7617\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.3176 - reconstruction_loss: 3.5375 - kl_loss: 6.7801\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1982\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3975\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.4588 - reconstruction_loss: 2.6597 - kl_loss: 6.7991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.4953 - reconstruction_loss: 1.6703 - kl_loss: 6.8249\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6202\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6495\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.7263 - reconstruction_loss: 1.9120 - kl_loss: 5.8143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 896us/step - loss: 8.6017 - reconstruction_loss: 2.7679 - kl_loss: 5.8338\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6477\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.1559 - reconstruction_loss: 6.6679 - kl_loss: 6.4881\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.1357 - reconstruction_loss: 1.6313 - kl_loss: 6.5044\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5218\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.6653 - reconstruction_loss: 2.5364 - kl_loss: 6.1290\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.0531 - reconstruction_loss: 3.9185 - kl_loss: 6.1347\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 796.6009\n",
      "Success in episode 60 at time step 92\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=60, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.train_vae = False\n",
    "daifa.model_vae.show_training = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make the HABIT ACTION NET\n",
    "habit_net = HabitualAction(latent_dim, 1, [16, 16], train_epochs=2, show_training=True)\n",
    "habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "daifa.habit_action_model = habit_net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.habit_action_model.show_training = True\n",
    "daifa.train_habit_net = True\n",
    "daifa.train_after_exploring = True\n",
    "daifa.use_kl_intrinsic = True\n",
    "daifa.use_kl_extrinsic = False\n",
    "daifa.use_fast_thinking = True\n",
    "daifa.uncertainty_tolerance = 0.1\n",
    "\n",
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(obs_pos)\n",
    "\n",
    "utils = daifa.prior_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(vel_pos)\n",
    "\n",
    "utils = daifa.prior_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(obs_pos)\n",
    "\n",
    "# utils = daifa.habit_action_model.actor_model(latent_mean)\n",
    "utils = daifa.habit_action_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(vel_pos)\n",
    "\n",
    "# utils = daifa.habit_action_model.actor_model(latent_mean)\n",
    "utils = daifa.habit_action_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "obs_stddev = [0.05, 0.05]\n",
    "# obs_stddev = [0, 0]\n",
    "\n",
    "\n",
    "t_max = 999\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    done = False\n",
    "    rewards = []\n",
    "\n",
    "    t = 0\n",
    "    while not done:\n",
    "\n",
    "        obs = obs.reshape(1, obs.shape[0])\n",
    "        obs = transform_observations(obs, observation_max, observation_min, obs_stddev)\n",
    "\n",
    "        # print(obs)\n",
    "\n",
    "        latent_mean, _, _ = daifa.model_vae.encoder(obs)\n",
    "        # action = daifa.habit_action_model.actor_model(latent_mean)\n",
    "        action = daifa.habit_action_model(latent_mean)\n",
    "        action = action.numpy()\n",
    "\n",
    "        for k in range(daifa.agent_time_ratio):\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            env.render()\n",
    "\n",
    "            # print(obs)\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "            t += 1\n",
    "\n",
    "            if t == t_max:\n",
    "                done = True\n",
    "                break\n",
    "            elif done:\n",
    "                break\n",
    "\n",
    "    print(t)\n",
    "    if t < t_max:\n",
    "        print(\"success\")\n",
    "    else:\n",
    "        print(\"Failure\")\n",
    "        print(\"max obs\", obs)\n",
    "\n",
    "    print(np.sum(rewards))\n",
    "    # print(rewards)\n",
    "\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}