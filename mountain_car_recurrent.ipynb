{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run The Agent on Mountain Car"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from vae_recurrent import VAE, create_decoder, create_encoder\n",
    "from transition_gru import TransitionGRU\n",
    "from recurrent_agent import DAIFAgentRecurrent\n",
    "from prior_model import PriorModelBellman\n",
    "from habitual_action_network import HabitualAction, compute_discounted_cumulative_reward\n",
    "from ddpg import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations\n",
    "from train_agent import train_single_agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from identity_vae import IdentityVAE, identity_encoder, identity_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with no prior model FEEF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PriorModelBellman(keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 observation_dim,\n",
    "                 output_dim=1,\n",
    "                 iterate_train=1,\n",
    "                 discount_factor=0.99,\n",
    "                 training_epochs=1,\n",
    "                 show_training=True,\n",
    "                 use_tanh_on_output=True):\n",
    "\n",
    "        super(PriorModelBellman, self).__init__()\n",
    "        self.observation_dim = observation_dim\n",
    "        self.iterate_train = iterate_train\n",
    "        self.discount_factor = discount_factor\n",
    "        self.train_epochs = 1\n",
    "\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.train_epochs = training_epochs\n",
    "        self.show_training = show_training\n",
    "\n",
    "        # make the model\n",
    "        transition_inputs = layers.Input(observation_dim)\n",
    "        h = layers.Dense(observation_dim * 20, activation=\"silu\")(transition_inputs)\n",
    "        if use_tanh_on_output:\n",
    "            h = layers.Dense(output_dim, activation=\"tanh\")(h)\n",
    "        else:\n",
    "            h = layers.Dense(output_dim)(h)\n",
    "\n",
    "        self.prior_model = keras.Model(transition_inputs, h, name=\"prior_model\")\n",
    "        self.prior_model.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "    def call(self, observations):\n",
    "        return self.prior_model(observations)\n",
    "\n",
    "    def extrinsic_kl(self, observations):\n",
    "        return 1.0 - self(observations)  # map from [-1, 1] to [2, 0]\n",
    "\n",
    "    def train(self, observations, rewards):\n",
    "        \"\"\"\n",
    "        :param observations: o_0, o_1, ... , o_n\n",
    "        :param rewards: list with r_0, r_1, ... , r_n\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        num_observations = len(observations)\n",
    "\n",
    "        print(rewards)\n",
    "\n",
    "        for i in range(self.iterate_train):\n",
    "\n",
    "            # TODO Still seems a little strange that we add 0 to the end and discount the way we do but I think it makes sense. Check what predicted utilities are in practice\n",
    "            utility_t = self.prior_model(observations)\n",
    "            # utility_t_plus_one = tf.concat([utility_t[1:], tf.zeros((1, self.output_dim), dtype=utility_t.dtype)], axis=0)\n",
    "            utility_t_plus_one = tf.concat([utility_t[1:], 0], axis=0)\n",
    "\n",
    "            # OR just have constant gamma\n",
    "            discount_factors = np.ones_like(utility_t_plus_one) * self.discount_factor\n",
    "\n",
    "            # reducing discount factors through time\n",
    "            # discount_factors = np.power([self.discount_factor]*num_observations, np.arange(num_observations)).reshape(observations.shape[0], 1)\n",
    "            # discount_factors = np.flip(discount_factors)\n",
    "\n",
    "            # print(discount_factors)\n",
    "\n",
    "            # print(predicted_utility, pred_next_v)\n",
    "\n",
    "            expected_utility = rewards_stacked + discount_factors * utility_t_plus_one\n",
    "\n",
    "            # print(rewards_stacked)\n",
    "            # print(discount_factors * utility_t_plus_one)\n",
    "\n",
    "            self.prior_model.fit(observations, expected_utility, epochs=self.train_epochs, verbose=self.show_training)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "# from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 habitual_action_net,\n",
    "                 given_prior_mean=None,\n",
    "                 given_prior_stddev=None,\n",
    "                 agent_time_ratio=6,\n",
    "                 actions_to_execute_when_exploring=2,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True,\n",
    "                 train_prior_model=True,\n",
    "                 train_habit_net=True,\n",
    "                 train_with_replay=True,\n",
    "                 train_after_exploring=True,\n",
    "                 use_kl_extrinsic=True,\n",
    "                 use_kl_intrinsic=True,\n",
    "                 use_FEEF=True,\n",
    "                 use_fast_thinking=False,\n",
    "                 uncertainty_tolerance=0.05,\n",
    "                 habit_model_type=\"name_of_model\"):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        # parameters for slow policy planning\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        # flags for whether or not we are training models or using pretrained models and when we should train\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "        self.train_habit_net = train_habit_net\n",
    "        self.train_prior = train_prior_model\n",
    "        self.train_with_replay = train_with_replay\n",
    "        self.train_after_exploring = train_after_exploring\n",
    "\n",
    "        # do we use the kl divergence for extrinsic vs intrinsic\n",
    "        self.use_kl_intrinsic = use_kl_intrinsic\n",
    "        self.use_kl_extrinsic = use_kl_extrinsic\n",
    "\n",
    "        # do we use the FEEF or EFE?\n",
    "        self.use_FEEF = use_FEEF\n",
    "\n",
    "        # given prior values\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.tran = tran\n",
    "        self.prior_model = prior_model\n",
    "        self.habit_action_model = habitual_action_net\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "        self.actions_to_execute_when_exploring = actions_to_execute_when_exploring\n",
    "        self.time_step = 0\n",
    "        self.exploring = False\n",
    "\n",
    "        # track the hidden state of the transition gru model so we can use it to train\n",
    "        self.tran_hidden_state = None\n",
    "        self.tran_hidden_state_pre_exploring = None\n",
    "        self.prev_tran_hidden_state = None\n",
    "\n",
    "        # store the full observations for the episode so we can train using replay\n",
    "        self.full_observation_sequence = []\n",
    "        self.full_action_sequence = []\n",
    "        self.full_reward_sequence = []\n",
    "\n",
    "        # store the observations while the agent is in exploration mode\n",
    "        self.exploring_observation_sequence = []\n",
    "        self.exploring_action_sequence = []\n",
    "        self.exploring_reward_sequence = []\n",
    "\n",
    "        # store the observations at the world time scale\n",
    "        self.env_time_scale_observations = []\n",
    "\n",
    "        self.policy_left_to_execute = [None]\n",
    "        self.previous_observation = None\n",
    "        self.action_being_executed = None\n",
    "        self.action_being_executed = 0\n",
    "\n",
    "        self.use_fast_thinking = use_fast_thinking\n",
    "        self.habit_model_type = habit_model_type\n",
    "        self.uncertainty_tolerance = uncertainty_tolerance\n",
    "\n",
    "\n",
    "    def perceive_and_act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        The function called to have the agent interact with the environment\n",
    "        We assume the agent gets a transformed/noisy observation from the environment and then returns an action\n",
    "\n",
    "        TODO: possibly the agent returns some other information for logging and showing experiments\n",
    "\n",
    "        :param observation:\n",
    "        :param reward:\n",
    "        :param done:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # track the world time scale observation sequence\n",
    "        self.env_time_scale_observations.append(observation)\n",
    "\n",
    "        # if the episode is finished, then do any training on the full data set\n",
    "        if done:\n",
    "\n",
    "            if self.train_with_replay:\n",
    "                print(\"training on full data\")\n",
    "\n",
    "                # add the final observation and reward we observed to the sequences\n",
    "                self.full_observation_sequence.append(observation)\n",
    "                self.full_reward_sequence.append(reward)\n",
    "\n",
    "                # Call the training function on the observation sequences to train everything we need to train\n",
    "                self.train_models(np.vstack(self.full_observation_sequence),\n",
    "                                  np.vstack(self.full_action_sequence),\n",
    "                                  np.array(self.full_reward_sequence),\n",
    "                                  None)\n",
    "\n",
    "\n",
    "        # Otherwise are we at a point where we can reconsider our policy and maybe train the world model\n",
    "        elif self.time_step % self.agent_time_ratio == 0:\n",
    "\n",
    "            # add the reward only if it's not the first observation\n",
    "            if self.time_step != 0:\n",
    "                self.full_reward_sequence.append(reward)\n",
    "\n",
    "            # add the observation to the sequence\n",
    "            self.full_observation_sequence.append(observation)\n",
    "\n",
    "            # We only update the model during the episode when we were exploring using the planning method and we have executed all of the actions in the policy\n",
    "            if self.exploring and len(self.policy_left_to_execute) == 0:\n",
    "\n",
    "                # print(\"f\", self.full_observation_sequence)\n",
    "                # print(\"e\", self.full_observation_sequence[-1*(self.actions_to_execute_when_exploring + 1):])\n",
    "\n",
    "                if self.train_after_exploring:\n",
    "\n",
    "                    # the actions done while exploring were the last self.actions_to_execute_when_exploring\n",
    "                    self.exploring_action_sequence = self.full_action_sequence[-1*self.actions_to_execute_when_exploring:]\n",
    "                    self.exploring_reward_sequence = self.full_reward_sequence[-1*self.actions_to_execute_when_exploring:]\n",
    "                    self.exploring_observation_sequence = self.full_observation_sequence[-1*(self.actions_to_execute_when_exploring + 1):]\n",
    "\n",
    "                    # Call the training function on the observation sequences to train everything we need to train\n",
    "                    self.train_models(np.vstack(self.exploring_observation_sequence),\n",
    "                                                np.vstack(self.exploring_action_sequence),\n",
    "                                                np.array(self.exploring_reward_sequence),\n",
    "                                                self.tran_hidden_state_pre_exploring)\n",
    "\n",
    "                self.exploring = False\n",
    "\n",
    "\n",
    "            # Predict the expected observation\n",
    "            action_as_array = np.array(self.action_being_executed).reshape(1, self.tran.action_dim)\n",
    "            expected_observation, self.tran_hidden_state = self.predict_next_observation(self.previous_observation, action_as_array, self.prev_tran_hidden_state)\n",
    "            # pred_next_observation, next_tran_hidden_state = None, None\n",
    "            # print(self.previous_observation, expected_observation, observation)\n",
    "            # Now we select our action. If we aren't exploring then either we act out of habit or we might need to explore\n",
    "            # I think I can check this based on whether or not there are actions left to execute in the current policy\n",
    "            if not self.exploring:\n",
    "\n",
    "                if self.use_fast_thinking and self.previous_observation is None:\n",
    "                    # self.policy_left_to_execute = self.habit_action_model(observation)\n",
    "                    self.policy_left_to_execute = self.select_fast_thinking_policy(observation)\n",
    "                    self.policy_left_to_execute = self.policy_left_to_execute.numpy().tolist()  # tf tensor to list\n",
    "                    print(\"fast thinking\")\n",
    "\n",
    "                # TDOD Fix this to work however it needs to\n",
    "                # we need to see what the generative model now thinks about what the expected current observation is\n",
    "                elif self.use_fast_thinking and np.allclose(observation, expected_observation, atol=self.uncertainty_tolerance):  # within some tolerance\n",
    "\n",
    "                    self.policy_left_to_execute = self.select_fast_thinking_policy(observation)\n",
    "                    # self.policy_left_to_execute = self.policy_left_to_execute + np.random.normal(0, scale=self.habit_action_model.action_std_dev)\n",
    "                    self.policy_left_to_execute = self.policy_left_to_execute.numpy().tolist()\n",
    "\n",
    "                    # self.tran_hidden_state = next_tran_hidden_state\n",
    "\n",
    "                    print(\"fast thinking\")\n",
    "\n",
    "                # the generative model is surprised so we should use the slow deliberation for planning out a policy that balances exploration and exploitation\n",
    "                else:\n",
    "                    # print(\"slow thinking\")\n",
    "                    policy = self.select_policy(observation)\n",
    "                    # print(policy.mean())\n",
    "                    # TODO should we actually sample here?\n",
    "                    policy = policy.mean().numpy()\n",
    "                    policy = policy.reshape(policy.shape[0], self.tran.action_dim).tolist()\n",
    "                    self.policy_left_to_execute = policy[0: self.actions_to_execute_when_exploring]\n",
    "\n",
    "                    self.tran_hidden_state_pre_exploring = self.tran_hidden_state\n",
    "\n",
    "                    self.exploring = True\n",
    "\n",
    "                # print(observation)\n",
    "                # print(pred_next_observation)\n",
    "\n",
    "            # finally update the previous observation and action to be the one we just had/did\n",
    "            self.previous_observation = observation\n",
    "            self.prev_tran_hidden_state = self.tran_hidden_state\n",
    "            self.action_being_executed = self.policy_left_to_execute[0]\n",
    "            self.full_action_sequence.append(self.action_being_executed)\n",
    "            self.policy_left_to_execute.pop(0)\n",
    "\n",
    "        # final updates increment the current timestep and return the action specified by the policy\n",
    "        self.time_step += 1\n",
    "\n",
    "        return self.action_being_executed\n",
    "\n",
    "\n",
    "    def predict_next_observation(self, obs, action, tran_hidden_state):\n",
    "\n",
    "        # TODO: Fix this with the transition hidden states\n",
    "        if obs is None:\n",
    "            return None, None\n",
    "        else:\n",
    "            z_mean, z_std, z = self.model_vae.encoder(obs)\n",
    "            # print(z_mean.shape)\n",
    "            # print(action.shape)\n",
    "            z_mean = z_mean.numpy()\n",
    "            z_plus_action = np.concatenate([z_mean, action], axis=1)\n",
    "            # print(z_mean)\n",
    "            # print(action)\n",
    "            # print(z_plus_action)\n",
    "\n",
    "            z_plus_action = z_plus_action.reshape(1, 1, z_plus_action.shape[1])\n",
    "            # print(z_plus_action)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((z_plus_action, tran_hidden_state))\n",
    "\n",
    "            next_observation = self.model_vae.decoder(next_latent_mean)\n",
    "            # print(next_observation)\n",
    "            return next_observation.numpy(), next_hidden_state\n",
    "\n",
    "\n",
    "    # We use this function to reset the hidden state of the transition model when we want to train on the full data set\n",
    "    def reset_tran_hidden_state(self):\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "\n",
    "    def reset_all_states(self):\n",
    "        self.time_step = 0\n",
    "        self.exploring = False\n",
    "\n",
    "        # track the hidden state of the transition gru model so we can use it to train\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "        # store the full observations for the episode so we can train using replay\n",
    "        self.complete_observation_sequence = []\n",
    "        self.full_observation_sequence = []\n",
    "        self.full_action_sequence = []\n",
    "        self.full_reward_sequence = []\n",
    "\n",
    "        # store the observations while the agent is in exploration mode\n",
    "        self.exploring_observation_sequence = []\n",
    "        self.exploring_action_sequence = []\n",
    "        self.exploring_reward_sequence = []\n",
    "\n",
    "        self.policy_left_to_execute = []\n",
    "        self.previous_observation = None\n",
    "        self.previous_action_executed = None\n",
    "\n",
    "\n",
    "    def train_models(self, observations_full, actions, rewards, tran_hidden_state_pre_obs):\n",
    "\n",
    "        pre_observations = observations_full[:-1]\n",
    "        post_observations = observations_full[1:]\n",
    "\n",
    "        # find the actual observed latent states using the vae\n",
    "        pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "        post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "        #### TRAIN THE TRANSITION MODEL ####\n",
    "        if self.train_tran:\n",
    "\n",
    "            num_observations = pre_observations.shape[0]\n",
    "            # observation_dim = pre_observations.shape[1]\n",
    "            action_dim = actions.shape[1]\n",
    "            latent_dim = self.model_vae.latent_dim\n",
    "\n",
    "            # set up the input training data that we use to train the transition model\n",
    "            z_train = np.concatenate([np.array(pre_latent_mean), actions], axis=1)\n",
    "\n",
    "            # we use the sequence to find the right hidden states to use as input\n",
    "            z_train_seq = z_train.reshape((1, num_observations, latent_dim + action_dim))\n",
    "            z_train_singles = z_train.reshape(num_observations, 1, latent_dim + action_dim)\n",
    "\n",
    "            # the previous hidden state is the memory after observing some sequences but it might be None if we're just starting\n",
    "            if tran_hidden_state_pre_obs is None:\n",
    "                tran_hidden_state_pre_obs = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, tran_hidden_state_pre_obs))\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([tran_hidden_state_pre_obs, h_states_for_training], axis=0)\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran.train_epochs, verbose=self.tran.show_training, batch_size=z_train_singles.shape[0])\n",
    "\n",
    "            # now find the new predicted hidden state that we will use for finding the policy\n",
    "            # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "            _, _, final_hidden_state, h_states = self.tran((z_train_seq, tran_hidden_state_pre_obs))\n",
    "            # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "            z_pred, _, _, _ = self.tran((z_train_singles, h_states_for_training))\n",
    "            # print(h_states)\n",
    "            # print(final_hidden_state)\n",
    "            # print(h_states[:, -2, :])\n",
    "            self.prev_tran_hidden_state = h_states[:, -2, :]\n",
    "            self.tran_hidden_state = final_hidden_state\n",
    "\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            # self.model_vae.fit(pre_observations_raw, epochs=self.vae_train_epochs, verbose=self.show_vae_training)\n",
    "            self.model_vae.fit(pre_observations, epochs=self.model_vae.train_epochs, verbose=self.model_vae.show_training, batch_size=pre_observations.shape[0])\n",
    "\n",
    "\n",
    "        #### TRAIN THE PRIOR MODEL ####\n",
    "        # TODO fix how this part should work\n",
    "        if self.train_prior:\n",
    "            # self.prior_model.train(post_observations, rewards, verbose=self.show_prior_training)\n",
    "            if max(rewards) > 0:\n",
    "                # self.prior_model.train(post_observations, rewards)\n",
    "                self.prior_model.train(post_latent_mean, rewards)\n",
    "\n",
    "\n",
    "        #### TRAIN THE HABIT ACTION NET ####\n",
    "        if self.train_habit_net:\n",
    "\n",
    "            # prior_preferences_mean = tf.convert_to_tensor(self.given_prior_mean, dtype=\"float32\")\n",
    "            # prior_preferences_stddev = tf.convert_to_tensor(self.given_prior_stddev, dtype=\"float32\")\n",
    "            #\n",
    "            # prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "            #\n",
    "            # external_efe = -1 * tf.math.log(prior_dist.prob(post_observations))\n",
    "            # external_efe = external_efe.numpy().reshape(external_efe.shape[0], 1)\n",
    "            #\n",
    "            # one_over_external_efe = 1/external_efe\n",
    "            #\n",
    "            # ten_minus_external_efe = -1*external_efe + 10\n",
    "\n",
    "            # ten_minus_external_efe = ten_minus_external_efe.numpy().reshape(ten_minus_external_efe.shape[0], 1)\n",
    "\n",
    "            # one_over_external_efe = one_over_external_efe.numpy().reshape(one_over_external_efe.shape[0], 1)\n",
    "            # print(one_over_external_efe.shape)\n",
    "\n",
    "            # print(post_observations)\n",
    "            # print(one_over_external_efe)\n",
    "\n",
    "            # obs_utilities = self.prior_model(pre_observations)\n",
    "            # obs_utilities = tf.reduce_sum(obs_utilities, axis=-1)\n",
    "            # obs_utilities = obs_utilities.numpy().reshape(obs_utilities.shape[0], 1)\n",
    "            # # print(obs_utilities)\n",
    "            #\n",
    "            # cum_rewards = compute_discounted_cumulative_reward(obs_utilities, self.habit_action_model.discount_factor)\n",
    "\n",
    "            if self.habit_model_type == \"PG\":\n",
    "                rewards = rewards.reshape(rewards.shape[0], 1)\n",
    "                cum_rewards = compute_discounted_cumulative_reward(rewards, self.habit_action_model.discount_factor)\n",
    "                rewards_to_train_on = cum_rewards\n",
    "\n",
    "                # print(cum_rewards)\n",
    "                # print(cum_rewards.sum())\n",
    "\n",
    "                # rewards_to_train_on = rewards\n",
    "\n",
    "                # DDPG and policy gradient interface with same function\n",
    "                self.habit_action_model.train(pre_latent_mean, actions, rewards_to_train_on, post_latent_mean)\n",
    "\n",
    "            if self.habit_model_type == \"DDPG\":\n",
    "                self.habit_action_model.train(pre_latent_mean, actions, rewards, post_latent_mean)\n",
    "\n",
    "\n",
    "    def select_fast_thinking_policy(self, observation):\n",
    "\n",
    "        # TODO should you select the mean here?\n",
    "        # _,  _, latent_state = self.model_vae.encoder(observation)\n",
    "        latent_state,  _, _ = self.model_vae.encoder(observation)\n",
    "        if self.habit_model_type == \"DDPG\":\n",
    "            action = self.habit_action_model.actor_model(latent_state)\n",
    "        elif self.habit_model_type == \"PG\":\n",
    "            action = self.habit_action_model(latent_state)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "        \"\"\"\n",
    "        :param observation: needs to be [n, observation_dim] shape np array or tf tensor\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO do you take the mean or that latent here?\n",
    "        # get the latent state from this observation\n",
    "        # TODO should I use the mean here?\n",
    "        _,  _, latent_state = self.model_vae.encoder(observation)\n",
    "        # latent_state,  _, _ = self.model_vae.encoder(observation)\n",
    "        # latent_state = latent_state.numpy().reshape((1, latent_state.shape[0]))\n",
    "        # print(latent_state)\n",
    "        # print(latent_state)\n",
    "        # select the policy\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(latent_state)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    # TODO Fix this so we can use different action dimensions\n",
    "    def cem_policy_optimisation(self, latent_z):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros((self.planning_horizon, self.tran.action_dim))\n",
    "        std_best_policies = tf.ones((self.planning_horizon, self.tran.action_dim))\n",
    "\n",
    "        # print(mean_best_policies)\n",
    "        # print(mean_best_policies.shape)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            # print(\"p\", policies.shape)\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "            # policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), latent_z)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            # print(\"POLICIES\", policies)\n",
    "            # print(\"FEEFS\", FEEFs)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by -1 to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape (self.n_policies, latent_dim) when z_t_minus is tensor with shape (1, latent_dim)\n",
    "        prev_latent_mean = tf.squeeze(tf.stack([z_t_minus_one]*self.n_policies, axis=1))\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.tran_hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.tran_hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # print(prev_latent_mean)\n",
    "            # print(policies[:, t, :].shape)\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t, :]], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            # print(tran_input.shape)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        if self.use_FEEF:\n",
    "            return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "        else:\n",
    "            return self.EFE(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "                # Compute the extrinisc approximation with the prior model\n",
    "                else:\n",
    "                    kl_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    kl_extrinsic = tf.reduce_sum(kl_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                kl_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"Extrinsic\", kl_extrinsic)\n",
    "            # print(\"Intrinsic\", kl_intrinsic)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    # TODO Find out how this works with the log probability extrinsic term\n",
    "    def EFE(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        EFEs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack(self.given_prior_mean), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack(self.given_prior_stddev), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    # compute extrinsic prior preferences term\n",
    "                    efe_extrinsic = -1 * tf.math.log(prior_dist.prob(predicted_likelihood))\n",
    "\n",
    "                # TODO Can I use the learned prior model here?\n",
    "                else:\n",
    "                    # efe_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    efe_extrinsic = self.prior_model.extrinsic_kl(predicted_posterior)\n",
    "                    efe_extrinsic = tf.reduce_sum(efe_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                efe_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"EX\", efe_extrinsic)\n",
    "            # print(\"IN\", kl_intrinsic)\n",
    "\n",
    "            EFE = efe_extrinsic - kl_intrinsic\n",
    "\n",
    "            EFEs.append(EFE)\n",
    "\n",
    "        return EFEs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "pln_hrzn = 5\n",
    "latent_dim = 2\n",
    "obs_dim = 2\n",
    "\n",
    "# make the VAE\n",
    "enc = create_encoder(2, latent_dim, [20])\n",
    "dec = create_decoder(latent_dim, 2, [20])\n",
    "vae = VAE(enc, dec, latent_dim,  [0]*latent_dim, [0.3]*latent_dim, train_epochs=2, show_training=True)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the TRANSITION\n",
    "tran = TransitionGRU(latent_dim, 1, 2*pln_hrzn*latent_dim, 2, train_epochs=2, show_training=True)\n",
    "tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the HABIT ACTION NET\n",
    "habit_net = HabitualAction(latent_dim, 1, [16, 16], train_epochs=2, show_training=True)\n",
    "habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# actor_model = get_actor(latent_dim, 1)\n",
    "# critic_model = get_critic(latent_dim, 1)\n",
    "#\n",
    "# target_actor = get_actor(latent_dim, 1)\n",
    "# target_critic = get_critic(latent_dim, 1)\n",
    "#\n",
    "# # Making the weights equal initially\n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "#\n",
    "# critic_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "# actor_optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "#\n",
    "# habit_net = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau=0.005, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer)\n",
    "\n",
    "# make the PRIOR NET\n",
    "prior_model = PriorModelBellman(latent_dim, output_dim=1, show_training=True)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "\n",
    "# daifa = DAIFAgentRecurrent(None,\n",
    "#                            vae,\n",
    "#                            tran,\n",
    "#                            habit_net,\n",
    "#                            scaled_prior_mean,\n",
    "#                            prior_stddev,\n",
    "#                            planning_horizon=pln_hrzn,\n",
    "#                            use_kl_extrinsic=True,\n",
    "#                            use_kl_intrinsic=True,\n",
    "#                            use_FEEF=False,\n",
    "#                            train_habit_net=True,\n",
    "#                            train_prior_model=False,\n",
    "#                            train_with_replay=True,\n",
    "#                            train_after_exploring=True,\n",
    "#                            use_fast_thinking=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           habit_net,\n",
    "                           planning_horizon=pln_hrzn,\n",
    "                           use_kl_extrinsic=False,  # maybe this works\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           train_habit_net=True,\n",
    "                           train_prior_model=True,\n",
    "                           train_tran=True,\n",
    "                           train_after_exploring=True,\n",
    "                           train_with_replay=True,\n",
    "                           use_fast_thinking=False,\n",
    "                           habit_model_type=\"PG\")\n",
    "\n",
    "\n",
    "\n",
    "daifa.train_prior = True\n",
    "daifa.prior_model.show_training = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.48151875  0.        ]\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 528ms/step - kl_loss: 0.0653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0598\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 45.8163 - reconstruction_loss: 38.4364 - kl_loss: 7.3799\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21.5488 - reconstruction_loss: 14.2295 - kl_loss: 7.3193\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 94ms/step - loss: -81.7286\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -83.3360\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0248\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.4684 - reconstruction_loss: 113.6694 - kl_loss: 7.7990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48.4176 - reconstruction_loss: 40.6887 - kl_loss: 7.7289\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.4721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.3641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2358\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2271\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.6199 - reconstruction_loss: 87.3092 - kl_loss: 6.3107\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42.4681 - reconstruction_loss: 36.2032 - kl_loss: 6.2649\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -77.9364\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -77.9284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.2000 - reconstruction_loss: 7.0639 - kl_loss: 6.1361\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62.0491 - reconstruction_loss: 55.9433 - kl_loss: 6.1059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.5063\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.5056\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0400\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 101.4456 - reconstruction_loss: 93.8918 - kl_loss: 7.5538\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.8086 - reconstruction_loss: 28.2895 - kl_loss: 7.5191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -45.3623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.4117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0755\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0729\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 43.4781 - reconstruction_loss: 34.7330 - kl_loss: 8.7451\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.1200 - reconstruction_loss: 20.4202 - kl_loss: 8.6998\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -90.3782\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -90.7896\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68.8018 - reconstruction_loss: 60.8689 - kl_loss: 7.9329\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 224.1586 - reconstruction_loss: 216.2723 - kl_loss: 7.8862\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -92.0947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -93.0722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 222.8707 - reconstruction_loss: 216.8210 - kl_loss: 6.0497\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 97.7913 - reconstruction_loss: 91.7954 - kl_loss: 5.9959\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -72.3125\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -71.9887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3521\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3381\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 156.5485 - reconstruction_loss: 151.9880 - kl_loss: 4.5605\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 173.0469 - reconstruction_loss: 168.4869 - kl_loss: 4.5600\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -94.4915\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -95.0850\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 336ms/step - kl_loss: 0.1013\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0966\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 158.6084 - reconstruction_loss: 152.2175 - kl_loss: 6.3909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103.4439 - reconstruction_loss: 97.0867 - kl_loss: 6.3572\n",
      "[-7.17073520e-02 -9.26769618e-02 -2.29248433e-03 -2.70656365e-02\n",
      " -7.44839842e-02 -8.30576987e-02 -2.28602764e-03 -9.27606637e-03\n",
      " -2.62700533e-02 -8.50666721e-02 -8.37080628e-02 -8.46299135e-02\n",
      " -8.48515069e-02 -8.98994599e-02 -6.06443025e-02 -8.21732383e-02\n",
      " -7.88351265e-02 -8.69200352e-02 -8.61805339e-02  9.99124697e+01]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Ranks of all input tensors should match: shape[0] = [19,1] vs. shape[1] = [] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Input \u001B[0;32mIn [43]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# train the agent on the env\u001B[39;00m\n\u001B[1;32m      2\u001B[0m env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMountainCarContinuous-v0\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m daifa, results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_single_agent\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdaifa\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservation_max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservation_min\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservation_noise_stddev\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m60\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrender_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Masters/Thesis/DAIF_Agents/train_agent.py:54\u001B[0m, in \u001B[0;36mtrain_single_agent\u001B[0;34m(mcc_env, agent, obs_max, obs_min, observation_noise_stddev, num_episodes, render_env)\u001B[0m\n\u001B[1;32m     51\u001B[0m     t \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# final training when the episode is done\u001B[39;00m\n\u001B[0;32m---> 54\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperceive_and_act\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation_noisy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreward\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdone\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m success \u001B[38;5;241m=\u001B[39m t \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m999\u001B[39m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m# get the VFE of the model for the run\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# VFE = float(tf.reduce_mean(agent.model_vae.compute_loss(all_post_observations)))\u001B[39;00m\n",
      "Input \u001B[0;32mIn [39]\u001B[0m, in \u001B[0;36mDAIFAgentRecurrent.perceive_and_act\u001B[0;34m(self, observation, reward, done)\u001B[0m\n\u001B[1;32m    127\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfull_reward_sequence\u001B[38;5;241m.\u001B[39mappend(reward)\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;66;03m# Call the training function on the observation sequences to train everything we need to train\u001B[39;00m\n\u001B[0;32m--> 130\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvstack\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfull_observation_sequence\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    131\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvstack\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfull_action_sequence\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    132\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfull_reward_sequence\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[43m                          \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;66;03m# Otherwise are we at a point where we can reconsider our policy and maybe train the world model\u001B[39;00m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent_time_ratio \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    138\u001B[0m \n\u001B[1;32m    139\u001B[0m     \u001B[38;5;66;03m# add the reward only if it's not the first observation\u001B[39;00m\n",
      "Input \u001B[0;32mIn [39]\u001B[0m, in \u001B[0;36mDAIFAgentRecurrent.train_models\u001B[0;34m(self, observations_full, actions, rewards, tran_hidden_state_pre_obs)\u001B[0m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_prior:\n\u001B[1;32m    345\u001B[0m     \u001B[38;5;66;03m# self.prior_model.train(post_observations, rewards, verbose=self.show_prior_training)\u001B[39;00m\n\u001B[1;32m    346\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(rewards) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    347\u001B[0m         \u001B[38;5;66;03m# self.prior_model.train(post_observations, rewards)\u001B[39;00m\n\u001B[0;32m--> 348\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprior_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpost_latent_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrewards\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m#### TRAIN THE HABIT ACTION NET ####\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_habit_net:\n\u001B[1;32m    353\u001B[0m \n\u001B[1;32m    354\u001B[0m     \u001B[38;5;66;03m# prior_preferences_mean = tf.convert_to_tensor(self.given_prior_mean, dtype=\"float32\")\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;66;03m# cum_rewards = compute_discounted_cumulative_reward(obs_utilities, self.habit_action_model.discount_factor)\u001B[39;00m\n",
      "Input \u001B[0;32mIn [38]\u001B[0m, in \u001B[0;36mPriorModelBellman.train\u001B[0;34m(self, observations, rewards)\u001B[0m\n\u001B[1;32m     61\u001B[0m utility_t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprior_model(observations)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;66;03m# utility_t_plus_one = tf.concat([utility_t[1:], tf.zeros((1, self.output_dim), dtype=utility_t.dtype)], axis=0)\u001B[39;00m\n\u001B[0;32m---> 63\u001B[0m utility_t_plus_one \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mutility_t\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;66;03m# OR just have constant gamma\u001B[39;00m\n\u001B[1;32m     66\u001B[0m discount_factors \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mones_like(utility_t_plus_one) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscount_factor\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif_car_race/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif_car_race/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7186\u001B[0m, in \u001B[0;36mraise_from_not_ok_status\u001B[0;34m(e, name)\u001B[0m\n\u001B[1;32m   7184\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraise_from_not_ok_status\u001B[39m(e, name):\n\u001B[1;32m   7185\u001B[0m   e\u001B[38;5;241m.\u001B[39mmessage \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m name: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m name \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 7186\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: ConcatOp : Ranks of all input tensors should match: shape[0] = [19,1] vs. shape[1] = [] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=60, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "daifa.train_vae = False\n",
    "daifa.model_vae.show_training = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.5360193  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6375\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6102\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -45.6262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.3539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9904\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9144\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -52.5604\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -52.2518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6778\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6394\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -52.9761\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -52.7046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4790\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -40.5776\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -40.5031\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4801\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.4427\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 900us/step - loss: -2.4553\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3271\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3398\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0170\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 893us/step - loss: -0.0169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3790\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2435\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -51.2801\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -51.2069\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2407\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2331\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -41.9829\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -41.9082\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6932\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.9357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 996us/step - loss: -22.8672\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.3993\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3035850.2500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3026596.5000\n",
      "Success in episode 1 at time step 109\n",
      "Episode 2\n",
      "[-0.508594  0.      ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0742\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0286\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -25.1260\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -25.0270\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6262\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -46.8852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -46.7250\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9242\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9337\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -52.3084\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -52.1233\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5009\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4896\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -13.9923\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 993us/step - loss: -13.9540\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7396\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7260\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -19.6924\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -19.7667\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2727\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1224\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0705\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0710\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.5928\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0601\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9382\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8744\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -72.5988\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -72.4742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.0497\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -22.0027\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4810\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2334189.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2324852.0000\n",
      "Success in episode 2 at time step 114\n",
      "Episode 3\n",
      "[-0.5855634  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4799\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4420\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -35.0780\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -34.9254\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3171\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -49.5563\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -49.3446\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3537\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3653\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -29.6209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -29.4736\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9181\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -32.8680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -32.8542\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -15.6276\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -15.6428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.1138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.6774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0191\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 929us/step - loss: -0.0191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2638\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -17.4453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: -17.4163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7802\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.9393\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -42.8499\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0649\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0687\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.1494\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.1513\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.1438\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.0730\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2506566.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2498277.2500\n",
      "Success in episode 3 at time step 112\n",
      "Episode 4\n",
      "[-0.58186567  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9054\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9014\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -27.8774\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -27.6968\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2435\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -44.0733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 954us/step - loss: -43.8904\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0946\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0936\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -21.6979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 911us/step - loss: -21.6918\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0515\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -21.0777\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 989us/step - loss: -21.1515\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6298\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6004\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.3964\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 979us/step - loss: -0.3978\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.7740\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.3590\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.9271\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.9526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8964\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -80.9325\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -80.7807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9593\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9494\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -18.6276\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -18.5833\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8634\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.7609\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2044172.7500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2037526.0000\n",
      "Success in episode 4 at time step 104\n",
      "Episode 5\n",
      "[-0.4256832  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7761\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -61.3406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 975us/step - loss: -61.3429\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6168\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -86.0723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -85.8857\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6170\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -39.6362\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -39.5092\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.5815\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 959us/step - loss: -12.5645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6837\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.5394\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.5470\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8503\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8790\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.8842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.8840\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9072\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.8788\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.5665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 998us/step - loss: -1.5648\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4854\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -80.3733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -80.2821\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0253\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -27.2430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -27.1963\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.6917\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2936570.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2927603.5000\n",
      "Success in episode 5 at time step 118\n",
      "Episode 6\n",
      "[-0.46279934  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7438\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -37.9536\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 902us/step - loss: -37.8224\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5881\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5710\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -37.4988\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -37.3616\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2091\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.5645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 988us/step - loss: -42.4434\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1131\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -14.8883\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -14.8814\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8359\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8322\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -36.7640\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -36.8666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4678\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.4801\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0321\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1689\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1724\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -53.7513\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 967us/step - loss: -53.6453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1030\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -14.5440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -14.5137\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5330\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2294011.7500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2287121.0000\n",
      "Success in episode 6 at time step 115\n",
      "Episode 7\n",
      "[-0.5932944  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2575\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -28.0154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -27.8769\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3413\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -37.8159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -37.6533\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6311\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6390\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -15.7455\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -15.7378\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6639\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.3167\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.3123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5843\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4094\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.4086\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.0230\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8455\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.2460\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.2445\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3759\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2537\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -57.4890\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 994us/step - loss: -57.3787\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4474\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4539\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.3390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -22.2857\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1383456.2500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1379107.0000\n",
      "Success in episode 7 at time step 105\n",
      "Episode 8\n",
      "[-0.48914003  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6336\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -63.1656\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -63.0897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3726\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3392\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.0096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -41.8861\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3411\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3474\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.8403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.8318\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1926\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1925\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -31.5722\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 974us/step - loss: -31.5863\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9206\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.7425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.7443\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2629\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.3441\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.3472\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -41.2274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -41.1670\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6295\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5944\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -25.1740\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -25.1330\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7786\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7914\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.6371\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.6331\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.4272\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.4094\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2697674.7500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2693430.2500\n",
      "Success in episode 8 at time step 110\n",
      "Episode 9\n",
      "[-0.56875086  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5656\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5869\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -26.5169\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 946us/step - loss: -26.4240\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5981\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5803\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.3603\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 965us/step - loss: -7.3554\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4848\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -35.7022\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -35.7022\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7273\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -29.5690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 968us/step - loss: -29.6093\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3438\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3347\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.7937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 887us/step - loss: -0.7986\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6820\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1883\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.6206\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 985us/step - loss: -9.5903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9616\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5714\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -31.4766\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -31.4051\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1945\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.6818\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.6797\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9008\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.8161\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1556671.7500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1554266.5000\n",
      "Success in episode 9 at time step 102\n",
      "Episode 10\n",
      "[-0.42177007  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1107\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -66.5595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 937us/step - loss: -66.5892\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3698\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3687\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -60.5066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -60.4384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1375\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.8589\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.8397\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2419\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -20.8983\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -20.8956\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5437\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5153\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -33.6784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 887us/step - loss: -33.6387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8634\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8950\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.1333\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 987us/step - loss: -1.1323\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0345\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9563\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -14.6548\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -14.6844\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5457\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -54.1703\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 942us/step - loss: -54.1074\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9942\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -18.8302\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 919us/step - loss: -18.8158\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0759\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0208\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2712279.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2708018.0000\n",
      "Success in episode 10 at time step 116\n",
      "Episode 11\n",
      "[-0.55589515  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0006\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9731\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -32.3353\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -32.1938\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0914\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0720\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -25.9905\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -25.8823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1114\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.5249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.5181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1642\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -53.6565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -53.7489\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2639\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2682\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.7116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.7125\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6753\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.2985\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 918us/step - loss: -9.3214\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5724\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5587\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -41.8733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -41.7904\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -15.2717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 885us/step - loss: -15.2400\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4017\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3686\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1606654.7500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1602296.6250\n",
      "Success in episode 11 at time step 104\n",
      "Episode 12\n",
      "[-0.50535643  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8926\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9138\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -20.7050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -20.6392\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4998\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4854\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -39.4703\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -39.3404\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4544\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4658\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -11.0696\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 951us/step - loss: -11.0301\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9108\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -65.8494\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -65.8533\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7901\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.1231\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 996us/step - loss: -4.1121\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.1164\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0579\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 901us/step - loss: -0.0576\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7052\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.7299\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -20.8592\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -20.8272\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2918\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2930\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -17.1413\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 903us/step - loss: -17.1089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0455\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.5042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 951us/step - loss: -42.5287\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8604\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.8489\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2201484.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2198335.0000\n",
      "Success in episode 12 at time step 110\n",
      "Episode 13\n",
      "[-0.5049282  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5911\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.1990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 931us/step - loss: -22.1148\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4082\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -38.7114\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -38.5939\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9523\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -48.9094\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -48.7901\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0339\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0323\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -52.5201\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -52.5293\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5926\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5811\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.0964\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.0997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1995\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0904\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1710\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0086\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -19.2004\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 848us/step - loss: -19.1651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5227\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5066\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -19.2083\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -19.1778\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8273\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -11.2386\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -11.2428\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0506\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0299\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2026383.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2021665.0000\n",
      "Success in episode 13 at time step 110\n",
      "Episode 14\n",
      "[-0.58757573  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1716\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1424\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -17.5637\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 978us/step - loss: -17.4841\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0188\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9992\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -36.1108\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -35.9636\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -20.2449\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 887us/step - loss: -20.2558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3242\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3218\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -54.4430\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 920us/step - loss: -54.4944\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 935us/step - loss: -0.0651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2016\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2296\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.2984\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.3058\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1417\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1586\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -29.8908\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -29.8152\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8256\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8100\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.2240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 903us/step - loss: -5.2092\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3373\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3232\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1433634.7500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1431019.2500\n",
      "Success in episode 14 at time step 104\n",
      "Episode 15\n",
      "[-0.45855004  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2849\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2826\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -64.2420\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 979us/step - loss: -64.3071\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2410\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2339\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -36.5108\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 960us/step - loss: -36.4359\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1606\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1575\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -17.1711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -17.1370\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6449\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6211\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -62.2300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 910us/step - loss: -62.2493\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4384\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -17.9078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 964us/step - loss: -17.8638\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 971us/step - loss: -0.4812\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4215\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4442\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1569\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1564\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2393\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1704\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -20.7974\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -20.7637\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0598\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0638\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -20.3635\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 930us/step - loss: -20.3698\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.9391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.9287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2525306.2500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2523314.5000\n",
      "Success in episode 15 at time step 116\n",
      "Episode 16\n",
      "[-0.5324216  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2003\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2016\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.7734\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 886us/step - loss: -16.7308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1505\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1528\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -31.9599\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -31.8797\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6699\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.9244\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.9267\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6244\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -21.8972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 970us/step - loss: -21.9276\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2769\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.7485\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.7480\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4552\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1767\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.4093\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.4077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4443\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3767\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -36.2702\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -36.2175\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4620\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4811\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.5495\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 957us/step - loss: -7.5379\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.0491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.9964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1113546.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1111494.6250\n",
      "Success in episode 16 at time step 106\n",
      "Episode 17\n",
      "[-0.42931733  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9427\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9104\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -14.7468\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 901us/step - loss: -14.7322\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7644\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7782\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.3403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.3393\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6128\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6512\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -107.8890\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 988us/step - loss: -107.9129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8219\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8151\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -14.8873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -14.9028\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1649\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1682\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -167.5529\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -167.3895\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4097\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2799\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -18.0510\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 927us/step - loss: -18.0229\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1189\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.7162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 916us/step - loss: -10.7053\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7707\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3073205.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3068348.7500\n",
      "Success in episode 17 at time step 92\n",
      "Episode 18\n",
      "[-0.45769086  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0953\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0953\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -36.5303\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 933us/step - loss: -36.5677\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2916\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -32.8291\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -32.8412\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8672\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8597\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.6806\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.6809\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7015\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -46.9437\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -46.9062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.6150\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 917us/step - loss: -7.6081\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7088\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7493\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.4662\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 915us/step - loss: -1.4709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3440\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -14.5584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 939us/step - loss: -14.5222\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9075\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -14.7084\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -14.7039\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9901\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9911\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -59.7675\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -59.7425\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.9643\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9332\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2236673.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2234516.5000\n",
      "Success in episode 18 at time step 112\n",
      "Episode 19\n",
      "[-0.45847616  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4728\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4505\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -34.6247\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -34.5207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2951\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -32.5823\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -32.5276\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2915\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2889\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -58.7640\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 858us/step - loss: -58.7725\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8230\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8118\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -26.6591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -26.6787\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0155\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.1186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 925us/step - loss: -1.1197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3915\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.2216\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.2228\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1335\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -15.7789\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -15.7552\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7127\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.3807\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.3820\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.2509\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.2157\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1653100.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1651273.3750\n",
      "Success in episode 19 at time step 103\n",
      "Episode 20\n",
      "[-0.5580536  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3601\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -25.8545\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -25.7755\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4776\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4591\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -27.5594\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 903us/step - loss: -27.4798\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3737\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3713\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -40.7492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 856us/step - loss: -40.7723\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6059\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6058\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -32.6377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -32.6333\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9254\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8834\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1041\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 934us/step - loss: -0.1036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5164\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4525\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.5489\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 949us/step - loss: -0.5486\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7347\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7761\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -17.4237\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 951us/step - loss: -17.3963\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2010\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2057\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.7887\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 819us/step - loss: -9.7934\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8231\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7983\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1489040.2500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1487113.8750\n",
      "Success in episode 20 at time step 103\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "daifa.train_prior = False\n",
    "daifa.prior_model.show_training = False\n",
    "\n",
    "# actor_model = get_actor(latent_dim, 1)\n",
    "# critic_model = get_critic(latent_dim, 1)\n",
    "#\n",
    "# target_actor = get_actor(latent_dim, 1)\n",
    "# target_critic = get_critic(latent_dim, 1)\n",
    "#\n",
    "# # Making the weights equal initially\n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "#\n",
    "# critic_optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "# actor_optimizer = tf.keras.optimizers.Adam(0.0005)\n",
    "#\n",
    "# habit_net = BasicDDPG(actor_model, critic_model, target_actor, target_critic, tau=0.005, critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer, observation_dim=latent_dim)\n",
    "#\n",
    "# daifa.habit_model_type = \"DDPG\"\n",
    "\n",
    "# # make the HABIT ACTION NET\n",
    "# habit_net = HabitualAction(latent_dim, 1, [16, 16], train_epochs=2, show_training=True)\n",
    "# habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "# daifa.habit_model_type = \"PG\"\n",
    "#\n",
    "# daifa.habit_action_model = habit_net\n",
    "\n",
    "daifa.habit_action_model.show_training = True\n",
    "daifa.train_habit_net = False\n",
    "daifa.train_after_exploring = True\n",
    "daifa.use_kl_intrinsic = True\n",
    "daifa.use_kl_extrinsic = False\n",
    "daifa.use_fast_thinking = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.59662616  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 3.2428\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.2278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 96ms/step - loss: -47.8857\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -48.2584\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -119.0805\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -120.6901\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0884\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.1601\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -22.4742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0546\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -70.1201\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -70.2886\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9130\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8417\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -89.6176\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -90.6986\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6668\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -21.4285\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -21.6689\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6057\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5454\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -187.5963\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -188.8662\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.1314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0024\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -149.5500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -151.6626\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.8568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7762\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6004060.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5962035.5000\n",
      "Success in episode 1 at time step 106\n",
      "Episode 2\n",
      "[-0.42516613  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3846\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4183\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -103.7141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -103.2851\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7741\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8114\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.8353\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -22.6892\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1289\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1888\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -89.1878\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -89.3814\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2400\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2918\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -92.8930\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -92.8957\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2158\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1543\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -13.8062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -13.7975\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2851\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4534\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -141.5254\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -140.7933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6707\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -142.0780\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -141.2530\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3058\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1091\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4311486.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4283993.0000\n",
      "Success in episode 2 at time step 95\n",
      "Episode 3\n",
      "[-0.57418054  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8636\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -95.1649\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 930us/step - loss: -95.7667\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3430\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -63.3773\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -62.8637\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6638\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7567\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -100.6072\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -99.9200\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -32.2284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -32.2199\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3400\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -130.2256\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -130.6425\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5573\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -95.6036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -95.6521\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.6503\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.4702\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -126.3109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -126.1302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7075\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.5384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -106.7909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -106.3281\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9046\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -79.9628\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -79.9650\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5087\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5469\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -154.3399\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -154.6393\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4230\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4397\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -112.9302\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -113.0666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9658\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9342\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -123.8042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -123.8709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4323\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8570\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -106.3813\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -106.3671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9397\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1302\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -157.1911\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -157.0041\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1306\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1334\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -100.9931\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -100.8697\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.4131\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3315\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19832524.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19655558.0000\n",
      "Success in episode 3 at time step 188\n",
      "Episode 4\n",
      "[-0.5030183  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4441\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4610\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -65.9567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -65.5696\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0917\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -71.6114\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -71.5067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1821\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2569\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -84.7212\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: -84.5336\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5516\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5828\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -94.0974\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -94.1027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9797\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -93.6025\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -94.0684\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4471\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2543\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -78.8697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -79.4185\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4079\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1233\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -84.2339\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -84.7446\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8420\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2189\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -82.7920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -83.1888\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4940\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -55.2425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -55.1644\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0797\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0773\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -61.7733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 912us/step - loss: -61.8677\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6322\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -55.0226\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -55.1746\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7236\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -114.0674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -113.8021\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -83.4115\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -83.1988\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 34.1673\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 32.1066\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -70.4018\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -70.3421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.8205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.5287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.5458\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3853\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2560\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -97.1040\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -97.0036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2121\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -74.1304\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -74.1069\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.7876\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5447\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17489048.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17391348.0000\n",
      "Success in episode 4 at time step 210\n",
      "Episode 5\n",
      "[-0.43734002  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0273\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7470\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -17.8462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -17.9736\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2532\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9907\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -77.1780\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -76.1929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0593\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.6746\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -56.2839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 980us/step - loss: -55.6575\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2635\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -48.9165\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -48.2929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5259\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7383\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -66.0045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -66.1665\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2121\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -117.6664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -118.2561\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1051\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.0774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -60.9110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 893us/step - loss: -60.9837\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0605\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9234\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2848901.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2843987.5000\n",
      "Success in episode 5 at time step 90\n",
      "Episode 6\n",
      "[-0.5650215  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.6783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -84.8978\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -84.6752\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6967\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5918\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -76.6120\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 937us/step - loss: -76.4155\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2965\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.9623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.9451\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2534\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.7975\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -118.1866\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -118.1664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8136\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6968\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -37.6143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -37.6092\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2385\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -79.7236\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -79.7437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2543\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2523\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -71.9367\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -71.8369\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 40.6429\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 38.4829\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -53.3453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -53.2599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6845\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.6018\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -50.7967\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -50.7925\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3298\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -138.8183\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -138.7865\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2334\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1469\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -75.9682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -75.9236\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.6540\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5860\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7803919.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7782959.0000\n",
      "Success in episode 6 at time step 139\n",
      "Episode 7\n",
      "[-0.48808995  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4345\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -81.2386\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -80.9636\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.5776\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -11.6856\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -11.6381\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7086\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -31.3467\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -31.3098\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.7731\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.7634\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -42.8576\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3132\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -101.1832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -101.2418\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0521\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9672\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -84.0985\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -83.9881\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3640\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2868\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -66.0591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 982us/step - loss: -65.9543\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6008\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5205\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -70.6401\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 962us/step - loss: -70.5457\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3262\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2435\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -47.8529\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -47.8480\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9652\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9502\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -104.9508\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -104.8580\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.4206\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -66.2841\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -66.2523\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.6118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7489953.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7465094.0000\n",
      "Success in episode 7 at time step 140\n",
      "Episode 8\n",
      "[-0.44041052  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9912\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9739\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -87.4845\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -86.7438\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9977\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8418\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -67.3942\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 967us/step - loss: -66.8595\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9462\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.6731\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -62.4291\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -61.9851\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4573\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.0168\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -57.3812\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -57.0904\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7336\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -35.1275\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -34.9817\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2823\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2023\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -115.7619\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -116.0167\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4909\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -57.4858\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -57.5215\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -67.2987\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -67.3258\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2729\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2502\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -72.6805\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -72.6610\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9337\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.6576\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -46.4288\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -46.3440\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 36.9033\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 34.5944\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -89.3330\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -89.3759\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0013\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -121.1751\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -121.2395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0604\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.3257\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -64.8671\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -64.8544\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3655\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9851088.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9802476.0000\n",
      "Success in episode 8 at time step 161\n",
      "Episode 9\n",
      "[-0.57235885  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5976\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6692\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -67.9760\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 950us/step - loss: -67.4201\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1209\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.8970\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.9048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5330\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1810\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -55.4075\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -55.2442\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.4560\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -50.8357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -50.6964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2321\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.5710\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -49.9889\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 953us/step - loss: -49.7917\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1136\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5327\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -43.1109\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -42.9691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8722\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2449\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -57.7180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -57.5962\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8262\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -51.4508\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 938us/step - loss: -51.3497\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1027\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.6307\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -29.5710\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -29.5632\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3630\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -37.3126\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 978us/step - loss: -37.3189\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3942\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2223\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -45.6796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.6413\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6749\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -21.7926\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -21.7663\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -68.9309\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -68.8772\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5914\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.4540\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -15.5749\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -15.5739\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8953\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 15.1602\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -120.2872\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -120.2707\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0710\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.4742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -75.6971\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -75.7061\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.6913\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12018594.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11907035.0000\n",
      "Success in episode 9 at time step 203\n",
      "Episode 10\n",
      "[-0.52606106  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8187\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5483\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -101.8717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -103.4636\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7996\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4407\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -30.9839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -30.7680\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6643\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7107\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -36.2178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -35.7585\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5515\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -37.5089\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 982us/step - loss: -36.9688\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8985\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.8692\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -26.2923\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -25.8507\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0072\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -130.5986\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -131.3747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6455\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -85.1053\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -85.4130\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8291\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8250\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3383314.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3377262.5000\n",
      "Success in episode 10 at time step 95\n",
      "Episode 11\n",
      "[-0.49402633  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5080\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5050\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -54.6120\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -54.5091\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.7712\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -85.0138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -85.0680\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9421\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.7966\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -46.7587\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -46.6612\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0557\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9488\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -94.4143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 952us/step - loss: -94.1665\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8953\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -40.8865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -40.9195\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4673\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -46.8216\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 974us/step - loss: -46.7933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9507\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -25.9524\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -25.9144\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.0854\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.0436\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -38.2499\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -38.1946\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5999\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6888\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -120.9701\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -120.7413\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0959\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -69.5352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -69.4459\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1415\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0960\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5535071.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5500801.5000\n",
      "Success in episode 11 at time step 124\n",
      "Episode 12\n",
      "[-0.46963203  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1871\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1541\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -154.8043\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 981us/step - loss: -154.5197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1533\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -126.0106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 918us/step - loss: -125.7103\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0644\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -123.5442\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -123.3237\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3245\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2505\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -140.1391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -139.9904\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2883\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -30.2047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -30.1956\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2718\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1340\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -32.6232\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -32.6255\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.0174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.3632\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.4085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8829\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -108.2999\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -107.7956\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -63.0579\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -62.8075\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0651\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5834242.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5778860.5000\n",
      "Success in episode 12 at time step 115\n",
      "Episode 13\n",
      "[-0.43228698  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4098\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -144.6996\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -142.9863\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1975\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -131.6006\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -129.8044\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0705\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.9738\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -51.5299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 981us/step - loss: -52.3071\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8007\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -31.1733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -31.4015\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7617\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.5539\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.0701\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 987us/step - loss: -10.1115\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.8538\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -125.8586\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -125.0745\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0199\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -46.2005\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.7884\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.8079\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7586\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3198288.7500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3178235.5000\n",
      "Success in episode 13 at time step 94\n",
      "Episode 14\n",
      "[-0.5905618  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5048\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -60.0224\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -60.6405\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.1139\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -53.9265\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -54.2890\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2608\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.0670\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -58.0741\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 985us/step - loss: -58.5326\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7856\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.5812\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -74.6517\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -75.1981\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4007\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -90.1239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -90.6467\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4601\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -104.9338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -105.3527\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4452\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -84.4125\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -84.6822\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -69.9656\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -70.1344\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8528\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -74.1469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -74.2961\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8656\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -112.7951\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -112.6163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2643\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -44.7858\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -44.6909\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0132\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -39.0801\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 995us/step - loss: -39.0123\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2580\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1999\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9159547.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9074075.0000\n",
      "Success in episode 14 at time step 154\n",
      "Episode 15\n",
      "[-0.575275  0.      ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3157\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3164\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -98.2296\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -100.1221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5469\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4808\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -70.2667\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -70.8632\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9752\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -81.7009\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -82.0578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4543\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4541\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -99.1332\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -99.3929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4283\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3766\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -15.8855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -15.7794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4159\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.8207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -21.9412\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 964us/step - loss: -21.7544\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6506\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.2639\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -190.1530\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -190.9554\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2273\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2060\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -78.1632\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -78.6978\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3482\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -32.4758\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -32.5289\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5090\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4677\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5167749.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5131316.5000\n",
      "Success in episode 15 at time step 112\n",
      "Episode 16\n",
      "[-0.41502452  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2954\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2833\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -143.6978\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -142.2230\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8427\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -102.2992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -101.1073\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4023\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -76.5964\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -77.4141\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9243\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -33.8163\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -34.0330\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.8509\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.2492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 924us/step - loss: -42.5149\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7519\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -48.9908\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 948us/step - loss: -49.4134\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0734\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0399\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -72.9458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -72.4904\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6044\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.5462\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -27.2674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -27.0665\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9436\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8976\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4891682.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4887389.5000\n",
      "Success in episode 16 at time step 105\n",
      "Episode 17\n",
      "[-0.46473056  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -73.4035\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -73.4936\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2995\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2935\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -49.7591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: -49.7129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7496\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -161.9648\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -161.9389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6821\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6656\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -51.8990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 981us/step - loss: -51.7762\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.3399\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.4823\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.4762\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.1852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1182\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1736759.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1734704.8750\n",
      "Success in episode 17 at time step 69\n",
      "Episode 18\n",
      "[-0.50295705  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7104\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -120.2989\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -120.1817\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9768\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -80.2780\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 973us/step - loss: -80.2142\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9688\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -107.7963\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -107.8034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1989\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -37.6784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -37.6526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2355\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.8426\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -35.7941\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -35.8108\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3384\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.4628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -41.8769\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -41.8854\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4376\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4264\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -46.2740\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -46.1773\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.3061\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 956us/step - loss: -16.2876\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3557\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3113245.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3108664.5000\n",
      "Success in episode 18 at time step 99\n",
      "Episode 19\n",
      "[-0.4217114  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7122\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -142.5036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -142.3681\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0590\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -85.0520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 954us/step - loss: -85.1657\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2416\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -43.9287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -43.8884\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7829\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 9.2526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.6165\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -42.6094\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0168\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -67.3693\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -67.4887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4306\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9758\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -25.1406\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -25.0591\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8907\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -27.3276\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -27.2448\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7680\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2630932.2500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2627612.5000\n",
      "Success in episode 19 at time step 94\n",
      "Episode 20\n",
      "[-0.44991374  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6919\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.6222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -22.5819\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5384\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -50.3785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -50.4233\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6901\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8003\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -19.7709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 913us/step - loss: -19.7602\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.0750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9683\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -48.5417\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -48.5051\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.6933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -83.2735\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -83.2467\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3084\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.8159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -66.0289\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 949us/step - loss: -65.9803\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5364\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -58.8204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -58.7651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2008\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -70.7125\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -70.6688\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -74.3681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 984us/step - loss: -74.3272\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8067\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4812\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -107.2891\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -107.2749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5199\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -98.2154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 983us/step - loss: -98.2044\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0632\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1435\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -78.6763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -78.6608\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0269\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0206\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -60.2286\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -60.2166\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6013\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.7464\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -63.9981\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 979us/step - loss: -63.9845\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5616\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -13.1483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -13.1459\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2552\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -60.2014\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -60.2011\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3674\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -13.0129\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -13.0101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5927\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -59.6080\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -59.6110\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.7517\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -83.2202\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 999us/step - loss: -83.2188\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.1770\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 10.0898\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.6592\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -22.6565\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 24.7511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 23.9352\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -61.6448\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 986us/step - loss: -61.6464\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8353\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6484\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -54.4154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -54.4148\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.7054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -18.7813\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -18.7799\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9897\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24680888.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24085174.0000\n",
      "Success in episode 20 at time step 280\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, render_env=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/core.py:57: DeprecationWarning: \u001B[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.5802331  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 1.5104\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5189\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -30.5062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -30.4193\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4896\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.6822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.6606\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7443\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7650\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.5774\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 893us/step - loss: -0.5818\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9216\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8626\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.2250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.2075\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5304\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 336352.4688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 335497.0000\n",
      "Success in episode 1 at time step 117\n",
      "Episode 2\n",
      "[-0.47586903  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2137\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2276\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -33.7087\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -33.6440\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6311\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6047\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -39.9850\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -39.9482\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2631\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2475\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -49.9770\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -49.9823\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3333\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3281\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.1483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.1356\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4434\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4010\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.3717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 919us/step - loss: -2.3711\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7269\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7094\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.2062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.1898\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.1913\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1880\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1389553.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1387941.8750\n",
      "Success in episode 2 at time step 129\n",
      "Episode 3\n",
      "[-0.5756734  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0799\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0886\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.7584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.7601\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2471\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2345\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -29.6687\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -29.6690\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 26.6118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.9628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.8002\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.8025\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6747\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8619\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -57.9710\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -57.9759\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.2546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.9604\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 966051.6250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 963453.5000\n",
      "Success in episode 3 at time step 121\n",
      "Episode 4\n",
      "[-0.41252407  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4580\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5245\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -28.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -28.4186\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6984\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -56.8006\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -56.7767\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.3623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.3438\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.6570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 938us/step - loss: -0.6617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.1453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1397\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.7560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.7227\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.1595\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1435\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.8621\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.8641\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2936\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1126\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1415829.3750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1412925.3750\n",
      "Success in episode 4 at time step 145\n",
      "Episode 5\n",
      "[-0.46047693  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3030\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -19.2481\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -19.2433\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.2000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0831\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.4050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.4008\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4397\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -11.8068\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 914us/step - loss: -11.7756\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0686\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3627\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.4062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.4059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2773\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -43.0151\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -42.9775\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7859\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.7355\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1115692.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1113185.3750\n",
      "Success in episode 5 at time step 136\n",
      "Episode 6\n",
      "[-0.42048392  0.        ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3736\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -45.4425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.4647\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7982\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7316\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -99.0774\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -99.1813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0486\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -28.7623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -28.7259\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7082\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -49.1468\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -49.1463\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7813\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8246\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -54.3375\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -54.2878\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6385\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.5084\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.5069\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5730\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.5938\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -16.5963\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.3879\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3605\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3560688.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3553600.7500\n",
      "Success in episode 6 at time step 153\n",
      "Episode 7\n",
      "[-0.5221552  0.       ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4467\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -47.1609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -46.6838\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6821\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6406\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -13.2683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 920us/step - loss: -13.4961\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7572\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.8920\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.9878\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1948\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1572\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -15.2613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -15.1866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 4.9044\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.9302\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.9207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8965\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8933\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.0001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.0005\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.9370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.8726\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 995565.5625\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 994472.2500\n",
      "Success in episode 7 at time step 109\n",
      "Episode 8\n",
      "[-0.5510469  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5355\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5211\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -88.7122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -88.6831\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.9452\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8957\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -40.7104\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -40.5951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4499\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4884\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.7792\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.7741\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6210\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -46.8842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -46.8882\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.2909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.2910\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3193\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1011\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.3421\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.3421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.0537\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.0393\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.8611\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.8616\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6461\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -11.4334\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -11.4269\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.5185\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4251\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3566917.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3555760.0000\n",
      "Success in episode 8 at time step 191\n",
      "Episode 9\n",
      "[-0.5066246  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.9053\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9845\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -24.0907\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -24.0800\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2803\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.0996\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.1077\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.1370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.1075\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7699\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8355\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.8520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.8366\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5512\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4652\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 494726.5625\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 492686.1250\n",
      "Success in episode 9 at time step 180\n",
      "Episode 10\n",
      "[-0.46680188  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4263\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4413\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -39.4424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -39.4048\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9330\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9576\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -212.8827\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -212.6812\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6974\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7155\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.3050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.3190\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.7604\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -11.1723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 907us/step - loss: -11.1199\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5601\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5176\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3215752.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3187177.0000\n",
      "Success in episode 10 at time step 175\n",
      "Episode 11\n",
      "[-0.56625867  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.1217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 928us/step - loss: -6.1027\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6335\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6930\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.9071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -17.4632\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5628\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1344\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -18.5752\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -19.0802\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6065\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1889\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.7049\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 946us/step - loss: -8.7618\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4080\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3830\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.6476\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.6178\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5855\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5610\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 744841.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 743716.9375\n",
      "Success in episode 11 at time step 121\n",
      "Episode 12\n",
      "[-0.54848975  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4301\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4271\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -24.5516\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -24.4634\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8515\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8739\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -49.1349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -49.2184\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8347\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9166\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.3809\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.3248\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7080\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6932\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.4669\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.4418\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5863\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 733489.6250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 729920.1875\n",
      "Success in episode 12 at time step 117\n",
      "Episode 13\n",
      "[-0.48362118  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4622\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4610\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -31.8041\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -31.8641\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -64.1888\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -64.4875\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.1865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1738\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.0889\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.0743\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3141\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3005\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -77.9458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -77.9398\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1875\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1929\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.0488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 941us/step - loss: -15.9961\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.8292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8132\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0773\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0775\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.1021\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.4422\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 928us/step - loss: -6.4320\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.8279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 932us/step - loss: -9.8307\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.9458\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8738\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3748495.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3740804.5000\n",
      "Success in episode 13 at time step 201\n",
      "Episode 14\n",
      "[-0.54755765  0.        ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6394\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5432\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -24.3132\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 966us/step - loss: -24.2743\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3741\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3532\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -25.3408\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 968us/step - loss: -25.3063\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7336\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8270\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.1442\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: -1.1535\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.0246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6157\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -25.5730\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -25.5006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.4047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3053\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.5334\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.5269\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.5934\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4545\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 879656.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 877233.0000\n",
      "Success in episode 14 at time step 106\n",
      "Episode 15\n",
      "[-0.54578614  0.        ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.1376\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0708\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -24.2689\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -24.1849\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4144\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4312\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -49.0886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -49.1066\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5206\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5118\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.6309\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.6502\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5402\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5589\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -13.1951\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -13.2043\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0382\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0829\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.9148\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.9127\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0290\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.9586\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 874327.1875\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 873652.6250\n",
      "Success in episode 15 at time step 108\n",
      "Episode 16\n",
      "[-0.4629212  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8801\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8298\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -26.6473\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -26.5952\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.6697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -44.2761\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -44.2552\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1994\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1424\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1231564.7500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1226860.6250\n",
      "Success in episode 16 at time step 127\n",
      "Episode 17\n",
      "[-0.45039886  0.        ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9778\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.4073\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 951us/step - loss: -1.4296\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2603\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -65.1744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -65.1074\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -35.6136\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -35.3922\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2590\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -88.1374\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -88.2913\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1890\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.2274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.2363\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5271\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2593634.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2583222.0000\n",
      "Success in episode 17 at time step 151\n",
      "Episode 18\n",
      "[-0.44386464  0.        ]\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1479\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3000\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -40.1386\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 945us/step - loss: -40.4073\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2117\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2738\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.5110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 872us/step - loss: -16.5743\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5987\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6120\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -39.9461\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -39.7687\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4472\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4682\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.7822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.7797\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.4044\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 17.9920\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -77.7249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -77.5567\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7654\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7520\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.4282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.4285\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6132\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -93.2043\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -93.1572\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.1939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.7748\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3329061.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3320217.0000\n",
      "Success in episode 18 at time step 120\n",
      "Episode 19\n",
      "[-0.4001952  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8658\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -37.2535\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 989us/step - loss: -36.9975\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.0940\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1253\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -30.6090\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -30.9166\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3564\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3342\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.2238\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.2419\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8343\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3143\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.2076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.1598\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 15.8069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.8945\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.3773\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.3566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2611\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2541\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.2556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: -6.2523\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.8803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6977\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 845517.2500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 843631.8125\n",
      "Success in episode 19 at time step 131\n",
      "Episode 20\n",
      "[-0.5760464  0.       ]\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7057\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -24.0771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -24.0582\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1789\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1538\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.4866\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.4755\n",
      "fast thinking\n",
      "fast thinking\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0161\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1549\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.9209\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 984us/step - loss: -8.8473\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3782\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1246\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.1266\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.1168\n",
      "fast thinking\n",
      "fast thinking\n",
      "fast thinking\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.3635\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3342\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 329941.9375\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 328162.5312\n",
      "Success in episode 20 at time step 110\n"
     ]
    }
   ],
   "source": [
    "daifa.habit_action_model.show_training = True\n",
    "daifa.train_habit_net = True\n",
    "daifa.train_after_exploring = True\n",
    "daifa.use_kl_intrinsic = True\n",
    "daifa.use_kl_extrinsic = False\n",
    "daifa.use_fast_thinking = True\n",
    "daifa.uncertainty_tolerance = 0.1\n",
    "\n",
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, render_env=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x29b92c5b0>,\n <matplotlib.lines.Line2D at 0x29b4da730>]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(obs_pos)\n",
    "\n",
    "utils = daifa.prior_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x29c905af0>,\n <matplotlib.lines.Line2D at 0x29c905e80>]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(vel_pos)\n",
    "\n",
    "utils = daifa.prior_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x16ec0f610>,\n <matplotlib.lines.Line2D at 0x16ec0f670>]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(obs_pos)\n",
    "\n",
    "# utils = daifa.habit_action_model.actor_model(latent_mean)\n",
    "utils = daifa.habit_action_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x16de9cc70>,\n <matplotlib.lines.Line2D at 0x16de9cd30>]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "latent_mean, _ , _ = daifa.model_vae.encoder(vel_pos)\n",
    "\n",
    "# utils = daifa.habit_action_model.actor_model(latent_mean)\n",
    "utils = daifa.habit_action_model(latent_mean)\n",
    "# print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/core.py:57: DeprecationWarning: \u001B[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/utils/passive_env_checker.py:165: UserWarning: \u001B[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001B[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292\n",
      "success\n",
      "94.28041628059832\n",
      "246\n",
      "success\n",
      "96.14228589939898\n",
      "237\n",
      "success\n",
      "95.62813478373828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/envs/classic_control/continuous_mountain_car.py:173: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  self.state = np.array([position, velocity], dtype=np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "success\n",
      "93.572889227936\n",
      "234\n",
      "success\n",
      "96.29431717532314\n",
      "195\n",
      "success\n",
      "94.50258531396743\n",
      "196\n",
      "success\n",
      "95.35582070840101\n",
      "158\n",
      "success\n",
      "96.50849545897857\n",
      "158\n",
      "success\n",
      "96.55365197283321\n",
      "199\n",
      "success\n",
      "95.70399826793675\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "obs_stddev = [0.05, 0.05]\n",
    "# obs_stddev = [0, 0]\n",
    "\n",
    "\n",
    "t_max = 999\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    done = False\n",
    "    rewards = []\n",
    "\n",
    "    t = 0\n",
    "    while not done:\n",
    "\n",
    "        obs = obs.reshape(1, obs.shape[0])\n",
    "        obs = transform_observations(obs, observation_max, observation_min, obs_stddev)\n",
    "\n",
    "        # print(obs)\n",
    "\n",
    "        latent_mean, _, _ = daifa.model_vae.encoder(obs)\n",
    "        # action = daifa.habit_action_model.actor_model(latent_mean)\n",
    "        action = daifa.habit_action_model(latent_mean)\n",
    "        action = action.numpy()\n",
    "\n",
    "        for k in range(daifa.agent_time_ratio):\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            env.render()\n",
    "\n",
    "            # print(obs)\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "            t += 1\n",
    "\n",
    "            if t == t_max:\n",
    "                done = True\n",
    "                break\n",
    "            elif done:\n",
    "                break\n",
    "\n",
    "    print(t)\n",
    "    if t < t_max:\n",
    "        print(\"success\")\n",
    "    else:\n",
    "        print(\"Failure\")\n",
    "        print(\"max obs\", obs)\n",
    "\n",
    "    print(np.sum(rewards))\n",
    "    # print(rewards)\n",
    "\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10998, 2) (10998, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 500\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "actions = []\n",
    "\n",
    "pre_obs = []\n",
    "post_obs = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "    actions.append(a)\n",
    "\n",
    "    pre_obs.append(o[:-1])\n",
    "    post_obs.append(o[1:])\n",
    "\n",
    "\n",
    "    observations.append(o)\n",
    "    # ob_seqs.append(train)\n",
    "    # next_obs.append(test)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "actions = np.vstack(actions)\n",
    "\n",
    "pre_obs = np.vstack(pre_obs)\n",
    "post_obs = np.vstack(post_obs)\n",
    "\n",
    "\n",
    "print(pre_obs.shape, actions.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [119]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241m.\u001B[39mhabit_action_model(pre_obs)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent.habit_action_model(pre_obs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = agent.model_vae(observations)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m, s, z_pre = agent.model_vae.encoder(pre_obs)\n",
    "m, s, z_post = agent.model_vae.encoder(post_obs)\n",
    "z_pre"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_plus_a = np.concatenate([z_pre, actions], axis=1)\n",
    "z_plus_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((z, np.zeros_like(z)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test without the replay training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2,  [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 20, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=5,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=40, action_repeats=6, num_actions_to_execute=2, train_on_full_data=False, show_replay_training=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = agent.model_vae(observations)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with the prior model FEEF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2, [0, 0], [0.3, 0.3], llik_scaling=1, recon_stddev=0.05)\n",
    "\n",
    "pl_hoz = 5\n",
    "latent_dim = 2\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 2*latent_dim*pl_hoz, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(2)\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           None,\n",
    "                           train_prior_model=True,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=40, action_repeats=6, num_actions_to_execute=5, train_on_full_data=False, show_replay_training=True, train_during_episode=True, train_vae=True, train_tran=True, train_prior=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the models produced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.model_vae(observations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.reduce_mean(agent.model_vae.compute_loss(observations))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.prior_model(observations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.prior_model.extrinsic_kl(observations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "pl_hoz = 5\n",
    "latent_dim = 2\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 2*pl_hoz*latent_dim, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(2)\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "# without prior model\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "# with prior model\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           None,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, action_repeats=10, num_actions_to_execute=2, train_on_full_data=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Test the models produced\n",
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.model_vae(observations)\n",
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Identity VAE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = agent.tran((ob_seqs[0:1], None))\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = ob_seqs[0:1, -1].reshape(1,1,3)\n",
    "h = out[3]\n",
    "h = h[0, -2, :]\n",
    "h = h.numpy().reshape(1,30)\n",
    "h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((t, h))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ob_seqs[0:1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test to see how the agent trains on standard observation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, vae_train_epochs=1, tran_train_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "success, agent, t, pre_obs, post_obs, acts = run_episode(env, daifa, observation_max, observation_min, observation_noise_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_np = np.array(pre_obs)\n",
    "a = np.array(acts)\n",
    "a.shape\n",
    "pre_a = np.concatenate([pre_np, a], axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(a.max(), a.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict = np.array(post_obs)[:, 14, :]\n",
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((pre_a, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine training the model on the observation data\n",
    "\n",
    "Does it eventually converge to a good model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_runs = 1\n",
    "for i in range(num_train_runs):\n",
    "\n",
    "    for j in range(len(pre)):\n",
    "        pre = pre_obs[j]\n",
    "        post = post_obs[j]\n",
    "        actions = acts[j]\n",
    "\n",
    "        daifa.train(pre, post, actions, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation(np.array([0.5, 0.1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the FEEF computations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, planning_horizon=15, n_policy_candidates=70, n_policies=1500, n_cem_policy_iterations=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_policy(agent, env, policy, action_repeats):\n",
    "\n",
    "    observation = env.reset()\n",
    "    obs = transform_observations(observation, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    z_t_minus_1 = obs\n",
    "    p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "    p\n",
    "    print(obs)\n",
    "    print(p)\n",
    "\n",
    "    for action in p:\n",
    "        for t in range(action_repeats):\n",
    "            res = env.step(np.array([action]))\n",
    "            print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([0, 0])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p\n",
    "\n",
    "agent.forward_policies(p, z_t_minus_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "test_policy(agent, env, p.numpy(), 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([-0.27691475,  0.01688306])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 given_prior_mean,\n",
    "                 given_prior_stddev,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 tran_train_epochs=1,\n",
    "                 vae_train_epochs=1,\n",
    "                 agent_time_ratio=6,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        self.prior_model = prior_model\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        self.vae_train_epochs = vae_train_epochs\n",
    "        self.tran_train_epochs = tran_train_epochs\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.model_vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        # transition\n",
    "        # takes action plus last state and outputs next latent state\n",
    "        self.tran = tran\n",
    "        self.tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        self.hidden_state = None\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(observation)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    def train(self, pre_observations_raw, post_observations_raw, actions_complete, rewards, verbose=0):\n",
    "\n",
    "        # compress the observations based on the agents time compression factor\n",
    "        # pre_observations = pre_observations_raw[::self.agent_time_ratio]  # for example take every 6th element\n",
    "        # post_observations = np.array([post_observations_raw[i] for i in range(len(post_observations_raw)) if i % self.agent_time_ratio == self.agent_time_ratio - 1])\n",
    "        #\n",
    "        # print(pre_observations_raw)\n",
    "        # print(pre_observations)\n",
    "        # print(post_observations_raw)\n",
    "        # print(post_observations)\n",
    "\n",
    "        pre_observations = pre_observations_raw\n",
    "        post_observations = post_observations_raw\n",
    "\n",
    "        # only look at the first n actions that we took\n",
    "        actions = actions_complete[0: len(pre_observations)]\n",
    "\n",
    "        num_observations = pre_observations.shape[0]\n",
    "        observation_dim = pre_observations.shape[1]\n",
    "        action_dim = actions.shape[1]\n",
    "        # action_dim = 1  # TODO fix this to allow different actions\n",
    "\n",
    "        # find the actual observed latent states using the vae\n",
    "        pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "        post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "        # set up the input training data that we use to train the transition model\n",
    "        z_train = np.concatenate([np.array(pre_latent_mean), np.array(actions)], axis=1)\n",
    "\n",
    "        # we use the sequence to find the right hidden states to use as input\n",
    "        z_train_seq = z_train.reshape((1, num_observations, observation_dim + action_dim))\n",
    "        z_train_singles = z_train.reshape(num_observations, 1, observation_dim + action_dim)\n",
    "\n",
    "        # the previous hidden state is the memory after observing some sequences but it might be None\n",
    "        if self.hidden_state is None:\n",
    "            self.hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "        if self.train_tran:\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.hidden_state))\n",
    "\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran_train_epochs, verbose=verbose)\n",
    "\n",
    "        # now find the new predicted hidden state that we will use for finding the policy\n",
    "        # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "        _, _, final_hidden_state, _ = self.tran((z_train_seq, self.hidden_state))\n",
    "        # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "        self.hidden_state = final_hidden_state\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            self.model_vae.fit(post_observations, epochs=self.vae_train_epochs, verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by one to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        print(z_t_minus_one)\n",
    "        print(mean_best_policies)\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape [self.n_policies, len(z_t_minus_one)]\n",
    "        prev_latent_mean = np.stack([z_t_minus_one]*self.n_policies)\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "            likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "            if self.prior_model is None:\n",
    "\n",
    "                # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                # create the prior distribution\n",
    "                prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "            # TODO Fix the learned prior model\n",
    "            else:\n",
    "                prior_dist = self.prior_model()\n",
    "\n",
    "            kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "            predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "            kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    def EFE(self, policy_posteriors, predicted_likelihood, predicted_posterior):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing with a pretrained transition model\n",
    "\n",
    "This works well! So the problem can't lie with the transition model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0.07]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, train_vae=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 200\n",
    "seq_length = 500\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "next_obs_stddev = []\n",
    "actions = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.2)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    # train = np.concatenate([o[:-1], a], axis=1)\n",
    "    train = o[:-1]\n",
    "    test = o[1:]\n",
    "\n",
    "    actions.append(a)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "    ob_seqs_stddev = np.ones_like(train)\n",
    "    next_stddev = np.ones_like(test)\n",
    "\n",
    "    next_obs_stddev.append(next_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(num_seqs):\n",
    "\n",
    "    pre = ob_seqs[i]\n",
    "    next = next_obs[i]\n",
    "    acts = actions[i]\n",
    "\n",
    "    next_sd = next_obs_stddev[i]\n",
    "\n",
    "    daifa.train(pre, next, acts, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 150\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    test = o[-1]\n",
    "\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "ob_seqs = np.array(ob_seqs)[:, -5:, :]\n",
    "next_obs = np.array(next_obs)\n",
    "ob_seqs.shape\n",
    "\n",
    "ob_seqs_stddev = np.ones_like(ob_seqs)\n",
    "next_obs_stddev = np.ones_like(next_obs)\n",
    "\n",
    "ob_seqs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.tran((ob_seqs, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_obs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks fantastic!!! With enough data the transition model is training very well"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.hidden_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([0.4, 0.5])\n",
    "daifa.hidden_state = None\n",
    "p, s = daifa.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.tran((np.array([[[0.4, 0.5, 1]]]), None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "daifa.train_tran = False\n",
    "daifa.train_vae = False\n",
    "\n",
    "daifa.hidden_state = None\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}