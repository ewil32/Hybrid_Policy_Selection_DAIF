{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run The Agent on Mountain Car"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from vae_recurrent import VAE, create_decoder, create_encoder\n",
    "from transition_gru import TransitionGRU\n",
    "from recurrent_agent import DAIFAgentRecurrent\n",
    "from prior_model import PriorModelBellman"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations\n",
    "from train_agent import train_single_agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from identity_vae import IdentityVAE, identity_encoder, identity_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with no prior model FEEF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from util import transform_observations\n",
    "\n",
    "\n",
    "def train_single_agent(mcc_env,\n",
    "                       agent,\n",
    "                       obs_max,\n",
    "                       obs_min,\n",
    "                       observation_noise_stddev,\n",
    "                       action_repeats,\n",
    "                       num_actions_to_execute,\n",
    "                       num_episodes=100,\n",
    "                       train_on_full_data=True,\n",
    "                       show_replay_training=False,\n",
    "                       replay_train_epochs=2,\n",
    "                       train_during_episode=True,\n",
    "                       train_vae=True,\n",
    "                       train_tran=True,\n",
    "                       train_prior=False):\n",
    "\n",
    "    # Set up to store results in pandas frame\n",
    "    cols = [\"episode\", \"success\", \"sim_steps\", \"VFE_post_run\", \"noise_stddev\"]\n",
    "    rows = []\n",
    "\n",
    "    for n in range(num_episodes):\n",
    "\n",
    "        print(\"Episode\", n+1)\n",
    "        # arrays to store observations, actions and rewards\n",
    "\n",
    "        all_pre_observations = []\n",
    "        all_post_observations = []\n",
    "        all_action = []\n",
    "        observation_sequence = []\n",
    "        actions_executed = []\n",
    "        reward_sequence = []\n",
    "\n",
    "        # get the first observation from the environment\n",
    "        first_observation = mcc_env.reset()\n",
    "        print(first_observation)\n",
    "        # first_observation = np.array([first_observation, 0])\n",
    "\n",
    "        # apply noise to and scaling to first observation\n",
    "        first_observation_noisy = transform_observations(first_observation, obs_max, obs_min, observation_noise_stddev)\n",
    "\n",
    "        # find the first policy\n",
    "        policy_observation = first_observation_noisy\n",
    "        policy = agent.select_policy(policy_observation)\n",
    "\n",
    "        # loop until episode ends or the agent succeeds\n",
    "        t = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            # get the actions from the policy and reshape to desired form\n",
    "            actions = policy.mean()\n",
    "            actions = tf.reshape(actions, (actions.shape[0], agent.tran.action_dim))  # [num_actions, action_dim]\n",
    "            actions = actions.numpy()\n",
    "\n",
    "            # get the actions that we will execute before changing policy\n",
    "            actions_to_execute = []\n",
    "            for action in actions[0:num_actions_to_execute]:\n",
    "                actions_to_execute = actions_to_execute + [action]*action_repeats\n",
    "\n",
    "            # agent executes policy and gathers observations\n",
    "            for action in actions_to_execute:\n",
    "                observation, reward, done, info = mcc_env.step(action)  # action should be array to satisfy gym requirements\n",
    "\n",
    "                actions_executed.append(action)\n",
    "                observation_sequence.append(observation)\n",
    "                reward_sequence.append(reward)\n",
    "\n",
    "                t += 1\n",
    "                if done:\n",
    "\n",
    "                    # did we succeed\n",
    "                    if t < 999:\n",
    "                        print(policy_observation)\n",
    "                        print(policy.mean())\n",
    "\n",
    "                        success = True\n",
    "\n",
    "                    else:\n",
    "                        success = False\n",
    "\n",
    "                    # get a full observations set\n",
    "                    all_post_observations = np.vstack(all_post_observations)\n",
    "                    all_pre_observations = np.vstack(all_pre_observations)\n",
    "                    all_action = np.vstack(all_action)\n",
    "\n",
    "                    # should we train on final full data run\n",
    "                    if train_on_full_data:\n",
    "                        # agent.model_vae.fit(all_post_observations, epochs=replay_train_epochs, verbose=show_replay_training)\n",
    "                        agent.train(all_pre_observations, all_post_observations, all_action, rewards=None, train_vae=train_vae, train_tran=train_tran, train_prior=train_prior)\n",
    "\n",
    "\n",
    "                    # get the VFE of the model for the run\n",
    "                    VFE = float(tf.reduce_mean(agent.model_vae.compute_loss(all_post_observations)))\n",
    "\n",
    "                    # finally break free from the loop\n",
    "                    break\n",
    "\n",
    "            if not done:\n",
    "\n",
    "                actions_executed = np.array(actions_executed).reshape((len(actions_executed), agent.tran.action_dim))\n",
    "\n",
    "                # scale and add noise to the observation\n",
    "                observation_sequence = transform_observations(observation_sequence, obs_max, obs_min, observation_noise_stddev)\n",
    "\n",
    "                # get the noisy observations for pre and post actions\n",
    "                pre_observation_sequence = np.vstack([policy_observation, observation_sequence[:-1]])\n",
    "                post_action_observation_sequence = observation_sequence\n",
    "\n",
    "                # print(post_action_observation_sequence)\n",
    "\n",
    "                all_pre_observations.append(pre_observation_sequence)\n",
    "                all_post_observations.append(post_action_observation_sequence)\n",
    "                all_action.append(actions_executed)\n",
    "\n",
    "                # print(\"pol\", policy_observation)\n",
    "                # print(\"obs\", observation_sequence)\n",
    "                # print(\"pre\", pre_observation_sequence)\n",
    "                # print(\"post\", post_action_observation_sequence)\n",
    "\n",
    "                # if time to train the agent\n",
    "                if train_during_episode:\n",
    "                    agent.train(pre_observation_sequence, post_action_observation_sequence, actions_executed, reward_sequence, train_vae=train_vae, train_tran=train_tran, train_prior=train_prior)\n",
    "\n",
    "                # the new observation we use to select a policy is the last observation in observation_sequences\n",
    "                policy_observation = observation_sequence[-1]\n",
    "\n",
    "                # select a new policy and clear everything\n",
    "                policy = agent.select_policy(policy_observation)\n",
    "\n",
    "                # clear the observations\n",
    "                observation_sequence = []\n",
    "                reward_sequence = []\n",
    "                actions_executed = []\n",
    "\n",
    "        rows.append(dict(zip(cols, [n, success, t, VFE, observation_noise_stddev])))\n",
    "\n",
    "        if success:\n",
    "            print(\"Success in episode\", n+1, \"at time step\", t)\n",
    "        else:\n",
    "            print(\"No Success\")\n",
    "\n",
    "    results = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "    mcc_env.close()\n",
    "\n",
    "    return agent, results\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "# from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 given_prior_mean,\n",
    "                 given_prior_stddev,\n",
    "                 agent_time_ratio=6,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 tran_train_epochs=1,\n",
    "                 vae_train_epochs=1,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True,\n",
    "                 train_prior_model=False,\n",
    "                 use_kl_extrinsic=True,\n",
    "                 use_kl_intrinsic=True,\n",
    "                 use_FEEF=True,\n",
    "                 show_vae_training=False,\n",
    "                 show_tran_training=False,\n",
    "                 show_prior_training=False):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        self.vae_train_epochs = vae_train_epochs\n",
    "        self.tran_train_epochs = tran_train_epochs\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "\n",
    "        # do we use the kl divergence for extrinsic vs intrinsic\n",
    "        self.use_kl_intrinsic = use_kl_intrinsic\n",
    "        self.use_kl_extrinsic = use_kl_extrinsic\n",
    "\n",
    "        # do we use the FEEF or EFE?\n",
    "        self.use_FEEF = use_FEEF\n",
    "\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.model_vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "        self.show_vae_training = show_vae_training\n",
    "\n",
    "        # transition\n",
    "        # takes action plus last state and outputs next latent state\n",
    "        self.tran = tran\n",
    "        self.tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "        self.show_tran_training = show_tran_training\n",
    "        # track the hidden state of the transition gru model\n",
    "        self.hidden_state = None\n",
    "\n",
    "        # Prior model\n",
    "        self.prior_model = prior_model\n",
    "        self.train_prior = train_prior_model\n",
    "        self.show_prior_training = show_prior_training\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "\n",
    "\n",
    "    def train(self, pre_observations_raw, post_observations_raw, actions_complete, rewards, train_vae=True, train_tran=True, train_prior=True):\n",
    "\n",
    "        # compress the observations based on the agents time compression factor\n",
    "        pre_observations = pre_observations_raw[::self.agent_time_ratio]  # for example take every 6th element\n",
    "        post_observations = post_observations_raw[self.agent_time_ratio - 1:][::self.agent_time_ratio]\n",
    "\n",
    "        # only look at the unique actions we took rather than the repeated actions\n",
    "        actions = actions_complete[::self.agent_time_ratio]\n",
    "\n",
    "        # there is a chance pre_observations is longer than post if we are training on the whole data set because the episode ended\n",
    "        if len(pre_observations) > len(post_observations):\n",
    "            print(\"yes\")\n",
    "            print(pre_observations)\n",
    "            print(post_observations)\n",
    "            print(actions)\n",
    "            pre_observations = pre_observations[:-1]\n",
    "            actions = actions[:-1]\n",
    "\n",
    "        # print(pre_observations_raw)\n",
    "        # print(pre_observations)\n",
    "        # print(post_observations_raw)\n",
    "        # print(post_observations)\n",
    "\n",
    "        #### TRAIN THE TRANSITION MODEL ####\n",
    "        if train_tran:\n",
    "\n",
    "            num_observations = pre_observations.shape[0]\n",
    "            observation_dim = pre_observations.shape[1]\n",
    "            action_dim = actions.shape[1]\n",
    "            latent_dim = self.model_vae.latent_dim\n",
    "\n",
    "            # find the actual observed latent states using the vae\n",
    "            pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "            post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "            # set up the input training data that we use to train the transition model\n",
    "            z_train = np.concatenate([np.array(pre_latent_mean), np.array(actions)], axis=1)\n",
    "\n",
    "            # we use the sequence to find the right hidden states to use as input\n",
    "            z_train_seq = z_train.reshape((1, num_observations, latent_dim + action_dim))\n",
    "            z_train_singles = z_train.reshape(num_observations, 1, latent_dim + action_dim)\n",
    "\n",
    "            # the previous hidden state is the memory after observing some sequences but it might be None if we're just starting\n",
    "            if self.hidden_state is None:\n",
    "                self.hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.hidden_state))\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran_train_epochs, verbose=self.show_tran_training)\n",
    "\n",
    "            # now find the new predicted hidden state that we will use for finding the policy\n",
    "            # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "            _, _, final_hidden_state, _ = self.tran((z_train_seq, self.hidden_state))\n",
    "            # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "            z_pred, _, _, _ = self.tran((z_train_singles, h_states_for_training))\n",
    "\n",
    "            # print(\"LOSS\")\n",
    "            # print(tf.reduce_mean(self.tran.compute_loss((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev))))\n",
    "            #\n",
    "            # print(\"PRED VS REAL\")\n",
    "            # for i in range(len(z_pred)):\n",
    "            #     print(z_pred[i], post_latent_mean[i])\n",
    "\n",
    "            self.hidden_state = final_hidden_state\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            # self.model_vae.fit(pre_observations_raw, epochs=self.vae_train_epochs, verbose=self.show_vae_training)\n",
    "            self.model_vae.fit(pre_observations, epochs=self.vae_train_epochs, verbose=self.show_vae_training)\n",
    "\n",
    "            # print(\"true\", pre_observations)\n",
    "            # print(\"pred\", self.model_vae(pre_observations))\n",
    "\n",
    "        #### TRAIN THE PRIOR MODEL ####\n",
    "        if train_prior:\n",
    "            # self.prior_model.train(post_observations, rewards, verbose=self.show_prior_training)\n",
    "            self.prior_model.train(post_observations_raw, rewards, verbose=self.show_prior_training)\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "\n",
    "        # TODO do you take the mean or that latent here?\n",
    "        # get the latent state from this observation\n",
    "        _,  _, latent_state = self.model_vae.encoder(observation.reshape(1, observation.shape[0]))\n",
    "        # latent_state = latent_state.numpy().reshape((1, latent_state.shape[0]))\n",
    "\n",
    "        # print(latent_state)\n",
    "        # select the policy\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(latent_state)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "    # TODO Fix this so we can use different action dimensions\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by one to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape (self.n_policies, latent_dim) when z_t_minus is tensor with shape (1, latent_dim\n",
    "        prev_latent_mean = tf.squeeze(tf.stack([z_t_minus_one]*self.n_policies, axis=1))\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # print(prev_latent_mean)\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            # print(tran_input)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        if self.use_FEEF:\n",
    "            return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "        else:\n",
    "            return self.EFE(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "                # Compute the extrinisc approximation with the prior model\n",
    "                else:\n",
    "                    kl_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    kl_extrinsic = tf.reduce_sum(kl_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                kl_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"Extrinsic\", kl_extrinsic)\n",
    "            # print(\"Intrinsic\", kl_intrinsic)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    # TODO Find out how this works with the log probability extrinsic term\n",
    "    def EFE(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        EFEs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack(self.given_prior_mean), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack(self.given_prior_stddev), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    # compute extrinsic prior preferences term\n",
    "                    efe_extrinsic = -1 * tf.math.log(prior_dist.prob(predicted_likelihood))\n",
    "\n",
    "                # TODO Can I use the learned prior model here?\n",
    "                else:\n",
    "                    efe_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    efe_extrinsic = tf.reduce_sum(efe_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                efe_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            EFE = efe_extrinsic - kl_intrinsic\n",
    "\n",
    "            EFEs.append(EFE)\n",
    "\n",
    "        return EFEs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.83333333, 0.        ])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2,  [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 20, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=5,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False,\n",
    "                           show_tran_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.5804541  0.       ]\n",
      "[0.5820793  0.56995112]\n",
      "tf.Tensor([0.9595216  0.9533301  0.9519167  0.94522595 0.2565902 ], shape=(5,), dtype=float32)\n",
      "Success in episode 1 at time step 296\n",
      "Episode 2\n",
      "[-0.43930787  0.        ]\n",
      "[0.3297406  0.74181763]\n",
      "tf.Tensor([-0.9264326   0.62588334  0.67739195 -0.0683539  -0.7863491 ], shape=(5,), dtype=float32)\n",
      "Success in episode 2 at time step 307\n",
      "Episode 3\n",
      "[-0.59120166  0.        ]\n",
      "[-0.2037634   0.82579449]\n",
      "tf.Tensor([ 0.9585774   0.9229164   0.9190242   0.8587515  -0.25404596], shape=(5,), dtype=float32)\n",
      "Success in episode 3 at time step 115\n",
      "Episode 4\n",
      "[-0.57110834  0.        ]\n",
      "[0.1775421  0.67526242]\n",
      "tf.Tensor([ 0.9277285   0.8787425   0.34034693 -0.60928476 -0.85705173], shape=(5,), dtype=float32)\n",
      "Success in episode 4 at time step 258\n",
      "Episode 5\n",
      "[-0.58970606  0.        ]\n",
      "[-0.1078984   0.64276213]\n",
      "tf.Tensor([0.9711694  0.87211853 0.90033376 0.89951986 0.6705164 ], shape=(5,), dtype=float32)\n",
      "Success in episode 5 at time step 525\n",
      "Episode 6\n",
      "[-0.5292909  0.       ]\n",
      "[0.52284658 0.78118728]\n",
      "tf.Tensor([-0.90617716  0.5071469   0.30269462 -0.12301601 -0.46339864], shape=(5,), dtype=float32)\n",
      "Success in episode 6 at time step 392\n",
      "Episode 7\n",
      "[-0.47921568  0.        ]\n",
      "[-0.16942397  0.59764503]\n",
      "tf.Tensor([0.9405389  0.9408276  0.91070193 0.8485386  0.69145936], shape=(5,), dtype=float32)\n",
      "Success in episode 7 at time step 168\n",
      "Episode 8\n",
      "[-0.40778214  0.        ]\n",
      "[0.4866984  0.46192549]\n",
      "tf.Tensor([0.97289187 0.88700914 0.93283015 0.9370212  0.7124357 ], shape=(5,), dtype=float32)\n",
      "Success in episode 8 at time step 250\n",
      "Episode 9\n",
      "[-0.571513  0.      ]\n",
      "[-0.17543759  0.71827116]\n",
      "tf.Tensor([-0.8950079  0.9809849  0.9412154  0.9308681  0.6903705], shape=(5,), dtype=float32)\n",
      "Success in episode 9 at time step 336\n",
      "Episode 10\n",
      "[-0.5026279  0.       ]\n",
      "[0.22768184 0.73515791]\n",
      "tf.Tensor([0.548955   0.9657235  0.9655308  0.9308851  0.84660447], shape=(5,), dtype=float32)\n",
      "Success in episode 10 at time step 85\n",
      "Episode 11\n",
      "[-0.5594456  0.       ]\n",
      "[0.54203782 0.63201332]\n",
      "tf.Tensor([0.9633074  0.92062235 0.90628016 0.886197   0.59912866], shape=(5,), dtype=float32)\n",
      "Success in episode 11 at time step 390\n",
      "Episode 12\n",
      "[-0.55188906  0.        ]\n",
      "[0.05747003 0.61691566]\n",
      "tf.Tensor([0.961461   0.89376086 0.92861646 0.89412796 0.7877849 ], shape=(5,), dtype=float32)\n",
      "Success in episode 12 at time step 237\n",
      "Episode 13\n",
      "[-0.54302305  0.        ]\n",
      "[0.73890841 0.3180199 ]\n",
      "tf.Tensor([ 0.1622865  -0.49043176 -0.69184166 -0.5837944  -0.0740151 ], shape=(5,), dtype=float32)\n",
      "Success in episode 13 at time step 173\n",
      "Episode 14\n",
      "[-0.56402487  0.        ]\n",
      "[0.79127377 0.80232596]\n",
      "tf.Tensor([0.95989186 0.9526755  0.9434239  0.89003867 0.724184  ], shape=(5,), dtype=float32)\n",
      "Success in episode 14 at time step 242\n",
      "Episode 15\n",
      "[-0.47996408  0.        ]\n",
      "[-0.48167606  0.79215901]\n",
      "tf.Tensor([0.95447475 0.9609329  0.9251539  0.8751794  0.6465607 ], shape=(5,), dtype=float32)\n",
      "Success in episode 15 at time step 114\n",
      "Episode 16\n",
      "[-0.5570547  0.       ]\n",
      "[0.79598808 0.16788482]\n",
      "tf.Tensor([0.9176003 0.9524982 0.9355107 0.8681182 0.7159685], shape=(5,), dtype=float32)\n",
      "Success in episode 16 at time step 217\n",
      "Episode 17\n",
      "[-0.5438392  0.       ]\n",
      "[0.18596905 0.9949731 ]\n",
      "tf.Tensor([0.9353169  0.9412539  0.9540993  0.9044746  0.70066416], shape=(5,), dtype=float32)\n",
      "Success in episode 17 at time step 322\n",
      "Episode 18\n",
      "[-0.5838132  0.       ]\n",
      "[-0.17870788  0.64137878]\n",
      "tf.Tensor([0.94199765 0.915203   0.9089176  0.90573853 0.77282363], shape=(5,), dtype=float32)\n",
      "Success in episode 18 at time step 983\n",
      "Episode 19\n",
      "[-0.5308108  0.       ]\n",
      "[0.56065426 0.34814413]\n",
      "tf.Tensor([0.9768994  0.84919906 0.89788026 0.8263786  0.6944323 ], shape=(5,), dtype=float32)\n",
      "Success in episode 19 at time step 298\n",
      "Episode 20\n",
      "[-0.56708837  0.        ]\n",
      "[-0.06232178  1.        ]\n",
      "tf.Tensor([0.9309665  0.9511737  0.87675256 0.8674725  0.80409676], shape=(5,), dtype=float32)\n",
      "Success in episode 20 at time step 110\n",
      "Episode 21\n",
      "[-0.54312956  0.        ]\n",
      "[0.51296725 0.28641849]\n",
      "tf.Tensor([0.930832   0.93391365 0.87005335 0.8456869  0.6712671 ], shape=(5,), dtype=float32)\n",
      "Success in episode 21 at time step 308\n",
      "Episode 22\n",
      "[-0.5929846  0.       ]\n",
      "[0.53498708 0.68012956]\n",
      "tf.Tensor([0.9402804  0.947012   0.9365569  0.92913884 0.7623882 ], shape=(5,), dtype=float32)\n",
      "Success in episode 22 at time step 150\n",
      "Episode 23\n",
      "[-0.5765887  0.       ]\n",
      "[-0.48457658  0.72110993]\n",
      "tf.Tensor([0.92907363 0.955535   0.94476473 0.91818905 0.63632005], shape=(5,), dtype=float32)\n",
      "Success in episode 23 at time step 96\n",
      "Episode 24\n",
      "[-0.52537715  0.        ]\n",
      "[0.38669184 0.58987969]\n",
      "tf.Tensor([0.94110936 0.92640924 0.92373484 0.8188806  0.72082263], shape=(5,), dtype=float32)\n",
      "Success in episode 24 at time step 302\n",
      "Episode 25\n",
      "[-0.5961741  0.       ]\n",
      "[0.75878002 0.35521585]\n",
      "tf.Tensor([0.87856096 0.90602857 0.937032   0.92419696 0.85126334], shape=(5,), dtype=float32)\n",
      "Success in episode 25 at time step 74\n",
      "Episode 26\n",
      "[-0.4945468  0.       ]\n",
      "[0.54192234 0.80154344]\n",
      "tf.Tensor([0.92993003 0.9323434  0.91874444 0.88558686 0.7189655 ], shape=(5,), dtype=float32)\n",
      "Success in episode 26 at time step 126\n",
      "Episode 27\n",
      "[-0.43354335  0.        ]\n",
      "[-0.0263402   0.77984291]\n",
      "tf.Tensor([0.92411846 0.9216772  0.9145471  0.9191561  0.75565696], shape=(5,), dtype=float32)\n",
      "Success in episode 27 at time step 138\n",
      "Episode 28\n",
      "[-0.43774644  0.        ]\n",
      "[0.32522545 0.75485626]\n",
      "tf.Tensor([0.8894987  0.9454093  0.9127305  0.88684154 0.7772608 ], shape=(5,), dtype=float32)\n",
      "Success in episode 28 at time step 82\n",
      "Episode 29\n",
      "[-0.4484572  0.       ]\n",
      "[0.22896765 0.96962804]\n",
      "tf.Tensor([0.9456069  0.95764226 0.9024703  0.90212095 0.6433638 ], shape=(5,), dtype=float32)\n",
      "Success in episode 29 at time step 274\n",
      "Episode 30\n",
      "[-0.43062934  0.        ]\n",
      "[-0.07387516  0.95453561]\n",
      "tf.Tensor([0.93994   0.9588781 0.9175844 0.9472764 0.7171719], shape=(5,), dtype=float32)\n",
      "Success in episode 30 at time step 158\n",
      "Episode 31\n",
      "[-0.49161097  0.        ]\n",
      "[0.42511716 0.43411744]\n",
      "tf.Tensor([0.9497563  0.94824785 0.9240871  0.8908818  0.67562467], shape=(5,), dtype=float32)\n",
      "Success in episode 31 at time step 276\n",
      "Episode 32\n",
      "[-0.56137305  0.        ]\n",
      "[0.59939    0.62477007]\n",
      "tf.Tensor([0.92233306 0.8927421  0.91821116 0.940482   0.8121197 ], shape=(5,), dtype=float32)\n",
      "Success in episode 32 at time step 296\n",
      "Episode 33\n",
      "[-0.5329587  0.       ]\n",
      "[0.35903539 0.81459243]\n",
      "tf.Tensor([0.95280814 0.9387788  0.8956799  0.860617   0.7182659 ], shape=(5,), dtype=float32)\n",
      "Success in episode 33 at time step 152\n",
      "Episode 34\n",
      "[-0.44056174  0.        ]\n",
      "[-0.11157587  0.63634862]\n",
      "tf.Tensor([0.95645154 0.94018424 0.91444856 0.88777286 0.69247395], shape=(5,), dtype=float32)\n",
      "Success in episode 34 at time step 118\n",
      "Episode 35\n",
      "[-0.5651275  0.       ]\n",
      "[0.61756943 0.3701388 ]\n",
      "tf.Tensor([0.9287558  0.9458033  0.89524966 0.88920695 0.7116761 ], shape=(5,), dtype=float32)\n",
      "Success in episode 35 at time step 174\n",
      "Episode 36\n",
      "[-0.5409051  0.       ]\n",
      "[0.70143202 0.22233711]\n",
      "tf.Tensor([0.9363251  0.9386344  0.8984186  0.8789467  0.85810715], shape=(5,), dtype=float32)\n",
      "Success in episode 36 at time step 83\n",
      "Episode 37\n",
      "[-0.43007937  0.        ]\n",
      "[-0.0144807   0.64891861]\n",
      "tf.Tensor([0.9041168  0.93880177 0.9316597  0.916302   0.7823063 ], shape=(5,), dtype=float32)\n",
      "Success in episode 37 at time step 307\n",
      "Episode 38\n",
      "[-0.48683885  0.        ]\n",
      "[0.63582341 0.58545926]\n",
      "tf.Tensor([0.9349598  0.91025925 0.92015404 0.91451734 0.7705266 ], shape=(5,), dtype=float32)\n",
      "Success in episode 38 at time step 148\n",
      "Episode 39\n",
      "[-0.4460061  0.       ]\n",
      "[0.24109189 0.91072152]\n",
      "tf.Tensor([0.97513527 0.94103634 0.8986696  0.9033248  0.7042136 ], shape=(5,), dtype=float32)\n",
      "Success in episode 39 at time step 83\n",
      "Episode 40\n",
      "[-0.48793584  0.        ]\n",
      "[-0.35835309  0.60968031]\n",
      "tf.Tensor([0.9601251 0.9451403 0.9040467 0.8561258 0.6124627], shape=(5,), dtype=float32)\n",
      "Success in episode 40 at time step 118\n",
      "Episode 41\n",
      "[-0.54031676  0.        ]\n",
      "[0.77191109 0.72798148]\n",
      "tf.Tensor([0.9243465  0.9273241  0.9243104  0.8937442  0.73140585], shape=(5,), dtype=float32)\n",
      "Success in episode 41 at time step 217\n",
      "Episode 42\n",
      "[-0.44799525  0.        ]\n",
      "[-0.44855487  0.84601879]\n",
      "tf.Tensor([0.9677124 0.9690211 0.8959778 0.8616603 0.6196601], shape=(5,), dtype=float32)\n",
      "Success in episode 42 at time step 619\n",
      "Episode 43\n",
      "[-0.41892716  0.        ]\n",
      "[-0.0427382   0.82392809]\n",
      "tf.Tensor([0.9668225 0.9750692 0.921468  0.8506129 0.5084207], shape=(5,), dtype=float32)\n",
      "Success in episode 43 at time step 68\n",
      "Episode 44\n",
      "[-0.57754403  0.        ]\n",
      "[0.19955726 0.49953561]\n",
      "tf.Tensor([0.92784226 0.9443136  0.92656106 0.92047566 0.7066847 ], shape=(5,), dtype=float32)\n",
      "Success in episode 44 at time step 89\n",
      "Episode 45\n",
      "[-0.48631507  0.        ]\n",
      "[-0.36222886  0.91896018]\n",
      "tf.Tensor([0.9804901  0.95148444 0.9321664  0.89216644 0.62380445], shape=(5,), dtype=float32)\n",
      "Success in episode 45 at time step 209\n",
      "Episode 46\n",
      "[-0.5412607  0.       ]\n",
      "[-0.53058891  0.75605933]\n",
      "tf.Tensor([0.9645704  0.93759614 0.89742655 0.8441215  0.77066505], shape=(5,), dtype=float32)\n",
      "Success in episode 46 at time step 116\n",
      "Episode 47\n",
      "[-0.42613053  0.        ]\n",
      "[0.55963068 0.59583666]\n",
      "tf.Tensor([0.9410186  0.90531    0.89229906 0.89307797 0.7331317 ], shape=(5,), dtype=float32)\n",
      "Success in episode 47 at time step 151\n",
      "Episode 48\n",
      "[-0.52736974  0.        ]\n",
      "[0.72119607 0.13010755]\n",
      "tf.Tensor([0.98065925 0.9375108  0.9512862  0.9085938  0.27431303], shape=(5,), dtype=float32)\n",
      "Success in episode 48 at time step 663\n",
      "Episode 49\n",
      "[-0.5116934  0.       ]\n",
      "[0.74708285 0.20845752]\n",
      "tf.Tensor([0.96063656 0.9550291  0.9251421  0.887197   0.543323  ], shape=(5,), dtype=float32)\n",
      "Success in episode 49 at time step 339\n",
      "Episode 50\n",
      "[-0.40340006  0.        ]\n",
      "[-0.03962834  0.59033703]\n",
      "tf.Tensor([0.9621669  0.93075836 0.930942   0.8921292  0.4388255 ], shape=(5,), dtype=float32)\n",
      "Success in episode 50 at time step 166\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=50, action_repeats=6, num_actions_to_execute=4, train_on_full_data=True, show_replay_training=True, train_during_episode=True, train_vae=True, train_tran=True, train_prior=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=5,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=4,\n",
    "                           tran_train_epochs=4,\n",
    "                           show_vae_training=True,\n",
    "                           show_tran_training=True,\n",
    "                           train_vae=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.54905313  0.        ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7648\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8307\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8431\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8065\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.3455\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3123\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2779\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2427\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2738\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2699\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2668\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2642\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0856\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0829\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0796\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0758\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0331\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0401\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0447\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0473\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1266\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1155\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0949\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0660\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7452\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7519\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7473\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7324\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7876\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7738\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7591\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7436\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.6088\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6012\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5914\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5795\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2626\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2622\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2566\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2464\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2355\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2368\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2376\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2380\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4754\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4531\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4234\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3872\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7175\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7177\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7153\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7108\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4268\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4210\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4146\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4079\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1944\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1976\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1712\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1186\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5220\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5382\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5498\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5570\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.7357\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7322\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7253\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7154\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4675\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4641\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4603\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4559\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2865\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2784\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2668\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2520\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1718\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1495\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1240\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0959\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8557\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8324\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7989\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7570\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.4109\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 12.7783\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.8688\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7878\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2397\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2065\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.1017\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9995\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2350\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.8398\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.1367\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2029\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4427\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7995\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2145\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7201\n",
      "[0.54912316 0.8208157 ]\n",
      "tf.Tensor([0.964448   0.94589734 0.9385642  0.85845494 0.66371644], shape=(5,), dtype=float32)\n",
      "Epoch 1/4\n",
      "2/2 [==============================] - 0s 2ms/step - kl_loss: 2.4289\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 0s 2ms/step - kl_loss: 2.4062\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 0s 2ms/step - kl_loss: 2.2286\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 0s 1ms/step - kl_loss: 2.0231\n",
      "Success in episode 1 at time step 304\n",
      "Episode 2\n",
      "[-0.50351393  0.        ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9449\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2173\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4160\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5372\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8993\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9377\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9601\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9675\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2891\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2535\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2102\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1604\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2208\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2225\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2241\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2254\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6054\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5868\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5592\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5236\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3587\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3554\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3511\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3458\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2767\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2787\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2795\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2792\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4709\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4606\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4479\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4331\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8000\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8021\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7982\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7887\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.9068\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9076\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9031\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8936\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9756\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9581\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9375\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9142\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6481\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6505\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6526\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6546\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9426\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9289\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9130\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8952\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7614\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7240\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6814\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6348\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4601\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4737\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4833\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4891\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5525\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5467\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5375\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5253\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6725\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6832\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6887\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6892\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.9527\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.9486\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9385\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9230\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6226\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6105\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5970\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5821\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6364\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6375\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6351\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6296\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8516\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8478\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8273\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7923\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0633\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0776\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0729\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0514\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3663\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2821\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1647\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0220\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4381\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3461\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2454\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1403\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 4.3056\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3580\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3785\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3695\n",
      "[0.73398223 0.42913277]\n",
      "tf.Tensor([0.9694725  0.9224093  0.9269504  0.8764665  0.57731193], shape=(5,), dtype=float32)\n",
      "Epoch 1/4\n",
      "2/2 [==============================] - 0s 1ms/step - kl_loss: 1.5021\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 0s 1ms/step - kl_loss: 1.4940\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 0s 2ms/step - kl_loss: 1.4681\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 0s 2ms/step - kl_loss: 1.4451\n",
      "Success in episode 2 at time step 305\n",
      "Episode 3\n",
      "[-0.54028404  0.        ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6958\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.2466\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6510\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.9408\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5112\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6269\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7218\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7947\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.9299\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9320\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9230\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9038\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0517\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0844\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0959\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0878\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.4413\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4323\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4226\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4125\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2825\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2696\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2562\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2426\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3897\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3797\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3675\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3535\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2411\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2427\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2257\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1929\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.6009\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2490\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.4288\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2562\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5879\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8477\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0817\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2737\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7322\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6752\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5548\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3838\n",
      "[0.70752211 0.82007548]\n",
      "tf.Tensor([0.95968604 0.9566123  0.9335953  0.89472556 0.56333447], shape=(5,), dtype=float32)\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9717\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9588\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.9392\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9146\n",
      "Success in episode 3 at time step 134\n",
      "Episode 4\n",
      "[-0.5207106  0.       ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5929\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5577\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5014\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4281\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.6653\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6716\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6470\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5950\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5718\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5425\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5070\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4664\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3151\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3176\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3202\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3226\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1669\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1369\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1033\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0668\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1693\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0994\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9922\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8545\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.4199\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.3850\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2463\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.0218\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.7899\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.8089\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2480\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.2353\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8832\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6040\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3165\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0435\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5010\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4069\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3006\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1926\n",
      "[0.7950401  0.95503043]\n",
      "tf.Tensor([0.97974604 0.95158607 0.91632015 0.8599679  0.3190096 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7837\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7913\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7409\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6471\n",
      "Success in episode 4 at time step 122\n",
      "Episode 5\n",
      "[-0.4490788  0.       ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8737\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5315\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1265\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6796\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0466\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9406\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8496\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7720\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.5240\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5387\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5557\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5735\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3909\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4466\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4852\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5076\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6887\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6854\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6773\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6649\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0548\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0182\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9498\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8537\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0032\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9400\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8767\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8140\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.1955\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1350\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0664\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9911\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1571\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0567\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9487\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8351\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.5768\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5935\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6096\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6246\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0923\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0355\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9717\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9021\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0606\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0391\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0202\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0036\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6013\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.6153\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6223\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6229\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5835\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5878\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5890\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5872\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9257\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9292\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9307\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9304\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2695\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2696\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2668\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2613\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5469\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5359\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5226\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5073\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2979\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2860\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2738\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2616\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5827\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5974\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6044\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6045\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9333\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9312\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9215\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9052\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0177\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0280\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0328\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0324\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1939\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1923\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1904\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1882\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5213\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5140\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4975\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4728\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3333\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3338\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3324\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3293\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3598\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3502\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3369\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3204\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5889\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5806\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5629\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5369\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0507\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0725\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0707\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0479\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3416\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3392\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3359\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3316\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7804\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7872\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7886\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7851\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3093\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3116\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3113\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3088\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4186\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4157\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4106\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4037\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0923\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0946\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0944\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0918\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.4100\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4063\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4001\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3917\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6068\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5990\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5906\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5819\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7737\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7543\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.7325\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7089\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8894\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8328\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7423\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6228\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4743\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4632\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4517\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4397\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6333\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6573\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6622\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6496\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4952\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5039\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5102\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5142\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.1680\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1677\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1673\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1668\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9801\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9683\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9498\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9255\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.7431\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7378\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.7294\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7185\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3609\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3598\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3591\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3587\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5785\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5672\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5550\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5421\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2172\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2087\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1992\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1890\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8756\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8882\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8876\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.8750\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.6285\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6334\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.6364\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6378\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3164\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3135\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3097\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3051\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.4054\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4041\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3992\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3911\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3266\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3303\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3318\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3313\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5159\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4929\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4590\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4159\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7260\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7468\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7620\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7719\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7906\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7906\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7885\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.7844\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.6433\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6102\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5594\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4935\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 6.2392\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2374\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 6.1499\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.9877\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9976\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9530\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.8944\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8240\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.0123\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9619\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9146\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8706\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4145\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4131\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4123\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4116\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.7789\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8011\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8153\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.8222\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3137\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3134\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3105\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3052\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2072\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2040\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1994\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1936\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8461\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8395\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8328\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8259\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0703\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0439\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0124\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9769\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3651\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3618\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3586\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3552\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1910\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1949\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1979\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1998\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.6543\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6488\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6387\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6247\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0188\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0269\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0323\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0354\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.2485\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2365\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2139\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1824\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3835\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3831\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3821\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3804\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6560\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5938\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5064\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3982\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5293\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5764\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 2.6024\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6085\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4985\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4994\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4995\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4987\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.6582\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6607\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6572\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6484\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6583\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6543\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6379\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6105\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4818\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4660\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4412\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4090\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3603\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3335\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3066\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2802\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0030\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9980\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9909\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9819\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0993\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0634\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0136\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9521\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.1350\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1165\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0860\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0451\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4764\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4855\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4929\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4983\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3194\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3337\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3408\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3415\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5760\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5718\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5662\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5595\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7689\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7570\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7339\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7013\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 0s 1ms/step - kl_loss: 1.2838\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 0s 995us/step - kl_loss: 1.2545\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 0s 973us/step - kl_loss: 1.2388\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 0s 895us/step - kl_loss: 1.2386\n",
      "No Success\n",
      "Episode 6\n",
      "[-0.5883788  0.       ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9353\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8405\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7157\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5677\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3433\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3655\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3863\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4049\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9430\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9474\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9513\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9546\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5990\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5968\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5893\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5771\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0996\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0795\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0481\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0069\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5632\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5227\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4783\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4310\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.2121\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2222\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2301\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2357\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.4803\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4841\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4804\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4700\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7857\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7877\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7851\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7786\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0917\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0857\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0752\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 4.0607\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1786\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1722\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1651\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1574\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6013\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6018\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5862\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.5565\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.7059\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6912\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6736\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6536\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5512\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5405\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5300\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5196\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3663\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3532\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3380\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3211\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6590\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6571\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6115\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5281\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4418\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4460\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4500\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4532\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2418\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2393\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2357\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2310\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5788\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5655\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5478\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5265\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3196\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2812\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2370\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1883\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2249\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1317\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0265\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9118\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5954\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5959\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5977\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6002\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.2305\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2489\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2483\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2308\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9906\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9587\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9167\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8681\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5474\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2565\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8327\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3362\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.5776\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.2055\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.5957\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9121\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.9605\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9010\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8875\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9119\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0890\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0923\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8990\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5424\n",
      "[0.82550487 0.85658963]\n",
      "tf.Tensor([0.970343   0.95783114 0.96247035 0.841144   0.3952935 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/4\n",
      "2/2 [==============================] - 0s 1ms/step - kl_loss: 2.7071\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 0s 2ms/step - kl_loss: 2.3980\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 0s 2ms/step - kl_loss: 2.0959\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 0s 2ms/step - kl_loss: 1.9256\n",
      "Success in episode 6 at time step 337\n",
      "Episode 7\n",
      "[-0.559665  0.      ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4521\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.7416\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9792\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1583\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6510\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6852\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6942\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6801\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 13.2334\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.1876\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.9893\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.6591\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3155\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2428\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1769\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1176\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8769\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8307\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7778\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7189\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9785\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9105\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8274\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7316\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2276\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1637\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0974\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0301\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.1279\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0506\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9635\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8715\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1698\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9345\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6610\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3682\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0741\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8144\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5512\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2949\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.6728\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7669\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7823\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7256\n",
      "[0.61378621 0.48205287]\n",
      "tf.Tensor([0.9717114  0.95356554 0.9491357  0.8724616  0.26710677], shape=(5,), dtype=float32)\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.6425\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5880\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4839\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3556\n",
      "Success in episode 7 at time step 137\n",
      "Episode 8\n",
      "[-0.42329755  0.        ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5673\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3878\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2416\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1250\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.0665\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1627\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2522\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3308\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0234\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1305\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1936\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2152\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 5.5058\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4704\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4123\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3348\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1580\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0420\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9058\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7533\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 5.2502\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0227\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7794\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5256\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8675\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7724\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6875\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6120\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9599\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8713\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7889\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7128\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5052\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5340\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5620\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5880\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.9252\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9011\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8781\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8565\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5530\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5676\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5797\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5891\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8383\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8202\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7969\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7691\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3301\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3251\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3191\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3123\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.6257\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6424\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6441\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6323\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7159\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6736\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6101\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5289\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.5067\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4660\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4267\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3892\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.1460\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1246\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1029\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0811\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3531\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3739\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3902\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4019\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1681\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1713\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1736\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1749\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.8481\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8469\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8445\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8410\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.5258\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5207\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5122\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5009\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7367\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7346\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7314\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7273\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.7678\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7570\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7445\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7308\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4220\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4164\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4094\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4011\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5606\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5600\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5595\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5591\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3860\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3917\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3858\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3695\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4478\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4568\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4538\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4400\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7603\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7623\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7526\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7326\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5980\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5976\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5975\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5975\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9414\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9307\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9187\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9057\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5446\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5340\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5236\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5136\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1542\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1528\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1467\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1362\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.8754\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8600\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8434\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8262\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7294\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7111\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6906\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6685\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2950\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3190\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3145\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2844\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3338\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3289\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3246\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3210\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2870\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2831\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2787\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2738\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3562\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3164\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2650\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2039\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0803\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1329\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1466\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1250\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.2481\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2651\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2684\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2594\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3510\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3455\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3343\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3184\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.1307\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0943\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0487\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9960\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8387\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8597\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8761\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8879\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9804\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9761\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9717\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9672\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5063\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4994\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4917\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4833\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3714\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3788\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3823\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3821\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6474\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6451\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6407\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6344\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6174\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6060\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5784\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5370\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2107\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2052\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1967\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1857\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9427\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9522\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9570\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9574\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6609\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6627\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6611\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6563\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8006\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7985\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7960\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7931\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9858\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9679\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9448\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9174\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3467\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3400\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3325\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3245\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8219\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8381\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8448\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8429\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5126\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4827\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4358\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3749\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7765\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7579\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7395\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7215\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 7.2628\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2872\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2843\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2566\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0907\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0721\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0483\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0203\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6613\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6463\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6320\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6182\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4499\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4454\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4418\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4388\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3957\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4027\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4072\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4094\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9106\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9117\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9107\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9077\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4401\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4377\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4305\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4193\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7544\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7498\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7439\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7368\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0282\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0234\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0180\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0121\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7928\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7964\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7989\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8003\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7353\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7344\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7301\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7229\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6179\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6188\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5987\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5600\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8258\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8437\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8543\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8580\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4453\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4407\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4260\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4025\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4521\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4676\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4741\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4722\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2770\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2789\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2784\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2759\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8351\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8368\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8370\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8358\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0681\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0588\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0411\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0162\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1734\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1877\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1955\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1973\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1095\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1089\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1070\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1040\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6455\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6482\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6484\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6464\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1994\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1998\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1993\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1982\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2761\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2728\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2683\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2628\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3442\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3483\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3503\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3501\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3745\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3721\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3685\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3639\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5830\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5253\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4365\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3222\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 0s 1ms/step - kl_loss: 1.3223\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 0s 1ms/step - kl_loss: 1.3137\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 0s 997us/step - kl_loss: 1.2990\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 0s 821us/step - kl_loss: 1.2888\n",
      "No Success\n",
      "Episode 9\n",
      "[-0.5984904  0.       ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5103\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4033\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2210\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9773\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5163\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5374\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5613\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5858\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0025\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9873\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9446\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8779\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8171\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7796\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7369\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6900\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5757\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5135\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4471\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3776\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8759\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8643\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8459\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8215\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6166\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6153\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5875\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5368\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6709\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7080\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7270\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7289\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2462\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 6.2309\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2075\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1774\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6998\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.6911\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6786\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6630\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7389\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6986\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6468\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5860\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7286\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7260\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7185\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7065\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6903\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6569\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5969\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5137\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6833\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7182\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7366\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7398\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5320\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5136\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4768\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4242\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7123\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6813\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6492\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6169\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6394\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6432\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6451\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6453\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5087\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5064\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4984\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4859\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2246\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2197\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1724\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0891\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9338\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9473\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9467\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9329\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.1620\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.8091\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 21.2192\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 20.4331\n",
      "[0.93917788 0.22309613]\n",
      "tf.Tensor([0.9266151  0.96850806 0.956694   0.82554036 0.53651226], shape=(5,), dtype=float32)\n",
      "Epoch 1/4\n",
      "2/2 [==============================] - 0s 2ms/step - kl_loss: 3.6347\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 0s 2ms/step - kl_loss: 3.4436\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 0s 2ms/step - kl_loss: 3.3991\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 0s 1ms/step - kl_loss: 3.3383\n",
      "Success in episode 9 at time step 253\n",
      "Episode 10\n",
      "[-0.4393004  0.       ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9663\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3771\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6722\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 7.8863\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3786\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4020\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4322\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4656\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7910\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8555\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8782\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8621\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1199\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0650\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9754\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8558\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8308\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6841\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5133\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3234\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1512\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0013\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8386\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6679\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1586\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9475\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7088\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4609\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7310\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2177\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6862\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2121\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4926\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0786\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6116\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1292\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7698\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0258\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8869\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4210\n",
      "[0.30694024 0.84386266]\n",
      "tf.Tensor([0.9738341  0.9521471  0.93254393 0.77038205 0.3661881 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.3287\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1182\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.8205\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.5350\n",
      "Success in episode 10 at time step 130\n",
      "Episode 11\n",
      "[-0.46151564  0.        ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 8.3353\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0311\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7345\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4390\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1268\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2967\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3906\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4077\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.6584\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6730\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6704\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6514\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1406\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0244\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8776\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7061\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0389\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0057\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9778\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9546\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6050\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5583\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5049\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4471\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7565\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2222\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4601\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5938\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4879\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9359\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2814\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6107\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.6524\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9956\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.8761\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3468\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 14.5311\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.8070\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.7144\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5529\n",
      "[0.61833178 0.90691401]\n",
      "tf.Tensor([0.96017563 0.9559546  0.9212431  0.87177604 0.53013456], shape=(5,), dtype=float32)\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4862\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2697\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9348\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6014\n",
      "Success in episode 11 at time step 125\n",
      "Episode 12\n",
      "[-0.42530268  0.        ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.5768\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9772\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2966\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5194\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6107\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7186\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8193\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9082\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7349\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7600\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7788\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7911\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7384\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7361\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7245\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7047\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9920\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9971\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9942\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9840\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2743\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2798\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2592\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2158\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.7450\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2923\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4977\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.4801\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.1374\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.7001\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1815\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7079\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.3448\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2958\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9575\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.3987\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.2333\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8585\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.2155\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5161\n",
      "[0.60462056 0.70123029]\n",
      "tf.Tensor([0.9786489  0.95973295 0.95093143 0.5540763  0.25054553], shape=(5,), dtype=float32)\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4109\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2601\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0421\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8109\n",
      "Success in episode 12 at time step 124\n",
      "Episode 13\n",
      "[-0.42523405  0.        ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.4958\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8206\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1315\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4095\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3237\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3798\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4123\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4204\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8363\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8053\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7465\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6633\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9563\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9726\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9868\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9985\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3863\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3593\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3322\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3055\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8293\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8517\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8677\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8775\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.7919\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7962\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7974\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7957\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8890\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8852\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8805\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8751\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3462\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3363\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3122\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2760\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3019\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2641\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2154\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1578\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6439\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6124\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5803\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5477\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4955\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5119\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5228\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5283\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1232\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1329\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1221\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0932\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4303\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3754\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3101\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2370\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.7803\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7361\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6929\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6511\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3021\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3112\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3201\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3282\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1235\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1200\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0832\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0168\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8765\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8952\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9060\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9093\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9639\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9681\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9683\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9650\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6814\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6775\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6726\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6670\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2211\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2181\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2152\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2125\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.7015\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6931\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.6819\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6684\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.2407\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2516\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2552\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2520\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9481\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9306\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9067\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8774\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8624\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8504\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8391\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8286\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7503\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7626\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7705\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7741\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3201\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3061\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2802\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2441\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3230\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3313\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3383\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3440\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8681\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8688\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8577\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8362\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4361\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4010\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3597\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3138\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4269\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4197\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4101\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3985\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1918\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1952\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1936\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1877\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.8819\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8830\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8828\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8814\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.6516\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.6541\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6483\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6351\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9335\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9409\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9370\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9231\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8330\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8295\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8241\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8169\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6656\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6625\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6582\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6529\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5320\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5245\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5167\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5087\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2859\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2890\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2916\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2938\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6365\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6293\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6044\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5639\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8040\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8037\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8016\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7977\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3728\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3815\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3749\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3549\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4153\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3973\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3723\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3416\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9890\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9962\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0027\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0082\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7657\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7657\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7627\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7572\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5174\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5182\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5189\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5196\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3762\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3682\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3488\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3194\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9816\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9586\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9333\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.9066\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1787\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1724\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1674\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1636\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6624\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6753\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6788\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6735\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1531\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1535\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1527\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1510\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.9873\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9851\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9722\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9498\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2835\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2599\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2155\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1533\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3581\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3384\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3193\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3008\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3400\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2964\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2370\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1651\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.7168\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7171\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7197\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7237\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9363\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8956\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8481\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7955\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8698\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8029\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7286\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6488\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4071\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3905\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3759\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3630\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1783\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.2088\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2100\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1847\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2196\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1782\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1293\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0748\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4692\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3653\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2405\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0998\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9267\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9071\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8764\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8356\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1949\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1367\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0750\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0111\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.1667\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2386\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2848\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3063\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.4366\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4007\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3423\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2648\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.5882\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6103\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6266\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6371\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7583\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7543\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7001\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6021\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4514\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4368\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4172\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3929\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1699\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1415\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1031\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0565\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.4424\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4726\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4759\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4549\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6950\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6883\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6788\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6667\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3361\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3480\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3567\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3623\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3097\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3098\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3071\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3021\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.6696\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6650\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6601\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6549\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.1034\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0737\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0297\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9740\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3338\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3277\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3099\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2819\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7325\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7178\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7005\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6813\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7477\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7106\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6711\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6301\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1782\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1573\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1341\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1090\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.3067\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3215\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3195\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3022\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.3953\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3653\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3197\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2613\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1509\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1266\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1053\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0870\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 0s 1ms/step - kl_loss: 1.4313\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 0s 993us/step - kl_loss: 1.4142\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 0s 984us/step - kl_loss: 1.3952\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 0s 986us/step - kl_loss: 1.3917\n",
      "No Success\n",
      "Episode 14\n",
      "[-0.59320503  0.        ]\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2866\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2972\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2889\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2640\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3040\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3002\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2830\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2542\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.6893\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6805\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6722\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6644\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1373\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0982\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0482\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9890\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4937\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4817\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4710\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4616\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5435\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5388\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5272\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5098\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.9209\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9045\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8549\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7766\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1487\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0972\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0312\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9540\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9825\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0209\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0484\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0646\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6185\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6097\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5956\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5770\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2694\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2743\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2780\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2804\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5129\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5076\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4984\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4858\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3937\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3717\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3461\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3177\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3445\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3662\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3751\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3722\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.7176\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6569\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5604\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4343\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4006\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3896\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3765\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3618\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3126\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3291\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3318\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3213\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.0128\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9994\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9641\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9102\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0552\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0815\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0938\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0929\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3677\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3507\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3247\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2912\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6837\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6904\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6922\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6894\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5603\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5437\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5244\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5031\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8167\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8326\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8344\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8235\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0454\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0484\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0506\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0518\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [135]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# train the agent on the env\u001B[39;00m\n\u001B[1;32m      2\u001B[0m env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMountainCarContinuous-v0\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m agent, results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_single_agent\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdaifa\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservation_max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservation_min\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservation_noise_stddev\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_repeats\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_actions_to_execute\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_on_full_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_replay_training\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_during_episode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [123]\u001B[0m, in \u001B[0;36mtrain_single_agent\u001B[0;34m(mcc_env, agent, obs_max, obs_min, observation_noise_stddev, action_repeats, num_actions_to_execute, num_episodes, train_on_full_data, show_replay_training, replay_train_epochs, train_during_episode)\u001B[0m\n\u001B[1;32m    130\u001B[0m policy_observation \u001B[38;5;241m=\u001B[39m observation_sequence[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    132\u001B[0m \u001B[38;5;66;03m# select a new policy and clear everything\u001B[39;00m\n\u001B[0;32m--> 133\u001B[0m policy \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_policy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolicy_observation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;66;03m# clear the observations\u001B[39;00m\n\u001B[1;32m    136\u001B[0m observation_sequence \u001B[38;5;241m=\u001B[39m []\n",
      "Input \u001B[0;32mIn [124]\u001B[0m, in \u001B[0;36mDAIFAgentRecurrent.select_policy\u001B[0;34m(self, observation)\u001B[0m\n\u001B[1;32m    177\u001B[0m _,  _, latent_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_vae\u001B[38;5;241m.\u001B[39mencoder(observation\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, observation\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]))\n\u001B[1;32m    178\u001B[0m \u001B[38;5;66;03m# latent_state = latent_state.numpy().reshape((1, latent_state.shape[0]))\u001B[39;00m\n\u001B[1;32m    179\u001B[0m \n\u001B[1;32m    180\u001B[0m \u001B[38;5;66;03m# print(latent_state)\u001B[39;00m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;66;03m# select the policy\u001B[39;00m\n\u001B[0;32m--> 182\u001B[0m policy_mean, policy_stddev \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcem_policy_optimisation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlatent_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;66;03m# return a distribution that we can sample from\u001B[39;00m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tfp\u001B[38;5;241m.\u001B[39mdistributions\u001B[38;5;241m.\u001B[39mMultivariateNormalDiag(loc\u001B[38;5;241m=\u001B[39mpolicy_mean, scale_diag\u001B[38;5;241m=\u001B[39mpolicy_stddev)\n",
      "Input \u001B[0;32mIn [124]\u001B[0m, in \u001B[0;36mDAIFAgentRecurrent.cem_policy_optimisation\u001B[0;34m(self, z_t_minus_one)\u001B[0m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_cem_policy_iterations):\n\u001B[1;32m    195\u001B[0m     policy_distr \u001B[38;5;241m=\u001B[39m tfp\u001B[38;5;241m.\u001B[39mdistributions\u001B[38;5;241m.\u001B[39mMultivariateNormalDiag(loc\u001B[38;5;241m=\u001B[39mmean_best_policies, scale_diag\u001B[38;5;241m=\u001B[39mstd_best_policies)\n\u001B[0;32m--> 196\u001B[0m     policies \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy_distr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_policies\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    197\u001B[0m     policies \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mclip_by_value(policies, clip_value_min\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, clip_value_max\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;66;03m# project trajectory into the future using transition model and calculate FEEF for each policy\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:1233\u001B[0m, in \u001B[0;36mDistribution.sample\u001B[0;34m(self, sample_shape, seed, name, **kwargs)\u001B[0m\n\u001B[1;32m   1218\u001B[0m \u001B[38;5;124;03m\"\"\"Generate samples of the specified shape.\u001B[39;00m\n\u001B[1;32m   1219\u001B[0m \n\u001B[1;32m   1220\u001B[0m \u001B[38;5;124;03mNote that a call to `sample()` without arguments will generate a single\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;124;03m  samples: a `Tensor` with prepended dimensions `sample_shape`.\u001B[39;00m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1232\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name_and_control_scope(name):\n\u001B[0;32m-> 1233\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_sample_n\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif/lib/python3.8/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py:344\u001B[0m, in \u001B[0;36mTransformedDistribution._call_sample_n\u001B[0;34m(self, sample_shape, seed, **kwargs)\u001B[0m\n\u001B[1;32m    341\u001B[0m distribution_kwargs, bijector_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_kwargs_split_fn(kwargs)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;66;03m# First, generate samples from the base distribution.\u001B[39;00m\n\u001B[0;32m--> 344\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_broadcast_distribution_batch_shape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdistribution_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# Apply the bijector's forward transformation. For caching to\u001B[39;00m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;66;03m# work, it is imperative that this is the last modification to the\u001B[39;00m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;66;03m# returned result.\u001B[39;00m\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbijector\u001B[38;5;241m.\u001B[39mforward(x, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbijector_kwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:1233\u001B[0m, in \u001B[0;36mDistribution.sample\u001B[0;34m(self, sample_shape, seed, name, **kwargs)\u001B[0m\n\u001B[1;32m   1218\u001B[0m \u001B[38;5;124;03m\"\"\"Generate samples of the specified shape.\u001B[39;00m\n\u001B[1;32m   1219\u001B[0m \n\u001B[1;32m   1220\u001B[0m \u001B[38;5;124;03mNote that a call to `sample()` without arguments will generate a single\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;124;03m  samples: a `Tensor` with prepended dimensions `sample_shape`.\u001B[39;00m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1232\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name_and_control_scope(name):\n\u001B[0;32m-> 1233\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_sample_n\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:1209\u001B[0m, in \u001B[0;36mDistribution._call_sample_n\u001B[0;34m(self, sample_shape, seed, **kwargs)\u001B[0m\n\u001B[1;32m   1205\u001B[0m sample_shape \u001B[38;5;241m=\u001B[39m ps\u001B[38;5;241m.\u001B[39mconvert_to_shape_tensor(\n\u001B[1;32m   1206\u001B[0m     ps\u001B[38;5;241m.\u001B[39mcast(sample_shape, tf\u001B[38;5;241m.\u001B[39mint32), name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msample_shape\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m   1207\u001B[0m sample_shape, n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_sample_shape_to_vector(\n\u001B[1;32m   1208\u001B[0m     sample_shape, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msample_shape\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m-> 1209\u001B[0m samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample_n\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1210\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcallable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1211\u001B[0m batch_event_shape \u001B[38;5;241m=\u001B[39m ps\u001B[38;5;241m.\u001B[39mshape(samples)[\u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m   1212\u001B[0m final_shape \u001B[38;5;241m=\u001B[39m ps\u001B[38;5;241m.\u001B[39mconcat([sample_shape, batch_event_shape], \u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif/lib/python3.8/site-packages/tensorflow_probability/python/distributions/sample.py:238\u001B[0m, in \u001B[0;36mSample._sample_n\u001B[0;34m(self, n, seed, **kwargs)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sample_n\u001B[39m(\u001B[38;5;28mself\u001B[39m, n, seed, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    237\u001B[0m   sample_shape \u001B[38;5;241m=\u001B[39m ps\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample_shape, shape\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m--> 238\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribution\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m(ps\u001B[38;5;241m.\u001B[39mconcat([[n], sample_shape], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m),\n\u001B[1;32m    239\u001B[0m                                seed\u001B[38;5;241m=\u001B[39mseed,\n\u001B[1;32m    240\u001B[0m                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    241\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mtranspose(a\u001B[38;5;241m=\u001B[39mx, perm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampling_permutation(sample_ndims\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=30, action_repeats=6, num_actions_to_execute=2, train_on_full_data=True, show_replay_training=True, train_during_episode=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12651, 2) (12651, 1)\n"
     ]
    }
   ],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 500\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "actions = []\n",
    "\n",
    "pre_obs = []\n",
    "post_obs = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "    actions.append(a)\n",
    "\n",
    "    pre_obs.append(o[:-1])\n",
    "    post_obs.append(o[1:])\n",
    "\n",
    "\n",
    "    observations.append(o)\n",
    "    # ob_seqs.append(train)\n",
    "    # next_obs.append(test)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "actions = np.vstack(actions)\n",
    "\n",
    "pre_obs = np.vstack(pre_obs)\n",
    "post_obs = np.vstack(post_obs)\n",
    "\n",
    "\n",
    "print(pre_obs.shape, actions.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.25044785,  0.03038468],\n       [-0.19236082,  0.06095558],\n       [-0.24001593,  0.03986869],\n       ...,\n       [-0.42658775,  0.59245269],\n       [-0.4112652 ,  0.65043436],\n       [-0.35537884,  0.62402917]])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(12671, 2), dtype=float32, numpy=\narray([[-0.26148793,  0.03625497],\n       [-0.14099051,  0.08441392],\n       [-0.21544485,  0.0564904 ],\n       ...,\n       [-0.36538196,  0.6092098 ],\n       [-0.386122  ,  0.5987153 ],\n       [-0.32891673,  0.5738698 ]], dtype=float32)>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = agent.model_vae(observations)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(12651, 2), dtype=float32, numpy=\narray([[-0.18604551,  0.03065864],\n       [-0.05877355, -0.03745528],\n       [-0.13142759,  0.01794346],\n       ...,\n       [-0.7772355 , -0.49976996],\n       [-0.585173  , -0.5954007 ],\n       [-0.6535084 , -0.52987987]], dtype=float32)>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, s, z_pre = agent.model_vae.encoder(pre_obs)\n",
    "m, s, z_post = agent.model_vae.encoder(post_obs)\n",
    "z_pre"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2 and the array at index 1 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [47]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m z_plus_a \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactions\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m z_plus_a\n",
      "File \u001B[0;32m<__array_function__ internals>:5\u001B[0m, in \u001B[0;36mconcatenate\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2 and the array at index 1 has size 1"
     ]
    }
   ],
   "source": [
    "z_plus_a = np.concatenate([z, actions], axis=0)\n",
    "z_plus_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"transition_gru_5\" (type TransitionGRU).\n\nInput 1 of layer \"transition\" is incompatible with the layer: expected shape=(None, 20), found shape=(10371, 2)\n\nCall arguments received:\n  ‚Ä¢ inputs=('tf.Tensor(shape=(10371, 2), dtype=float32)', 'tf.Tensor(shape=(10371, 2), dtype=float32)')\n  ‚Ä¢ training=None\n  ‚Ä¢ mask=None",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [38]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtran\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/Documents/Masters/Thesis/DAIF_Agents/transition_gru.py:50\u001B[0m, in \u001B[0;36mTransitionGRU.call\u001B[0;34m(self, inputs, training, mask)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m initial_state \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     49\u001B[0m     initial_state \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_units))  \u001B[38;5;66;03m# start as zeros with number of examples times hidden dimension\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransition_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: Exception encountered when calling layer \"transition_gru_5\" (type TransitionGRU).\n\nInput 1 of layer \"transition\" is incompatible with the layer: expected shape=(None, 20), found shape=(10371, 2)\n\nCall arguments received:\n  ‚Ä¢ inputs=('tf.Tensor(shape=(10371, 2), dtype=float32)', 'tf.Tensor(shape=(10371, 2), dtype=float32)')\n  ‚Ä¢ training=None\n  ‚Ä¢ mask=None"
     ]
    }
   ],
   "source": [
    "agent.tran((z, np.zeros_like(z)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test without the replay training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.83333333, 0.        ])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2,  [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 20, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=5,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.5452394  0.       ]\n",
      "WARNING:tensorflow:From /Users/Ethan/miniconda3/envs/tf_daif_car_race/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 14:32:26.876947: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80860598 0.09458183]\n",
      "tf.Tensor([ 0.9830013   0.7846042   0.3220308  -0.01687961  0.16555098], shape=(5,), dtype=float32)\n",
      "Success in episode 1 at time step 369\n",
      "Episode 2\n",
      "[-0.57064813  0.        ]\n",
      "[0.47878008 0.40288668]\n",
      "tf.Tensor([ 0.957456    0.79769796  0.22657327 -0.27285808 -0.09558801], shape=(5,), dtype=float32)\n",
      "Success in episode 2 at time step 106\n",
      "Episode 3\n",
      "[-0.4439096  0.       ]\n",
      "[0.55124285 0.51222228]\n",
      "tf.Tensor([ 0.9207449  -0.5341525  -0.88466763 -0.8933735  -0.15289223], shape=(5,), dtype=float32)\n",
      "Success in episode 3 at time step 92\n",
      "Episode 4\n",
      "[-0.55050933  0.        ]\n",
      "No Success\n",
      "Episode 5\n",
      "[-0.53231597  0.        ]\n",
      "[0.7296093  0.15947908]\n",
      "tf.Tensor([ 0.9820634   0.93507713  0.9540813   0.42379785 -0.84602374], shape=(5,), dtype=float32)\n",
      "Success in episode 5 at time step 125\n",
      "Episode 6\n",
      "[-0.4112631  0.       ]\n",
      "No Success\n",
      "Episode 7\n",
      "[-0.53678113  0.        ]\n",
      "No Success\n",
      "Episode 8\n",
      "[-0.42946008  0.        ]\n",
      "[0.74362838 0.34122923]\n",
      "tf.Tensor([0.7967912  0.9772121  0.9674017  0.93885607 0.19883062], shape=(5,), dtype=float32)\n",
      "Success in episode 8 at time step 758\n",
      "Episode 9\n",
      "[-0.5263604  0.       ]\n",
      "[0.84034789 0.53523016]\n",
      "tf.Tensor([ 0.9681933   0.97307837  0.94899935  0.8507929  -0.10709003], shape=(5,), dtype=float32)\n",
      "Success in episode 9 at time step 122\n",
      "Episode 10\n",
      "[-0.5852645  0.       ]\n",
      "[0.69803108 0.45851984]\n",
      "tf.Tensor([ 0.93200725  0.07335937  0.35393855 -0.43221903 -0.99461627], shape=(5,), dtype=float32)\n",
      "Success in episode 10 at time step 438\n",
      "Episode 11\n",
      "[-0.42456153  0.        ]\n",
      "[0.90410014 0.03039351]\n",
      "tf.Tensor([0.955181   0.9668999  0.94788754 0.9243209  0.03093042], shape=(5,), dtype=float32)\n",
      "Success in episode 11 at time step 196\n",
      "Episode 12\n",
      "[-0.5621345  0.       ]\n",
      "[0.60916363 0.64281688]\n",
      "tf.Tensor([0.9673787  0.9515966  0.9198448  0.83164215 0.37397212], shape=(5,), dtype=float32)\n",
      "Success in episode 12 at time step 353\n",
      "Episode 13\n",
      "[-0.44907644  0.        ]\n",
      "[0.6238851  0.56028498]\n",
      "tf.Tensor([0.962473   0.96590596 0.91697454 0.80474585 0.36682007], shape=(5,), dtype=float32)\n",
      "Success in episode 13 at time step 245\n",
      "Episode 14\n",
      "[-0.5084155  0.       ]\n",
      "[0.52400454 0.49420985]\n",
      "tf.Tensor([0.96537584 0.95639193 0.8497883  0.8272771  0.53330594], shape=(5,), dtype=float32)\n",
      "Success in episode 14 at time step 139\n",
      "Episode 15\n",
      "[-0.4963757  0.       ]\n",
      "[0.5967929  0.78715146]\n",
      "tf.Tensor([0.9737183  0.96602696 0.92889744 0.8075658  0.27599594], shape=(5,), dtype=float32)\n",
      "Success in episode 15 at time step 472\n",
      "Episode 16\n",
      "[-0.41137737  0.        ]\n",
      "[0.64697548 0.55997122]\n",
      "tf.Tensor([ 0.7319554   0.5848996   0.45162955 -0.20630382 -0.1557222 ], shape=(5,), dtype=float32)\n",
      "Success in episode 16 at time step 114\n",
      "Episode 17\n",
      "[-0.47954446  0.        ]\n",
      "[0.66216164 0.5763151 ]\n",
      "tf.Tensor([0.9872321 0.9339481 0.8323261 0.7743125 0.7955457], shape=(5,), dtype=float32)\n",
      "Success in episode 17 at time step 126\n",
      "Episode 18\n",
      "[-0.54798496  0.        ]\n",
      "[0.80955268 0.84861332]\n",
      "tf.Tensor([0.9511565  0.91198766 0.914686   0.92453223 0.4987105 ], shape=(5,), dtype=float32)\n",
      "Success in episode 18 at time step 109\n",
      "Episode 19\n",
      "[-0.41204917  0.        ]\n",
      "[0.76391228 0.86990232]\n",
      "tf.Tensor([0.9856899  0.91792905 0.67700845 0.79191905 0.81273013], shape=(5,), dtype=float32)\n",
      "Success in episode 19 at time step 122\n",
      "Episode 20\n",
      "[-0.5644128  0.       ]\n",
      "[0.67947636 0.68230617]\n",
      "tf.Tensor([ 0.9875462   0.9684505   0.82187605  0.0949681  -0.51842016], shape=(5,), dtype=float32)\n",
      "Success in episode 20 at time step 135\n",
      "Episode 21\n",
      "[-0.5304834  0.       ]\n",
      "[0.0453475 0.9325196]\n",
      "tf.Tensor([ 0.99650794  0.9938319   0.9598182   0.10676017 -0.1436152 ], shape=(5,), dtype=float32)\n",
      "Success in episode 21 at time step 120\n",
      "Episode 22\n",
      "[-0.58897793  0.        ]\n",
      "[0.32071005 0.86789848]\n",
      "tf.Tensor([ 0.96506894  0.97593784  0.9567569   0.37466732 -0.29258507], shape=(5,), dtype=float32)\n",
      "Success in episode 22 at time step 105\n",
      "Episode 23\n",
      "[-0.4831836  0.       ]\n",
      "[0.55355523 0.84974087]\n",
      "tf.Tensor([ 0.8452918  -0.92526656 -0.96639925 -0.9039103  -0.2935575 ], shape=(5,), dtype=float32)\n",
      "Success in episode 23 at time step 90\n",
      "Episode 24\n",
      "[-0.4490086  0.       ]\n",
      "[0.15431216 0.96833963]\n",
      "tf.Tensor([ 0.9827694   0.66276896 -0.89383835 -0.9488957  -0.9110807 ], shape=(5,), dtype=float32)\n",
      "Success in episode 24 at time step 94\n",
      "Episode 25\n",
      "[-0.51117045  0.        ]\n",
      "[0.42419586 0.93457236]\n",
      "tf.Tensor([-0.8502137  -0.9579167  -0.9561652  -0.8854989  -0.45486552], shape=(5,), dtype=float32)\n",
      "Success in episode 25 at time step 105\n",
      "Episode 26\n",
      "[-0.47662842  0.        ]\n",
      "[0.23855552 0.93985884]\n",
      "tf.Tensor([ 0.0436843  -0.96799046 -0.9648949  -0.92606574 -0.81029797], shape=(5,), dtype=float32)\n",
      "Success in episode 26 at time step 96\n",
      "Episode 27\n",
      "[-0.5792641  0.       ]\n",
      "[0.39285086 0.89285321]\n",
      "tf.Tensor([-0.8838425  -0.9465538  -0.9562758  -0.8997264  -0.53602725], shape=(5,), dtype=float32)\n",
      "Success in episode 27 at time step 117\n",
      "Episode 28\n",
      "[-0.4025429  0.       ]\n",
      "[0.16296143 0.88732952]\n",
      "tf.Tensor([ 0.96141434 -0.7195983  -0.9567334  -0.9061281  -0.70640635], shape=(5,), dtype=float32)\n",
      "Success in episode 28 at time step 84\n",
      "Episode 29\n",
      "[-0.45679152  0.        ]\n",
      "[0.75470664 0.31699938]\n",
      "tf.Tensor([-0.97266483 -0.9755431  -0.9096328  -0.63750523 -0.53149885], shape=(5,), dtype=float32)\n",
      "Success in episode 29 at time step 89\n",
      "Episode 30\n",
      "[-0.5462031  0.       ]\n",
      "[0.34965257 0.95319361]\n",
      "tf.Tensor([ 0.30918306 -0.9515482  -0.9745215  -0.934501   -0.7999848 ], shape=(5,), dtype=float32)\n",
      "Success in episode 30 at time step 140\n",
      "Episode 31\n",
      "[-0.41845554  0.        ]\n",
      "[0.42680894 0.5955394 ]\n",
      "tf.Tensor([ 0.0122298  -0.97020054 -0.97598225 -0.92446953 -0.71551466], shape=(5,), dtype=float32)\n",
      "Success in episode 31 at time step 273\n",
      "Episode 32\n",
      "[-0.41770574  0.        ]\n",
      "[0.77922881 0.63037632]\n",
      "tf.Tensor([-0.9758603  -0.988826   -0.92702734  0.09244861  0.27263066], shape=(5,), dtype=float32)\n",
      "Success in episode 32 at time step 253\n",
      "Episode 33\n",
      "[-0.463659  0.      ]\n",
      "[0.62340402 0.5435404 ]\n",
      "tf.Tensor([-0.9704191  -0.9395351  -0.89592385 -0.80561024 -0.7685855 ], shape=(5,), dtype=float32)\n",
      "Success in episode 33 at time step 283\n",
      "Episode 34\n",
      "[-0.47814003  0.        ]\n",
      "[0.79361985 0.38159733]\n",
      "tf.Tensor([-0.9939903  -0.8851764   0.33940306 -0.06241485 -0.8044955 ], shape=(5,), dtype=float32)\n",
      "Success in episode 34 at time step 219\n",
      "Episode 35\n",
      "[-0.5698738  0.       ]\n",
      "[0.80840458 0.25253609]\n",
      "tf.Tensor([-0.65526855 -0.10981813  0.9148883  -0.66293246 -0.93880355], shape=(5,), dtype=float32)\n",
      "Success in episode 35 at time step 436\n",
      "Episode 36\n",
      "[-0.4417323  0.       ]\n",
      "No Success\n",
      "Episode 37\n",
      "[-0.5901701  0.       ]\n",
      "[0.73845693 0.1457434 ]\n",
      "tf.Tensor([0.50353736 0.82554203 0.9732157  0.9508254  0.50749767], shape=(5,), dtype=float32)\n",
      "Success in episode 37 at time step 238\n",
      "Episode 38\n",
      "[-0.45247665  0.        ]\n",
      "[0.7372389  0.34390254]\n",
      "tf.Tensor([-0.30627385 -0.20624106  0.9204775   0.93034166 -0.22478874], shape=(5,), dtype=float32)\n",
      "Success in episode 38 at time step 471\n",
      "Episode 39\n",
      "[-0.41200373  0.        ]\n",
      "[0.70348916 0.06563696]\n",
      "tf.Tensor([ 0.9063062   0.90392214  0.8661124  -0.79284    -0.9465599 ], shape=(5,), dtype=float32)\n",
      "Success in episode 39 at time step 576\n",
      "Episode 40\n",
      "[-0.57113284  0.        ]\n",
      "[0.74960369 0.26518259]\n",
      "tf.Tensor([ 0.8939229  -0.29449376  0.42528403 -0.7624439  -0.9759877 ], shape=(5,), dtype=float32)\n",
      "Success in episode 40 at time step 229\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=40, action_repeats=6, num_actions_to_execute=2, train_on_full_data=False, show_replay_training=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'observations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mobservations\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'observations' is not defined"
     ]
    }
   ],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'observations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m res \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mmodel_vae(\u001B[43mobservations\u001B[49m)\n\u001B[1;32m      2\u001B[0m res\n",
      "\u001B[0;31mNameError\u001B[0m: name 'observations' is not defined"
     ]
    }
   ],
   "source": [
    "res = agent.model_vae(observations)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with the prior model FEEF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2, [0, 0], [0.3, 0.3], llik_scaling=1, recon_stddev=0.05)\n",
    "\n",
    "pl_hoz = 5\n",
    "latent_dim = 2\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 2*latent_dim*pl_hoz, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(2)\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           None,\n",
    "                           train_prior_model=True,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.5484944  0.       ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 599us/step - loss: 58.4202 - reconstruction_loss: 45.0543 - kl_loss: 6.8987\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 627us/step - loss: 36.1541 - reconstruction_loss: 31.3868 - kl_loss: 5.3182\n",
      "No Success\n",
      "Episode 2\n",
      "[-0.518806  0.      ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 514us/step - loss: 23.7571 - reconstruction_loss: 19.5673 - kl_loss: 3.8580\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 470us/step - loss: 22.4032 - reconstruction_loss: 18.4973 - kl_loss: 2.9210\n",
      "No Success\n",
      "Episode 3\n",
      "[-0.48708177  0.        ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 488us/step - loss: 44.9584 - reconstruction_loss: 41.6020 - kl_loss: 2.8819\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 453us/step - loss: 37.5611 - reconstruction_loss: 33.4581 - kl_loss: 3.1555\n",
      "No Success\n",
      "Episode 4\n",
      "[-0.59569204  0.        ]\n",
      "[0.43393912 0.60661473]\n",
      "tf.Tensor([ 0.53942364 -0.80796605 -0.9376626  -0.9712407  -0.96946347], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "19/19 [==============================] - 0s 583us/step - loss: 28.1456 - reconstruction_loss: 25.0616 - kl_loss: 2.8645\n",
      "Epoch 2/2\n",
      "19/19 [==============================] - 0s 588us/step - loss: 23.4992 - reconstruction_loss: 20.2202 - kl_loss: 3.0532\n",
      "Success in episode 4 at time step 610\n",
      "Episode 5\n",
      "[-0.52210057  0.        ]\n",
      "[0.43684663 0.46498674]\n",
      "tf.Tensor([ 0.92546123  0.68108666 -0.6756315  -0.9598786  -0.95649064], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 0s 475us/step - loss: 24.0181 - reconstruction_loss: 15.6585 - kl_loss: 6.4340\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 0s 469us/step - loss: 17.0986 - reconstruction_loss: 9.3317 - kl_loss: 7.0323\n",
      "Success in episode 5 at time step 639\n",
      "Episode 6\n",
      "[-0.5314198  0.       ]\n",
      "[-0.64987119  0.68371874]\n",
      "tf.Tensor([ 0.90175134  0.9434608  -0.32570428 -0.9429536  -0.9643723 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 753us/step - loss: 8.7762 - reconstruction_loss: 5.5566 - kl_loss: 3.6357\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 686us/step - loss: 8.4378 - reconstruction_loss: 4.6241 - kl_loss: 3.6613\n",
      "Success in episode 6 at time step 179\n",
      "Episode 7\n",
      "[-0.4605715  0.       ]\n",
      "[-0.00353998  0.9392916 ]\n",
      "tf.Tensor([ 0.68352056 -0.5560999  -0.9425656  -0.9672263  -0.95395947], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 13.1592 - reconstruction_loss: 5.6714 - kl_loss: 7.1696\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 792us/step - loss: 12.0877 - reconstruction_loss: 4.8470 - kl_loss: 7.1424\n",
      "Success in episode 7 at time step 105\n",
      "Episode 8\n",
      "[-0.5544191  0.       ]\n",
      "[0.07817301 0.7160106 ]\n",
      "tf.Tensor([ 0.5129768  -0.8201981  -0.95326155 -0.9582234  -0.9605208 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - 0s 523us/step - loss: 20.3771 - reconstruction_loss: 9.2486 - kl_loss: 11.3828\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 0s 499us/step - loss: 19.7152 - reconstruction_loss: 8.1764 - kl_loss: 11.3159\n",
      "Success in episode 8 at time step 377\n",
      "Episode 9\n",
      "[-0.45428526  0.        ]\n",
      "[-0.71513816  0.66045801]\n",
      "tf.Tensor([-0.310609    0.9450641   0.69586194 -0.9487002  -0.9586462 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 0s 477us/step - loss: 18.8558 - reconstruction_loss: 7.1300 - kl_loss: 11.9460\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 0s 460us/step - loss: 17.5985 - reconstruction_loss: 5.9947 - kl_loss: 10.9378\n",
      "Success in episode 9 at time step 660\n",
      "Episode 10\n",
      "[-0.5995278  0.       ]\n",
      "[0.6379365  0.49749467]\n",
      "tf.Tensor([-0.95766777  0.3941255  -0.37302512 -0.70233625 -0.48940507], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "11/11 [==============================] - 0s 508us/step - loss: 12.0112 - reconstruction_loss: 4.9172 - kl_loss: 6.8543\n",
      "Epoch 2/2\n",
      "11/11 [==============================] - 0s 509us/step - loss: 10.5292 - reconstruction_loss: 3.8701 - kl_loss: 6.7893\n",
      "Success in episode 10 at time step 347\n",
      "Episode 11\n",
      "[-0.55990255  0.        ]\n",
      "[-0.60410505  0.60038473]\n",
      "tf.Tensor([ 0.4334197   0.89764506  0.9182931  -0.89183265 -0.94067025], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "15/15 [==============================] - 0s 611us/step - loss: 6.4572 - reconstruction_loss: 3.3261 - kl_loss: 3.0807\n",
      "Epoch 2/2\n",
      "15/15 [==============================] - 0s 581us/step - loss: 6.1785 - reconstruction_loss: 3.2316 - kl_loss: 3.1090\n",
      "Success in episode 11 at time step 476\n",
      "Episode 12\n",
      "[-0.5676446  0.       ]\n",
      "[0.78900151 0.01502983]\n",
      "tf.Tensor([ 0.9525033  -0.5266449  -0.90737504 -0.9485466  -0.9685088 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "4/4 [==============================] - 0s 756us/step - loss: 7.7062 - reconstruction_loss: 3.8728 - kl_loss: 4.3860\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 0s 908us/step - loss: 7.3972 - reconstruction_loss: 3.1801 - kl_loss: 4.4344\n",
      "Success in episode 12 at time step 125\n",
      "Episode 13\n",
      "[-0.4126645  0.       ]\n",
      "[0.6035577  0.39746463]\n",
      "tf.Tensor([ 0.94505864  0.8305527  -0.84912413 -0.9223308  -0.91720754], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 781us/step - loss: 8.1907 - reconstruction_loss: 2.4208 - kl_loss: 5.7350\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.4131 - reconstruction_loss: 2.0390 - kl_loss: 5.7346\n",
      "Success in episode 13 at time step 101\n",
      "Episode 14\n",
      "[-0.57494265  0.        ]\n",
      "[0.05225397 0.75246499]\n",
      "tf.Tensor([ 0.9236813   0.88682705 -0.8156639  -0.9238964  -0.8818617 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 896us/step - loss: 10.7618 - reconstruction_loss: 3.4984 - kl_loss: 6.9304\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 692us/step - loss: 10.3217 - reconstruction_loss: 3.4507 - kl_loss: 6.9022\n",
      "Success in episode 14 at time step 199\n",
      "Episode 15\n",
      "[-0.4938598  0.       ]\n",
      "[-0.53897305  0.59568419]\n",
      "tf.Tensor([ 0.6923169  0.9413038  0.9228984 -0.9531834 -0.9176845], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "11/11 [==============================] - 0s 750us/step - loss: 11.6503 - reconstruction_loss: 3.0920 - kl_loss: 8.6630\n",
      "Epoch 2/2\n",
      "11/11 [==============================] - 0s 661us/step - loss: 12.0253 - reconstruction_loss: 2.8702 - kl_loss: 8.5575\n",
      "Success in episode 15 at time step 359\n",
      "Episode 16\n",
      "[-0.58367604  0.        ]\n",
      "[-0.22943367  0.84849797]\n",
      "tf.Tensor([ 0.887139    0.9392145  -0.5319459  -0.94324166 -0.92820805], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 670us/step - loss: 8.8991 - reconstruction_loss: 2.4107 - kl_loss: 6.2945\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 659us/step - loss: 8.4455 - reconstruction_loss: 2.4163 - kl_loss: 6.2123\n",
      "Success in episode 16 at time step 200\n",
      "Episode 17\n",
      "[-0.4923572  0.       ]\n",
      "[0.48803502 0.54621775]\n",
      "tf.Tensor([ 0.95160884 -0.59898406 -0.9622428  -0.9601188  -0.7568512 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 794us/step - loss: 8.7988 - reconstruction_loss: 2.3113 - kl_loss: 6.6859\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 741us/step - loss: 9.3523 - reconstruction_loss: 2.6106 - kl_loss: 6.6324\n",
      "Success in episode 17 at time step 166\n",
      "Episode 18\n",
      "[-0.4676559  0.       ]\n",
      "[0.55960618 0.32915288]\n",
      "tf.Tensor([ 0.9601318   0.76323575 -0.8307218  -0.9234122  -0.86427575], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 616us/step - loss: 9.1163 - reconstruction_loss: 2.5926 - kl_loss: 6.2077\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 654us/step - loss: 7.9493 - reconstruction_loss: 1.8056 - kl_loss: 6.1592\n",
      "Success in episode 18 at time step 158\n",
      "Episode 19\n",
      "[-0.55600256  0.        ]\n",
      "[0.78381184 0.43816678]\n",
      "tf.Tensor([ 0.94960546  0.93194854 -0.7137635  -0.91298646 -0.8813957 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "7/7 [==============================] - 0s 669us/step - loss: 7.3899 - reconstruction_loss: 2.3247 - kl_loss: 5.0661\n",
      "Epoch 2/2\n",
      "7/7 [==============================] - 0s 774us/step - loss: 7.0945 - reconstruction_loss: 2.2097 - kl_loss: 5.0609\n",
      "Success in episode 19 at time step 214\n",
      "Episode 20\n",
      "[-0.43542972  0.        ]\n",
      "[-0.77829979  0.38725577]\n",
      "tf.Tensor([ 0.67858493  0.937512    0.96516037  0.54972255 -0.9547544 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "4/4 [==============================] - 0s 756us/step - loss: 5.8008 - reconstruction_loss: 1.5050 - kl_loss: 4.2047\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 5.9445 - reconstruction_loss: 1.8158 - kl_loss: 4.1713\n",
      "Success in episode 20 at time step 150\n",
      "Episode 21\n",
      "[-0.55849165  0.        ]\n",
      "[-0.33124466  0.86650486]\n",
      "tf.Tensor([ 0.9371447   0.94848025  0.8995507  -0.7638814  -0.8855443 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 836us/step - loss: 6.6571 - reconstruction_loss: 1.6020 - kl_loss: 5.1269\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.5744 - reconstruction_loss: 1.5278 - kl_loss: 5.1054\n",
      "Success in episode 21 at time step 109\n",
      "Episode 22\n",
      "[-0.4310394  0.       ]\n",
      "[-0.03981547  0.61139527]\n",
      "tf.Tensor([ 0.9779087  0.9653669  0.7095495 -0.6561639 -0.8852587], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 876us/step - loss: 5.3512 - reconstruction_loss: 1.3716 - kl_loss: 4.0622\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 897us/step - loss: 6.0304 - reconstruction_loss: 1.9029 - kl_loss: 4.0604\n",
      "Success in episode 22 at time step 112\n",
      "Episode 23\n",
      "[-0.5496855  0.       ]\n",
      "[-0.44668815  0.56953237]\n",
      "tf.Tensor([ 0.96393096  0.95517796  0.9484314  -0.02067748 -0.93797463], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - 0s 680us/step - loss: 9.4566 - reconstruction_loss: 2.6261 - kl_loss: 6.7383\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 0s 645us/step - loss: 8.8589 - reconstruction_loss: 2.1317 - kl_loss: 6.8827\n",
      "Success in episode 23 at time step 384\n",
      "Episode 24\n",
      "[-0.5740722  0.       ]\n",
      "[0.76662139 0.44852311]\n",
      "tf.Tensor([ 0.96741486  0.92653435 -0.6716142  -0.9289489  -0.81193256], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "13/13 [==============================] - 0s 504us/step - loss: 5.4406 - reconstruction_loss: 1.7470 - kl_loss: 3.6913\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 0s 539us/step - loss: 5.2857 - reconstruction_loss: 1.5390 - kl_loss: 3.6619\n",
      "Success in episode 24 at time step 391\n",
      "Episode 25\n",
      "[-0.47093832  0.        ]\n",
      "[0.34536269 0.62146267]\n",
      "tf.Tensor([ 0.982638   0.9438238 -0.2913136 -0.9403439 -0.8967593], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 622us/step - loss: 7.5697 - reconstruction_loss: 2.1730 - kl_loss: 5.3664\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 638us/step - loss: 6.7834 - reconstruction_loss: 1.7488 - kl_loss: 5.3519\n",
      "Success in episode 25 at time step 159\n",
      "Episode 26\n",
      "[-0.4050906  0.       ]\n",
      "[0.52380358 0.55880912]\n",
      "tf.Tensor([ 0.97054344  0.91447043 -0.82860225 -0.93091756 -0.85550815], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 695us/step - loss: 6.9088 - reconstruction_loss: 1.5761 - kl_loss: 5.1277\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 667us/step - loss: 6.9870 - reconstruction_loss: 1.7676 - kl_loss: 5.1074\n",
      "Success in episode 26 at time step 157\n",
      "Episode 27\n",
      "[-0.5262916  0.       ]\n",
      "[0.60023659 0.53565799]\n",
      "tf.Tensor([ 0.96012986 -0.0096777  -0.94669604 -0.948937   -0.85035473], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "7/7 [==============================] - 0s 828us/step - loss: 7.6614 - reconstruction_loss: 2.0994 - kl_loss: 5.5271\n",
      "Epoch 2/2\n",
      "7/7 [==============================] - 0s 672us/step - loss: 7.2678 - reconstruction_loss: 1.8238 - kl_loss: 5.5268\n",
      "Success in episode 27 at time step 215\n",
      "Episode 28\n",
      "[-0.47990215  0.        ]\n",
      "[-0.69654455  0.47187729]\n",
      "tf.Tensor([ 0.8633668   0.9329309   0.9395837  -0.7648681  -0.92263836], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "8/8 [==============================] - 0s 578us/step - loss: 5.7185 - reconstruction_loss: 1.4130 - kl_loss: 4.4546\n",
      "Epoch 2/2\n",
      "8/8 [==============================] - 0s 682us/step - loss: 5.8526 - reconstruction_loss: 1.4291 - kl_loss: 4.4411\n",
      "Success in episode 28 at time step 270\n",
      "Episode 29\n",
      "[-0.47733557  0.        ]\n",
      "[0.72197514 0.24306388]\n",
      "tf.Tensor([-0.29253593 -0.960111   -0.96267974 -0.95452476 -0.83690506], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 638us/step - loss: 8.2226 - reconstruction_loss: 1.9444 - kl_loss: 6.2108\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 622us/step - loss: 8.2735 - reconstruction_loss: 1.7529 - kl_loss: 6.1656\n",
      "Success in episode 29 at time step 184\n",
      "Episode 30\n",
      "[-0.5131528  0.       ]\n",
      "[-0.14345096  0.80473539]\n",
      "tf.Tensor([ 0.9552281   0.9496022   0.01079097 -0.94117475 -0.94041866], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "24/24 [==============================] - 0s 542us/step - loss: 3.7798 - reconstruction_loss: 1.2655 - kl_loss: 2.5184\n",
      "Epoch 2/2\n",
      "24/24 [==============================] - 0s 510us/step - loss: 3.7290 - reconstruction_loss: 1.2098 - kl_loss: 2.5490\n",
      "Success in episode 30 at time step 768\n",
      "Episode 31\n",
      "[-0.46932176  0.        ]\n",
      "[0.39546656 0.66208924]\n",
      "tf.Tensor([ 0.9668821  -0.04248648 -0.93477327 -0.94676405 -0.8885452 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 851us/step - loss: 6.7260 - reconstruction_loss: 1.5653 - kl_loss: 5.2926\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 784us/step - loss: 6.4667 - reconstruction_loss: 1.3425 - kl_loss: 5.2772\n",
      "Success in episode 31 at time step 99\n",
      "Episode 32\n",
      "[-0.5092919  0.       ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 507us/step - loss: 7.0663 - reconstruction_loss: 1.4210 - kl_loss: 5.6891\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 431us/step - loss: 6.9905 - reconstruction_loss: 1.5410 - kl_loss: 5.4485\n",
      "No Success\n",
      "Episode 33\n",
      "[-0.541249  0.      ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 600us/step - loss: 4.6203 - reconstruction_loss: 1.3204 - kl_loss: 3.2881\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 550us/step - loss: 4.4882 - reconstruction_loss: 1.1874 - kl_loss: 3.3315\n",
      "No Success\n",
      "Episode 34\n",
      "[-0.42544827  0.        ]\n",
      "[-0.0741573   0.94230029]\n",
      "tf.Tensor([ 0.73010695 -0.5771215  -0.9590358  -0.973611   -0.9504563 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "14/14 [==============================] - 0s 634us/step - loss: 7.4225 - reconstruction_loss: 1.1921 - kl_loss: 6.1865\n",
      "Epoch 2/2\n",
      "14/14 [==============================] - 0s 652us/step - loss: 7.2913 - reconstruction_loss: 1.2582 - kl_loss: 6.0974\n",
      "Success in episode 34 at time step 438\n",
      "Episode 35\n",
      "[-0.48182368  0.        ]\n",
      "[-0.54787218  0.75677811]\n",
      "tf.Tensor([ 0.76725364  0.93586665 -0.04508896 -0.95973086 -0.95905685], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 902us/step - loss: 6.1304 - reconstruction_loss: 1.3611 - kl_loss: 4.7358\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 729us/step - loss: 5.8535 - reconstruction_loss: 1.3062 - kl_loss: 4.6978\n",
      "Success in episode 35 at time step 204\n",
      "Episode 36\n",
      "[-0.45295024  0.        ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 530us/step - loss: 5.7639 - reconstruction_loss: 1.3331 - kl_loss: 4.5020\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 536us/step - loss: 5.7395 - reconstruction_loss: 1.2709 - kl_loss: 4.5028\n",
      "No Success\n",
      "Episode 37\n",
      "[-0.44485533  0.        ]\n",
      "[0.44641612 0.37350669]\n",
      "tf.Tensor([ 0.9651801   0.9173072  -0.9163242  -0.9164517  -0.61982554], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "7/7 [==============================] - 0s 630us/step - loss: 6.5600 - reconstruction_loss: 1.1956 - kl_loss: 5.5112\n",
      "Epoch 2/2\n",
      "7/7 [==============================] - 0s 588us/step - loss: 6.7416 - reconstruction_loss: 1.1619 - kl_loss: 5.4893\n",
      "Success in episode 37 at time step 226\n",
      "Episode 38\n",
      "[-0.57085204  0.        ]\n",
      "[0.48467547 0.29315904]\n",
      "tf.Tensor([ 0.9787136   0.9768982  -0.49663472 -0.94532764 -0.6978502 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - 0s 597us/step - loss: 6.4877 - reconstruction_loss: 1.1846 - kl_loss: 5.2208\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 0s 575us/step - loss: 6.3580 - reconstruction_loss: 1.2805 - kl_loss: 5.1756\n",
      "Success in episode 38 at time step 380\n",
      "Episode 39\n",
      "[-0.44514108  0.        ]\n",
      "[-0.76237081  0.42460001]\n",
      "tf.Tensor([ 0.89983445  0.949412    0.91550064  0.5047957  -0.9490105 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "4/4 [==============================] - 0s 757us/step - loss: 4.4097 - reconstruction_loss: 1.2832 - kl_loss: 3.1965\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 0s 775us/step - loss: 4.6610 - reconstruction_loss: 1.3046 - kl_loss: 3.2002\n",
      "Success in episode 39 at time step 149\n",
      "Episode 40\n",
      "[-0.45099804  0.        ]\n",
      "[0.49815578 0.49676182]\n",
      "tf.Tensor([ 0.9588886   0.94861776 -0.69989896 -0.9204575  -0.84000653], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5.0808 - reconstruction_loss: 0.9404 - kl_loss: 4.2115\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 832us/step - loss: 5.5306 - reconstruction_loss: 1.1334 - kl_loss: 4.2071\n",
      "Success in episode 40 at time step 99\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=40, action_repeats=6, num_actions_to_execute=5, train_on_full_data=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the models produced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": "(10740, 2)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.10825755,  0.05147636],\n       [-0.15054854, -0.03395247],\n       [-0.16731468, -0.08817429],\n       ...,\n       [ 0.83856054,  0.35450622],\n       [ 0.77111532,  0.34275715],\n       [ 0.85586775,  0.30161132]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=9.67474>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(agent.model_vae.compute_loss(observations))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Test the models produced\n",
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape\n",
    "observations\n",
    "agent.model_vae(observations)\n",
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.83333333, 0.        ])"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "pl_hoz = 5\n",
    "latent_dim = 2\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 2*pl_hoz*latent_dim, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(2)\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "# without prior model\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "# with prior model\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           None,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[0.32907545 0.81204165]\n",
      "tf.Tensor([0.93988144 0.9176549  0.9576125  0.92440426 0.50135976], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - 0s 647us/step - loss: 79.1247 - reconstruction_loss: 66.5922 - kl_loss: 6.1959\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 0s 544us/step - loss: 56.3993 - reconstruction_loss: 49.0732 - kl_loss: 5.4928\n",
      "Success in episode 1 at time step 389\n",
      "Episode 2\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 438us/step - loss: 70.1578 - reconstruction_loss: 65.3552 - kl_loss: 3.8945\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 442us/step - loss: 62.2310 - reconstruction_loss: 58.4824 - kl_loss: 3.3635\n",
      "No Success\n",
      "Episode 3\n",
      "[0.69670431 0.33783742]\n",
      "tf.Tensor([ 0.9314696  -0.9435271   0.9645834   0.8415556   0.56516707], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 0s 606us/step - loss: 52.3677 - reconstruction_loss: 45.6785 - kl_loss: 3.5939\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 0s 501us/step - loss: 46.2414 - reconstruction_loss: 41.3540 - kl_loss: 3.5405\n",
      "Success in episode 3 at time step 306\n",
      "Episode 4\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 490us/step - loss: 17.5091 - reconstruction_loss: 12.6776 - kl_loss: 1.7703\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 470us/step - loss: 9.0485 - reconstruction_loss: 6.6526 - kl_loss: 1.8981\n",
      "No Success\n",
      "Episode 5\n",
      "[0.49734786 0.77757351]\n",
      "tf.Tensor([ 0.98152953 -0.9801048   0.346932    0.32506007 -0.20486811], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "14/14 [==============================] - 0s 522us/step - loss: 51.1933 - reconstruction_loss: 45.8139 - kl_loss: 2.3554\n",
      "Epoch 2/2\n",
      "14/14 [==============================] - 0s 481us/step - loss: 39.7211 - reconstruction_loss: 36.3924 - kl_loss: 2.5372\n",
      "Success in episode 5 at time step 426\n",
      "Episode 6\n",
      "[-0.08916565  0.69457532]\n",
      "tf.Tensor([ 0.86979437  0.9163867   0.45106322 -0.9567182  -0.96376824], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "17/17 [==============================] - 0s 620us/step - loss: 30.2887 - reconstruction_loss: 26.7621 - kl_loss: 1.6930\n",
      "Epoch 2/2\n",
      "17/17 [==============================] - 0s 506us/step - loss: 24.3142 - reconstruction_loss: 21.4270 - kl_loss: 2.6086\n",
      "Success in episode 6 at time step 560\n",
      "Episode 7\n",
      "[0.7740761  0.43105478]\n",
      "tf.Tensor([-0.8406548   0.22958903 -0.86530244  0.16205291  0.9585254 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "8/8 [==============================] - 0s 502us/step - loss: 51.1668 - reconstruction_loss: 44.3108 - kl_loss: 4.4013\n",
      "Epoch 2/2\n",
      "8/8 [==============================] - 0s 642us/step - loss: 43.8689 - reconstruction_loss: 40.3531 - kl_loss: 4.6428\n",
      "Success in episode 7 at time step 242\n",
      "Episode 8\n",
      "[0.60109328 0.73302718]\n",
      "tf.Tensor([-0.9034789  -0.93242174 -0.9142734  -0.9228895  -0.82804745], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "30/30 [==============================] - 0s 599us/step - loss: 26.6581 - reconstruction_loss: 23.5741 - kl_loss: 3.7185\n",
      "Epoch 2/2\n",
      "30/30 [==============================] - 0s 631us/step - loss: 25.3964 - reconstruction_loss: 22.4093 - kl_loss: 3.5270\n",
      "Success in episode 8 at time step 945\n",
      "Episode 9\n",
      "[0.58582333 0.70690177]\n",
      "tf.Tensor([-0.7801693  -0.82969654 -0.92583454 -0.9501423  -0.8357265 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "14/14 [==============================] - 0s 526us/step - loss: 37.6207 - reconstruction_loss: 32.8728 - kl_loss: 5.0782\n",
      "Epoch 2/2\n",
      "14/14 [==============================] - 0s 585us/step - loss: 35.9430 - reconstruction_loss: 31.7605 - kl_loss: 4.9600\n",
      "Success in episode 9 at time step 446\n",
      "Episode 10\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 468us/step - loss: 30.1776 - reconstruction_loss: 25.5544 - kl_loss: 3.9946\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 503us/step - loss: 26.9137 - reconstruction_loss: 18.8948 - kl_loss: 4.4838\n",
      "No Success\n",
      "Episode 11\n",
      "[-0.13406156  0.91068   ]\n",
      "tf.Tensor([0.9825429  0.450113   0.8094051  0.96760887 0.9726515 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "15/15 [==============================] - 0s 635us/step - loss: 19.5421 - reconstruction_loss: 12.5481 - kl_loss: 6.4961\n",
      "Epoch 2/2\n",
      "15/15 [==============================] - 0s 612us/step - loss: 15.8965 - reconstruction_loss: 7.9619 - kl_loss: 6.8670\n",
      "Success in episode 11 at time step 495\n",
      "Episode 12\n",
      "[-0.26456655  0.80451077]\n",
      "tf.Tensor([0.9721597  0.9052161  0.9302739  0.83734506 0.75346595], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 580us/step - loss: 12.4143 - reconstruction_loss: 5.5323 - kl_loss: 7.1294\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 607us/step - loss: 11.0802 - reconstruction_loss: 3.7463 - kl_loss: 6.8431\n",
      "Success in episode 12 at time step 818\n",
      "Episode 13\n",
      "[0.11189345 0.84674417]\n",
      "tf.Tensor([ 0.9897314   0.5497189  -0.90938026  0.67785156  0.9646587 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 638us/step - loss: 8.1991 - reconstruction_loss: 3.2571 - kl_loss: 4.5878\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 620us/step - loss: 7.5046 - reconstruction_loss: 2.8409 - kl_loss: 4.5775\n",
      "Success in episode 13 at time step 192\n",
      "Episode 14\n",
      "[0.50627537 0.85736673]\n",
      "tf.Tensor([-0.95708704 -0.93841213 -0.939945   -0.30885416  0.9591544 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 0s 524us/step - loss: 12.8243 - reconstruction_loss: 3.9857 - kl_loss: 8.7120\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 0s 645us/step - loss: 11.9485 - reconstruction_loss: 3.3041 - kl_loss: 8.4940\n",
      "Success in episode 14 at time step 307\n",
      "Episode 15\n",
      "[0.40779439 0.87726324]\n",
      "tf.Tensor([-0.89853    -0.92443955 -0.9282573   0.4178334   0.95529646], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "16/16 [==============================] - 0s 541us/step - loss: 6.3207 - reconstruction_loss: 1.3305 - kl_loss: 5.0124\n",
      "Epoch 2/2\n",
      "16/16 [==============================] - 0s 582us/step - loss: 6.2795 - reconstruction_loss: 1.4447 - kl_loss: 4.9020\n",
      "Success in episode 15 at time step 509\n",
      "Episode 16\n",
      "[-0.12792678  0.90065215]\n",
      "tf.Tensor([ 0.97663975  0.85461074 -0.6157359  -0.80765086  0.89361745], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 784us/step - loss: 9.5037 - reconstruction_loss: 3.4068 - kl_loss: 6.1400\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 830us/step - loss: 9.5505 - reconstruction_loss: 2.9323 - kl_loss: 6.2233\n",
      "Success in episode 16 at time step 175\n",
      "Episode 17\n",
      "[0.34461867 0.76592857]\n",
      "tf.Tensor([-0.71878654 -0.7026098  -0.9614596   0.92464733  0.9633947 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 636us/step - loss: 10.0144 - reconstruction_loss: 2.2789 - kl_loss: 7.3443\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 611us/step - loss: 9.1990 - reconstruction_loss: 2.2846 - kl_loss: 7.2655\n",
      "Success in episode 17 at time step 195\n",
      "Episode 18\n",
      "[0.22763537 0.87905425]\n",
      "tf.Tensor([-0.50013196 -0.8199611  -0.9188756  -0.9499112   0.9782911 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "14/14 [==============================] - 0s 474us/step - loss: 6.2820 - reconstruction_loss: 1.0543 - kl_loss: 5.1769\n",
      "Epoch 2/2\n",
      "14/14 [==============================] - 0s 524us/step - loss: 6.0942 - reconstruction_loss: 1.0928 - kl_loss: 5.0239\n",
      "Success in episode 18 at time step 434\n",
      "Episode 19\n",
      "[0.82519335 0.04704075]\n",
      "tf.Tensor([ 0.9419391  -0.95463955 -0.9360158  -0.943113    0.44610092], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "8/8 [==============================] - 0s 587us/step - loss: 9.0316 - reconstruction_loss: 2.9396 - kl_loss: 6.2023\n",
      "Epoch 2/2\n",
      "8/8 [==============================] - 0s 541us/step - loss: 8.5861 - reconstruction_loss: 2.1607 - kl_loss: 6.2835\n",
      "Success in episode 19 at time step 242\n",
      "Episode 20\n",
      "[0.56574249 0.72304362]\n",
      "tf.Tensor([-0.9569217  -0.91001046 -0.92691606 -0.58486927  0.96289206], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 598us/step - loss: 6.3744 - reconstruction_loss: 1.6380 - kl_loss: 4.6993\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 674us/step - loss: 6.1429 - reconstruction_loss: 1.4956 - kl_loss: 4.6901\n",
      "Success in episode 20 at time step 147\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, action_repeats=10, num_actions_to_execute=2, train_on_full_data=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "(12806, 2)"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test the models produced\n",
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.27734923,  0.        ],\n       [-0.27835019, -0.01286914],\n       [-0.28034457, -0.02564205],\n       ...,\n       [ 0.78566315,  0.35808382],\n       [ 0.81395161,  0.36370895],\n       [ 0.8428795 ,  0.37193014]])"
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(12806, 2), dtype=float32, numpy=\narray([[-0.26162374, -0.03878228],\n       [-0.27726045, -0.06711522],\n       [-0.24541792, -0.04369228],\n       ...,\n       [ 0.7668527 ,  0.39962938],\n       [ 0.93341327,  0.4722877 ],\n       [ 0.8112131 ,  0.41610983]], dtype=float32)>"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model_vae(observations)\n",
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Identity VAE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = agent.tran((ob_seqs[0:1], None))\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = ob_seqs[0:1, -1].reshape(1,1,3)\n",
    "h = out[3]\n",
    "h = h[0, -2, :]\n",
    "h = h.numpy().reshape(1,30)\n",
    "h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((t, h))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ob_seqs[0:1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test to see how the agent trains on standard observation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, vae_train_epochs=1, tran_train_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "success, agent, t, pre_obs, post_obs, acts = run_episode(env, daifa, observation_max, observation_min, observation_noise_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_np = np.array(pre_obs)\n",
    "a = np.array(acts)\n",
    "a.shape\n",
    "pre_a = np.concatenate([pre_np, a], axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(a.max(), a.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict = np.array(post_obs)[:, 14, :]\n",
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((pre_a, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine training the model on the observation data\n",
    "\n",
    "Does it eventually converge to a good model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_runs = 1\n",
    "for i in range(num_train_runs):\n",
    "\n",
    "    for j in range(len(pre)):\n",
    "        pre = pre_obs[j]\n",
    "        post = post_obs[j]\n",
    "        actions = acts[j]\n",
    "\n",
    "        daifa.train(pre, post, actions, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation(np.array([0.5, 0.1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the FEEF computations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  0.5]\n"
     ]
    }
   ],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, planning_horizon=15, n_policy_candidates=70, n_policies=1500, n_cem_policy_iterations=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "WARNING:tensorflow:From /Users/Ethan/miniconda3/envs/tf_daif/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-06 15:23:48.777636: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_policy(agent, env, policy, action_repeats):\n",
    "\n",
    "    observation = env.reset()\n",
    "    obs = transform_observations(observation, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    z_t_minus_1 = obs\n",
    "    p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "    p\n",
    "    print(obs)\n",
    "    print(p)\n",
    "\n",
    "    for action in p:\n",
    "        for t in range(action_repeats):\n",
    "            res = env.step(np.array([action]))\n",
    "            print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([0, 0])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p\n",
    "\n",
    "agent.forward_policies(p, z_t_minus_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "test_policy(agent, env, p.numpy(), 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([-0.27691475,  0.01688306])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 given_prior_mean,\n",
    "                 given_prior_stddev,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 tran_train_epochs=1,\n",
    "                 vae_train_epochs=1,\n",
    "                 agent_time_ratio=6,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        self.prior_model = prior_model\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        self.vae_train_epochs = vae_train_epochs\n",
    "        self.tran_train_epochs = tran_train_epochs\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.model_vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        # transition\n",
    "        # takes action plus last state and outputs next latent state\n",
    "        self.tran = tran\n",
    "        self.tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        self.hidden_state = None\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(observation)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    def train(self, pre_observations_raw, post_observations_raw, actions_complete, rewards, verbose=0):\n",
    "\n",
    "        # compress the observations based on the agents time compression factor\n",
    "        # pre_observations = pre_observations_raw[::self.agent_time_ratio]  # for example take every 6th element\n",
    "        # post_observations = np.array([post_observations_raw[i] for i in range(len(post_observations_raw)) if i % self.agent_time_ratio == self.agent_time_ratio - 1])\n",
    "        #\n",
    "        # print(pre_observations_raw)\n",
    "        # print(pre_observations)\n",
    "        # print(post_observations_raw)\n",
    "        # print(post_observations)\n",
    "\n",
    "        pre_observations = pre_observations_raw\n",
    "        post_observations = post_observations_raw\n",
    "\n",
    "        # only look at the first n actions that we took\n",
    "        actions = actions_complete[0: len(pre_observations)]\n",
    "\n",
    "        num_observations = pre_observations.shape[0]\n",
    "        observation_dim = pre_observations.shape[1]\n",
    "        action_dim = actions.shape[1]\n",
    "        # action_dim = 1  # TODO fix this to allow different actions\n",
    "\n",
    "        # find the actual observed latent states using the vae\n",
    "        pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "        post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "        # set up the input training data that we use to train the transition model\n",
    "        z_train = np.concatenate([np.array(pre_latent_mean), np.array(actions)], axis=1)\n",
    "\n",
    "        # we use the sequence to find the right hidden states to use as input\n",
    "        z_train_seq = z_train.reshape((1, num_observations, observation_dim + action_dim))\n",
    "        z_train_singles = z_train.reshape(num_observations, 1, observation_dim + action_dim)\n",
    "\n",
    "        # the previous hidden state is the memory after observing some sequences but it might be None\n",
    "        if self.hidden_state is None:\n",
    "            self.hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "        if self.train_tran:\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.hidden_state))\n",
    "\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran_train_epochs, verbose=verbose)\n",
    "\n",
    "        # now find the new predicted hidden state that we will use for finding the policy\n",
    "        # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "        _, _, final_hidden_state, _ = self.tran((z_train_seq, self.hidden_state))\n",
    "        # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "        self.hidden_state = final_hidden_state\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            self.model_vae.fit(post_observations, epochs=self.vae_train_epochs, verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by one to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        print(z_t_minus_one)\n",
    "        print(mean_best_policies)\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape [self.n_policies, len(z_t_minus_one)]\n",
    "        prev_latent_mean = np.stack([z_t_minus_one]*self.n_policies)\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "            likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "            if self.prior_model is None:\n",
    "\n",
    "                # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                # create the prior distribution\n",
    "                prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "            # TODO Fix the learned prior model\n",
    "            else:\n",
    "                prior_dist = self.prior_model()\n",
    "\n",
    "            kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "            predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "            kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    def EFE(self, policy_posteriors, predicted_likelihood, predicted_posterior):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing with a pretrained transition model\n",
    "\n",
    "This works well! So the problem can't lie with the transition model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0.07]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, train_vae=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "num_seqs = 200\n",
    "seq_length = 500\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "next_obs_stddev = []\n",
    "actions = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.2)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    # train = np.concatenate([o[:-1], a], axis=1)\n",
    "    train = o[:-1]\n",
    "    test = o[1:]\n",
    "\n",
    "    actions.append(a)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "    ob_seqs_stddev = np.ones_like(train)\n",
    "    next_stddev = np.ones_like(test)\n",
    "\n",
    "    next_obs_stddev.append(next_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 1ms/step - kl_loss: 0.1993\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0497\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0132\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0103\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0097\n",
      "6/6 [==============================] - 0s 1ms/step - kl_loss: 0.0108\n",
      "16/16 [==============================] - 0s 967us/step - kl_loss: 0.0029\n",
      "16/16 [==============================] - 0s 940us/step - kl_loss: 0.0020\n",
      "15/15 [==============================] - 0s 882us/step - kl_loss: 0.0047\n",
      "16/16 [==============================] - 0s 853us/step - kl_loss: 0.0054\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 0.0074\n",
      "16/16 [==============================] - 0s 943us/step - kl_loss: 0.0113\n",
      "8/8 [==============================] - 0s 920us/step - kl_loss: 0.0256\n",
      "16/16 [==============================] - 0s 891us/step - kl_loss: 0.0049\n",
      "16/16 [==============================] - 0s 923us/step - kl_loss: 0.0037\n",
      "16/16 [==============================] - 0s 905us/step - kl_loss: 0.0038\n",
      "16/16 [==============================] - 0s 860us/step - kl_loss: 0.0033\n",
      "14/14 [==============================] - 0s 896us/step - kl_loss: 0.0033\n",
      "16/16 [==============================] - 0s 928us/step - kl_loss: 0.0012\n",
      "7/7 [==============================] - 0s 964us/step - kl_loss: 0.0033\n",
      "16/16 [==============================] - 0s 853us/step - kl_loss: 6.2965e-04\n",
      "16/16 [==============================] - 0s 895us/step - kl_loss: 3.8473e-04\n",
      "16/16 [==============================] - 0s 886us/step - kl_loss: 9.0204e-04\n",
      "16/16 [==============================] - 0s 866us/step - kl_loss: 3.5106e-04\n",
      "7/7 [==============================] - 0s 948us/step - kl_loss: 0.0014\n",
      "16/16 [==============================] - 0s 921us/step - kl_loss: 0.0011\n",
      "16/16 [==============================] - 0s 897us/step - kl_loss: 2.8698e-04\n",
      "16/16 [==============================] - 0s 918us/step - kl_loss: 3.7622e-04\n",
      "12/12 [==============================] - 0s 979us/step - kl_loss: 7.3435e-04\n",
      "16/16 [==============================] - 0s 872us/step - kl_loss: 6.9894e-04\n",
      "16/16 [==============================] - 0s 845us/step - kl_loss: 3.9676e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 9.3522e-04\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 5.7943e-04\n",
      "16/16 [==============================] - 0s 861us/step - kl_loss: 3.5893e-04\n",
      "16/16 [==============================] - 0s 927us/step - kl_loss: 4.9170e-04\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 2.7267e-04\n",
      "16/16 [==============================] - 0s 838us/step - kl_loss: 1.5856e-04\n",
      "6/6 [==============================] - 0s 1ms/step - kl_loss: 9.0418e-04\n",
      "16/16 [==============================] - 0s 886us/step - kl_loss: 5.7081e-04\n",
      "12/12 [==============================] - 0s 907us/step - kl_loss: 4.9615e-04\n",
      "16/16 [==============================] - 0s 878us/step - kl_loss: 6.9546e-04\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 2.8895e-04\n",
      "13/13 [==============================] - 0s 899us/step - kl_loss: 7.5522e-04\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 6.0688e-04\n",
      "11/11 [==============================] - 0s 921us/step - kl_loss: 5.8398e-04\n",
      "9/9 [==============================] - 0s 976us/step - kl_loss: 0.0012\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 6.1929e-04\n",
      "16/16 [==============================] - 0s 836us/step - kl_loss: 2.2836e-04\n",
      "10/10 [==============================] - 0s 916us/step - kl_loss: 5.1460e-04\n",
      "16/16 [==============================] - 0s 867us/step - kl_loss: 4.9099e-04\n",
      "16/16 [==============================] - 0s 858us/step - kl_loss: 3.6082e-04\n",
      "16/16 [==============================] - 0s 889us/step - kl_loss: 3.1299e-04\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 8.5156e-04\n",
      "16/16 [==============================] - 0s 921us/step - kl_loss: 0.0018\n",
      "16/16 [==============================] - 0s 852us/step - kl_loss: 0.0030\n",
      "10/10 [==============================] - 0s 915us/step - kl_loss: 0.0085\n",
      "16/16 [==============================] - 0s 897us/step - kl_loss: 0.0071\n",
      "16/16 [==============================] - 0s 917us/step - kl_loss: 0.0035\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 0.0035\n",
      "16/16 [==============================] - 0s 890us/step - kl_loss: 0.0026\n",
      "16/16 [==============================] - 0s 882us/step - kl_loss: 0.0020\n",
      "7/7 [==============================] - 0s 966us/step - kl_loss: 0.0028\n",
      "16/16 [==============================] - 0s 896us/step - kl_loss: 0.0013\n",
      "12/12 [==============================] - 0s 911us/step - kl_loss: 0.0014\n",
      "14/14 [==============================] - 0s 918us/step - kl_loss: 0.0015\n",
      "16/16 [==============================] - 0s 880us/step - kl_loss: 8.1093e-04\n",
      "16/16 [==============================] - 0s 859us/step - kl_loss: 5.0124e-04\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 2.6348e-04\n",
      "8/8 [==============================] - 0s 976us/step - kl_loss: 5.0341e-04\n",
      "10/10 [==============================] - 0s 871us/step - kl_loss: 4.9221e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 2.3029e-04\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 6.5155e-05\n",
      "10/10 [==============================] - 0s 918us/step - kl_loss: 5.9321e-04\n",
      "16/16 [==============================] - 0s 912us/step - kl_loss: 3.2684e-04\n",
      "16/16 [==============================] - 0s 874us/step - kl_loss: 1.7592e-04\n",
      "16/16 [==============================] - 0s 863us/step - kl_loss: 1.6582e-04\n",
      "16/16 [==============================] - 0s 864us/step - kl_loss: 1.8564e-04\n",
      "16/16 [==============================] - 0s 925us/step - kl_loss: 1.4003e-04\n",
      "16/16 [==============================] - 0s 915us/step - kl_loss: 1.7503e-04\n",
      "12/12 [==============================] - 0s 975us/step - kl_loss: 2.7620e-04\n",
      "16/16 [==============================] - 0s 935us/step - kl_loss: 2.7183e-04\n",
      "11/11 [==============================] - 0s 942us/step - kl_loss: 4.2113e-04\n",
      "16/16 [==============================] - 0s 867us/step - kl_loss: 4.4761e-04\n",
      "12/12 [==============================] - 0s 916us/step - kl_loss: 2.7003e-04\n",
      "13/13 [==============================] - 0s 886us/step - kl_loss: 5.0503e-04\n",
      "12/12 [==============================] - 0s 891us/step - kl_loss: 4.9012e-04\n",
      "16/16 [==============================] - 0s 888us/step - kl_loss: 2.4893e-04\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 1.1987e-04\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 2.8243e-04\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 1.1809e-04\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 7.0822e-05\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 6.8286e-05\n",
      "16/16 [==============================] - 0s 900us/step - kl_loss: 2.7148e-04\n",
      "15/15 [==============================] - 0s 960us/step - kl_loss: 1.1328e-04\n",
      "16/16 [==============================] - 0s 859us/step - kl_loss: 1.6294e-04\n",
      "16/16 [==============================] - 0s 899us/step - kl_loss: 9.3267e-05\n",
      "13/13 [==============================] - 0s 930us/step - kl_loss: 2.7863e-04\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 3.4847e-04\n",
      "16/16 [==============================] - 0s 909us/step - kl_loss: 6.9290e-05\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 2.5452e-04\n",
      "16/16 [==============================] - 0s 904us/step - kl_loss: 2.0722e-04\n",
      "16/16 [==============================] - 0s 903us/step - kl_loss: 1.6057e-04\n",
      "16/16 [==============================] - 0s 996us/step - kl_loss: 1.6325e-04\n",
      "10/10 [==============================] - 0s 981us/step - kl_loss: 2.9550e-04\n",
      "16/16 [==============================] - 0s 905us/step - kl_loss: 2.2208e-04\n",
      "16/16 [==============================] - 0s 880us/step - kl_loss: 1.4792e-04\n",
      "14/14 [==============================] - 0s 876us/step - kl_loss: 2.6079e-04\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 2.2629e-04\n",
      "16/16 [==============================] - 0s 927us/step - kl_loss: 1.7176e-04\n",
      "16/16 [==============================] - 0s 863us/step - kl_loss: 7.7746e-05\n",
      "16/16 [==============================] - 0s 907us/step - kl_loss: 4.9998e-05\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 9.7333e-05\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 9.3879e-06\n",
      "16/16 [==============================] - 0s 871us/step - kl_loss: 3.5369e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 6.1430e-05\n",
      "16/16 [==============================] - 0s 913us/step - kl_loss: 3.4472e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 4.7960e-05\n",
      "11/11 [==============================] - 0s 988us/step - kl_loss: 1.9969e-04\n",
      "16/16 [==============================] - 0s 886us/step - kl_loss: 1.3500e-04\n",
      "9/9 [==============================] - 0s 959us/step - kl_loss: 1.3400e-04\n",
      "15/15 [==============================] - 0s 927us/step - kl_loss: 1.8833e-04\n",
      "16/16 [==============================] - 0s 889us/step - kl_loss: 1.6533e-04\n",
      "16/16 [==============================] - 0s 855us/step - kl_loss: 4.5978e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 6.0070e-05\n",
      "16/16 [==============================] - 0s 878us/step - kl_loss: 1.2162e-04\n",
      "16/16 [==============================] - 0s 942us/step - kl_loss: 9.9278e-05\n",
      "16/16 [==============================] - 0s 919us/step - kl_loss: 6.8983e-05\n",
      "16/16 [==============================] - 0s 935us/step - kl_loss: 3.9110e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 3.2605e-05\n",
      "16/16 [==============================] - 0s 908us/step - kl_loss: 5.6698e-05\n",
      "16/16 [==============================] - 0s 912us/step - kl_loss: 4.7928e-05\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 1.7173e-04\n",
      "10/10 [==============================] - 0s 866us/step - kl_loss: 1.5600e-04\n",
      "16/16 [==============================] - 0s 848us/step - kl_loss: 1.7683e-04\n",
      "10/10 [==============================] - 0s 940us/step - kl_loss: 1.8164e-04\n",
      "10/10 [==============================] - 0s 920us/step - kl_loss: 2.5693e-04\n",
      "16/16 [==============================] - 0s 887us/step - kl_loss: 1.4039e-04\n",
      "16/16 [==============================] - 0s 869us/step - kl_loss: 3.1678e-04\n",
      "16/16 [==============================] - 0s 877us/step - kl_loss: 2.0544e-04\n",
      "16/16 [==============================] - 0s 910us/step - kl_loss: 1.3164e-04\n",
      "16/16 [==============================] - 0s 920us/step - kl_loss: 3.7877e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 1.7913e-05\n",
      "16/16 [==============================] - 0s 896us/step - kl_loss: 8.9225e-06\n",
      "16/16 [==============================] - 0s 894us/step - kl_loss: 2.8919e-05\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 8.2512e-05\n",
      "16/16 [==============================] - 0s 852us/step - kl_loss: 6.1667e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 6.0950e-05\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 4.6845e-05\n",
      "16/16 [==============================] - 0s 872us/step - kl_loss: 1.2996e-04\n",
      "16/16 [==============================] - 0s 934us/step - kl_loss: 8.1739e-05\n",
      "11/11 [==============================] - 0s 936us/step - kl_loss: 1.4202e-04\n",
      "16/16 [==============================] - 0s 919us/step - kl_loss: 1.3208e-04\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 2.5416e-05\n",
      "16/16 [==============================] - 0s 879us/step - kl_loss: 2.3686e-04\n",
      "16/16 [==============================] - 0s 893us/step - kl_loss: 8.3754e-05\n",
      "16/16 [==============================] - 0s 910us/step - kl_loss: 5.5621e-05\n",
      "16/16 [==============================] - 0s 908us/step - kl_loss: 4.7069e-05\n",
      "16/16 [==============================] - 0s 863us/step - kl_loss: 3.4773e-05\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 6.9179e-05\n",
      "16/16 [==============================] - 0s 892us/step - kl_loss: 5.2090e-05\n",
      "14/14 [==============================] - 0s 910us/step - kl_loss: 2.1028e-04\n",
      "9/9 [==============================] - 0s 919us/step - kl_loss: 3.0514e-04\n",
      "16/16 [==============================] - 0s 907us/step - kl_loss: 1.1390e-04\n",
      "16/16 [==============================] - 0s 879us/step - kl_loss: 2.2734e-05\n",
      "16/16 [==============================] - 0s 883us/step - kl_loss: 2.9648e-05\n",
      "16/16 [==============================] - 0s 841us/step - kl_loss: 4.2551e-04\n",
      "16/16 [==============================] - 0s 858us/step - kl_loss: 8.8995e-05\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 2.1523e-04\n",
      "16/16 [==============================] - 0s 878us/step - kl_loss: 2.3479e-04\n",
      "16/16 [==============================] - 0s 884us/step - kl_loss: 3.5033e-05\n",
      "16/16 [==============================] - 0s 893us/step - kl_loss: 2.2248e-05\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 2.9066e-05\n",
      "16/16 [==============================] - 0s 865us/step - kl_loss: 2.1744e-05\n",
      "11/11 [==============================] - 0s 894us/step - kl_loss: 6.2629e-05\n",
      "16/16 [==============================] - 0s 853us/step - kl_loss: 2.1039e-04\n",
      "16/16 [==============================] - 0s 911us/step - kl_loss: 7.2563e-05\n",
      "16/16 [==============================] - 0s 911us/step - kl_loss: 7.8947e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 1.6788e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 3.6060e-04\n",
      "16/16 [==============================] - 0s 871us/step - kl_loss: 7.9603e-05\n",
      "16/16 [==============================] - 0s 888us/step - kl_loss: 3.5523e-05\n",
      "16/16 [==============================] - 0s 905us/step - kl_loss: 1.2547e-04\n",
      "16/16 [==============================] - 0s 884us/step - kl_loss: 2.6244e-04\n",
      "8/8 [==============================] - 0s 905us/step - kl_loss: 1.9605e-04\n",
      "16/16 [==============================] - 0s 880us/step - kl_loss: 7.6105e-05\n",
      "16/16 [==============================] - 0s 904us/step - kl_loss: 1.9218e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 1.0097e-04\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 1.8222e-04\n",
      "16/16 [==============================] - 0s 913us/step - kl_loss: 5.0225e-05\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 1.4844e-04\n",
      "12/12 [==============================] - 0s 916us/step - kl_loss: 7.4900e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 8.2255e-05\n",
      "7/7 [==============================] - 0s 998us/step - kl_loss: 1.0713e-04\n",
      "16/16 [==============================] - 0s 877us/step - kl_loss: 8.7325e-05\n",
      "16/16 [==============================] - 0s 884us/step - kl_loss: 2.1384e-05\n",
      "16/16 [==============================] - 0s 890us/step - kl_loss: 6.9913e-05\n",
      "16/16 [==============================] - 0s 861us/step - kl_loss: 4.8478e-05\n",
      "16/16 [==============================] - 0s 858us/step - kl_loss: 5.1530e-05\n",
      "16/16 [==============================] - 0s 928us/step - kl_loss: 9.6592e-06\n",
      "13/13 [==============================] - 0s 938us/step - kl_loss: 9.6443e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_seqs):\n",
    "\n",
    "    pre = ob_seqs[i]\n",
    "    next = next_obs[i]\n",
    "    acts = actions[i]\n",
    "\n",
    "    next_sd = next_obs_stddev[i]\n",
    "\n",
    "    daifa.train(pre, next, acts, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "data": {
      "text/plain": "(20, 5, 3)"
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 150\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    test = o[-1]\n",
    "\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "ob_seqs = np.array(ob_seqs)[:, -5:, :]\n",
    "next_obs = np.array(next_obs)\n",
    "ob_seqs.shape\n",
    "\n",
    "ob_seqs_stddev = np.ones_like(ob_seqs)\n",
    "next_obs_stddev = np.ones_like(next_obs)\n",
    "\n",
    "ob_seqs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: shape=(20, 2), dtype=float32, numpy=\n array([[0.42757505, 0.6529479 ],\n        [0.38936174, 0.5224733 ],\n        [0.5177275 , 0.5719767 ],\n        [0.32569912, 0.5357033 ],\n        [0.5789795 , 0.37268484],\n        [0.50462687, 0.16914466],\n        [0.10576638, 0.2804633 ],\n        [0.50895584, 0.26160803],\n        [0.84278893, 0.7405859 ],\n        [0.41768757, 0.8168086 ],\n        [0.6252694 , 0.524398  ],\n        [0.29302755, 0.6640839 ],\n        [0.7982165 , 0.4708082 ],\n        [0.0011582 , 0.43203047],\n        [0.15381938, 0.3239351 ],\n        [0.51816386, 0.7722647 ],\n        [0.4246617 , 0.18837008],\n        [0.62402654, 0.48184252],\n        [0.6900874 , 0.6458197 ],\n        [0.48874134, 0.24390756]], dtype=float32)>,\n <tf.Tensor: shape=(20, 2), dtype=float32, numpy=\n array([[1.0015444 , 1.0040803 ],\n        [1.0029435 , 1.0013554 ],\n        [1.00249   , 1.0037054 ],\n        [1.0025867 , 1.0013175 ],\n        [1.0037783 , 1.0024655 ],\n        [1.0045325 , 0.9992334 ],\n        [1.0020174 , 1.0017799 ],\n        [1.0041207 , 1.0012053 ],\n        [1.0033425 , 1.0089743 ],\n        [1.0012552 , 1.0062354 ],\n        [1.0033436 , 1.0037761 ],\n        [1.0008956 , 1.0037417 ],\n        [1.0051221 , 1.0040122 ],\n        [1.0008185 , 1.0032822 ],\n        [1.0018778 , 1.0018895 ],\n        [0.99936193, 1.0098908 ],\n        [1.0041473 , 1.001115  ],\n        [1.0043585 , 1.0022985 ],\n        [1.0025626 , 1.0072029 ],\n        [1.0040324 , 1.0017579 ]], dtype=float32)>,\n <tf.Tensor: shape=(20, 60), dtype=float32, numpy=\n array([[ 0.15026245,  0.02147635,  0.08431074, ..., -0.10982125,\n          0.05334226,  0.06565161],\n        [ 0.1892961 ,  0.0171166 ,  0.14129707, ..., -0.02967911,\n          0.19479504,  0.10307508],\n        [ 0.1276658 ,  0.0259374 ,  0.09705178, ..., -0.09186851,\n          0.05516341,  0.06683648],\n        ...,\n        [ 0.12049596,  0.0347067 ,  0.12938446, ..., -0.06724519,\n          0.1028895 ,  0.08167858],\n        [ 0.067205  ,  0.0587694 ,  0.05941856, ..., -0.19308257,\n         -0.05885605,  0.03636646],\n        [-0.04927346,  0.04336642,  0.01616201, ..., -0.09991126,\n         -0.13079813,  0.00147478]], dtype=float32)>,\n <tf.Tensor: shape=(20, 5, 60), dtype=float32, numpy=\n array([[[ 0.04525499, -0.01716381,  0.02350051, ..., -0.04238965,\n           0.01428884,  0.0045097 ],\n         [ 0.09317333, -0.00512476,  0.05376579, ..., -0.07720339,\n           0.02760326,  0.0271852 ],\n         [ 0.12649374,  0.00985319,  0.0731228 , ..., -0.09722769,\n           0.03932348,  0.04809814],\n         [ 0.14352977,  0.01830245,  0.08136631, ..., -0.10614912,\n           0.04794318,  0.06033898],\n         [ 0.15026245,  0.02147635,  0.08431074, ..., -0.10982125,\n           0.05334226,  0.06565161]],\n \n        [[ 0.06360728, -0.0279705 ,  0.04693291, ...,  0.01876127,\n           0.07270339,  0.02625261],\n         [ 0.12016988, -0.01528889,  0.09293082, ...,  0.00722509,\n           0.12378819,  0.05685403],\n         [ 0.15799734,  0.00201168,  0.12204836, ..., -0.00917845,\n           0.15883154,  0.08118919],\n         [ 0.17884886,  0.0125097 ,  0.13581392, ..., -0.02153258,\n           0.1813325 ,  0.09584883],\n         [ 0.1892961 ,  0.0171166 ,  0.14129707, ..., -0.02967911,\n           0.19479504,  0.10307508]],\n \n        [[ 0.01899024, -0.00537003,  0.01712187, ..., -0.05791149,\n          -0.00749379, -0.00342179],\n         [ 0.05159665,  0.01008601,  0.04413131, ..., -0.09687613,\n          -0.01088502,  0.01538653],\n         [ 0.09476812,  0.01297409,  0.07409859, ..., -0.08821176,\n           0.01896192,  0.04428795],\n         [ 0.11822307,  0.02161739,  0.09018441, ..., -0.08948812,\n           0.0409235 ,  0.05975568],\n         [ 0.1276658 ,  0.0259374 ,  0.09705178, ..., -0.09186851,\n           0.05516341,  0.06683648]],\n \n        ...,\n \n        [[-0.01287574,  0.00913438,  0.01037965, ..., -0.07211688,\n          -0.03120288, -0.01136945],\n         [ 0.04945046, -0.00161371,  0.06758605, ..., -0.04407822,\n           0.02909646,  0.0303868 ],\n         [ 0.09147461,  0.0191132 ,  0.10595157, ..., -0.05436005,\n           0.06481314,  0.0583218 ],\n         [ 0.11274984,  0.03069214,  0.12332863, ..., -0.0628503 ,\n           0.08860222,  0.07462525],\n         [ 0.12049596,  0.0347067 ,  0.12938446, ..., -0.06724519,\n           0.1028895 ,  0.08167858]],\n \n        [[ 0.01436141, -0.00646611,  0.01604711, ..., -0.07796118,\n          -0.01446355, -0.00974533],\n         [ 0.05152211,  0.0144128 ,  0.04881622, ..., -0.12986496,\n          -0.0225791 ,  0.01168774],\n         [ 0.08065747,  0.03350948,  0.06986689, ..., -0.15249343,\n          -0.02346765,  0.03437518],\n         [ 0.09257582,  0.04136415,  0.07716379, ..., -0.15684438,\n          -0.02214691,  0.04622274],\n         [ 0.067205  ,  0.0587694 ,  0.05941856, ..., -0.19308257,\n          -0.05885605,  0.03636646]],\n \n        [[-0.04124852,  0.02623192, -0.00106705, ..., -0.07540748,\n          -0.05910962, -0.01571499],\n         [-0.0516727 ,  0.0423548 ,  0.00782672, ..., -0.10877656,\n          -0.10001399, -0.01105171],\n         [-0.05289169,  0.04889357,  0.01271122, ..., -0.11655216,\n          -0.12273175, -0.00386084],\n         [-0.04969812,  0.04598803,  0.01593486, ..., -0.10666266,\n          -0.12809363,  0.0011813 ],\n         [-0.04927346,  0.04336642,  0.01616201, ..., -0.09991126,\n          -0.13079813,  0.00147478]]], dtype=float32)>]"
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daifa.tran((ob_seqs, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.41424149, 0.64817536],\n       [0.37549612, 0.51434882],\n       [0.50395287, 0.56238058],\n       [0.31282919, 0.53150581],\n       [0.56830029, 0.36537232],\n       [0.49751557, 0.15729226],\n       [0.10200014, 0.28023718],\n       [0.49930485, 0.25297206],\n       [0.83264549, 0.76498072],\n       [0.40183171, 0.8195579 ],\n       [0.61205382, 0.51735449],\n       [0.28334383, 0.66585968],\n       [0.78955368, 0.48945753],\n       [0.        , 0.5       ],\n       [0.15014838, 0.32678897],\n       [0.5064109 , 0.78005801],\n       [0.41774712, 0.18253206],\n       [0.61022634, 0.47224011],\n       [0.67659513, 0.6480953 ],\n       [0.48181082, 0.23722683]])"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_obs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks fantastic!!! With enough data the transition model is training very well"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 60), dtype=float32, numpy=\narray([[ 0.1668222 ,  0.05080901,  0.20015422,  0.38899958, -0.08546768,\n        -0.04040675,  0.03512116,  0.00171472,  0.03852477,  0.20108685,\n        -0.03330585,  0.01087453, -0.03944991, -0.23539615,  0.19884759,\n         0.15129937,  0.08765514,  0.15757117, -0.16009761, -0.02254741,\n        -0.17335685, -0.09706004,  0.05607434,  0.03711884, -0.0560054 ,\n        -0.27313083,  0.02705026,  0.14458522, -0.25310335, -0.08086976,\n        -0.10635097, -0.28293777,  0.00502296,  0.2793439 ,  0.07475004,\n        -0.09199525, -0.23762226,  0.05454395, -0.07554322, -0.06423084,\n         0.11491245,  0.03344171, -0.03258195,  0.04890673,  0.07888647,\n         0.11464167,  0.31568897,  0.01460155, -0.23916677,  0.24096602,\n         0.1589966 ,  0.0215495 , -0.38883814,  0.2073881 ,  0.17495394,\n         0.30218056, -0.14856315, -0.09490789,  0.20044254,  0.12068783]],\n      dtype=float32)>"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daifa.hidden_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.5]\n",
      "tf.Tensor(\n",
      "[ 0.7089493   0.76423895  0.8047202   0.8124183   0.73879427  0.76978123\n",
      "  0.61311316  0.5821078   0.4306626   0.40520254  0.17977683  0.27913105\n",
      " -0.00298302  0.03768509  0.0850338 ], shape=(15,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(15,), dtype=float32, numpy=\narray([ 0.7089493 ,  0.76423895,  0.8047202 ,  0.8124183 ,  0.73879427,\n        0.76978123,  0.61311316,  0.5821078 ,  0.4306626 ,  0.40520254,\n        0.17977683,  0.27913105, -0.00298302,  0.03768509,  0.0850338 ],\n      dtype=float32)>"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_t_minus_1 = np.array([0.4, 0.5])\n",
    "daifa.hidden_state = None\n",
    "p, s = daifa.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.17485519, 0.28052184]], dtype=float32)>,\n <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.998175 , 0.9848511]], dtype=float32)>,\n <tf.Tensor: shape=(1, 60), dtype=float32, numpy=\n array([[ 0.06734794, -0.03086371,  0.04919352,  0.06776053,  0.05020184,\n         -0.02855486,  0.05532712,  0.00621268,  0.03898622,  0.06404883,\n         -0.06092996,  0.02080428, -0.03182369, -0.09051668,  0.02938318,\n          0.02920253,  0.09898859,  0.04242412, -0.0937261 , -0.07689307,\n         -0.04255384, -0.01035933,  0.0217123 ,  0.0560016 , -0.06113841,\n         -0.08259731, -0.04008626,  0.02852438, -0.09232952, -0.09844272,\n         -0.09578703, -0.0808928 ,  0.07631816,  0.10538951,  0.03266784,\n         -0.06981869, -0.13518177, -0.00443301, -0.0766279 ,  0.03137437,\n          0.10195947,  0.03933517,  0.01060682, -0.02867689,  0.05144734,\n          0.02094595,  0.07122529,  0.09150924, -0.11927285,  0.11849919,\n          0.06474774, -0.00642365, -0.15364884,  0.10189001,  0.09677684,\n          0.10298578,  0.00337658,  0.01931385,  0.07793737,  0.02707184]],\n       dtype=float32)>,\n <tf.Tensor: shape=(1, 1, 60), dtype=float32, numpy=\n array([[[ 0.06734794, -0.03086371,  0.04919352,  0.06776053,\n           0.05020184, -0.02855486,  0.05532712,  0.00621268,\n           0.03898622,  0.06404883, -0.06092996,  0.02080428,\n          -0.03182369, -0.09051668,  0.02938318,  0.02920253,\n           0.09898859,  0.04242412, -0.0937261 , -0.07689307,\n          -0.04255384, -0.01035933,  0.0217123 ,  0.0560016 ,\n          -0.06113841, -0.08259731, -0.04008626,  0.02852438,\n          -0.09232952, -0.09844272, -0.09578703, -0.0808928 ,\n           0.07631816,  0.10538951,  0.03266784, -0.06981869,\n          -0.13518177, -0.00443301, -0.0766279 ,  0.03137437,\n           0.10195947,  0.03933517,  0.01060682, -0.02867689,\n           0.05144734,  0.02094595,  0.07122529,  0.09150924,\n          -0.11927285,  0.11849919,  0.06474774, -0.00642365,\n          -0.15364884,  0.10189001,  0.09677684,  0.10298578,\n           0.00337658,  0.01931385,  0.07793737,  0.02707184]]],\n       dtype=float32)>]"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daifa.tran((np.array([[[0.4, 0.5, 1]]]), None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[0.3531864 0.5      ]\n",
      "tf.Tensor(\n",
      "[0.75776505 0.808785   0.823482   0.7384236  0.66807634 0.75550365\n",
      " 0.65706307 0.57430935 0.44249344 0.50059706 0.30967107 0.3025037\n",
      " 0.23872639 0.13167822 0.03006317], shape=(15,), dtype=float32)\n",
      "[0.41072467 0.60575192]\n",
      "tf.Tensor(\n",
      "[0.80091256 0.831186   0.80336916 0.743643   0.67225486 0.7388898\n",
      " 0.6710419  0.49252346 0.45496187 0.39570916 0.33172902 0.4691094\n",
      " 0.18269321 0.02243686 0.13316137], shape=(15,), dtype=float32)\n",
      "[0.52321638 0.6161014 ]\n",
      "tf.Tensor(\n",
      "[0.7582315  0.7428887  0.78753793 0.70738727 0.6904973  0.6947737\n",
      " 0.7184091  0.63121426 0.51972836 0.44251725 0.4724567  0.35061508\n",
      " 0.20868196 0.29411447 0.1608929 ], shape=(15,), dtype=float32)\n",
      "[0.59332169 0.53282979]\n",
      "tf.Tensor(\n",
      "[ 0.8261601   0.7788568   0.7498009   0.68705684  0.71283436  0.7625619\n",
      "  0.6251273   0.59106356  0.47288027  0.37912145  0.37760547  0.27614397\n",
      "  0.17472365  0.0352636  -0.06415275], shape=(15,), dtype=float32)\n",
      "[0.57653528 0.43853916]\n",
      "tf.Tensor(\n",
      "[ 0.84993833  0.838954    0.836631    0.8009051   0.6381009   0.67451525\n",
      "  0.66360396  0.7313162   0.44090325  0.61107373  0.4429256   0.2499801\n",
      "  0.18311256  0.00455586 -0.03917548], shape=(15,), dtype=float32)\n",
      "[0.48622649 0.38311274]\n",
      "tf.Tensor(\n",
      "[0.8081694  0.80967754 0.7922341  0.7968606  0.7375868  0.6537518\n",
      " 0.64383316 0.62586427 0.55786794 0.3814907  0.40052238 0.48054183\n",
      " 0.37376764 0.13650063 0.09636682], shape=(15,), dtype=float32)\n",
      "[0.38618095 0.41815763]\n",
      "tf.Tensor(\n",
      "[0.808529   0.80657184 0.7916462  0.70533156 0.7616304  0.6811054\n",
      " 0.6398279  0.49607593 0.48863018 0.42573503 0.4255617  0.5032834\n",
      " 0.25781015 0.1669854  0.03199732], shape=(15,), dtype=float32)\n",
      "[0.36487361 0.53167849]\n",
      "tf.Tensor(\n",
      "[ 0.80258495  0.86818594  0.71748924  0.8466585   0.7656668   0.6802341\n",
      "  0.7175385   0.55456597  0.56965184  0.48494464  0.4133271   0.35727996\n",
      "  0.29771766  0.2542887  -0.04310569], shape=(15,), dtype=float32)\n",
      "[0.44369857 0.61764689]\n",
      "tf.Tensor(\n",
      "[0.8110079  0.6923667  0.79028475 0.6758299  0.71477115 0.7232198\n",
      " 0.6768123  0.6062628  0.5514061  0.47747606 0.4313433  0.3517048\n",
      " 0.39273486 0.17666398 0.08394014], shape=(15,), dtype=float32)\n",
      "[0.54803135 0.58811021]\n",
      "tf.Tensor(\n",
      "[0.77069455 0.7971718  0.76023203 0.76838154 0.70611453 0.7311224\n",
      " 0.6250644  0.6438176  0.47762975 0.5243256  0.41966495 0.36352235\n",
      " 0.40499055 0.01003213 0.061273  ], shape=(15,), dtype=float32)\n",
      "[0.58816123 0.5017748 ]\n",
      "tf.Tensor(\n",
      "[ 0.82042825  0.80002797  0.74095577  0.71816117  0.73239034  0.7092903\n",
      "  0.72302485  0.62576604  0.5700318   0.4367064   0.43498516  0.47891188\n",
      "  0.16538338  0.18202068 -0.19701585], shape=(15,), dtype=float32)\n",
      "[0.54586323 0.41759907]\n",
      "tf.Tensor(\n",
      "[0.7925538  0.797785   0.7362157  0.7193825  0.72075266 0.6847715\n",
      " 0.63142014 0.52450913 0.6858361  0.4741538  0.46578348 0.35045347\n",
      " 0.4875199  0.10972673 0.30347246], shape=(15,), dtype=float32)\n",
      "[0.44605739 0.38614578]\n",
      "tf.Tensor(\n",
      "[0.7413631  0.7655986  0.7876091  0.7940088  0.70736146 0.6543615\n",
      " 0.6855993  0.6544356  0.58999455 0.48650116 0.347798   0.12228234\n",
      " 0.24543785 0.20780389 0.03425851], shape=(15,), dtype=float32)\n",
      "[0.36494746 0.45144458]\n",
      "tf.Tensor(\n",
      "[0.8217085  0.73286784 0.7572981  0.7622988  0.7463897  0.71150637\n",
      " 0.5872628  0.6521302  0.51973563 0.41864827 0.4010417  0.27743676\n",
      " 0.38145977 0.16709988 0.10859277], shape=(15,), dtype=float32)\n",
      "[0.3799972  0.56624843]\n",
      "tf.Tensor(\n",
      "[0.7157361  0.8330341  0.7970764  0.7716476  0.79019064 0.6825232\n",
      " 0.58022153 0.49948868 0.45807794 0.5443946  0.4094528  0.3813852\n",
      " 0.3188499  0.09375567 0.2158043 ], shape=(15,), dtype=float32)\n",
      "[0.47352085 0.61658074]\n",
      "tf.Tensor(\n",
      "[0.7780647  0.80061483 0.722193   0.74317825 0.7437629  0.68641585\n",
      " 0.5962243  0.50690097 0.65318114 0.52816546 0.38895717 0.27855322\n",
      " 0.25044137 0.2727542  0.06080603], shape=(15,), dtype=float32)\n",
      "[0.56438515 0.56986869]\n",
      "tf.Tensor(\n",
      "[0.7811205  0.7339901  0.81370497 0.72263205 0.73430645 0.629454\n",
      " 0.6017497  0.6197657  0.56409466 0.43045133 0.26490685 0.2498877\n",
      " 0.31837606 0.00660494 0.18230386], shape=(15,), dtype=float32)\n",
      "[0.58373286 0.47584327]\n",
      "tf.Tensor(\n",
      "[ 0.786635    0.7676723   0.78109056  0.7734856   0.7349278   0.6434932\n",
      "  0.64729017  0.6477715   0.46817616  0.5103125   0.3484181   0.32719758\n",
      "  0.19347914  0.08783682 -0.12831853], shape=(15,), dtype=float32)\n",
      "[0.51849457 0.39698744]\n",
      "tf.Tensor(\n",
      "[0.7013822  0.82110566 0.8151903  0.83248204 0.69641477 0.7037499\n",
      " 0.65221685 0.62713647 0.6174364  0.46683326 0.47702065 0.45662794\n",
      " 0.22590521 0.12989476 0.18857364], shape=(15,), dtype=float32)\n",
      "[0.41009074 0.39329548]\n",
      "tf.Tensor(\n",
      "[0.78286713 0.75951535 0.79978824 0.7267528  0.71680486 0.65614986\n",
      " 0.6204901  0.54773957 0.50284386 0.5718347  0.25679275 0.19043566\n",
      " 0.2571739  0.09063831 0.13465643], shape=(15,), dtype=float32)\n",
      "[0.35496967 0.49114117]\n",
      "tf.Tensor(\n",
      "[ 0.7816021   0.81464404  0.8433688   0.76073045  0.763372    0.6750622\n",
      "  0.5641183   0.69435006  0.5063368   0.4080222   0.47917065  0.31884998\n",
      "  0.17305967 -0.06161011 -0.04073815], shape=(15,), dtype=float32)\n",
      "[0.40579482 0.60065321]\n",
      "tf.Tensor(\n",
      "[0.79529333 0.750566   0.79003304 0.76391315 0.73424697 0.6420629\n",
      " 0.6682913  0.6523948  0.5369906  0.5469905  0.33981997 0.26553097\n",
      " 0.0638757  0.28567824 0.23673962], shape=(15,), dtype=float32)\n",
      "[0.51507599 0.61200315]\n",
      "tf.Tensor(\n",
      "[ 0.79337245  0.7404581   0.7697803   0.73482007  0.70173734  0.73203075\n",
      "  0.5884999   0.60060364  0.46503568  0.4337764   0.4391953   0.34379748\n",
      "  0.26070306  0.07910137 -0.0782097 ], shape=(15,), dtype=float32)\n",
      "[0.58594846 0.53615845]\n",
      "tf.Tensor(\n",
      "[0.77882797 0.69763243 0.7354432  0.78261834 0.7548013  0.7395214\n",
      " 0.6853571  0.57952696 0.5142892  0.46434534 0.52490604 0.3411709\n",
      " 0.2709834  0.09081004 0.05274038], shape=(15,), dtype=float32)\n",
      "[0.57014214 0.43662953]\n",
      "tf.Tensor(\n",
      "[ 0.8492138   0.7675102   0.8595176   0.78523564  0.7543218   0.66763735\n",
      "  0.5548783   0.62393606  0.48151332  0.5518069   0.38802665  0.3985188\n",
      "  0.3526202   0.23939711 -0.12900242], shape=(15,), dtype=float32)\n",
      "[0.47905944 0.38159515]\n",
      "tf.Tensor(\n",
      "[ 0.82852745  0.81620026  0.76462984  0.6585787   0.680708    0.5028422\n",
      "  0.67517024  0.60897154  0.4863976   0.62219536  0.49263546  0.44589314\n",
      "  0.28722784  0.23585846 -0.02223484], shape=(15,), dtype=float32)\n",
      "[0.38228903 0.42519666]\n",
      "tf.Tensor(\n",
      "[0.79549915 0.73700064 0.78116643 0.7397113  0.7940053  0.78030664\n",
      " 0.5730892  0.49749538 0.5726207  0.3887233  0.46988285 0.3230513\n",
      " 0.03857424 0.06511506 0.09033238], shape=(15,), dtype=float32)\n",
      "[0.36676927 0.53437901]\n",
      "tf.Tensor(\n",
      "[0.8047022  0.75837153 0.8462696  0.7162242  0.72801363 0.65123004\n",
      " 0.59589547 0.54535824 0.6131784  0.4514287  0.6247042  0.36965016\n",
      " 0.20724636 0.18744104 0.0636511 ], shape=(15,), dtype=float32)\n",
      "[0.44490315 0.61066828]\n",
      "tf.Tensor(\n",
      "[0.81024164 0.7460855  0.7998578  0.74819976 0.7318434  0.7199478\n",
      " 0.6176514  0.6539627  0.45442376 0.54600036 0.46350846 0.27041775\n",
      " 0.35329205 0.08493165 0.01173883], shape=(15,), dtype=float32)\n",
      "[0.54393757 0.58586972]\n",
      "tf.Tensor(\n",
      "[0.83877844 0.7512684  0.8179611  0.7089987  0.62314284 0.6872689\n",
      " 0.666742   0.58867216 0.43242267 0.48708692 0.5365295  0.36782113\n",
      " 0.2323856  0.22591136 0.1930209 ], shape=(15,), dtype=float32)\n",
      "[0.58560245 0.50302988]\n",
      "tf.Tensor(\n",
      "[ 0.81615007  0.8337144   0.7280479   0.83652335  0.6593734   0.6673092\n",
      "  0.644081    0.6065941   0.49902397  0.597173    0.33554184  0.29067463\n",
      "  0.29533008  0.15974145 -0.04347519], shape=(15,), dtype=float32)\n",
      "[0.54541119 0.42172624]\n",
      "tf.Tensor(\n",
      "[0.77859366 0.7399454  0.80361915 0.8222891  0.6997945  0.7333289\n",
      " 0.5931555  0.5250654  0.38968724 0.4366026  0.41840667 0.2858042\n",
      " 0.20093226 0.11056146 0.28115508], shape=(15,), dtype=float32)\n",
      "[0.44752685 0.38479307]\n",
      "tf.Tensor(\n",
      "[ 0.82429093  0.7936777   0.8563681   0.7860239   0.7406905   0.5799222\n",
      "  0.7315543   0.5343847   0.460609    0.4562714   0.35923907  0.36962327\n",
      "  0.18014123  0.23509723 -0.02257505], shape=(15,), dtype=float32)\n",
      "[0.36856751 0.45480162]\n",
      "tf.Tensor(\n",
      "[0.8142052  0.7555646  0.8146456  0.76780176 0.7035508  0.6438452\n",
      " 0.6299063  0.6259845  0.56285286 0.4900472  0.4291155  0.2972585\n",
      " 0.23083456 0.0416389  0.11854338], shape=(15,), dtype=float32)\n",
      "[0.38437616 0.56566065]\n",
      "tf.Tensor(\n",
      "[0.8717742  0.79311955 0.8349144  0.7581415  0.6517145  0.639386\n",
      " 0.60070837 0.52159375 0.42648387 0.45765376 0.37017617 0.3854718\n",
      " 0.27083525 0.08882858 0.03711194], shape=(15,), dtype=float32)\n",
      "[0.48105746 0.61698049]\n",
      "tf.Tensor(\n",
      "[0.7816412  0.8154942  0.7475302  0.7307145  0.67497694 0.65167934\n",
      " 0.6135576  0.52580523 0.50448567 0.5064087  0.28986984 0.33856618\n",
      " 0.2111372  0.04163319 0.06080171], shape=(15,), dtype=float32)\n",
      "[0.56957929 0.56592761]\n",
      "tf.Tensor(\n",
      "[0.7415218  0.8276692  0.76559246 0.7420308  0.6718621  0.6524063\n",
      " 0.74649006 0.6595096  0.57205176 0.42472783 0.46960062 0.40188935\n",
      " 0.3381443  0.31225252 0.12252428], shape=(15,), dtype=float32)\n",
      "[0.58403423 0.47394873]\n",
      "tf.Tensor(\n",
      "[ 0.7820771   0.76585364  0.77992487  0.7481109   0.71760786  0.7291078\n",
      "  0.73734087  0.65609974  0.58259207  0.44035995  0.39198986  0.0941532\n",
      "  0.22869183 -0.15184657  0.09070029], shape=(15,), dtype=float32)\n",
      "[0.51688021 0.39510235]\n",
      "tf.Tensor(\n",
      "[0.7352284  0.75400406 0.78572464 0.801884   0.6663553  0.7583888\n",
      " 0.67039853 0.5376294  0.50376385 0.5414519  0.5258707  0.35912326\n",
      " 0.2996345  0.05289698 0.06059019], shape=(15,), dtype=float32)\n",
      "[0.40798668 0.39102628]\n",
      "tf.Tensor(\n",
      "[0.7611072  0.8020517  0.78023297 0.6550679  0.66480124 0.76778185\n",
      " 0.6363506  0.5958002  0.47454938 0.5027793  0.5354361  0.33032587\n",
      " 0.32072702 0.1383539  0.08724254], shape=(15,), dtype=float32)\n",
      "[0.3520312  0.49354863]\n",
      "tf.Tensor(\n",
      "[0.79392415 0.7739514  0.8184119  0.79044056 0.6628201  0.73477334\n",
      " 0.6351135  0.6286006  0.4333898  0.42271107 0.5196934  0.10632459\n",
      " 0.33844072 0.25389937 0.09108002], shape=(15,), dtype=float32)\n",
      "[0.40609846 0.60283605]\n",
      "tf.Tensor(\n",
      "[ 0.7884622   0.82729053  0.8360866   0.7258866   0.64380354  0.7157528\n",
      "  0.5779476   0.5710402   0.47314978  0.5921881   0.3813641   0.29816145\n",
      "  0.16894156  0.27616373 -0.02353296], shape=(15,), dtype=float32)\n",
      "[0.51798088 0.61750049]\n",
      "tf.Tensor(\n",
      "[ 0.81359965  0.763978    0.76793635  0.7462814   0.70802116  0.68962127\n",
      "  0.58806014  0.5863664   0.46981502  0.5895484   0.37318972  0.36630228\n",
      "  0.25067335  0.17080323 -0.03141759], shape=(15,), dtype=float32)\n",
      "[0.5938309  0.54126586]\n",
      "tf.Tensor(\n",
      "[0.8113294  0.768809   0.6856807  0.73540765 0.72331357 0.6783503\n",
      " 0.5949049  0.583399   0.61106104 0.5376641  0.33809215 0.40843654\n",
      " 0.18948479 0.06254447 0.08455604], shape=(15,), dtype=float32)\n",
      "[0.58345062 0.44372764]\n",
      "tf.Tensor(\n",
      "[0.8009942  0.8070976  0.7275021  0.69663674 0.71600336 0.70018804\n",
      " 0.65069294 0.66603565 0.7022384  0.45402578 0.34891218 0.22229828\n",
      " 0.2553366  0.20484611 0.02414317], shape=(15,), dtype=float32)\n",
      "[0.49264736 0.37778779]\n",
      "tf.Tensor(\n",
      "[0.74953175 0.75108176 0.7682707  0.7941111  0.6321828  0.7890561\n",
      " 0.59978455 0.5697439  0.5587085  0.5677614  0.34142888 0.45922023\n",
      " 0.34311283 0.18927541 0.04024025], shape=(15,), dtype=float32)\n",
      "[0.38186869 0.40307761]\n",
      "tf.Tensor(\n",
      "[0.8401727  0.83660525 0.7682112  0.7321585  0.7469177  0.6507884\n",
      " 0.6442639  0.64668775 0.45425525 0.34885836 0.329645   0.31632945\n",
      " 0.17799746 0.10585867 0.17687285], shape=(15,), dtype=float32)\n",
      "[0.35298689 0.53085447]\n",
      "tf.Tensor(\n",
      "[0.78200173 0.7954946  0.79973924 0.76843774 0.76992786 0.64292175\n",
      " 0.6476148  0.5449644  0.6215908  0.56584924 0.37136087 0.37221712\n",
      " 0.19877341 0.05788508 0.09599178], shape=(15,), dtype=float32)\n",
      "[0.43535255 0.62311142]\n",
      "tf.Tensor(\n",
      "[0.7988832  0.81995803 0.79670495 0.75617784 0.7537437  0.72877586\n",
      " 0.6120326  0.6481022  0.528509   0.44550523 0.54102105 0.4045983\n",
      " 0.20502405 0.10994437 0.01944509], shape=(15,), dtype=float32)\n",
      "[0.54961391 0.6057297 ]\n",
      "tf.Tensor(\n",
      "[0.82574815 0.8087345  0.79786146 0.726313   0.6651899  0.7448676\n",
      " 0.6447881  0.57326835 0.49473333 0.34929955 0.18304539 0.3073979\n",
      " 0.30910343 0.2195527  0.08602198], shape=(15,), dtype=float32)\n",
      "[0.60715892 0.51897679]\n",
      "tf.Tensor(\n",
      "[0.73939127 0.7683055  0.77412814 0.7428036  0.71010387 0.734558\n",
      " 0.6276515  0.5459412  0.57485867 0.42185855 0.32804263 0.21587849\n",
      " 0.03043543 0.26203704 0.07618994], shape=(15,), dtype=float32)\n",
      "[0.571388   0.41616956]\n",
      "tf.Tensor(\n",
      "[0.8084016  0.79118896 0.7125908  0.78489465 0.7554889  0.70314\n",
      " 0.6542661  0.6204031  0.5129748  0.53307694 0.3740126  0.1729145\n",
      " 0.2167828  0.13401434 0.3829881 ], shape=(15,), dtype=float32)\n",
      "[0.46168311 0.36681081]\n",
      "tf.Tensor(\n",
      "[ 0.80106664  0.8936466   0.76621354  0.7754078   0.77766734  0.63762605\n",
      "  0.644117    0.4937583   0.3810801   0.40484437  0.45562983  0.39988062\n",
      "  0.14110254  0.12532564 -0.0749072 ], shape=(15,), dtype=float32)\n",
      "[0.36223823 0.43672092]\n",
      "tf.Tensor(\n",
      "[ 0.8590739   0.85995823  0.7432653   0.7503428   0.80756474  0.72860914\n",
      "  0.69533503  0.65018004  0.58915186  0.49469632  0.26144204  0.31654248\n",
      "  0.1668773   0.16982254 -0.07862785], shape=(15,), dtype=float32)\n",
      "[0.37103283 0.57032034]\n",
      "tf.Tensor(\n",
      "[0.7883463  0.7421475  0.7198873  0.8076494  0.73724467 0.7076293\n",
      " 0.55192846 0.49001226 0.5639493  0.52365774 0.33413577 0.2639018\n",
      " 0.14376952 0.13097861 0.12782432], shape=(15,), dtype=float32)\n",
      "[0.47393083 0.62502963]\n",
      "tf.Tensor(\n",
      "[ 0.78464913  0.7832258   0.81469816  0.7778928   0.73397785  0.6671145\n",
      "  0.56024915  0.63776195  0.52743816  0.43552956  0.38730374  0.34354916\n",
      "  0.14792752 -0.00164434  0.15573315], shape=(15,), dtype=float32)\n",
      "[0.57156853 0.57477212]\n",
      "tf.Tensor(\n",
      "[0.8371124  0.72853917 0.8691501  0.73401505 0.6498376  0.73067075\n",
      " 0.63454086 0.55254984 0.5146528  0.4891952  0.43070522 0.43262103\n",
      " 0.1375717  0.04265443 0.3134485 ], shape=(15,), dtype=float32)\n",
      "[0.59580991 0.47936026]\n",
      "tf.Tensor(\n",
      "[ 0.81873614  0.83532137  0.7040841   0.7788191   0.6197398   0.61335325\n",
      "  0.68482035  0.5888923   0.61703384  0.4427971   0.6005854   0.20061466\n",
      "  0.14735143  0.22884987 -0.0335655 ], shape=(15,), dtype=float32)\n",
      "[0.5329938  0.39897234]\n",
      "tf.Tensor(\n",
      "[0.8065402  0.7822272  0.76538205 0.7681339  0.7753727  0.7742152\n",
      " 0.69478977 0.57195425 0.4704853  0.49332318 0.31115493 0.395126\n",
      " 0.28096202 0.04866501 0.1842206 ], shape=(15,), dtype=float32)\n",
      "[0.42374525 0.38484408]\n",
      "tf.Tensor(\n",
      "[0.85375875 0.85130006 0.82338494 0.7543496  0.5935664  0.68263274\n",
      " 0.6397417  0.6747055  0.4713774  0.47407013 0.29585087 0.24762736\n",
      " 0.16460706 0.19996603 0.0240241 ], shape=(15,), dtype=float32)\n",
      "[0.35941744 0.48232772]\n",
      "tf.Tensor(\n",
      "[0.8330857  0.7875589  0.7354576  0.7439057  0.73607373 0.72214407\n",
      " 0.57638764 0.573217   0.5422481  0.41029024 0.3359802  0.30563188\n",
      " 0.10840655 0.21389659 0.00672198], shape=(15,), dtype=float32)\n",
      "[0.40275122 0.59225201]\n",
      "tf.Tensor(\n",
      "[ 0.82999897  0.86564636  0.7875039   0.81777596  0.70565     0.67825115\n",
      "  0.61548394  0.5552539   0.42982295  0.41920453  0.4481033   0.38862363\n",
      "  0.21809898  0.06253474 -0.07581279], shape=(15,), dtype=float32)\n",
      "[0.51037026 0.6184226 ]\n",
      "tf.Tensor(\n",
      "[0.7676647  0.82916576 0.7767916  0.7539434  0.705458   0.64423805\n",
      " 0.69134396 0.44446015 0.62188345 0.49696687 0.4288368  0.4260155\n",
      " 0.28188017 0.08480137 0.05216508], shape=(15,), dtype=float32)\n",
      "[0.58867452 0.54802307]\n",
      "tf.Tensor(\n",
      "[0.8200514  0.8098292  0.80813813 0.74955344 0.6526869  0.66327983\n",
      " 0.609704   0.53753686 0.49178484 0.45022994 0.4564877  0.33983544\n",
      " 0.3784235  0.15109646 0.04174715], shape=(15,), dtype=float32)\n",
      "[0.58638319 0.45443259]\n",
      "tf.Tensor(\n",
      "[0.7696876  0.7509072  0.7973766  0.7715203  0.7091568  0.6614527\n",
      " 0.5852787  0.6455517  0.45499912 0.40769756 0.4184576  0.47384334\n",
      " 0.33827722 0.06488781 0.05048884], shape=(15,), dtype=float32)\n",
      "[0.50134118 0.37832446]\n",
      "tf.Tensor(\n",
      "[0.8192935  0.7438691  0.8174823  0.7081893  0.73018426 0.7355522\n",
      " 0.59525925 0.5514984  0.5196334  0.38062623 0.40469217 0.34332815\n",
      " 0.40626797 0.17307793 0.15640299], shape=(15,), dtype=float32)\n",
      "[0.38968613 0.39832184]\n",
      "tf.Tensor(\n",
      "[ 0.8166617   0.8328098   0.79810065  0.78592587  0.77027655  0.6313598\n",
      "  0.5805887   0.6444551   0.6290719   0.4495581   0.3667833   0.28382215\n",
      "  0.2628423  -0.02390542 -0.15235856], shape=(15,), dtype=float32)\n",
      "[0.35192616 0.51947195]\n",
      "tf.Tensor(\n",
      "[0.78434974 0.81409925 0.8017937  0.74562925 0.6691752  0.718595\n",
      " 0.60850894 0.50091076 0.5456703  0.45592985 0.43720937 0.29297298\n",
      " 0.25382063 0.32232195 0.18150227], shape=(15,), dtype=float32)\n",
      "[0.42643755 0.61913247]\n",
      "tf.Tensor(\n",
      "[0.7865445  0.8380168  0.7752155  0.7637406  0.7297546  0.70171916\n",
      " 0.6920485  0.54565537 0.52947325 0.47661933 0.4009594  0.3770377\n",
      " 0.32883    0.16308029 0.02899723], shape=(15,), dtype=float32)\n",
      "[0.54147191 0.61092481]\n",
      "tf.Tensor(\n",
      "[0.78888315 0.7469997  0.67738193 0.7490696  0.66438204 0.69212526\n",
      " 0.6257292  0.5625566  0.64628035 0.53416157 0.4505727  0.4250792\n",
      " 0.22938962 0.04641598 0.08701646], shape=(15,), dtype=float32)\n",
      "[0.60302623 0.52109476]\n",
      "tf.Tensor(\n",
      "[0.78477997 0.81664246 0.84776825 0.7173021  0.6917322  0.72397983\n",
      " 0.64939183 0.47706875 0.5227951  0.51427835 0.32041466 0.2011926\n",
      " 0.26265493 0.08005637 0.13914573], shape=(15,), dtype=float32)\n",
      "[0.57279901 0.42512558]\n",
      "tf.Tensor(\n",
      "[0.8355642  0.81322175 0.8219691  0.64820415 0.6749398  0.6151745\n",
      " 0.6576263  0.528807   0.5957073  0.31524938 0.5663655  0.27555418\n",
      " 0.14108284 0.23462236 0.08474787], shape=(15,), dtype=float32)\n",
      "[0.47154124 0.37436967]\n",
      "tf.Tensor(\n",
      "[ 0.77912587  0.8206091   0.8392971   0.6829742   0.81002873  0.71247864\n",
      "  0.6173976   0.53924984  0.5642729   0.44932613  0.42185163  0.31431144\n",
      "  0.1367163   0.162648   -0.05937104], shape=(15,), dtype=float32)\n",
      "[0.37085166 0.42617494]\n",
      "tf.Tensor(\n",
      "[ 0.7898823   0.7594188   0.79121584  0.78815037  0.7210286   0.71293074\n",
      "  0.6105649   0.70436573  0.4780255   0.39781225  0.34089234  0.3126574\n",
      "  0.20237492 -0.04093971  0.1548445 ], shape=(15,), dtype=float32)\n",
      "[0.36229389 0.54699585]\n",
      "tf.Tensor(\n",
      "[ 0.82355934  0.7344039   0.8172188   0.73402274  0.74502105  0.6534147\n",
      "  0.6435986   0.7128599   0.49431676  0.48380378  0.39066416  0.20980707\n",
      "  0.12757571  0.13026321 -0.0428597 ], shape=(15,), dtype=float32)\n",
      "[0.45297857 0.62130913]\n",
      "tf.Tensor(\n",
      "[ 0.7381621   0.71149874  0.7293028   0.7196442   0.6499199   0.7436134\n",
      "  0.6323719   0.70429844  0.6065198   0.37744427  0.3373286   0.3274003\n",
      "  0.21847181 -0.0329199   0.06734627], shape=(15,), dtype=float32)\n",
      "[0.55327263 0.58079277]\n",
      "tf.Tensor(\n",
      "[0.7875618  0.77709216 0.83968383 0.7608554  0.6851276  0.7055456\n",
      " 0.7077723  0.7634509  0.49213797 0.47112393 0.40849224 0.30878276\n",
      " 0.23586862 0.19032483 0.104571  ], shape=(15,), dtype=float32)\n",
      "[0.58605266 0.49302219]\n",
      "tf.Tensor(\n",
      "[ 0.77135384  0.77240855  0.76240754  0.7110033   0.7368906   0.73387945\n",
      "  0.7226573   0.69948393  0.53461146  0.4671657   0.29715195  0.20918204\n",
      "  0.28141868  0.21912421 -0.05751437], shape=(15,), dtype=float32)\n",
      "[0.53415447 0.40774971]\n",
      "tf.Tensor(\n",
      "[0.8164045  0.7889183  0.7453783  0.7584136  0.790255   0.69659835\n",
      " 0.55485296 0.5503677  0.4235517  0.59859073 0.27493244 0.30828068\n",
      " 0.2731018  0.03790416 0.07867999], shape=(15,), dtype=float32)\n",
      "[0.43198381 0.39000767]\n",
      "tf.Tensor(\n",
      "[ 0.84704113  0.84706455  0.757801    0.8020106   0.7303452   0.7478122\n",
      "  0.66020226  0.51668745  0.36881408  0.40496936  0.41804153  0.26775324\n",
      "  0.29054877  0.09941671 -0.02390235], shape=(15,), dtype=float32)\n",
      "[0.36699534 0.47671569]\n",
      "tf.Tensor(\n",
      "[0.777419   0.8151371  0.75728863 0.82728755 0.7308517  0.66778195\n",
      " 0.64413494 0.6297771  0.5729615  0.43044972 0.48582512 0.2614562\n",
      " 0.19151399 0.0993737  0.01213963], shape=(15,), dtype=float32)\n",
      "[0.40010066 0.58109861]\n",
      "tf.Tensor(\n",
      "[0.8289774  0.84249175 0.7708501  0.7566673  0.75550324 0.6980426\n",
      " 0.6896482  0.68317795 0.41909352 0.3745316  0.4521397  0.201769\n",
      " 0.19879925 0.13832897 0.14238796], shape=(15,), dtype=float32)\n",
      "[0.49982994 0.61265017]\n",
      "tf.Tensor(\n",
      "[0.78668094 0.8483236  0.76588315 0.74072427 0.69789535 0.76625866\n",
      " 0.5793121  0.65345925 0.49243197 0.5656529  0.30996072 0.2837328\n",
      " 0.3334923  0.18241735 0.21956131], shape=(15,), dtype=float32)\n",
      "[0.57826431 0.55251938]\n",
      "tf.Tensor(\n",
      "[0.7769138  0.7900274  0.787963   0.7225514  0.7607372  0.6719565\n",
      " 0.6733853  0.5608764  0.48230317 0.5253998  0.28176615 0.37010866\n",
      " 0.1746049  0.23498641 0.1128964 ], shape=(15,), dtype=float32)\n",
      "No Success\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "daifa.train_tran = False\n",
    "daifa.train_vae = False\n",
    "\n",
    "daifa.hidden_state = None\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}