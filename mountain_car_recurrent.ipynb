{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run The Agent on Mountain Car"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from vae_recurrent import VAE, create_decoder, create_encoder\n",
    "from transition_gru import TransitionGRU\n",
    "# from recurrent_agent import DAIFAgentRecurrent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from identity_vae import IdentityVAE, identity_encoder, identity_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "def run_episode(mcc_env, agent, obs_max, obs_min, observation_noise_stddev=[0.05, 0.05], action_repeats=6, num_actions_to_execute=2, episode_length=1000):\n",
    "\n",
    "    # arrays to store observations, actions and rewards\n",
    "    all_pre_observations = []\n",
    "    all_post_observations = []\n",
    "    all_action = []\n",
    "    observation_sequence = []\n",
    "    actions_executed = []\n",
    "    agent_scale_observation_sequence = []\n",
    "    reward_sequence = []\n",
    "\n",
    "    # get the first observation from the environment\n",
    "    first_observation, info = mcc_env.reset()\n",
    "    first_observation = np.array([first_observation, 0])\n",
    "\n",
    "    # apply noise to and scaling to first observation\n",
    "    first_observation_noisy = transform_observations(first_observation, obs_max, obs_min, observation_noise_stddev)\n",
    "\n",
    "    # find the first policy\n",
    "    policy_observation = first_observation_noisy\n",
    "    policy = agent.select_policy(policy_observation)\n",
    "\n",
    "    # loop until episode ends or the agent succeeds\n",
    "    t = 0\n",
    "    while True:\n",
    "\n",
    "        # if t % 10 == 0:\n",
    "        #     print(t)\n",
    "\n",
    "        # get the actions from the policy and reshape to desired form\n",
    "        actions = policy.mean()\n",
    "        actions = tf.reshape(actions, (actions.shape[0], agent.tran.action_dim))  # [num_actions, action_dim]\n",
    "        actions = actions.numpy()\n",
    "\n",
    "        # get the actions that we will execute before changing policy\n",
    "        actions_to_execute = actions[0:num_actions_to_execute]\n",
    "\n",
    "        # agent executes policy and gathers observations\n",
    "        for action in actions_to_execute:\n",
    "            for n in range(action_repeats):\n",
    "                observation, reward, done, info = mcc_env.step(action)  # action should be array to satisfy gym requirements\n",
    "\n",
    "                actions_executed.append(action)\n",
    "                observation_sequence.append(observation)\n",
    "                reward_sequence.append(reward)\n",
    "\n",
    "                t += 1\n",
    "                if done:\n",
    "                    if t < 999:\n",
    "                        print(policy.mean())\n",
    "                    return t < 999, agent, t, all_pre_observations, all_post_observations, all_action # the max for the environment\n",
    "\n",
    "        actions_executed = np.array(actions_executed).reshape((len(actions_executed), agent.tran.action_dim))\n",
    "\n",
    "        # scale and add noise to the observation\n",
    "        observation_sequence = transform_observations(observation_sequence, obs_max, obs_min, observation_noise_stddev)\n",
    "\n",
    "        # get the noisy observations for pre and post actions\n",
    "        pre_observation_sequence = np.vstack([policy_observation, observation_sequence[:-1]])\n",
    "        post_action_observation_sequence = observation_sequence\n",
    "\n",
    "        all_pre_observations.append(pre_observation_sequence)\n",
    "        all_post_observations.append(post_action_observation_sequence)\n",
    "        all_action.append(actions_executed)\n",
    "\n",
    "        # print(\"pol\", policy_observation)\n",
    "        # print(\"obs\", observation_sequence)\n",
    "        # print(\"pre\", pre_observation_sequence)\n",
    "        # print(\"post\", post_action_observation_sequence)\n",
    "\n",
    "        # if time to train the agent\n",
    "        agent.train(pre_observation_sequence, post_action_observation_sequence, actions_executed, reward_sequence)\n",
    "\n",
    "        # the new observation we use to select a policy is the last observation in observation_sequences\n",
    "        policy_observation = observation_sequence[-1]\n",
    "\n",
    "        # select a new policy and clear everything\n",
    "        policy = agent.select_policy(policy_observation)\n",
    "\n",
    "        # clear the observations\n",
    "        observation_sequence = []\n",
    "        reward_sequence = []\n",
    "        actions_executed = []\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def train_agent(mcc_env, agent, obs_max, obs_min, observation_noise_stddev, action_repeats, num_actions_to_execute, episode_length=1000, num_episodes=100):\n",
    "\n",
    "    time_to_success = []\n",
    "    did_succeed = []\n",
    "\n",
    "    for n in range(num_episodes):\n",
    "        print(\"Episode\", n+1)\n",
    "        success, agent, t, *rest = run_episode(mcc_env, agent, obs_max, obs_min, observation_noise_stddev, action_repeats=action_repeats, num_actions_to_execute=num_actions_to_execute)\n",
    "\n",
    "        did_succeed.append(success)\n",
    "        time_to_success.append(t)\n",
    "\n",
    "        if success:\n",
    "            print(\"Success in episode\", n+1, \"at time step\", t)\n",
    "        else:\n",
    "            print(\"No Success\")\n",
    "\n",
    "    return agent, did_succeed, time_to_success"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 given_prior_mean,\n",
    "                 given_prior_stddev,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 tran_train_epochs=1,\n",
    "                 vae_train_epochs=1,\n",
    "                 agent_time_ratio=6,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True,\n",
    "                 use_kl_extrinsic=True,\n",
    "                 use_kl_intrinsic=True,\n",
    "                 use_FEEF=True):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        self.prior_model = prior_model\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        self.vae_train_epochs = vae_train_epochs\n",
    "        self.tran_train_epochs = tran_train_epochs\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "\n",
    "        # do we use the kl divergence for extrinsic vs intrinsic\n",
    "        self.use_kl_intrinsic = use_kl_intrinsic\n",
    "        self.use_kl_extrinsic = use_kl_extrinsic\n",
    "\n",
    "        # do we use the FEEF or EFE?\n",
    "        self.use_FEEF = use_FEEF\n",
    "\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.model_vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        # transition\n",
    "        # takes action plus last state and outputs next latent state\n",
    "        self.tran = tran\n",
    "        self.tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        self.hidden_state = None\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(observation)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    def train(self, pre_observations_raw, post_observations_raw, actions_complete, rewards, verbose=0):\n",
    "\n",
    "        # compress the observations based on the agents time compression factor\n",
    "        # pre_observations = pre_observations_raw[::self.agent_time_ratio]  # for example take every 6th element\n",
    "        # post_observations = np.array([post_observations_raw[i] for i in range(len(post_observations_raw)) if i % self.agent_time_ratio == self.agent_time_ratio - 1])\n",
    "        #\n",
    "        # print(pre_observations_raw)\n",
    "        # print(pre_observations)\n",
    "        # print(post_observations_raw)\n",
    "        # print(post_observations)\n",
    "\n",
    "        pre_observations = pre_observations_raw\n",
    "        post_observations = post_observations_raw\n",
    "\n",
    "        #### TRAIN THE TRANSITION MODEL ####\n",
    "        if self.train_tran:\n",
    "            # only look at the first n actions that we took\n",
    "            actions = actions_complete[0: len(pre_observations)]\n",
    "\n",
    "            num_observations = pre_observations.shape[0]\n",
    "            observation_dim = pre_observations.shape[1]\n",
    "            action_dim = actions.shape[1]\n",
    "            # action_dim = 1  # TODO fix this to allow different actions\n",
    "\n",
    "            # find the actual observed latent states using the vae\n",
    "            pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "            post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "            # set up the input training data that we use to train the transition model\n",
    "            z_train = np.concatenate([np.array(pre_latent_mean), np.array(actions)], axis=1)\n",
    "\n",
    "            # we use the sequence to find the right hidden states to use as input\n",
    "            z_train_seq = z_train.reshape((1, num_observations, observation_dim + action_dim))\n",
    "            z_train_singles = z_train.reshape(num_observations, 1, observation_dim + action_dim)\n",
    "\n",
    "            # the previous hidden state is the memory after observing some sequences but it might be None\n",
    "            if self.hidden_state is None:\n",
    "                self.hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.hidden_state))\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran_train_epochs, verbose=verbose)\n",
    "\n",
    "            # now find the new predicted hidden state that we will use for finding the policy\n",
    "            # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "            # _, _, final_hidden_state, _ = self.tran((z_train_seq, self.hidden_state))\n",
    "            _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "            self.hidden_state = final_hidden_state\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            self.model_vae.fit(post_observations, epochs=self.vae_train_epochs, verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by one to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape [self.n_policies, len(z_t_minus_one)]\n",
    "        prev_latent_mean = np.stack([z_t_minus_one]*self.n_policies)\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        if self.use_FEEF:\n",
    "            return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "        else:\n",
    "            return self.EFE(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                # TODO Fix the learned prior model\n",
    "                else:\n",
    "                    prior_dist = self.prior_model()\n",
    "\n",
    "                kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                kl_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    # TODO Find out how this works with the log probability extrinsic term\n",
    "    def EFE(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        EFEs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack(self.given_prior_mean), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack(self.given_prior_stddev), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                # TODO Fix the learned prior model\n",
    "                else:\n",
    "                    prior_dist = self.prior_model()\n",
    "\n",
    "                # compute extrinsic surprisal term\n",
    "                surprisal = -1 * tf.math.log(prior_dist.prob(predicted_likelihood))\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                surprisal = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            EFE = surprisal - kl_intrinsic\n",
    "\n",
    "            EFEs.append(EFE)\n",
    "\n",
    "        return EFEs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91666667 0.5       ]\n"
     ]
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=10000)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, vae, tran, scaled_prior_mean, prior_stddev, planning_horizon=15, use_kl_extrinsic=True, use_kl_intrinsic=True, use_FEEF=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "tf.Tensor(\n",
      "[ 0.76252204  0.7992568   0.8287706   0.6856433   0.68857074  0.452326\n",
      "  0.03881958  0.02461082  0.05587012 -0.02622242  0.09428193  0.08782012\n",
      "  0.12979889  0.33948568  0.25739586], shape=(15,), dtype=float32)\n",
      "Success in episode 1 at time step 166\n",
      "Episode 2\n",
      "tf.Tensor(\n",
      "[0.8579513  0.8458588  0.8119089  0.73444873 0.73463625 0.6057264\n",
      " 0.49812087 0.50137895 0.38753495 0.3531708  0.34940052 0.3907418\n",
      " 0.49389654 0.35820103 0.19256409], shape=(15,), dtype=float32)\n",
      "Success in episode 2 at time step 699\n",
      "Episode 3\n",
      "No Success\n",
      "Episode 4\n",
      "No Success\n",
      "Episode 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=10, action_repeats=6, num_actions_to_execute=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(time_to_success), time_to_success))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the models produced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 1200\n",
    "seq_length = 12\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "ob_seqs = np.array(ob_seqs)\n",
    "next_obs = np.array(next_obs)\n",
    "ob_seqs_flat.shape\n",
    "\n",
    "ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "ob_seqs_flat.shape\n",
    "\n",
    "# ob_seqs = transform_observations(ob_seqs, observation_max, observation_min, [0,0])\n",
    "\n",
    "ob_seqs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ob_seqs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.model_vae(ob_seqs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z = agent.model_vae.encoder(ob_seqs)\n",
    "z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.model_vae.decoder(z[2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Identity VAE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = agent.tran((ob_seqs[0:1], None))\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = ob_seqs[0:1, -1].reshape(1,1,3)\n",
    "h = out[3]\n",
    "h = h[0, -2, :]\n",
    "h = h.numpy().reshape(1,30)\n",
    "h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((t, h))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ob_seqs[0:1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test to see how the agent trains on standard observation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, vae_train_epochs=1, tran_train_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "success, agent, t, pre_obs, post_obs, acts = run_episode(env, daifa, observation_max, observation_min, observation_noise_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_np = np.array(pre_obs)\n",
    "a = np.array(acts)\n",
    "a.shape\n",
    "pre_a = np.concatenate([pre_np, a], axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(a.max(), a.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict = np.array(post_obs)[:, 14, :]\n",
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((pre_a, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine training the model on the observation data\n",
    "\n",
    "Does it eventually converge to a good model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_runs = 1\n",
    "for i in range(num_train_runs):\n",
    "\n",
    "    for j in range(len(pre)):\n",
    "        pre = pre_obs[j]\n",
    "        post = post_obs[j]\n",
    "        actions = acts[j]\n",
    "\n",
    "        daifa.train(pre, post, actions, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation(np.array([0.5, 0.1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the FEEF computations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  0.5]\n"
     ]
    }
   ],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, planning_horizon=15, n_policy_candidates=70, n_policies=1500, n_cem_policy_iterations=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "WARNING:tensorflow:From /Users/Ethan/miniconda3/envs/tf_daif/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-06 15:23:48.777636: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_policy(agent, env, policy, action_repeats):\n",
    "\n",
    "    observation = env.reset()\n",
    "    obs = transform_observations(observation, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    z_t_minus_1 = obs\n",
    "    p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "    p\n",
    "    print(obs)\n",
    "    print(p)\n",
    "\n",
    "    for action in p:\n",
    "        for t in range(action_repeats):\n",
    "            res = env.step(np.array([action]))\n",
    "            print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([0, 0])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p\n",
    "\n",
    "agent.forward_policies(p, z_t_minus_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "test_policy(agent, env, p.numpy(), 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([-0.27691475,  0.01688306])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 given_prior_mean,\n",
    "                 given_prior_stddev,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 tran_train_epochs=1,\n",
    "                 vae_train_epochs=1,\n",
    "                 agent_time_ratio=6,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        self.prior_model = prior_model\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        self.vae_train_epochs = vae_train_epochs\n",
    "        self.tran_train_epochs = tran_train_epochs\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.model_vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        # transition\n",
    "        # takes action plus last state and outputs next latent state\n",
    "        self.tran = tran\n",
    "        self.tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        self.hidden_state = None\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(observation)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    def train(self, pre_observations_raw, post_observations_raw, actions_complete, rewards, verbose=0):\n",
    "\n",
    "        # compress the observations based on the agents time compression factor\n",
    "        # pre_observations = pre_observations_raw[::self.agent_time_ratio]  # for example take every 6th element\n",
    "        # post_observations = np.array([post_observations_raw[i] for i in range(len(post_observations_raw)) if i % self.agent_time_ratio == self.agent_time_ratio - 1])\n",
    "        #\n",
    "        # print(pre_observations_raw)\n",
    "        # print(pre_observations)\n",
    "        # print(post_observations_raw)\n",
    "        # print(post_observations)\n",
    "\n",
    "        pre_observations = pre_observations_raw\n",
    "        post_observations = post_observations_raw\n",
    "\n",
    "        # only look at the first n actions that we took\n",
    "        actions = actions_complete[0: len(pre_observations)]\n",
    "\n",
    "        num_observations = pre_observations.shape[0]\n",
    "        observation_dim = pre_observations.shape[1]\n",
    "        action_dim = actions.shape[1]\n",
    "        # action_dim = 1  # TODO fix this to allow different actions\n",
    "\n",
    "        # find the actual observed latent states using the vae\n",
    "        pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "        post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "        # set up the input training data that we use to train the transition model\n",
    "        z_train = np.concatenate([np.array(pre_latent_mean), np.array(actions)], axis=1)\n",
    "\n",
    "        # we use the sequence to find the right hidden states to use as input\n",
    "        z_train_seq = z_train.reshape((1, num_observations, observation_dim + action_dim))\n",
    "        z_train_singles = z_train.reshape(num_observations, 1, observation_dim + action_dim)\n",
    "\n",
    "        # the previous hidden state is the memory after observing some sequences but it might be None\n",
    "        if self.hidden_state is None:\n",
    "            self.hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "        if self.train_tran:\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.hidden_state))\n",
    "\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran_train_epochs, verbose=verbose)\n",
    "\n",
    "        # now find the new predicted hidden state that we will use for finding the policy\n",
    "        # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "        _, _, final_hidden_state, _ = self.tran((z_train_seq, self.hidden_state))\n",
    "        # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "        self.hidden_state = final_hidden_state\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            self.model_vae.fit(post_observations, epochs=self.vae_train_epochs, verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by one to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        print(z_t_minus_one)\n",
    "        print(mean_best_policies)\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape [self.n_policies, len(z_t_minus_one)]\n",
    "        prev_latent_mean = np.stack([z_t_minus_one]*self.n_policies)\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "            likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "            if self.prior_model is None:\n",
    "\n",
    "                # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                # create the prior distribution\n",
    "                prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "            # TODO Fix the learned prior model\n",
    "            else:\n",
    "                prior_dist = self.prior_model()\n",
    "\n",
    "            kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "            predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "            kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    def EFE(self, policy_posteriors, predicted_likelihood, predicted_posterior):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing with a pretrained transition model\n",
    "\n",
    "This works well! So the problem can't lie with the transition model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0.07]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, train_vae=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "num_seqs = 200\n",
    "seq_length = 500\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "next_obs_stddev = []\n",
    "actions = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.2)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    # train = np.concatenate([o[:-1], a], axis=1)\n",
    "    train = o[:-1]\n",
    "    test = o[1:]\n",
    "\n",
    "    actions.append(a)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "    ob_seqs_stddev = np.ones_like(train)\n",
    "    next_stddev = np.ones_like(test)\n",
    "\n",
    "    next_obs_stddev.append(next_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 1ms/step - kl_loss: 0.1993\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0497\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0132\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0103\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0097\n",
      "6/6 [==============================] - 0s 1ms/step - kl_loss: 0.0108\n",
      "16/16 [==============================] - 0s 967us/step - kl_loss: 0.0029\n",
      "16/16 [==============================] - 0s 940us/step - kl_loss: 0.0020\n",
      "15/15 [==============================] - 0s 882us/step - kl_loss: 0.0047\n",
      "16/16 [==============================] - 0s 853us/step - kl_loss: 0.0054\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 0.0074\n",
      "16/16 [==============================] - 0s 943us/step - kl_loss: 0.0113\n",
      "8/8 [==============================] - 0s 920us/step - kl_loss: 0.0256\n",
      "16/16 [==============================] - 0s 891us/step - kl_loss: 0.0049\n",
      "16/16 [==============================] - 0s 923us/step - kl_loss: 0.0037\n",
      "16/16 [==============================] - 0s 905us/step - kl_loss: 0.0038\n",
      "16/16 [==============================] - 0s 860us/step - kl_loss: 0.0033\n",
      "14/14 [==============================] - 0s 896us/step - kl_loss: 0.0033\n",
      "16/16 [==============================] - 0s 928us/step - kl_loss: 0.0012\n",
      "7/7 [==============================] - 0s 964us/step - kl_loss: 0.0033\n",
      "16/16 [==============================] - 0s 853us/step - kl_loss: 6.2965e-04\n",
      "16/16 [==============================] - 0s 895us/step - kl_loss: 3.8473e-04\n",
      "16/16 [==============================] - 0s 886us/step - kl_loss: 9.0204e-04\n",
      "16/16 [==============================] - 0s 866us/step - kl_loss: 3.5106e-04\n",
      "7/7 [==============================] - 0s 948us/step - kl_loss: 0.0014\n",
      "16/16 [==============================] - 0s 921us/step - kl_loss: 0.0011\n",
      "16/16 [==============================] - 0s 897us/step - kl_loss: 2.8698e-04\n",
      "16/16 [==============================] - 0s 918us/step - kl_loss: 3.7622e-04\n",
      "12/12 [==============================] - 0s 979us/step - kl_loss: 7.3435e-04\n",
      "16/16 [==============================] - 0s 872us/step - kl_loss: 6.9894e-04\n",
      "16/16 [==============================] - 0s 845us/step - kl_loss: 3.9676e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 9.3522e-04\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 5.7943e-04\n",
      "16/16 [==============================] - 0s 861us/step - kl_loss: 3.5893e-04\n",
      "16/16 [==============================] - 0s 927us/step - kl_loss: 4.9170e-04\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 2.7267e-04\n",
      "16/16 [==============================] - 0s 838us/step - kl_loss: 1.5856e-04\n",
      "6/6 [==============================] - 0s 1ms/step - kl_loss: 9.0418e-04\n",
      "16/16 [==============================] - 0s 886us/step - kl_loss: 5.7081e-04\n",
      "12/12 [==============================] - 0s 907us/step - kl_loss: 4.9615e-04\n",
      "16/16 [==============================] - 0s 878us/step - kl_loss: 6.9546e-04\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 2.8895e-04\n",
      "13/13 [==============================] - 0s 899us/step - kl_loss: 7.5522e-04\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 6.0688e-04\n",
      "11/11 [==============================] - 0s 921us/step - kl_loss: 5.8398e-04\n",
      "9/9 [==============================] - 0s 976us/step - kl_loss: 0.0012\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 6.1929e-04\n",
      "16/16 [==============================] - 0s 836us/step - kl_loss: 2.2836e-04\n",
      "10/10 [==============================] - 0s 916us/step - kl_loss: 5.1460e-04\n",
      "16/16 [==============================] - 0s 867us/step - kl_loss: 4.9099e-04\n",
      "16/16 [==============================] - 0s 858us/step - kl_loss: 3.6082e-04\n",
      "16/16 [==============================] - 0s 889us/step - kl_loss: 3.1299e-04\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 8.5156e-04\n",
      "16/16 [==============================] - 0s 921us/step - kl_loss: 0.0018\n",
      "16/16 [==============================] - 0s 852us/step - kl_loss: 0.0030\n",
      "10/10 [==============================] - 0s 915us/step - kl_loss: 0.0085\n",
      "16/16 [==============================] - 0s 897us/step - kl_loss: 0.0071\n",
      "16/16 [==============================] - 0s 917us/step - kl_loss: 0.0035\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 0.0035\n",
      "16/16 [==============================] - 0s 890us/step - kl_loss: 0.0026\n",
      "16/16 [==============================] - 0s 882us/step - kl_loss: 0.0020\n",
      "7/7 [==============================] - 0s 966us/step - kl_loss: 0.0028\n",
      "16/16 [==============================] - 0s 896us/step - kl_loss: 0.0013\n",
      "12/12 [==============================] - 0s 911us/step - kl_loss: 0.0014\n",
      "14/14 [==============================] - 0s 918us/step - kl_loss: 0.0015\n",
      "16/16 [==============================] - 0s 880us/step - kl_loss: 8.1093e-04\n",
      "16/16 [==============================] - 0s 859us/step - kl_loss: 5.0124e-04\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 2.6348e-04\n",
      "8/8 [==============================] - 0s 976us/step - kl_loss: 5.0341e-04\n",
      "10/10 [==============================] - 0s 871us/step - kl_loss: 4.9221e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 2.3029e-04\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 6.5155e-05\n",
      "10/10 [==============================] - 0s 918us/step - kl_loss: 5.9321e-04\n",
      "16/16 [==============================] - 0s 912us/step - kl_loss: 3.2684e-04\n",
      "16/16 [==============================] - 0s 874us/step - kl_loss: 1.7592e-04\n",
      "16/16 [==============================] - 0s 863us/step - kl_loss: 1.6582e-04\n",
      "16/16 [==============================] - 0s 864us/step - kl_loss: 1.8564e-04\n",
      "16/16 [==============================] - 0s 925us/step - kl_loss: 1.4003e-04\n",
      "16/16 [==============================] - 0s 915us/step - kl_loss: 1.7503e-04\n",
      "12/12 [==============================] - 0s 975us/step - kl_loss: 2.7620e-04\n",
      "16/16 [==============================] - 0s 935us/step - kl_loss: 2.7183e-04\n",
      "11/11 [==============================] - 0s 942us/step - kl_loss: 4.2113e-04\n",
      "16/16 [==============================] - 0s 867us/step - kl_loss: 4.4761e-04\n",
      "12/12 [==============================] - 0s 916us/step - kl_loss: 2.7003e-04\n",
      "13/13 [==============================] - 0s 886us/step - kl_loss: 5.0503e-04\n",
      "12/12 [==============================] - 0s 891us/step - kl_loss: 4.9012e-04\n",
      "16/16 [==============================] - 0s 888us/step - kl_loss: 2.4893e-04\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 1.1987e-04\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 2.8243e-04\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 1.1809e-04\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 7.0822e-05\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 6.8286e-05\n",
      "16/16 [==============================] - 0s 900us/step - kl_loss: 2.7148e-04\n",
      "15/15 [==============================] - 0s 960us/step - kl_loss: 1.1328e-04\n",
      "16/16 [==============================] - 0s 859us/step - kl_loss: 1.6294e-04\n",
      "16/16 [==============================] - 0s 899us/step - kl_loss: 9.3267e-05\n",
      "13/13 [==============================] - 0s 930us/step - kl_loss: 2.7863e-04\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 3.4847e-04\n",
      "16/16 [==============================] - 0s 909us/step - kl_loss: 6.9290e-05\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 2.5452e-04\n",
      "16/16 [==============================] - 0s 904us/step - kl_loss: 2.0722e-04\n",
      "16/16 [==============================] - 0s 903us/step - kl_loss: 1.6057e-04\n",
      "16/16 [==============================] - 0s 996us/step - kl_loss: 1.6325e-04\n",
      "10/10 [==============================] - 0s 981us/step - kl_loss: 2.9550e-04\n",
      "16/16 [==============================] - 0s 905us/step - kl_loss: 2.2208e-04\n",
      "16/16 [==============================] - 0s 880us/step - kl_loss: 1.4792e-04\n",
      "14/14 [==============================] - 0s 876us/step - kl_loss: 2.6079e-04\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 2.2629e-04\n",
      "16/16 [==============================] - 0s 927us/step - kl_loss: 1.7176e-04\n",
      "16/16 [==============================] - 0s 863us/step - kl_loss: 7.7746e-05\n",
      "16/16 [==============================] - 0s 907us/step - kl_loss: 4.9998e-05\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 9.7333e-05\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 9.3879e-06\n",
      "16/16 [==============================] - 0s 871us/step - kl_loss: 3.5369e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 6.1430e-05\n",
      "16/16 [==============================] - 0s 913us/step - kl_loss: 3.4472e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 4.7960e-05\n",
      "11/11 [==============================] - 0s 988us/step - kl_loss: 1.9969e-04\n",
      "16/16 [==============================] - 0s 886us/step - kl_loss: 1.3500e-04\n",
      "9/9 [==============================] - 0s 959us/step - kl_loss: 1.3400e-04\n",
      "15/15 [==============================] - 0s 927us/step - kl_loss: 1.8833e-04\n",
      "16/16 [==============================] - 0s 889us/step - kl_loss: 1.6533e-04\n",
      "16/16 [==============================] - 0s 855us/step - kl_loss: 4.5978e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 6.0070e-05\n",
      "16/16 [==============================] - 0s 878us/step - kl_loss: 1.2162e-04\n",
      "16/16 [==============================] - 0s 942us/step - kl_loss: 9.9278e-05\n",
      "16/16 [==============================] - 0s 919us/step - kl_loss: 6.8983e-05\n",
      "16/16 [==============================] - 0s 935us/step - kl_loss: 3.9110e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 3.2605e-05\n",
      "16/16 [==============================] - 0s 908us/step - kl_loss: 5.6698e-05\n",
      "16/16 [==============================] - 0s 912us/step - kl_loss: 4.7928e-05\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 1.7173e-04\n",
      "10/10 [==============================] - 0s 866us/step - kl_loss: 1.5600e-04\n",
      "16/16 [==============================] - 0s 848us/step - kl_loss: 1.7683e-04\n",
      "10/10 [==============================] - 0s 940us/step - kl_loss: 1.8164e-04\n",
      "10/10 [==============================] - 0s 920us/step - kl_loss: 2.5693e-04\n",
      "16/16 [==============================] - 0s 887us/step - kl_loss: 1.4039e-04\n",
      "16/16 [==============================] - 0s 869us/step - kl_loss: 3.1678e-04\n",
      "16/16 [==============================] - 0s 877us/step - kl_loss: 2.0544e-04\n",
      "16/16 [==============================] - 0s 910us/step - kl_loss: 1.3164e-04\n",
      "16/16 [==============================] - 0s 920us/step - kl_loss: 3.7877e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 1.7913e-05\n",
      "16/16 [==============================] - 0s 896us/step - kl_loss: 8.9225e-06\n",
      "16/16 [==============================] - 0s 894us/step - kl_loss: 2.8919e-05\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 8.2512e-05\n",
      "16/16 [==============================] - 0s 852us/step - kl_loss: 6.1667e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 6.0950e-05\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 4.6845e-05\n",
      "16/16 [==============================] - 0s 872us/step - kl_loss: 1.2996e-04\n",
      "16/16 [==============================] - 0s 934us/step - kl_loss: 8.1739e-05\n",
      "11/11 [==============================] - 0s 936us/step - kl_loss: 1.4202e-04\n",
      "16/16 [==============================] - 0s 919us/step - kl_loss: 1.3208e-04\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 2.5416e-05\n",
      "16/16 [==============================] - 0s 879us/step - kl_loss: 2.3686e-04\n",
      "16/16 [==============================] - 0s 893us/step - kl_loss: 8.3754e-05\n",
      "16/16 [==============================] - 0s 910us/step - kl_loss: 5.5621e-05\n",
      "16/16 [==============================] - 0s 908us/step - kl_loss: 4.7069e-05\n",
      "16/16 [==============================] - 0s 863us/step - kl_loss: 3.4773e-05\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 6.9179e-05\n",
      "16/16 [==============================] - 0s 892us/step - kl_loss: 5.2090e-05\n",
      "14/14 [==============================] - 0s 910us/step - kl_loss: 2.1028e-04\n",
      "9/9 [==============================] - 0s 919us/step - kl_loss: 3.0514e-04\n",
      "16/16 [==============================] - 0s 907us/step - kl_loss: 1.1390e-04\n",
      "16/16 [==============================] - 0s 879us/step - kl_loss: 2.2734e-05\n",
      "16/16 [==============================] - 0s 883us/step - kl_loss: 2.9648e-05\n",
      "16/16 [==============================] - 0s 841us/step - kl_loss: 4.2551e-04\n",
      "16/16 [==============================] - 0s 858us/step - kl_loss: 8.8995e-05\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 2.1523e-04\n",
      "16/16 [==============================] - 0s 878us/step - kl_loss: 2.3479e-04\n",
      "16/16 [==============================] - 0s 884us/step - kl_loss: 3.5033e-05\n",
      "16/16 [==============================] - 0s 893us/step - kl_loss: 2.2248e-05\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 2.9066e-05\n",
      "16/16 [==============================] - 0s 865us/step - kl_loss: 2.1744e-05\n",
      "11/11 [==============================] - 0s 894us/step - kl_loss: 6.2629e-05\n",
      "16/16 [==============================] - 0s 853us/step - kl_loss: 2.1039e-04\n",
      "16/16 [==============================] - 0s 911us/step - kl_loss: 7.2563e-05\n",
      "16/16 [==============================] - 0s 911us/step - kl_loss: 7.8947e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 1.6788e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 3.6060e-04\n",
      "16/16 [==============================] - 0s 871us/step - kl_loss: 7.9603e-05\n",
      "16/16 [==============================] - 0s 888us/step - kl_loss: 3.5523e-05\n",
      "16/16 [==============================] - 0s 905us/step - kl_loss: 1.2547e-04\n",
      "16/16 [==============================] - 0s 884us/step - kl_loss: 2.6244e-04\n",
      "8/8 [==============================] - 0s 905us/step - kl_loss: 1.9605e-04\n",
      "16/16 [==============================] - 0s 880us/step - kl_loss: 7.6105e-05\n",
      "16/16 [==============================] - 0s 904us/step - kl_loss: 1.9218e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 1.0097e-04\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 1.8222e-04\n",
      "16/16 [==============================] - 0s 913us/step - kl_loss: 5.0225e-05\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 1.4844e-04\n",
      "12/12 [==============================] - 0s 916us/step - kl_loss: 7.4900e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 8.2255e-05\n",
      "7/7 [==============================] - 0s 998us/step - kl_loss: 1.0713e-04\n",
      "16/16 [==============================] - 0s 877us/step - kl_loss: 8.7325e-05\n",
      "16/16 [==============================] - 0s 884us/step - kl_loss: 2.1384e-05\n",
      "16/16 [==============================] - 0s 890us/step - kl_loss: 6.9913e-05\n",
      "16/16 [==============================] - 0s 861us/step - kl_loss: 4.8478e-05\n",
      "16/16 [==============================] - 0s 858us/step - kl_loss: 5.1530e-05\n",
      "16/16 [==============================] - 0s 928us/step - kl_loss: 9.6592e-06\n",
      "13/13 [==============================] - 0s 938us/step - kl_loss: 9.6443e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_seqs):\n",
    "\n",
    "    pre = ob_seqs[i]\n",
    "    next = next_obs[i]\n",
    "    acts = actions[i]\n",
    "\n",
    "    next_sd = next_obs_stddev[i]\n",
    "\n",
    "    daifa.train(pre, next, acts, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "data": {
      "text/plain": "(20, 5, 3)"
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 150\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    test = o[-1]\n",
    "\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "ob_seqs = np.array(ob_seqs)[:, -5:, :]\n",
    "next_obs = np.array(next_obs)\n",
    "ob_seqs.shape\n",
    "\n",
    "ob_seqs_stddev = np.ones_like(ob_seqs)\n",
    "next_obs_stddev = np.ones_like(next_obs)\n",
    "\n",
    "ob_seqs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: shape=(20, 2), dtype=float32, numpy=\n array([[0.42757505, 0.6529479 ],\n        [0.38936174, 0.5224733 ],\n        [0.5177275 , 0.5719767 ],\n        [0.32569912, 0.5357033 ],\n        [0.5789795 , 0.37268484],\n        [0.50462687, 0.16914466],\n        [0.10576638, 0.2804633 ],\n        [0.50895584, 0.26160803],\n        [0.84278893, 0.7405859 ],\n        [0.41768757, 0.8168086 ],\n        [0.6252694 , 0.524398  ],\n        [0.29302755, 0.6640839 ],\n        [0.7982165 , 0.4708082 ],\n        [0.0011582 , 0.43203047],\n        [0.15381938, 0.3239351 ],\n        [0.51816386, 0.7722647 ],\n        [0.4246617 , 0.18837008],\n        [0.62402654, 0.48184252],\n        [0.6900874 , 0.6458197 ],\n        [0.48874134, 0.24390756]], dtype=float32)>,\n <tf.Tensor: shape=(20, 2), dtype=float32, numpy=\n array([[1.0015444 , 1.0040803 ],\n        [1.0029435 , 1.0013554 ],\n        [1.00249   , 1.0037054 ],\n        [1.0025867 , 1.0013175 ],\n        [1.0037783 , 1.0024655 ],\n        [1.0045325 , 0.9992334 ],\n        [1.0020174 , 1.0017799 ],\n        [1.0041207 , 1.0012053 ],\n        [1.0033425 , 1.0089743 ],\n        [1.0012552 , 1.0062354 ],\n        [1.0033436 , 1.0037761 ],\n        [1.0008956 , 1.0037417 ],\n        [1.0051221 , 1.0040122 ],\n        [1.0008185 , 1.0032822 ],\n        [1.0018778 , 1.0018895 ],\n        [0.99936193, 1.0098908 ],\n        [1.0041473 , 1.001115  ],\n        [1.0043585 , 1.0022985 ],\n        [1.0025626 , 1.0072029 ],\n        [1.0040324 , 1.0017579 ]], dtype=float32)>,\n <tf.Tensor: shape=(20, 60), dtype=float32, numpy=\n array([[ 0.15026245,  0.02147635,  0.08431074, ..., -0.10982125,\n          0.05334226,  0.06565161],\n        [ 0.1892961 ,  0.0171166 ,  0.14129707, ..., -0.02967911,\n          0.19479504,  0.10307508],\n        [ 0.1276658 ,  0.0259374 ,  0.09705178, ..., -0.09186851,\n          0.05516341,  0.06683648],\n        ...,\n        [ 0.12049596,  0.0347067 ,  0.12938446, ..., -0.06724519,\n          0.1028895 ,  0.08167858],\n        [ 0.067205  ,  0.0587694 ,  0.05941856, ..., -0.19308257,\n         -0.05885605,  0.03636646],\n        [-0.04927346,  0.04336642,  0.01616201, ..., -0.09991126,\n         -0.13079813,  0.00147478]], dtype=float32)>,\n <tf.Tensor: shape=(20, 5, 60), dtype=float32, numpy=\n array([[[ 0.04525499, -0.01716381,  0.02350051, ..., -0.04238965,\n           0.01428884,  0.0045097 ],\n         [ 0.09317333, -0.00512476,  0.05376579, ..., -0.07720339,\n           0.02760326,  0.0271852 ],\n         [ 0.12649374,  0.00985319,  0.0731228 , ..., -0.09722769,\n           0.03932348,  0.04809814],\n         [ 0.14352977,  0.01830245,  0.08136631, ..., -0.10614912,\n           0.04794318,  0.06033898],\n         [ 0.15026245,  0.02147635,  0.08431074, ..., -0.10982125,\n           0.05334226,  0.06565161]],\n \n        [[ 0.06360728, -0.0279705 ,  0.04693291, ...,  0.01876127,\n           0.07270339,  0.02625261],\n         [ 0.12016988, -0.01528889,  0.09293082, ...,  0.00722509,\n           0.12378819,  0.05685403],\n         [ 0.15799734,  0.00201168,  0.12204836, ..., -0.00917845,\n           0.15883154,  0.08118919],\n         [ 0.17884886,  0.0125097 ,  0.13581392, ..., -0.02153258,\n           0.1813325 ,  0.09584883],\n         [ 0.1892961 ,  0.0171166 ,  0.14129707, ..., -0.02967911,\n           0.19479504,  0.10307508]],\n \n        [[ 0.01899024, -0.00537003,  0.01712187, ..., -0.05791149,\n          -0.00749379, -0.00342179],\n         [ 0.05159665,  0.01008601,  0.04413131, ..., -0.09687613,\n          -0.01088502,  0.01538653],\n         [ 0.09476812,  0.01297409,  0.07409859, ..., -0.08821176,\n           0.01896192,  0.04428795],\n         [ 0.11822307,  0.02161739,  0.09018441, ..., -0.08948812,\n           0.0409235 ,  0.05975568],\n         [ 0.1276658 ,  0.0259374 ,  0.09705178, ..., -0.09186851,\n           0.05516341,  0.06683648]],\n \n        ...,\n \n        [[-0.01287574,  0.00913438,  0.01037965, ..., -0.07211688,\n          -0.03120288, -0.01136945],\n         [ 0.04945046, -0.00161371,  0.06758605, ..., -0.04407822,\n           0.02909646,  0.0303868 ],\n         [ 0.09147461,  0.0191132 ,  0.10595157, ..., -0.05436005,\n           0.06481314,  0.0583218 ],\n         [ 0.11274984,  0.03069214,  0.12332863, ..., -0.0628503 ,\n           0.08860222,  0.07462525],\n         [ 0.12049596,  0.0347067 ,  0.12938446, ..., -0.06724519,\n           0.1028895 ,  0.08167858]],\n \n        [[ 0.01436141, -0.00646611,  0.01604711, ..., -0.07796118,\n          -0.01446355, -0.00974533],\n         [ 0.05152211,  0.0144128 ,  0.04881622, ..., -0.12986496,\n          -0.0225791 ,  0.01168774],\n         [ 0.08065747,  0.03350948,  0.06986689, ..., -0.15249343,\n          -0.02346765,  0.03437518],\n         [ 0.09257582,  0.04136415,  0.07716379, ..., -0.15684438,\n          -0.02214691,  0.04622274],\n         [ 0.067205  ,  0.0587694 ,  0.05941856, ..., -0.19308257,\n          -0.05885605,  0.03636646]],\n \n        [[-0.04124852,  0.02623192, -0.00106705, ..., -0.07540748,\n          -0.05910962, -0.01571499],\n         [-0.0516727 ,  0.0423548 ,  0.00782672, ..., -0.10877656,\n          -0.10001399, -0.01105171],\n         [-0.05289169,  0.04889357,  0.01271122, ..., -0.11655216,\n          -0.12273175, -0.00386084],\n         [-0.04969812,  0.04598803,  0.01593486, ..., -0.10666266,\n          -0.12809363,  0.0011813 ],\n         [-0.04927346,  0.04336642,  0.01616201, ..., -0.09991126,\n          -0.13079813,  0.00147478]]], dtype=float32)>]"
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daifa.tran((ob_seqs, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.41424149, 0.64817536],\n       [0.37549612, 0.51434882],\n       [0.50395287, 0.56238058],\n       [0.31282919, 0.53150581],\n       [0.56830029, 0.36537232],\n       [0.49751557, 0.15729226],\n       [0.10200014, 0.28023718],\n       [0.49930485, 0.25297206],\n       [0.83264549, 0.76498072],\n       [0.40183171, 0.8195579 ],\n       [0.61205382, 0.51735449],\n       [0.28334383, 0.66585968],\n       [0.78955368, 0.48945753],\n       [0.        , 0.5       ],\n       [0.15014838, 0.32678897],\n       [0.5064109 , 0.78005801],\n       [0.41774712, 0.18253206],\n       [0.61022634, 0.47224011],\n       [0.67659513, 0.6480953 ],\n       [0.48181082, 0.23722683]])"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_obs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks fantastic!!! With enough data the transition model is training very well"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 60), dtype=float32, numpy=\narray([[ 0.1668222 ,  0.05080901,  0.20015422,  0.38899958, -0.08546768,\n        -0.04040675,  0.03512116,  0.00171472,  0.03852477,  0.20108685,\n        -0.03330585,  0.01087453, -0.03944991, -0.23539615,  0.19884759,\n         0.15129937,  0.08765514,  0.15757117, -0.16009761, -0.02254741,\n        -0.17335685, -0.09706004,  0.05607434,  0.03711884, -0.0560054 ,\n        -0.27313083,  0.02705026,  0.14458522, -0.25310335, -0.08086976,\n        -0.10635097, -0.28293777,  0.00502296,  0.2793439 ,  0.07475004,\n        -0.09199525, -0.23762226,  0.05454395, -0.07554322, -0.06423084,\n         0.11491245,  0.03344171, -0.03258195,  0.04890673,  0.07888647,\n         0.11464167,  0.31568897,  0.01460155, -0.23916677,  0.24096602,\n         0.1589966 ,  0.0215495 , -0.38883814,  0.2073881 ,  0.17495394,\n         0.30218056, -0.14856315, -0.09490789,  0.20044254,  0.12068783]],\n      dtype=float32)>"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daifa.hidden_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.5]\n",
      "tf.Tensor(\n",
      "[ 0.7089493   0.76423895  0.8047202   0.8124183   0.73879427  0.76978123\n",
      "  0.61311316  0.5821078   0.4306626   0.40520254  0.17977683  0.27913105\n",
      " -0.00298302  0.03768509  0.0850338 ], shape=(15,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(15,), dtype=float32, numpy=\narray([ 0.7089493 ,  0.76423895,  0.8047202 ,  0.8124183 ,  0.73879427,\n        0.76978123,  0.61311316,  0.5821078 ,  0.4306626 ,  0.40520254,\n        0.17977683,  0.27913105, -0.00298302,  0.03768509,  0.0850338 ],\n      dtype=float32)>"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_t_minus_1 = np.array([0.4, 0.5])\n",
    "daifa.hidden_state = None\n",
    "p, s = daifa.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.17485519, 0.28052184]], dtype=float32)>,\n <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.998175 , 0.9848511]], dtype=float32)>,\n <tf.Tensor: shape=(1, 60), dtype=float32, numpy=\n array([[ 0.06734794, -0.03086371,  0.04919352,  0.06776053,  0.05020184,\n         -0.02855486,  0.05532712,  0.00621268,  0.03898622,  0.06404883,\n         -0.06092996,  0.02080428, -0.03182369, -0.09051668,  0.02938318,\n          0.02920253,  0.09898859,  0.04242412, -0.0937261 , -0.07689307,\n         -0.04255384, -0.01035933,  0.0217123 ,  0.0560016 , -0.06113841,\n         -0.08259731, -0.04008626,  0.02852438, -0.09232952, -0.09844272,\n         -0.09578703, -0.0808928 ,  0.07631816,  0.10538951,  0.03266784,\n         -0.06981869, -0.13518177, -0.00443301, -0.0766279 ,  0.03137437,\n          0.10195947,  0.03933517,  0.01060682, -0.02867689,  0.05144734,\n          0.02094595,  0.07122529,  0.09150924, -0.11927285,  0.11849919,\n          0.06474774, -0.00642365, -0.15364884,  0.10189001,  0.09677684,\n          0.10298578,  0.00337658,  0.01931385,  0.07793737,  0.02707184]],\n       dtype=float32)>,\n <tf.Tensor: shape=(1, 1, 60), dtype=float32, numpy=\n array([[[ 0.06734794, -0.03086371,  0.04919352,  0.06776053,\n           0.05020184, -0.02855486,  0.05532712,  0.00621268,\n           0.03898622,  0.06404883, -0.06092996,  0.02080428,\n          -0.03182369, -0.09051668,  0.02938318,  0.02920253,\n           0.09898859,  0.04242412, -0.0937261 , -0.07689307,\n          -0.04255384, -0.01035933,  0.0217123 ,  0.0560016 ,\n          -0.06113841, -0.08259731, -0.04008626,  0.02852438,\n          -0.09232952, -0.09844272, -0.09578703, -0.0808928 ,\n           0.07631816,  0.10538951,  0.03266784, -0.06981869,\n          -0.13518177, -0.00443301, -0.0766279 ,  0.03137437,\n           0.10195947,  0.03933517,  0.01060682, -0.02867689,\n           0.05144734,  0.02094595,  0.07122529,  0.09150924,\n          -0.11927285,  0.11849919,  0.06474774, -0.00642365,\n          -0.15364884,  0.10189001,  0.09677684,  0.10298578,\n           0.00337658,  0.01931385,  0.07793737,  0.02707184]]],\n       dtype=float32)>]"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daifa.tran((np.array([[[0.4, 0.5, 1]]]), None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[0.3531864 0.5      ]\n",
      "tf.Tensor(\n",
      "[0.75776505 0.808785   0.823482   0.7384236  0.66807634 0.75550365\n",
      " 0.65706307 0.57430935 0.44249344 0.50059706 0.30967107 0.3025037\n",
      " 0.23872639 0.13167822 0.03006317], shape=(15,), dtype=float32)\n",
      "[0.41072467 0.60575192]\n",
      "tf.Tensor(\n",
      "[0.80091256 0.831186   0.80336916 0.743643   0.67225486 0.7388898\n",
      " 0.6710419  0.49252346 0.45496187 0.39570916 0.33172902 0.4691094\n",
      " 0.18269321 0.02243686 0.13316137], shape=(15,), dtype=float32)\n",
      "[0.52321638 0.6161014 ]\n",
      "tf.Tensor(\n",
      "[0.7582315  0.7428887  0.78753793 0.70738727 0.6904973  0.6947737\n",
      " 0.7184091  0.63121426 0.51972836 0.44251725 0.4724567  0.35061508\n",
      " 0.20868196 0.29411447 0.1608929 ], shape=(15,), dtype=float32)\n",
      "[0.59332169 0.53282979]\n",
      "tf.Tensor(\n",
      "[ 0.8261601   0.7788568   0.7498009   0.68705684  0.71283436  0.7625619\n",
      "  0.6251273   0.59106356  0.47288027  0.37912145  0.37760547  0.27614397\n",
      "  0.17472365  0.0352636  -0.06415275], shape=(15,), dtype=float32)\n",
      "[0.57653528 0.43853916]\n",
      "tf.Tensor(\n",
      "[ 0.84993833  0.838954    0.836631    0.8009051   0.6381009   0.67451525\n",
      "  0.66360396  0.7313162   0.44090325  0.61107373  0.4429256   0.2499801\n",
      "  0.18311256  0.00455586 -0.03917548], shape=(15,), dtype=float32)\n",
      "[0.48622649 0.38311274]\n",
      "tf.Tensor(\n",
      "[0.8081694  0.80967754 0.7922341  0.7968606  0.7375868  0.6537518\n",
      " 0.64383316 0.62586427 0.55786794 0.3814907  0.40052238 0.48054183\n",
      " 0.37376764 0.13650063 0.09636682], shape=(15,), dtype=float32)\n",
      "[0.38618095 0.41815763]\n",
      "tf.Tensor(\n",
      "[0.808529   0.80657184 0.7916462  0.70533156 0.7616304  0.6811054\n",
      " 0.6398279  0.49607593 0.48863018 0.42573503 0.4255617  0.5032834\n",
      " 0.25781015 0.1669854  0.03199732], shape=(15,), dtype=float32)\n",
      "[0.36487361 0.53167849]\n",
      "tf.Tensor(\n",
      "[ 0.80258495  0.86818594  0.71748924  0.8466585   0.7656668   0.6802341\n",
      "  0.7175385   0.55456597  0.56965184  0.48494464  0.4133271   0.35727996\n",
      "  0.29771766  0.2542887  -0.04310569], shape=(15,), dtype=float32)\n",
      "[0.44369857 0.61764689]\n",
      "tf.Tensor(\n",
      "[0.8110079  0.6923667  0.79028475 0.6758299  0.71477115 0.7232198\n",
      " 0.6768123  0.6062628  0.5514061  0.47747606 0.4313433  0.3517048\n",
      " 0.39273486 0.17666398 0.08394014], shape=(15,), dtype=float32)\n",
      "[0.54803135 0.58811021]\n",
      "tf.Tensor(\n",
      "[0.77069455 0.7971718  0.76023203 0.76838154 0.70611453 0.7311224\n",
      " 0.6250644  0.6438176  0.47762975 0.5243256  0.41966495 0.36352235\n",
      " 0.40499055 0.01003213 0.061273  ], shape=(15,), dtype=float32)\n",
      "[0.58816123 0.5017748 ]\n",
      "tf.Tensor(\n",
      "[ 0.82042825  0.80002797  0.74095577  0.71816117  0.73239034  0.7092903\n",
      "  0.72302485  0.62576604  0.5700318   0.4367064   0.43498516  0.47891188\n",
      "  0.16538338  0.18202068 -0.19701585], shape=(15,), dtype=float32)\n",
      "[0.54586323 0.41759907]\n",
      "tf.Tensor(\n",
      "[0.7925538  0.797785   0.7362157  0.7193825  0.72075266 0.6847715\n",
      " 0.63142014 0.52450913 0.6858361  0.4741538  0.46578348 0.35045347\n",
      " 0.4875199  0.10972673 0.30347246], shape=(15,), dtype=float32)\n",
      "[0.44605739 0.38614578]\n",
      "tf.Tensor(\n",
      "[0.7413631  0.7655986  0.7876091  0.7940088  0.70736146 0.6543615\n",
      " 0.6855993  0.6544356  0.58999455 0.48650116 0.347798   0.12228234\n",
      " 0.24543785 0.20780389 0.03425851], shape=(15,), dtype=float32)\n",
      "[0.36494746 0.45144458]\n",
      "tf.Tensor(\n",
      "[0.8217085  0.73286784 0.7572981  0.7622988  0.7463897  0.71150637\n",
      " 0.5872628  0.6521302  0.51973563 0.41864827 0.4010417  0.27743676\n",
      " 0.38145977 0.16709988 0.10859277], shape=(15,), dtype=float32)\n",
      "[0.3799972  0.56624843]\n",
      "tf.Tensor(\n",
      "[0.7157361  0.8330341  0.7970764  0.7716476  0.79019064 0.6825232\n",
      " 0.58022153 0.49948868 0.45807794 0.5443946  0.4094528  0.3813852\n",
      " 0.3188499  0.09375567 0.2158043 ], shape=(15,), dtype=float32)\n",
      "[0.47352085 0.61658074]\n",
      "tf.Tensor(\n",
      "[0.7780647  0.80061483 0.722193   0.74317825 0.7437629  0.68641585\n",
      " 0.5962243  0.50690097 0.65318114 0.52816546 0.38895717 0.27855322\n",
      " 0.25044137 0.2727542  0.06080603], shape=(15,), dtype=float32)\n",
      "[0.56438515 0.56986869]\n",
      "tf.Tensor(\n",
      "[0.7811205  0.7339901  0.81370497 0.72263205 0.73430645 0.629454\n",
      " 0.6017497  0.6197657  0.56409466 0.43045133 0.26490685 0.2498877\n",
      " 0.31837606 0.00660494 0.18230386], shape=(15,), dtype=float32)\n",
      "[0.58373286 0.47584327]\n",
      "tf.Tensor(\n",
      "[ 0.786635    0.7676723   0.78109056  0.7734856   0.7349278   0.6434932\n",
      "  0.64729017  0.6477715   0.46817616  0.5103125   0.3484181   0.32719758\n",
      "  0.19347914  0.08783682 -0.12831853], shape=(15,), dtype=float32)\n",
      "[0.51849457 0.39698744]\n",
      "tf.Tensor(\n",
      "[0.7013822  0.82110566 0.8151903  0.83248204 0.69641477 0.7037499\n",
      " 0.65221685 0.62713647 0.6174364  0.46683326 0.47702065 0.45662794\n",
      " 0.22590521 0.12989476 0.18857364], shape=(15,), dtype=float32)\n",
      "[0.41009074 0.39329548]\n",
      "tf.Tensor(\n",
      "[0.78286713 0.75951535 0.79978824 0.7267528  0.71680486 0.65614986\n",
      " 0.6204901  0.54773957 0.50284386 0.5718347  0.25679275 0.19043566\n",
      " 0.2571739  0.09063831 0.13465643], shape=(15,), dtype=float32)\n",
      "[0.35496967 0.49114117]\n",
      "tf.Tensor(\n",
      "[ 0.7816021   0.81464404  0.8433688   0.76073045  0.763372    0.6750622\n",
      "  0.5641183   0.69435006  0.5063368   0.4080222   0.47917065  0.31884998\n",
      "  0.17305967 -0.06161011 -0.04073815], shape=(15,), dtype=float32)\n",
      "[0.40579482 0.60065321]\n",
      "tf.Tensor(\n",
      "[0.79529333 0.750566   0.79003304 0.76391315 0.73424697 0.6420629\n",
      " 0.6682913  0.6523948  0.5369906  0.5469905  0.33981997 0.26553097\n",
      " 0.0638757  0.28567824 0.23673962], shape=(15,), dtype=float32)\n",
      "[0.51507599 0.61200315]\n",
      "tf.Tensor(\n",
      "[ 0.79337245  0.7404581   0.7697803   0.73482007  0.70173734  0.73203075\n",
      "  0.5884999   0.60060364  0.46503568  0.4337764   0.4391953   0.34379748\n",
      "  0.26070306  0.07910137 -0.0782097 ], shape=(15,), dtype=float32)\n",
      "[0.58594846 0.53615845]\n",
      "tf.Tensor(\n",
      "[0.77882797 0.69763243 0.7354432  0.78261834 0.7548013  0.7395214\n",
      " 0.6853571  0.57952696 0.5142892  0.46434534 0.52490604 0.3411709\n",
      " 0.2709834  0.09081004 0.05274038], shape=(15,), dtype=float32)\n",
      "[0.57014214 0.43662953]\n",
      "tf.Tensor(\n",
      "[ 0.8492138   0.7675102   0.8595176   0.78523564  0.7543218   0.66763735\n",
      "  0.5548783   0.62393606  0.48151332  0.5518069   0.38802665  0.3985188\n",
      "  0.3526202   0.23939711 -0.12900242], shape=(15,), dtype=float32)\n",
      "[0.47905944 0.38159515]\n",
      "tf.Tensor(\n",
      "[ 0.82852745  0.81620026  0.76462984  0.6585787   0.680708    0.5028422\n",
      "  0.67517024  0.60897154  0.4863976   0.62219536  0.49263546  0.44589314\n",
      "  0.28722784  0.23585846 -0.02223484], shape=(15,), dtype=float32)\n",
      "[0.38228903 0.42519666]\n",
      "tf.Tensor(\n",
      "[0.79549915 0.73700064 0.78116643 0.7397113  0.7940053  0.78030664\n",
      " 0.5730892  0.49749538 0.5726207  0.3887233  0.46988285 0.3230513\n",
      " 0.03857424 0.06511506 0.09033238], shape=(15,), dtype=float32)\n",
      "[0.36676927 0.53437901]\n",
      "tf.Tensor(\n",
      "[0.8047022  0.75837153 0.8462696  0.7162242  0.72801363 0.65123004\n",
      " 0.59589547 0.54535824 0.6131784  0.4514287  0.6247042  0.36965016\n",
      " 0.20724636 0.18744104 0.0636511 ], shape=(15,), dtype=float32)\n",
      "[0.44490315 0.61066828]\n",
      "tf.Tensor(\n",
      "[0.81024164 0.7460855  0.7998578  0.74819976 0.7318434  0.7199478\n",
      " 0.6176514  0.6539627  0.45442376 0.54600036 0.46350846 0.27041775\n",
      " 0.35329205 0.08493165 0.01173883], shape=(15,), dtype=float32)\n",
      "[0.54393757 0.58586972]\n",
      "tf.Tensor(\n",
      "[0.83877844 0.7512684  0.8179611  0.7089987  0.62314284 0.6872689\n",
      " 0.666742   0.58867216 0.43242267 0.48708692 0.5365295  0.36782113\n",
      " 0.2323856  0.22591136 0.1930209 ], shape=(15,), dtype=float32)\n",
      "[0.58560245 0.50302988]\n",
      "tf.Tensor(\n",
      "[ 0.81615007  0.8337144   0.7280479   0.83652335  0.6593734   0.6673092\n",
      "  0.644081    0.6065941   0.49902397  0.597173    0.33554184  0.29067463\n",
      "  0.29533008  0.15974145 -0.04347519], shape=(15,), dtype=float32)\n",
      "[0.54541119 0.42172624]\n",
      "tf.Tensor(\n",
      "[0.77859366 0.7399454  0.80361915 0.8222891  0.6997945  0.7333289\n",
      " 0.5931555  0.5250654  0.38968724 0.4366026  0.41840667 0.2858042\n",
      " 0.20093226 0.11056146 0.28115508], shape=(15,), dtype=float32)\n",
      "[0.44752685 0.38479307]\n",
      "tf.Tensor(\n",
      "[ 0.82429093  0.7936777   0.8563681   0.7860239   0.7406905   0.5799222\n",
      "  0.7315543   0.5343847   0.460609    0.4562714   0.35923907  0.36962327\n",
      "  0.18014123  0.23509723 -0.02257505], shape=(15,), dtype=float32)\n",
      "[0.36856751 0.45480162]\n",
      "tf.Tensor(\n",
      "[0.8142052  0.7555646  0.8146456  0.76780176 0.7035508  0.6438452\n",
      " 0.6299063  0.6259845  0.56285286 0.4900472  0.4291155  0.2972585\n",
      " 0.23083456 0.0416389  0.11854338], shape=(15,), dtype=float32)\n",
      "[0.38437616 0.56566065]\n",
      "tf.Tensor(\n",
      "[0.8717742  0.79311955 0.8349144  0.7581415  0.6517145  0.639386\n",
      " 0.60070837 0.52159375 0.42648387 0.45765376 0.37017617 0.3854718\n",
      " 0.27083525 0.08882858 0.03711194], shape=(15,), dtype=float32)\n",
      "[0.48105746 0.61698049]\n",
      "tf.Tensor(\n",
      "[0.7816412  0.8154942  0.7475302  0.7307145  0.67497694 0.65167934\n",
      " 0.6135576  0.52580523 0.50448567 0.5064087  0.28986984 0.33856618\n",
      " 0.2111372  0.04163319 0.06080171], shape=(15,), dtype=float32)\n",
      "[0.56957929 0.56592761]\n",
      "tf.Tensor(\n",
      "[0.7415218  0.8276692  0.76559246 0.7420308  0.6718621  0.6524063\n",
      " 0.74649006 0.6595096  0.57205176 0.42472783 0.46960062 0.40188935\n",
      " 0.3381443  0.31225252 0.12252428], shape=(15,), dtype=float32)\n",
      "[0.58403423 0.47394873]\n",
      "tf.Tensor(\n",
      "[ 0.7820771   0.76585364  0.77992487  0.7481109   0.71760786  0.7291078\n",
      "  0.73734087  0.65609974  0.58259207  0.44035995  0.39198986  0.0941532\n",
      "  0.22869183 -0.15184657  0.09070029], shape=(15,), dtype=float32)\n",
      "[0.51688021 0.39510235]\n",
      "tf.Tensor(\n",
      "[0.7352284  0.75400406 0.78572464 0.801884   0.6663553  0.7583888\n",
      " 0.67039853 0.5376294  0.50376385 0.5414519  0.5258707  0.35912326\n",
      " 0.2996345  0.05289698 0.06059019], shape=(15,), dtype=float32)\n",
      "[0.40798668 0.39102628]\n",
      "tf.Tensor(\n",
      "[0.7611072  0.8020517  0.78023297 0.6550679  0.66480124 0.76778185\n",
      " 0.6363506  0.5958002  0.47454938 0.5027793  0.5354361  0.33032587\n",
      " 0.32072702 0.1383539  0.08724254], shape=(15,), dtype=float32)\n",
      "[0.3520312  0.49354863]\n",
      "tf.Tensor(\n",
      "[0.79392415 0.7739514  0.8184119  0.79044056 0.6628201  0.73477334\n",
      " 0.6351135  0.6286006  0.4333898  0.42271107 0.5196934  0.10632459\n",
      " 0.33844072 0.25389937 0.09108002], shape=(15,), dtype=float32)\n",
      "[0.40609846 0.60283605]\n",
      "tf.Tensor(\n",
      "[ 0.7884622   0.82729053  0.8360866   0.7258866   0.64380354  0.7157528\n",
      "  0.5779476   0.5710402   0.47314978  0.5921881   0.3813641   0.29816145\n",
      "  0.16894156  0.27616373 -0.02353296], shape=(15,), dtype=float32)\n",
      "[0.51798088 0.61750049]\n",
      "tf.Tensor(\n",
      "[ 0.81359965  0.763978    0.76793635  0.7462814   0.70802116  0.68962127\n",
      "  0.58806014  0.5863664   0.46981502  0.5895484   0.37318972  0.36630228\n",
      "  0.25067335  0.17080323 -0.03141759], shape=(15,), dtype=float32)\n",
      "[0.5938309  0.54126586]\n",
      "tf.Tensor(\n",
      "[0.8113294  0.768809   0.6856807  0.73540765 0.72331357 0.6783503\n",
      " 0.5949049  0.583399   0.61106104 0.5376641  0.33809215 0.40843654\n",
      " 0.18948479 0.06254447 0.08455604], shape=(15,), dtype=float32)\n",
      "[0.58345062 0.44372764]\n",
      "tf.Tensor(\n",
      "[0.8009942  0.8070976  0.7275021  0.69663674 0.71600336 0.70018804\n",
      " 0.65069294 0.66603565 0.7022384  0.45402578 0.34891218 0.22229828\n",
      " 0.2553366  0.20484611 0.02414317], shape=(15,), dtype=float32)\n",
      "[0.49264736 0.37778779]\n",
      "tf.Tensor(\n",
      "[0.74953175 0.75108176 0.7682707  0.7941111  0.6321828  0.7890561\n",
      " 0.59978455 0.5697439  0.5587085  0.5677614  0.34142888 0.45922023\n",
      " 0.34311283 0.18927541 0.04024025], shape=(15,), dtype=float32)\n",
      "[0.38186869 0.40307761]\n",
      "tf.Tensor(\n",
      "[0.8401727  0.83660525 0.7682112  0.7321585  0.7469177  0.6507884\n",
      " 0.6442639  0.64668775 0.45425525 0.34885836 0.329645   0.31632945\n",
      " 0.17799746 0.10585867 0.17687285], shape=(15,), dtype=float32)\n",
      "[0.35298689 0.53085447]\n",
      "tf.Tensor(\n",
      "[0.78200173 0.7954946  0.79973924 0.76843774 0.76992786 0.64292175\n",
      " 0.6476148  0.5449644  0.6215908  0.56584924 0.37136087 0.37221712\n",
      " 0.19877341 0.05788508 0.09599178], shape=(15,), dtype=float32)\n",
      "[0.43535255 0.62311142]\n",
      "tf.Tensor(\n",
      "[0.7988832  0.81995803 0.79670495 0.75617784 0.7537437  0.72877586\n",
      " 0.6120326  0.6481022  0.528509   0.44550523 0.54102105 0.4045983\n",
      " 0.20502405 0.10994437 0.01944509], shape=(15,), dtype=float32)\n",
      "[0.54961391 0.6057297 ]\n",
      "tf.Tensor(\n",
      "[0.82574815 0.8087345  0.79786146 0.726313   0.6651899  0.7448676\n",
      " 0.6447881  0.57326835 0.49473333 0.34929955 0.18304539 0.3073979\n",
      " 0.30910343 0.2195527  0.08602198], shape=(15,), dtype=float32)\n",
      "[0.60715892 0.51897679]\n",
      "tf.Tensor(\n",
      "[0.73939127 0.7683055  0.77412814 0.7428036  0.71010387 0.734558\n",
      " 0.6276515  0.5459412  0.57485867 0.42185855 0.32804263 0.21587849\n",
      " 0.03043543 0.26203704 0.07618994], shape=(15,), dtype=float32)\n",
      "[0.571388   0.41616956]\n",
      "tf.Tensor(\n",
      "[0.8084016  0.79118896 0.7125908  0.78489465 0.7554889  0.70314\n",
      " 0.6542661  0.6204031  0.5129748  0.53307694 0.3740126  0.1729145\n",
      " 0.2167828  0.13401434 0.3829881 ], shape=(15,), dtype=float32)\n",
      "[0.46168311 0.36681081]\n",
      "tf.Tensor(\n",
      "[ 0.80106664  0.8936466   0.76621354  0.7754078   0.77766734  0.63762605\n",
      "  0.644117    0.4937583   0.3810801   0.40484437  0.45562983  0.39988062\n",
      "  0.14110254  0.12532564 -0.0749072 ], shape=(15,), dtype=float32)\n",
      "[0.36223823 0.43672092]\n",
      "tf.Tensor(\n",
      "[ 0.8590739   0.85995823  0.7432653   0.7503428   0.80756474  0.72860914\n",
      "  0.69533503  0.65018004  0.58915186  0.49469632  0.26144204  0.31654248\n",
      "  0.1668773   0.16982254 -0.07862785], shape=(15,), dtype=float32)\n",
      "[0.37103283 0.57032034]\n",
      "tf.Tensor(\n",
      "[0.7883463  0.7421475  0.7198873  0.8076494  0.73724467 0.7076293\n",
      " 0.55192846 0.49001226 0.5639493  0.52365774 0.33413577 0.2639018\n",
      " 0.14376952 0.13097861 0.12782432], shape=(15,), dtype=float32)\n",
      "[0.47393083 0.62502963]\n",
      "tf.Tensor(\n",
      "[ 0.78464913  0.7832258   0.81469816  0.7778928   0.73397785  0.6671145\n",
      "  0.56024915  0.63776195  0.52743816  0.43552956  0.38730374  0.34354916\n",
      "  0.14792752 -0.00164434  0.15573315], shape=(15,), dtype=float32)\n",
      "[0.57156853 0.57477212]\n",
      "tf.Tensor(\n",
      "[0.8371124  0.72853917 0.8691501  0.73401505 0.6498376  0.73067075\n",
      " 0.63454086 0.55254984 0.5146528  0.4891952  0.43070522 0.43262103\n",
      " 0.1375717  0.04265443 0.3134485 ], shape=(15,), dtype=float32)\n",
      "[0.59580991 0.47936026]\n",
      "tf.Tensor(\n",
      "[ 0.81873614  0.83532137  0.7040841   0.7788191   0.6197398   0.61335325\n",
      "  0.68482035  0.5888923   0.61703384  0.4427971   0.6005854   0.20061466\n",
      "  0.14735143  0.22884987 -0.0335655 ], shape=(15,), dtype=float32)\n",
      "[0.5329938  0.39897234]\n",
      "tf.Tensor(\n",
      "[0.8065402  0.7822272  0.76538205 0.7681339  0.7753727  0.7742152\n",
      " 0.69478977 0.57195425 0.4704853  0.49332318 0.31115493 0.395126\n",
      " 0.28096202 0.04866501 0.1842206 ], shape=(15,), dtype=float32)\n",
      "[0.42374525 0.38484408]\n",
      "tf.Tensor(\n",
      "[0.85375875 0.85130006 0.82338494 0.7543496  0.5935664  0.68263274\n",
      " 0.6397417  0.6747055  0.4713774  0.47407013 0.29585087 0.24762736\n",
      " 0.16460706 0.19996603 0.0240241 ], shape=(15,), dtype=float32)\n",
      "[0.35941744 0.48232772]\n",
      "tf.Tensor(\n",
      "[0.8330857  0.7875589  0.7354576  0.7439057  0.73607373 0.72214407\n",
      " 0.57638764 0.573217   0.5422481  0.41029024 0.3359802  0.30563188\n",
      " 0.10840655 0.21389659 0.00672198], shape=(15,), dtype=float32)\n",
      "[0.40275122 0.59225201]\n",
      "tf.Tensor(\n",
      "[ 0.82999897  0.86564636  0.7875039   0.81777596  0.70565     0.67825115\n",
      "  0.61548394  0.5552539   0.42982295  0.41920453  0.4481033   0.38862363\n",
      "  0.21809898  0.06253474 -0.07581279], shape=(15,), dtype=float32)\n",
      "[0.51037026 0.6184226 ]\n",
      "tf.Tensor(\n",
      "[0.7676647  0.82916576 0.7767916  0.7539434  0.705458   0.64423805\n",
      " 0.69134396 0.44446015 0.62188345 0.49696687 0.4288368  0.4260155\n",
      " 0.28188017 0.08480137 0.05216508], shape=(15,), dtype=float32)\n",
      "[0.58867452 0.54802307]\n",
      "tf.Tensor(\n",
      "[0.8200514  0.8098292  0.80813813 0.74955344 0.6526869  0.66327983\n",
      " 0.609704   0.53753686 0.49178484 0.45022994 0.4564877  0.33983544\n",
      " 0.3784235  0.15109646 0.04174715], shape=(15,), dtype=float32)\n",
      "[0.58638319 0.45443259]\n",
      "tf.Tensor(\n",
      "[0.7696876  0.7509072  0.7973766  0.7715203  0.7091568  0.6614527\n",
      " 0.5852787  0.6455517  0.45499912 0.40769756 0.4184576  0.47384334\n",
      " 0.33827722 0.06488781 0.05048884], shape=(15,), dtype=float32)\n",
      "[0.50134118 0.37832446]\n",
      "tf.Tensor(\n",
      "[0.8192935  0.7438691  0.8174823  0.7081893  0.73018426 0.7355522\n",
      " 0.59525925 0.5514984  0.5196334  0.38062623 0.40469217 0.34332815\n",
      " 0.40626797 0.17307793 0.15640299], shape=(15,), dtype=float32)\n",
      "[0.38968613 0.39832184]\n",
      "tf.Tensor(\n",
      "[ 0.8166617   0.8328098   0.79810065  0.78592587  0.77027655  0.6313598\n",
      "  0.5805887   0.6444551   0.6290719   0.4495581   0.3667833   0.28382215\n",
      "  0.2628423  -0.02390542 -0.15235856], shape=(15,), dtype=float32)\n",
      "[0.35192616 0.51947195]\n",
      "tf.Tensor(\n",
      "[0.78434974 0.81409925 0.8017937  0.74562925 0.6691752  0.718595\n",
      " 0.60850894 0.50091076 0.5456703  0.45592985 0.43720937 0.29297298\n",
      " 0.25382063 0.32232195 0.18150227], shape=(15,), dtype=float32)\n",
      "[0.42643755 0.61913247]\n",
      "tf.Tensor(\n",
      "[0.7865445  0.8380168  0.7752155  0.7637406  0.7297546  0.70171916\n",
      " 0.6920485  0.54565537 0.52947325 0.47661933 0.4009594  0.3770377\n",
      " 0.32883    0.16308029 0.02899723], shape=(15,), dtype=float32)\n",
      "[0.54147191 0.61092481]\n",
      "tf.Tensor(\n",
      "[0.78888315 0.7469997  0.67738193 0.7490696  0.66438204 0.69212526\n",
      " 0.6257292  0.5625566  0.64628035 0.53416157 0.4505727  0.4250792\n",
      " 0.22938962 0.04641598 0.08701646], shape=(15,), dtype=float32)\n",
      "[0.60302623 0.52109476]\n",
      "tf.Tensor(\n",
      "[0.78477997 0.81664246 0.84776825 0.7173021  0.6917322  0.72397983\n",
      " 0.64939183 0.47706875 0.5227951  0.51427835 0.32041466 0.2011926\n",
      " 0.26265493 0.08005637 0.13914573], shape=(15,), dtype=float32)\n",
      "[0.57279901 0.42512558]\n",
      "tf.Tensor(\n",
      "[0.8355642  0.81322175 0.8219691  0.64820415 0.6749398  0.6151745\n",
      " 0.6576263  0.528807   0.5957073  0.31524938 0.5663655  0.27555418\n",
      " 0.14108284 0.23462236 0.08474787], shape=(15,), dtype=float32)\n",
      "[0.47154124 0.37436967]\n",
      "tf.Tensor(\n",
      "[ 0.77912587  0.8206091   0.8392971   0.6829742   0.81002873  0.71247864\n",
      "  0.6173976   0.53924984  0.5642729   0.44932613  0.42185163  0.31431144\n",
      "  0.1367163   0.162648   -0.05937104], shape=(15,), dtype=float32)\n",
      "[0.37085166 0.42617494]\n",
      "tf.Tensor(\n",
      "[ 0.7898823   0.7594188   0.79121584  0.78815037  0.7210286   0.71293074\n",
      "  0.6105649   0.70436573  0.4780255   0.39781225  0.34089234  0.3126574\n",
      "  0.20237492 -0.04093971  0.1548445 ], shape=(15,), dtype=float32)\n",
      "[0.36229389 0.54699585]\n",
      "tf.Tensor(\n",
      "[ 0.82355934  0.7344039   0.8172188   0.73402274  0.74502105  0.6534147\n",
      "  0.6435986   0.7128599   0.49431676  0.48380378  0.39066416  0.20980707\n",
      "  0.12757571  0.13026321 -0.0428597 ], shape=(15,), dtype=float32)\n",
      "[0.45297857 0.62130913]\n",
      "tf.Tensor(\n",
      "[ 0.7381621   0.71149874  0.7293028   0.7196442   0.6499199   0.7436134\n",
      "  0.6323719   0.70429844  0.6065198   0.37744427  0.3373286   0.3274003\n",
      "  0.21847181 -0.0329199   0.06734627], shape=(15,), dtype=float32)\n",
      "[0.55327263 0.58079277]\n",
      "tf.Tensor(\n",
      "[0.7875618  0.77709216 0.83968383 0.7608554  0.6851276  0.7055456\n",
      " 0.7077723  0.7634509  0.49213797 0.47112393 0.40849224 0.30878276\n",
      " 0.23586862 0.19032483 0.104571  ], shape=(15,), dtype=float32)\n",
      "[0.58605266 0.49302219]\n",
      "tf.Tensor(\n",
      "[ 0.77135384  0.77240855  0.76240754  0.7110033   0.7368906   0.73387945\n",
      "  0.7226573   0.69948393  0.53461146  0.4671657   0.29715195  0.20918204\n",
      "  0.28141868  0.21912421 -0.05751437], shape=(15,), dtype=float32)\n",
      "[0.53415447 0.40774971]\n",
      "tf.Tensor(\n",
      "[0.8164045  0.7889183  0.7453783  0.7584136  0.790255   0.69659835\n",
      " 0.55485296 0.5503677  0.4235517  0.59859073 0.27493244 0.30828068\n",
      " 0.2731018  0.03790416 0.07867999], shape=(15,), dtype=float32)\n",
      "[0.43198381 0.39000767]\n",
      "tf.Tensor(\n",
      "[ 0.84704113  0.84706455  0.757801    0.8020106   0.7303452   0.7478122\n",
      "  0.66020226  0.51668745  0.36881408  0.40496936  0.41804153  0.26775324\n",
      "  0.29054877  0.09941671 -0.02390235], shape=(15,), dtype=float32)\n",
      "[0.36699534 0.47671569]\n",
      "tf.Tensor(\n",
      "[0.777419   0.8151371  0.75728863 0.82728755 0.7308517  0.66778195\n",
      " 0.64413494 0.6297771  0.5729615  0.43044972 0.48582512 0.2614562\n",
      " 0.19151399 0.0993737  0.01213963], shape=(15,), dtype=float32)\n",
      "[0.40010066 0.58109861]\n",
      "tf.Tensor(\n",
      "[0.8289774  0.84249175 0.7708501  0.7566673  0.75550324 0.6980426\n",
      " 0.6896482  0.68317795 0.41909352 0.3745316  0.4521397  0.201769\n",
      " 0.19879925 0.13832897 0.14238796], shape=(15,), dtype=float32)\n",
      "[0.49982994 0.61265017]\n",
      "tf.Tensor(\n",
      "[0.78668094 0.8483236  0.76588315 0.74072427 0.69789535 0.76625866\n",
      " 0.5793121  0.65345925 0.49243197 0.5656529  0.30996072 0.2837328\n",
      " 0.3334923  0.18241735 0.21956131], shape=(15,), dtype=float32)\n",
      "[0.57826431 0.55251938]\n",
      "tf.Tensor(\n",
      "[0.7769138  0.7900274  0.787963   0.7225514  0.7607372  0.6719565\n",
      " 0.6733853  0.5608764  0.48230317 0.5253998  0.28176615 0.37010866\n",
      " 0.1746049  0.23498641 0.1128964 ], shape=(15,), dtype=float32)\n",
      "No Success\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "daifa.train_tran = False\n",
    "daifa.train_vae = False\n",
    "\n",
    "daifa.hidden_state = None\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}