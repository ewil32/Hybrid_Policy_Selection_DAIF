{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run The Agent on Mountain Car"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from vae_recurrent import VAE, create_decoder, create_encoder\n",
    "from transition_gru import TransitionGRU\n",
    "from recurrent_agent import DAIFAgentRecurrent\n",
    "from prior_model import PriorModelBellman\n",
    "from habitual_action_network import HabitualAction, compute_discounted_cumulative_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations\n",
    "from train_agent import train_single_agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from identity_vae import IdentityVAE, identity_encoder, identity_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with no prior model FEEF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from util import transform_observations\n",
    "\n",
    "\n",
    "def train_single_agent(mcc_env,\n",
    "                       agent,\n",
    "                       obs_max,\n",
    "                       obs_min,\n",
    "                       observation_noise_stddev,\n",
    "                       num_episodes=100,\n",
    "                       render_env=False):\n",
    "\n",
    "    # Set up to store results in pandas frame\n",
    "    cols = [\"episode\", \"success\", \"sim_steps\", \"VFE_post_run\", \"noise_stddev\"]\n",
    "    rows = []\n",
    "\n",
    "    for n in range(num_episodes):\n",
    "\n",
    "        print(\"Episode\", n+1)\n",
    "\n",
    "        # reset the agent\n",
    "        agent.reset_all_states()\n",
    "\n",
    "        # get the first observation from the environment\n",
    "        first_observation = mcc_env.reset()\n",
    "        print(first_observation)\n",
    "\n",
    "        # apply noise to and scaling to first observation\n",
    "        observation_noisy = transform_observations(first_observation, obs_max, obs_min, observation_noise_stddev)\n",
    "        observation_noisy = observation_noisy.reshape(1, observation_noisy.shape[0])\n",
    "        # loop until episode ends or the agent succeeds\n",
    "        t = 0\n",
    "        reward = None\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            if render_env:\n",
    "                mcc_env.render()\n",
    "\n",
    "            action = agent.perceive_and_act(observation_noisy, reward=reward, done=done)\n",
    "            # print(action)\n",
    "            observation, reward, done, info = mcc_env.step(action)  # action should be array to satisfy gym requirements\n",
    "            # print(observation)\n",
    "            observation_noisy = transform_observations(observation, obs_max, obs_min, observation_noise_stddev)\n",
    "            observation_noisy = observation_noisy.reshape(1, observation_noisy.shape[0])\n",
    "\n",
    "            t += 1\n",
    "\n",
    "        # final training when the episode is done\n",
    "        agent.perceive_and_act(observation_noisy, reward=reward, done=done)\n",
    "\n",
    "        success = t < 999\n",
    "\n",
    "        # get the VFE of the model for the run\n",
    "        # VFE = float(tf.reduce_mean(agent.model_vae.compute_loss(all_post_observations)))\n",
    "        VFE = 0\n",
    "\n",
    "        rows.append(dict(zip(cols, [n, success, t, VFE, observation_noise_stddev])))\n",
    "\n",
    "        if success:\n",
    "            print(\"Success in episode\", n+1, \"at time step\", t)\n",
    "        else:\n",
    "            print(\"No Success\")\n",
    "\n",
    "    results = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "    mcc_env.close()\n",
    "\n",
    "    return agent, results\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PriorModelBellman(keras.Model):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 observation_dim,\n",
    "                 iterate_train=1,\n",
    "                 discount_factor=0.99,\n",
    "                 training_epochs=1,\n",
    "                 show_training=True):\n",
    "\n",
    "        super(PriorModelBellman, self).__init__()\n",
    "        self.observation_dim = observation_dim\n",
    "        self.iterate_train = iterate_train\n",
    "        self.discount_factor = discount_factor\n",
    "        self.train_epochs = 1\n",
    "\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.train_epochs = training_epochs\n",
    "        self.show_training = show_training\n",
    "\n",
    "        # make the model\n",
    "        transition_inputs = layers.Input(observation_dim)\n",
    "        h = layers.Dense(observation_dim * 20, activation=\"silu\")(transition_inputs)\n",
    "        h = layers.Dense(observation_dim, activation=\"tanh\")(h)\n",
    "        # h = layers.Dense(observation_dim)(h)\n",
    "\n",
    "        self.prior_model = keras.Model(transition_inputs, h, name=\"prior_model\")\n",
    "        self.prior_model.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "    def call(self, observations):\n",
    "        return self.prior_model(observations)\n",
    "\n",
    "    def extrinsic_kl(self, observations):\n",
    "        return 1.0 - self(observations)  # map from [-1, 1] to [2, 0]\n",
    "\n",
    "    def train(self, observations, rewards):\n",
    "        \"\"\"\n",
    "\n",
    "        :param observations: o_0, o_1, ... , o_n\n",
    "        :param rewards: list with r_0, r_1, ... , r_n\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        num_observations = len(observations)\n",
    "\n",
    "        # expand rewards to have the same dimension as observation dimension and transpose to give [num_observations, observation_dimension\n",
    "        rewards_stacked = np.stack([rewards]*self.observation_dim).T\n",
    "\n",
    "        for i in range(self.iterate_train):\n",
    "\n",
    "            # reducing discount factors through time\n",
    "            discount_factors = np.power([self.discount_factor]*num_observations, np.arange(num_observations)).reshape(observations.shape[0], 1)\n",
    "            discount_factors = np.flip(discount_factors)\n",
    "\n",
    "            # print(discount_factors)\n",
    "\n",
    "            # TODO Still seems a little strange that we add 0 to the end and discount the way we do but I think it makes sense. Check what predicted utilities are in practice\n",
    "            utility_t = self.prior_model(observations)\n",
    "            utility_t_plus_one = tf.concat([utility_t[1:], tf.zeros((1, self.observation_dim), dtype=utility_t.dtype)], axis=0)\n",
    "\n",
    "            # print(predicted_utility, pred_next_v)\n",
    "\n",
    "            expected_utility = rewards_stacked + discount_factors * utility_t_plus_one\n",
    "\n",
    "            # print(rewards_stacked)\n",
    "            # print(discount_factors * utility_t_plus_one)\n",
    "\n",
    "            self.prior_model.fit(observations, expected_utility, epochs=self.train_epochs, verbose=self.show_training)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "# from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 habitual_action_net,\n",
    "                 given_prior_mean=None,\n",
    "                 given_prior_stddev=None,\n",
    "                 agent_time_ratio=6,\n",
    "                 actions_to_execute_when_exploring=2,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True,\n",
    "                 train_prior_model=True,\n",
    "                 train_habit_net=True,\n",
    "                 train_with_replay=True,\n",
    "                 train_after_exploring=True,\n",
    "                 use_kl_extrinsic=True,\n",
    "                 use_kl_intrinsic=True,\n",
    "                 use_FEEF=True,\n",
    "                 use_fast_thinking=False):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        # parameters for slow policy planning\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        # flags for whether or not we are training models or using pretrained models and when we should train\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "        self.train_habit_net = train_habit_net\n",
    "        self.train_prior = train_prior_model\n",
    "        self.train_with_replay = train_with_replay\n",
    "        self.train_after_exploring = train_after_exploring\n",
    "\n",
    "        # do we use the kl divergence for extrinsic vs intrinsic\n",
    "        self.use_kl_intrinsic = use_kl_intrinsic\n",
    "        self.use_kl_extrinsic = use_kl_extrinsic\n",
    "\n",
    "        # do we use the FEEF or EFE?\n",
    "        self.use_FEEF = use_FEEF\n",
    "\n",
    "        # given prior values\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.tran = tran\n",
    "        self.prior_model = prior_model\n",
    "        self.habit_action_model = habitual_action_net\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "        self.actions_to_execute_when_exploring = actions_to_execute_when_exploring\n",
    "        self.time_step = 0\n",
    "        self.exploring = False\n",
    "\n",
    "        # track the hidden state of the transition gru model so we can use it to train\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "        # store the full observations for the episode so we can train using replay\n",
    "        self.full_observation_sequence = []\n",
    "        self.full_action_sequence = []\n",
    "        self.full_reward_sequence = []\n",
    "\n",
    "        # store the observations while the agent is in exploration mode\n",
    "        self.exploring_observation_sequence = []\n",
    "        self.exploring_action_sequence = []\n",
    "        self.exploring_reward_sequence = []\n",
    "\n",
    "        # store the observations at the world time scale\n",
    "        self.env_time_scale_observations = []\n",
    "\n",
    "        self.policy_being_executed = [None]\n",
    "        self.previous_observation = None\n",
    "        self.action_being_executed = None\n",
    "\n",
    "        self.use_fast_thinking = use_fast_thinking\n",
    "\n",
    "\n",
    "    def perceive_and_act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        The function called to have the agent interact with the environment\n",
    "        We assume the agent gets a transformed/noisy observation from the environment and then returns an action\n",
    "\n",
    "        TODO: possibly the agent returns some other information for logging and showing experiments\n",
    "\n",
    "        :param observation:\n",
    "        :param reward:\n",
    "        :param done:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # track the world time scale observation sequence\n",
    "        self.env_time_scale_observations.append(observation)\n",
    "\n",
    "        # if the episode is finished, then do any training on the full data set\n",
    "        if done:\n",
    "\n",
    "            if self.train_with_replay:\n",
    "                print(\"training on full data\")\n",
    "\n",
    "                # add the final observation and reward we observed to the sequences\n",
    "                self.full_observation_sequence.append(observation)\n",
    "                self.full_reward_sequence.append(reward)\n",
    "\n",
    "                # Call the training function on the observation sequences to train everything we need to train\n",
    "                self.train_models(np.vstack(self.full_observation_sequence),\n",
    "                                  np.vstack(self.full_action_sequence),\n",
    "                                  np.array(self.full_reward_sequence))\n",
    "\n",
    "\n",
    "        # Otherwise are we at a point where we can reconsider our policy and maybe train the world model\n",
    "        elif self.time_step % self.agent_time_ratio == 0:\n",
    "\n",
    "            # add the reward only if it's not the first observation\n",
    "            if self.time_step != 0:\n",
    "                self.full_reward_sequence.append(reward)\n",
    "\n",
    "            # add the observation to the sequence\n",
    "            self.full_observation_sequence.append(observation)\n",
    "\n",
    "            # We only update the model during the episode when we were exploring using the planning method and we have executed all of the actions in the policy\n",
    "            if self.exploring and len(self.policy_being_executed) == 0:\n",
    "\n",
    "                # print(\"f\", self.full_observation_sequence)\n",
    "                # print(\"e\", self.full_observation_sequence[-1*(self.actions_to_execute_when_exploring + 1):])\n",
    "\n",
    "                if self.train_after_exploring:\n",
    "                    # the actions done while exploring were the last self.actions_to_execute_when_exploring\n",
    "                    self.exploring_action_sequence = self.full_action_sequence[-1*self.actions_to_execute_when_exploring:]\n",
    "                    self.exploring_reward_sequence = self.full_reward_sequence[-1*self.actions_to_execute_when_exploring:]\n",
    "                    self.exploring_observation_sequence = self.full_observation_sequence[-1*(self.actions_to_execute_when_exploring + 1):]\n",
    "\n",
    "                    # Call the training function on the observation sequences to train everything we need to train\n",
    "                    self.train_models(np.vstack(self.exploring_observation_sequence),\n",
    "                                      np.vstack(self.exploring_action_sequence),\n",
    "                                      np.array(self.exploring_reward_sequence))\n",
    "\n",
    "                self.exploring = False\n",
    "\n",
    "\n",
    "            # Now we select our action. If we aren't exploring then either we act out of habit or we might need to explore\n",
    "            # I think I can check this based on whether or not there are actions left to execute in the current policy\n",
    "            if not self.exploring:\n",
    "\n",
    "                action_as_array = np.array(self.action_being_executed).reshape(1, self.tran.action_dim)\n",
    "                pred_next_observation, next_tran_hidden_state = self.predict_next_observation(self.previous_observation, action_as_array)\n",
    "\n",
    "                if self.previous_observation is None:\n",
    "                    self.policy_being_executed = self.habit_action_model(observation)\n",
    "                    self.policy_being_executed = self.policy_being_executed.numpy().tolist()  # tf tensor to list\n",
    "\n",
    "                # TDOD Fix this to work however it needs to\n",
    "                # we need to see what the generative model now thinks about what the expected current observation is\n",
    "                elif self.use_fast_thinking and np.allclose(observation, pred_next_observation, atol=self.model_vae.reconstruction_stddev):  # within some tolerance\n",
    "                    self.policy_being_executed = self.habit_action_model(observation)\n",
    "                    self.policy_being_executed = self.policy_being_executed + np.random.normal(0, scale=self.habit_action_model.action_std_dev)\n",
    "                    self.policy_being_executed = self.policy_being_executed.numpy().tolist()\n",
    "\n",
    "                    self.tran_hidden_state = next_tran_hidden_state\n",
    "\n",
    "                    print(\"fast thinking\")\n",
    "\n",
    "                # the generative model is surprised so we should use the slow deliberation for planning out a policy that balances exploration and exploitation\n",
    "                else:\n",
    "                    # print(\"slow thinking\")\n",
    "                    policy = self.select_policy(observation)\n",
    "                    # print(policy.mean())\n",
    "                    policy = policy.mean().numpy()\n",
    "                    policy = policy.reshape(policy.shape[0], self.tran.action_dim).tolist()\n",
    "                    self.policy_being_executed = policy[0: self.actions_to_execute_when_exploring]\n",
    "\n",
    "                    self.exploring = True\n",
    "\n",
    "                # print(observation)\n",
    "                # print(pred_next_observation)\n",
    "\n",
    "            # finally update the previous observation and action to be the one we just had/did\n",
    "            self.previous_observation = observation\n",
    "            self.action_being_executed = self.policy_being_executed[0]\n",
    "            self.full_action_sequence.append(self.action_being_executed)\n",
    "            self.policy_being_executed.pop(0)\n",
    "\n",
    "        # final updates increment the current timestep and return the action specified by the policy\n",
    "        self.time_step += 1\n",
    "\n",
    "        return self.action_being_executed\n",
    "\n",
    "\n",
    "    def predict_next_observation(self, obs, action):\n",
    "\n",
    "        # TODO: Fix this with the transition hidden states\n",
    "        if obs is None:\n",
    "            return None, None\n",
    "        else:\n",
    "            # print(obs.shape)\n",
    "            z_mean, z_std, z = self.model_vae.encoder(obs)\n",
    "            # print(z_mean.shape)\n",
    "            # print(action.shape)\n",
    "            z_mean = z_mean.numpy()\n",
    "            z_plus_action = np.concatenate([z_mean, action], axis=1)\n",
    "            # print(z_mean)\n",
    "            # print(action)\n",
    "            # print(z_plus_action)\n",
    "\n",
    "            z_plus_action = z_plus_action.reshape(1, 1, z_plus_action.shape[1])\n",
    "            # print(z_plus_action)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((z_plus_action, self.tran_hidden_state))\n",
    "\n",
    "            next_observation = self.model_vae.decoder(next_latent_mean)\n",
    "            # print(next_observation)\n",
    "            return next_observation.numpy(), next_hidden_state\n",
    "\n",
    "\n",
    "    # We use this function to reset the hidden state of the transition model when we want to train on the full data set\n",
    "    def reset_tran_hidden_state(self):\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "\n",
    "    def reset_all_states(self):\n",
    "        self.time_step = 0\n",
    "        self.exploring = False\n",
    "\n",
    "        # track the hidden state of the transition gru model so we can use it to train\n",
    "        self.tran_hidden_state = None\n",
    "\n",
    "        # store the full observations for the episode so we can train using replay\n",
    "        self.complete_observation_sequence = []\n",
    "        self.full_observation_sequence = []\n",
    "        self.full_action_sequence = []\n",
    "        self.full_reward_sequence = []\n",
    "\n",
    "        # store the observations while the agent is in exploration mode\n",
    "        self.exploring_observation_sequence = []\n",
    "        self.exploring_action_sequence = []\n",
    "        self.exploring_reward_sequence = []\n",
    "\n",
    "        self.policy_being_executed = [None]\n",
    "        self.previous_observation = None\n",
    "        self.previous_action_executed = None\n",
    "\n",
    "\n",
    "    def train_models(self, observations_full, actions, rewards):\n",
    "\n",
    "        pre_observations = observations_full[:-1]\n",
    "        post_observations = observations_full[1:]\n",
    "\n",
    "        #### TRAIN THE TRANSITION MODEL ####\n",
    "        if self.train_tran:\n",
    "\n",
    "            num_observations = pre_observations.shape[0]\n",
    "            # observation_dim = pre_observations.shape[1]\n",
    "            action_dim = actions.shape[1]\n",
    "            latent_dim = self.model_vae.latent_dim\n",
    "\n",
    "            # find the actual observed latent states using the vae\n",
    "            pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "            post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "            # set up the input training data that we use to train the transition model\n",
    "            z_train = np.concatenate([np.array(pre_latent_mean), actions], axis=1)\n",
    "\n",
    "            # we use the sequence to find the right hidden states to use as input\n",
    "            z_train_seq = z_train.reshape((1, num_observations, latent_dim + action_dim))\n",
    "            z_train_singles = z_train.reshape(num_observations, 1, latent_dim + action_dim)\n",
    "\n",
    "            # the previous hidden state is the memory after observing some sequences but it might be None if we're just starting\n",
    "            if self.tran_hidden_state is None:\n",
    "                self.tran_hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.tran_hidden_state))\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.tran_hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran.train_epochs, verbose=self.tran.show_training, batch_size=z_train_singles.shape[0])\n",
    "\n",
    "            # now find the new predicted hidden state that we will use for finding the policy\n",
    "            # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "            _, _, final_hidden_state, _ = self.tran((z_train_seq, self.tran_hidden_state))\n",
    "            # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "            z_pred, _, _, _ = self.tran((z_train_singles, h_states_for_training))\n",
    "\n",
    "            self.tran_hidden_state = final_hidden_state\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            # self.model_vae.fit(pre_observations_raw, epochs=self.vae_train_epochs, verbose=self.show_vae_training)\n",
    "            self.model_vae.fit(pre_observations, epochs=self.model_vae.train_epochs, verbose=self.model_vae.show_training, batch_size=pre_observations.shape[0])\n",
    "\n",
    "        #### TRAIN THE PRIOR MODEL ####\n",
    "        # TODO fix how this part should work\n",
    "        if self.train_prior:\n",
    "            # self.prior_model.train(post_observations, rewards, verbose=self.show_prior_training)\n",
    "            if max(rewards) > 0:\n",
    "                self.prior_model.train(post_observations, rewards)\n",
    "\n",
    "\n",
    "        #### TRAIN THE HABIT ACTION NET ####\n",
    "        if self.train_habit_net:\n",
    "\n",
    "            # TODO: Be careful with how the reward compares to EFE in terms of sign. Is the habit net maximising or minimising?\n",
    "\n",
    "            # prior_preferences_mean = tf.convert_to_tensor(self.given_prior_mean, dtype=\"float32\")\n",
    "            # prior_preferences_stddev = tf.convert_to_tensor(self.given_prior_stddev, dtype=\"float32\")\n",
    "            #\n",
    "            # prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "            #\n",
    "            # external_efe = -1 * tf.math.log(prior_dist.prob(post_observations))\n",
    "            # external_efe = external_efe.numpy().reshape(external_efe.shape[0], 1)\n",
    "            #\n",
    "            # one_over_external_efe = 1/external_efe\n",
    "            #\n",
    "            # ten_minus_external_efe = -1*external_efe + 10\n",
    "\n",
    "            # ten_minus_external_efe = ten_minus_external_efe.numpy().reshape(ten_minus_external_efe.shape[0], 1)\n",
    "\n",
    "            # one_over_external_efe = one_over_external_efe.numpy().reshape(one_over_external_efe.shape[0], 1)\n",
    "            # print(one_over_external_efe.shape)\n",
    "\n",
    "            # print(post_observations)\n",
    "            # print(one_over_external_efe)\n",
    "\n",
    "            # obs_utilities = self.prior_model(pre_observations)\n",
    "            # obs_utilities = tf.reduce_sum(obs_utilities, axis=-1)\n",
    "            # obs_utilities = obs_utilities.numpy().reshape(obs_utilities.shape[0], 1)\n",
    "            # # print(obs_utilities)\n",
    "            #\n",
    "            # cum_rewards = compute_discounted_cumulative_reward(obs_utilities, self.habit_action_model.discount_factor)\n",
    "\n",
    "            rewards = rewards.reshape(rewards.shape[0], 1)\n",
    "\n",
    "            cum_rewards = compute_discounted_cumulative_reward(rewards, self.habit_action_model.discount_factor)\n",
    "\n",
    "            # print(cum_rewards)\n",
    "            # print(cum_rewards.sum())\n",
    "\n",
    "            self.habit_action_model.fit(pre_observations, (actions, cum_rewards), epochs=self.habit_action_model.train_epochs, verbose=self.habit_action_model.show_training, batch_size=pre_observations.shape[0])\n",
    "\n",
    "            # else:\n",
    "            #     print(\"All negative\")\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "        \"\"\"\n",
    "        :param observation: needs to be [n, observation_dim] shape np array or tf tensor\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO do you take the mean or that latent here?\n",
    "        # get the latent state from this observation\n",
    "        _,  _, latent_state = self.model_vae.encoder(observation)\n",
    "        # latent_state = latent_state.numpy().reshape((1, latent_state.shape[0]))\n",
    "\n",
    "        # print(latent_state)\n",
    "        # select the policy\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(latent_state)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    # TODO Fix this so we can use different action dimensions\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            # print(\"POLICIES\", policies)\n",
    "            # print(\"FEEFS\", FEEFs)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by -1 to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape (self.n_policies, latent_dim) when z_t_minus is tensor with shape (1, latent_dim\n",
    "        prev_latent_mean = tf.squeeze(tf.stack([z_t_minus_one]*self.n_policies, axis=1))\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.tran_hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.tran_hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # print(prev_latent_mean)\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            # print(tran_input)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        if self.use_FEEF:\n",
    "            return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "        else:\n",
    "            return self.EFE(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "                # Compute the extrinisc approximation with the prior model\n",
    "                else:\n",
    "                    kl_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    kl_extrinsic = tf.reduce_sum(kl_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                kl_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"Extrinsic\", kl_extrinsic)\n",
    "            # print(\"Intrinsic\", kl_intrinsic)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    # TODO Find out how this works with the log probability extrinsic term\n",
    "    def EFE(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        EFEs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack(self.given_prior_mean), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack(self.given_prior_stddev), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    # compute extrinsic prior preferences term\n",
    "                    efe_extrinsic = -1 * tf.math.log(prior_dist.prob(predicted_likelihood))\n",
    "\n",
    "                # TODO Can I use the learned prior model here?\n",
    "                else:\n",
    "                    efe_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    efe_extrinsic = tf.reduce_sum(efe_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                efe_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"EX\", efe_extrinsic)\n",
    "            # print(\"IN\", kl_intrinsic)\n",
    "\n",
    "            EFE = efe_extrinsic - kl_intrinsic\n",
    "\n",
    "            EFEs.append(EFE)\n",
    "\n",
    "        return EFEs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "pln_hrzn = 5\n",
    "latent_dim = 2\n",
    "obs_dim = 2\n",
    "\n",
    "# make the VAE\n",
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2,  [0, 0], [0.3, 0.3], train_epochs=2, show_training=True)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the TRANSITION\n",
    "tran = TransitionGRU(2, 1, 2*pln_hrzn*latent_dim, 2, train_epochs=2, show_training=True)\n",
    "tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the HABIT ACTION NET\n",
    "habit_net = HabitualAction(latent_dim, 1, [20, 20], train_epochs=2, show_training=True)\n",
    "habit_net.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# make the PRIOR NET\n",
    "prior_model = PriorModelBellman(2, show_training=True)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "# prior_stddev = [0.1, 0.1]\n",
    "# prior_stddev = [3, 3]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "\n",
    "# daifa = DAIFAgentRecurrent(None,\n",
    "#                            vae,\n",
    "#                            tran,\n",
    "                           # habit_net,\n",
    "                           # scaled_prior_mean,\n",
    "                           # prior_stddev,\n",
    "                           # planning_horizon=pln_hrzn,\n",
    "                           # use_kl_extrinsic=True,\n",
    "                           # use_kl_intrinsic=True,\n",
    "                           # use_FEEF=False,\n",
    "                           # train_habit_net=True,\n",
    "                           # train_prior_model=False,\n",
    "                           # train_with_replay=True,\n",
    "                           # train_after_exploring=True,\n",
    "                           # use_fast_thinking=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.83333333, 0.        ])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           habit_net,\n",
    "                           planning_horizon=pln_hrzn,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           train_habit_net=True,\n",
    "                           train_prior_model=True,\n",
    "                           train_tran=True,\n",
    "                           train_after_exploring=True,\n",
    "                           use_fast_thinking=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "daifa.train_vae = False\n",
    "daifa.model_vae.show_training = False\n",
    "\n",
    "# daifa.train_tran = False\n",
    "# daifa.tran.show_training = False\n",
    "# daifa.train_prior = False\n",
    "# daifa.prior_model.show_training = False\n",
    "# #\n",
    "# daifa.habit_action_model.show_training = False\n",
    "# daifa.train_habit_net = False\n",
    "# # #\n",
    "# daifa.use_fast_thinking = True\n",
    "# daifa.habit_action_model.action_stddev = 0\n",
    "# #\n",
    "# daifa.model_vae.reconstruction_stddev = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.43311188  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9067\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9114\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.9096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.0716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.8155\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.9124\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -160.9153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -172.9368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.9261\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -56.8476\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -63.3173\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 52ms/step - kl_loss: 1.0012\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.0527\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -86.7293\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -93.3248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 37ms/step - kl_loss: 2.3747\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4310\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -154.2411\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -159.6034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 5.4409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4397\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -256.1680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -259.0957\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 29ms/step - kl_loss: 2.5829\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.6382\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.4142\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 30ms/step - kl_loss: 16.9437\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 15.7470\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -130.0941\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -130.5021\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 31ms/step - kl_loss: 12.5878\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.9613\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -240.3760\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -241.7776\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 32ms/step - kl_loss: 0.5450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4984\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -76.9710\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -79.1493\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 4.8420\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.7331\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -20.3974\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -21.3758\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.2242\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0408\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 407.9282\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15068500.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14916476.0000\n",
      "Success in episode 1 at time step 142\n",
      "Episode 2\n",
      "[-0.48809943  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5365\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6510\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -120.6019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 963us/step - loss: -119.0297\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 23ms/step - kl_loss: 1.3970\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5132\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -161.7666\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 982us/step - loss: -158.4948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 1.3478\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3809\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -116.0312\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -113.3982\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 1.1600\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1405\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -137.5484\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -135.5996\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 0.3833\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3743\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -253.6982\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -253.1117\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.3200\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3459\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -269.0041\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -268.6808\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 24ms/step - kl_loss: 0.2682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2843\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -244.4703\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -244.2330\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 4.3448\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3529\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -230.4172\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -230.0385\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 16ms/step - kl_loss: 3.2461\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1595\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -216.7373\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -216.1961\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 35ms/step - kl_loss: 1.4367\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3977\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -87.0466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -86.5053\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 1.1238\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.1357\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -114.1567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -113.7587\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 2.9744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9977\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -203.9348\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -203.7258\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 33ms/step - kl_loss: 0.4543\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -271.4946\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -271.4185\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 13ms/step - kl_loss: 3.2721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2144\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -263.4433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -263.3940\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 6.1539\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0545\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -248.9993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -248.9463\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 20ms/step - kl_loss: 1.5803\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.4818\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -221.8648\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -221.7518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 1.2761\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2011\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -84.4489\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -84.3294\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 13ms/step - kl_loss: 0.4597\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4820\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -90.0619\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -89.9614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 3.1723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -182.6395\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -182.5852\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 13ms/step - kl_loss: 0.4439\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4301\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -275.4266\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -275.4058\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1455\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1487\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -252.3400\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -252.3284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.5538\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 36ms/step - kl_loss: 1.5315\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -277.5491\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -277.5387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 2.0002\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9024\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -244.6122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -244.5963\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.3908\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3952\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -108.1540\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -108.1191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4899\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4335\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -53.9042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -53.8864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4192\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -126.0218\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -126.0057\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9002\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9296\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -224.1947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 969us/step - loss: -224.1870\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.9911\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9768\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -252.1363\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -252.1322\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7876\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7203\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -270.5360\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -270.5338\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9381\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9779\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -279.7390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -279.7371\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.1077\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0914\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -204.7396\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -204.7345\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4317\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4298\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -70.5535\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -70.5480\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0305\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -63.3701\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -63.3667\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9426\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9188\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -159.7897\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -159.7874\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -250.3365\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -250.3358\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 1.1808\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1693\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -269.2774\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -269.2770\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 2.3429\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2496\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -255.1154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -255.1151\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8471\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7902\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -243.4608\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -243.4605\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.0405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0297\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -216.6351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 953us/step - loss: -216.6342\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 14ms/step - kl_loss: 1.4744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -65.4336\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -65.4327\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3926\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -80.9802\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -80.9797\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 0.8146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8131\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -125.0946\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -125.0943\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6856\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6638\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -235.8287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -235.8287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -273.3042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -273.3043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 2.2315\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2845\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -253.8653\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -253.8654\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 0.5074\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4953\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -287.4670\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -287.4671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 0.9679\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9552\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -238.5334\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -238.5335\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.5237\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5238\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -73.2242\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -73.2242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 0.2427\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -69.2095\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -69.2096\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.7829\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7702\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -116.2975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -116.2977\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.5706\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.5694\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -220.0219\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 981us/step - loss: -220.0222\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 2.1788\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -278.2335\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -278.2339\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 1.7134\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5640\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -287.1492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -287.1494\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 38ms/step - kl_loss: 1.5805\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.5620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -266.1220\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -266.1222\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.5190\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4571\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -273.5720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -273.5721\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.4324\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4210\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -230.0445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -230.0446\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.0568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0661\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -76.5736\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -76.5737\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.5646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5444\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -52.3649\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -52.3650\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.9794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9955\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -95.7794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -95.7797\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 13ms/step - kl_loss: 0.4516\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4443\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -169.9493\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -169.9496\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5839\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5328\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -228.4267\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -228.4272\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 3.2789\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1379\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -273.4536\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -273.4540\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.9525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -286.0099\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -286.0104\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 9.9578\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.6860\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.8971\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.8970\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.8199\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -104.9569\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -104.9570\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 4.6697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4745\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -265.2062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -265.2063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 2.2965\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1521\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.0173\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -16.0174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 39ms/step - kl_loss: 1.4249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3424\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.8691\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.8691\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 3.1686\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.5282\n",
      "5/5 [==============================] - 0s 655us/step - loss: 71.1150\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 599152896.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 569919552.0000\n",
      "Success in episode 2 at time step 826\n",
      "Episode 3\n",
      "[-0.50109035  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9596\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.5759\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -168.5098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -153.6701\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 3.3202\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9888\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -54.5528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 886us/step - loss: -45.8232\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.7822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0198\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.4082\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -37.0687\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 3.4716\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0457\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -66.1923\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -60.4171\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 8.5391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.2248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -113.1709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -110.5007\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 6.0832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.3048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -47.8155\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -47.1996\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 8.0538\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0675\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -147.2424\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -144.7070\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 8.5834\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.4725\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -21.9142\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -20.6444\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 9.9947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.9111\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.0503\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.9608\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 16.4932\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 16.4096\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.0764\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.9385\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 25.5554\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.1962\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -117.7191\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 989us/step - loss: -118.6165\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 27.2352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 25.6528\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -64.5124\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -65.4435\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 3.9128\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2061\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.1897\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 961us/step - loss: -7.2749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 22.7944\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 22.3742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -131.3361\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -130.7646\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 7.6390\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1913\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1849\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 12.1328\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 11.4054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0688\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 7.2835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4941\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 287.9773\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10745146.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10697816.0000\n",
      "Success in episode 3 at time step 203\n",
      "Episode 4\n",
      "[-0.53553754  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 4.4279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3245\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -44.0493\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -44.3424\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.3460\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0130\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -26.1239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -25.7363\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 1.2915\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2792\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.2351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.2109\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.7267\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6669\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.0210\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.9660\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 2.5665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3083\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -45.8810\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.7806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.7144\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6545\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -63.4779\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -63.4673\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 4.9527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.8537\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -28.0258\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -27.9863\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 11.5255\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.6548\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -105.4728\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -105.0874\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 16.4228\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 16.0086\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.5308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.4497\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 4.0269\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.7050\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1497\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1499\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 4.1597\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7440\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 425.6190\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3694882.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3686366.0000\n",
      "Success in episode 4 at time step 138\n",
      "Episode 5\n",
      "[-0.55319786  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 42ms/step - kl_loss: 2.5665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4272\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.8721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.8475\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.0784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9161\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.1235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.1059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 2.1433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1784\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.0794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.0599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 3.6095\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6303\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -84.2418\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -84.1816\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4650\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4546\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.7459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.7448\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9108\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9321\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -43.2927\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -43.2389\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4041\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -78.0868\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -77.8927\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 986us/step - loss: -0.1217\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3250\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3261\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1378\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5401\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 466.3101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2279439.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2274051.2500\n",
      "Success in episode 5 at time step 124\n",
      "Episode 6\n",
      "[-0.5121811  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5883\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5796\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -67.0272\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -67.0519\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4885\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -20.3669\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -20.2801\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2833\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.0459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 976us/step - loss: -3.0385\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3528\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.8300\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.8112\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1407\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -85.5911\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -85.5387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6687\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6608\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -131.9174\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -131.8852\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8799\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8347\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -99.5894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 989us/step - loss: -99.5325\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4914\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -87.7744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -87.6973\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5028\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4777\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.7795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.7657\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6937\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6775\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.4860\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.4850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6930\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.9528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.9498\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9643\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9530\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -50.9931\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -50.9839\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8705\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8587\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -121.2122\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -121.2027\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2033\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -25.7518\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 957us/step - loss: -25.7494\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8020\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7692\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -69.0694\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 977us/step - loss: -69.0510\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5268\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -24.5369\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -24.5274\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2542\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2494\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0171\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0171\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2716\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2653\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0752\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0752\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.0326\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0138\n",
      "2/2 [==============================] - 0s 966us/step - loss: 257.7493\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13241542.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13178117.0000\n",
      "Success in episode 6 at time step 227\n",
      "Episode 7\n",
      "[-0.49968904  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6065\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -48.9762\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -49.1234\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3948\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3912\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.3331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -21.9311\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5465\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.9409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 993us/step - loss: -3.8962\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3112\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -19.3004\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -19.1739\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1870\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -71.6169\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -71.3740\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6837\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -130.2018\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -129.9531\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5085\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5099\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -51.7133\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 998us/step - loss: -51.5549\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5696\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5455\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -55.4495\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -55.2417\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0581\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.8537\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 974us/step - loss: -1.8436\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9153\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.8602\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.8573\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6942\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6892\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.6177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.6006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7714\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7694\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -24.7373\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -24.7077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2258\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -53.2302\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -53.1857\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.6631\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5643\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -59.3574\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 951us/step - loss: -59.3899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.8801\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7355\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -24.0257\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -24.0059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3176\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3213\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -35.2938\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -35.2496\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9700\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9348\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.6362\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 882us/step - loss: -0.6364\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9368\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9363\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.3685\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.3685\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1290\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1081\n",
      "2/2 [==============================] - 0s 992us/step - loss: 257.6680\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9656920.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9606576.0000\n",
      "Success in episode 7 at time step 224\n",
      "Episode 8\n",
      "[-0.5505428  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8367\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8384\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.5062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.5112\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2535\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2494\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.0929\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.0421\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5076\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4902\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.2358\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.1705\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2791\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2833\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -60.1741\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -59.8902\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4496\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4334\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -118.7013\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 985us/step - loss: -118.4516\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3752\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3743\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.4116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -16.3778\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1184\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1105\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -45.8001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.5229\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8726\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8251\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.2077\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.1849\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0612\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0611\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3266\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.3512\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.3495\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.3657\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3617\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 445.0880\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2375007.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2368015.5000\n",
      "Success in episode 8 at time step 131\n",
      "Episode 9\n",
      "[-0.5483442  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1965\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1688\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.6032\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.6172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6513\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.2320\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.2208\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6350\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6193\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.1974\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.1672\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4443\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.3861\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 973us/step - loss: -42.3055\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3179\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3108\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -118.4523\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 907us/step - loss: -118.3453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5694\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5691\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -137.4452\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -137.2570\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3006\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3009\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -94.7452\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -94.6090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3486\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.4089\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.3889\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2506\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.1828\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.1792\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9757\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9708\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.3974\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.3919\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4675\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -40.0391\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -40.0214\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2232\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2251\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -87.2188\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -87.1882\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4852\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4800\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -84.5645\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -84.5411\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2010\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1979\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -81.4380\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -81.4075\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8575\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.4567\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.4525\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0716\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.6482\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 905us/step - loss: -1.6477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7397\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7355\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.3731\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.3724\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7048\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6985\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.9129\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.9111\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7113\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7149\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -89.0001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 970us/step - loss: -88.9934\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9862\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9695\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.1660\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.1665\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7626\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7423\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -60.9524\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -60.9478\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.2450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.9477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.2615\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -16.2583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6376\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5048\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1367\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 954us/step - loss: -0.1367\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4498\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4769\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 926us/step - loss: -0.4119\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5735\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5542\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 191.9773\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18051310.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17807164.0000\n",
      "Success in episode 9 at time step 301\n",
      "Episode 10\n",
      "[-0.4058076  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6019\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5651\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -17.4449\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -16.6338\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6410\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6549\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.9036\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -22.1441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5219\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -32.8235\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -32.0927\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5925\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -48.2702\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 960us/step - loss: -47.5555\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4693\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -45.6244\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -44.9836\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1905\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1924\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -72.9582\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -72.0887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1473\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1508\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -60.8335\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 976us/step - loss: -60.1721\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6447\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -11.4046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -11.2566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.9765\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.9436\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9864\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9746\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -47.9829\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -47.7536\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9709\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9671\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -35.7428\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -35.5819\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7389\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7199\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -97.1558\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -96.8906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5325\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5094\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -59.0763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -58.8900\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2029\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1967\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.2129\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -42.1004\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5751\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5652\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -19.0785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -19.0326\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0332\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0416\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.6813\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.6664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9797\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9732\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -15.5797\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -15.5597\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4365\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -48.0886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -48.0419\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0425\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -92.4308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -92.3664\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6388\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6280\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -50.5175\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 997us/step - loss: -50.4793\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7114\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -18.6936\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -18.6784\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9636\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.9245\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.9221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.0817\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.0769\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5377\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5393\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -15.4612\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 916us/step - loss: -15.4563\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9527\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9504\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -30.8582\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -30.8515\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5910\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -84.6294\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -84.6153\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1381\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1328\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -56.7943\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -56.7839\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4094\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3974\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -37.7383\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 980us/step - loss: -37.7325\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5180\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.5405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.5401\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0954\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.2786\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.2783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7506\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -31.5873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -31.5855\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2432\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2415\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -55.8010\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -55.7986\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7924\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -85.1545\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -85.1510\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9001\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -56.9546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -56.9522\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6568\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6522\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -39.1482\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -39.1468\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0961\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0945\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.6416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.6415\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1846\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1865\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.1507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.1507\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7591\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -27.3746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -27.3743\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -73.9220\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -73.9213\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0809\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0702\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -72.3400\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -72.3393\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5008\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -57.6979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 991us/step - loss: -57.6975\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.0184\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -16.0183\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2476\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.1759\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.1759\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9094\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.7037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 961us/step - loss: -16.7036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4434\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4490\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -19.9070\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -19.9070\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6516\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6277\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -102.5905\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -102.5904\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4837\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4856\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -108.7403\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: -108.7402\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4948\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4831\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -65.8681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -65.8680\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2840\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2849\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.4980\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -16.4980\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9401\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9428\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.4453\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.4453\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3897\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3880\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.1526\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.1526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6499\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -28.6966\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 982us/step - loss: -28.6966\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2486\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -92.2571\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1000us/step - loss: -92.2571\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4355\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -112.1277\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -112.1278\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -55.7450\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -55.7450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1285\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1295\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -36.5972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 962us/step - loss: -36.5972\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0924\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0915\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.1444\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.1444\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9442\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.6189\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.6189\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7559\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7463\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -33.5724\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 966us/step - loss: -33.5724\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6178\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -104.2543\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -104.2544\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6396\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.6450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -104.7661\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 910us/step - loss: -104.7662\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1201\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1116\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -41.7932\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 975us/step - loss: -41.7932\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2199\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1846\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -13.0357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -13.0357\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0772\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0775\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.2565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.2565\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3719\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3804\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.7241\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.7241\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4801\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4777\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -27.4035\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 976us/step - loss: -27.4036\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3845\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -102.9440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -102.9441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7707\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7592\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -107.9032\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -107.9033\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8760\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8831\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -52.2672\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -52.2673\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8601\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -11.7956\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -11.7956\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1515\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1500\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.3685\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: -1.3685\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4420\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.9556\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.9556\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2825\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2772\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -34.0511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -34.0512\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8939\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8964\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -93.2287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 940us/step - loss: -93.2287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5285\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -108.1183\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -108.1184\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6602\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6432\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -45.1435\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.1436\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1780\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1522\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.0045\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.0045\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0113\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9911\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.4924\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.4924\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.4260\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 951us/step - loss: -6.4260\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7098\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7126\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -18.2452\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -18.2452\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0588\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -89.8445\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -89.8445\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8457\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8316\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -102.8785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -102.8785\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3347\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3128\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16300928.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16778186.0000\n",
      "No Success\n",
      "Episode 11\n",
      "[-0.56503767  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.1180\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.6865\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.8014\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9288\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0170\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.5081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.5847\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6519\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7339\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.3892\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.7125\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1651\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.1580\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -41.1671\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -41.7725\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3007\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3250\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -122.3214\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -123.4241\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9081\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -133.1075\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 970us/step - loss: -134.2294\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4179\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4177\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -77.2940\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -78.2210\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5193\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.3274\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.4535\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1090\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1067\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.0284\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.0515\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3092\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2944\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -5.2633\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.2987\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3851\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.3677\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -74.1872\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -74.3780\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6171\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6026\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -96.2919\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -96.4159\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6539\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.6537\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -97.1034\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -97.2518\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8646\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8687\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -100.0805\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -100.2824\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9322\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.4485\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -42.5574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3456\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3066\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.1409\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.1450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4238\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.3979\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 954us/step - loss: -1.4006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9246\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.1381\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -16.1533\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0810\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0363\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -38.1546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -38.1787\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6405\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.5922\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -40.5416\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 978us/step - loss: -40.5251\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3870\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2337\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.1246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.1258\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9021\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.7993\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -40.8475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -40.8802\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7788\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7549\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.2562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.2560\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7749\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.2792\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.2792\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.8291\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7831\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 195.8220\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19747866.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19530396.0000\n",
      "Success in episode 11 at time step 299\n",
      "Episode 12\n",
      "[-0.40935472  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9670\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9032\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -46.7682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.4705\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6675\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6692\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.5929\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.2807\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9707\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -36.9440\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -36.3870\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8185\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8212\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -15.4408\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -15.2156\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6446\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6629\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -89.0804\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -88.4349\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9629\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9537\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -63.4015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -62.9094\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1097\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1034\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -76.3227\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -75.7446\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4200\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4238\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.2871\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.1779\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7614\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7705\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.8330\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 986us/step - loss: -6.7775\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3243\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.3052\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.2682\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6902\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.6794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -23.1192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -23.0502\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0704\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0647\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -102.2471\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -102.0561\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8147\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8203\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -91.3708\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -91.2077\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8336\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8261\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -56.9041\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -56.7735\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3419\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.5052\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -22.4563\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5305\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5083\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.5955\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.5829\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7681\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.7229\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -18.0951\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -18.0761\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8702\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8585\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -49.6313\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -49.6008\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9369\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9477\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -43.3448\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -43.3190\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0061\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -93.6752\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -93.6358\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1739\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1652\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -94.7092\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 948us/step - loss: -94.6722\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7066\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7010\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -33.6514\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -33.6350\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2359\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2256\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.3575\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.3558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4248\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3743\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.9429\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.9410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5303\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5130\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -56.1488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -56.1410\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7345\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -71.6898\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -71.6823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8982\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8542\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -114.4715\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -114.4623\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5362\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -97.9766\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -97.9690\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5376\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5300\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -29.7341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -29.7310\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0841\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0805\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.9873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.9871\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1586\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.8796\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.8791\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3302\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3250\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -45.4866\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.4852\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9975\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9954\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -91.4954\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -91.4935\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5033\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -118.3047\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -118.3024\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1489\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.1201\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -104.2677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -104.2659\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.9872\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -11.9287\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -11.9284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8632\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8573\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.9368\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.9367\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2435\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -15.7065\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -15.7063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3729\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.3358\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -44.0733\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 975us/step - loss: -44.0730\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3191\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3260\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -70.0251\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -70.0248\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5946\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5728\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -121.1211\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -121.1206\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5968\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -78.6012\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -78.6009\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4195\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3760\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.1477\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.1476\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2977\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3119\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.2500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.2500\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2470\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2552\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.5012\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.5011\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4110\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4024\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -28.5578\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -28.5577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8178\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7928\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -117.3317\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 970us/step - loss: -117.3317\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4001\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -96.1998\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -96.1998\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8611\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -70.0696\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -70.0696\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7278\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7126\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -18.4461\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -18.4461\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6890\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6752\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.8750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.8750\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8677\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -17.6071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 987us/step - loss: -17.6071\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7540\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -30.0999\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -30.0999\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4100\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.4081\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -80.1185\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 973us/step - loss: -80.1185\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -117.1637\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 975us/step - loss: -117.1637\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8811\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8918\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -73.1699\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -73.1699\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8496\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8529\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -23.4950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -23.4951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0714\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0677\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.1770\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.1770\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3968\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3959\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.6169\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.6169\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0215\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -34.6216\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -34.6216\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3271\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3176\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -86.2510\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -86.2511\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8164\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -106.0690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -106.0690\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8100\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -99.1861\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -99.1862\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5266\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5287\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.0603\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.0603\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6132\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6058\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.5750\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.5750\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9467\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9490\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.2184\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.2184\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1776\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1737\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -11.7017\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -11.7017\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9802\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -63.6674\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 951us/step - loss: -63.6674\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3350\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -123.6784\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 918us/step - loss: -123.6784\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3452\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -98.7656\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -98.7656\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0831\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.0813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -28.0208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -28.0208\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7031\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7052\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.5836\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.5836\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.5515\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.0870\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.0870\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7169\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7105\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.9253\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 969us/step - loss: -16.9253\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.4066\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -74.7614\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -74.7615\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3441\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3387\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -143.1226\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -143.1227\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5723\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5493\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -98.2910\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -98.2911\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9525\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9591\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -38.1395\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -38.1395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5582\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.4193\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.4193\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1696\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.8596\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.8596\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.5806\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5814\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.7948\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.7948\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 24ms/step - kl_loss: 0.5197\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5163\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -48.3244\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 963us/step - loss: -48.3244\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4762\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4699\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -19002820.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -19353714.0000\n",
      "No Success\n",
      "Episode 13\n",
      "[-0.47527698  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.3106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3090\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.9245\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -18.0346\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 13ms/step - kl_loss: 1.3175\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3268\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -13.5275\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -14.1089\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 25ms/step - kl_loss: 0.8512\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8281\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.4116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 972us/step - loss: -7.6353\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.2192\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.2177\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -53.5291\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -54.3110\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.5180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5164\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -107.8960\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -108.8212\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.3748\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.3735\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -132.1794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -133.1414\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 15ms/step - kl_loss: 1.1565\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -91.7756\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -92.5172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.8534\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8525\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.8199\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.9588\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.7459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 0.7463\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.7862\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.8296\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.7528\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7478\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.7353\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.7978\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 2.1577\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1497\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -56.7017\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -56.8828\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.2096\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2145\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -104.8886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -105.0536\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 0.7549\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7452\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -129.8342\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -130.0228\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 1.7336\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7405\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -89.5931\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -89.7620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 2.6997\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6949\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.9536\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -16.9997\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.4657\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4583\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.2638\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 998us/step - loss: -8.2823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.2379\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2284\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -28.6190\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -28.6538\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.6105\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6086\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -41.7592\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 980us/step - loss: -41.7935\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 2.1980\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -72.6729\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -72.7106\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.3236\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3203\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -144.8773\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -144.9181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 1.3638\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3506\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -115.9011\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -115.9420\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 2.4895\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4941\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -50.2322\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -50.2522\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.8755\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8768\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.7886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.7906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 3.2767\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2745\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.0208\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.0232\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.3030\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3013\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -61.7977\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -61.8079\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.2684\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2544\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -159.7188\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 927us/step - loss: -159.7276\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 0.9892\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9750\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -141.0926\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -141.1017\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 2.1913\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -87.5562\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 986us/step - loss: -87.5645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.7123\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7174\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.4623\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.4640\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.7395\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7274\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -10.1203\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -10.1215\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.2830\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2761\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.3555\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.3562\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.8683\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8729\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -32.7822\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -32.7836\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 0.9986\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0000\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -146.5671\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -146.5694\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 0.3858\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3874\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -161.7102\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -161.7127\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.1035\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1016\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -107.8449\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 995us/step - loss: -107.8473\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 0.6690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6663\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -31.5612\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -31.5622\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 0.7078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.8571\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.8572\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 12ms/step - kl_loss: 0.2161\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2140\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.3425\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: -9.3427\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 6.0042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 5.9947\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -22.2663\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -22.2666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.2374\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -118.1808\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -118.1815\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6083\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6085\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -150.5541\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -150.5546\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 2.7828\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7748\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -104.7948\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -104.7953\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6884\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -68.5802\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -68.5806\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 2.3701\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3694\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.5399\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.5400\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.4342\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4309\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.0265\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.0265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 3.6332\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6247\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -39.0142\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 967us/step - loss: -39.0143\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.4385\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4338\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -75.7613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -75.7614\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.9008\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -150.1447\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -150.1448\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4868\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4900\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -127.8511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -127.8513\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 1.4587\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4539\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -76.8253\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -76.8254\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 2.1325\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1358\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.9360\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.9361\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 1.3907\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3782\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - loss: -2.4864\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.4864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 1.4798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4863\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -27.1948\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -27.1949\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 1.4507\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4505\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -68.3692\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -68.3692\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6712\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -156.9570\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 959us/step - loss: -156.9570\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.9301\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9356\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -74.6032\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -74.6032\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1954\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1961\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -104.0984\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -104.0985\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8041\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8050\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.4924\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.4924\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 22ms/step - kl_loss: 1.5005\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4923\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.0459\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.0459\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.9336\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9368\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.6951\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.6951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.8818\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8711\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -18.4372\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -18.4372\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0727\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.0074\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -40.9189\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -40.9189\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 4.1431\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0400\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -59.3930\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -59.3930\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 3.9664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.9749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -31.7113\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -31.7113\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 5.8028\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.6958\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -51.5508\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -51.5509\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7685\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.2951\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.2951\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7550\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.6798\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.3097\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.3097\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.6881\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6674\n",
      "5/5 [==============================] - 0s 590us/step - loss: 71.6325\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 193949136.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 182379392.0000\n",
      "Success in episode 13 at time step 820\n",
      "Episode 14\n",
      "[-0.52083135  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 1.6088\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.6346\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.4336\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.1659\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2948\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3318\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.2843\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.0221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7466\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.6620\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.9903\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.6774\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.3060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3513\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -41.2792\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -38.0924\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.6431\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6299\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -73.8299\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -69.7074\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7823\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8135\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -47.9848\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -45.8959\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 3.4731\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4945\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -18.2508\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -17.2032\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5352\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5380\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 967us/step - loss: -0.3787\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8004\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7987\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1716\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1588\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4762\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.6343\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.6066\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.4023\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3946\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.8355\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -9.6101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2058\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1992\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -23.1412\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -22.7322\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6829\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6757\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -49.8410\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -49.2636\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0481\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -27.5546\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -27.2013\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5762\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 970us/step - loss: -0.1433\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.3967\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3864\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0206\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0198\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9919\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9942\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1705\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 969us/step - loss: -0.1685\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3794\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3551\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.4864\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.4692\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0589\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -24.4046\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -24.3059\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8307\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8164\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -38.1771\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -38.0762\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1664\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1417\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -30.1583\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 990us/step - loss: -30.0826\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.7641\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7400\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.0680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.0585\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7197\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7052\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0207\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 975us/step - loss: -0.0208\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7324\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7405\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0305\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0304\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5311\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5290\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1788\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1783\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 9ms/step - kl_loss: 1.1164\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.8612\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.8532\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8328\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -33.0454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -33.0235\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3401\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3507\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -59.5721\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 858us/step - loss: -59.5442\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5697\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5637\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -25.2050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -25.1920\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 8ms/step - kl_loss: 0.7226\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7019\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.0249\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.0207\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8367\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0263\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0263\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3480\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3432\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0167\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0167\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3916\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.3887\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.5506\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.5504\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5239\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.4982\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.7265\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 950us/step - loss: -1.7260\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7205\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.6558\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.5890\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.5885\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 10.8781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.5617\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -102.4148\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -102.4184\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 9.4928\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 8.6626\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -133.6744\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -133.6813\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.9874\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.8219\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.2138\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.2134\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1156\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1388\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.9609\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 995us/step - loss: -0.9609\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 3.0909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0959\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.5392\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.5392\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0472\n",
      "3/3 [==============================] - 0s 672us/step - loss: 118.0362\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21896090.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21754134.0000\n",
      "Success in episode 14 at time step 496\n",
      "Episode 15\n",
      "[-0.49460235  0.        ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2483\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2645\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1542\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1434\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5073\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4976\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.3135\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 978us/step - loss: -0.3043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5338\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4798\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.2065\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 995us/step - loss: -1.1809\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7140\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.7286\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.9232\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.7381\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8593\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8073\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -36.4898\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 987us/step - loss: -36.0666\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2838\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2841\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -27.4304\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -27.1577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4244\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4306\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.1039\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.0437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4764\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4280\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4725\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 978us/step - loss: -0.4683\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 2.5659\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5529\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0964\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0955\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4170\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4320\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.8500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.8465\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0402\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0450\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.8917\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 956us/step - loss: -5.8704\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6073\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 3.5823\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -25.5131\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 947us/step - loss: -25.4374\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4677\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4481\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -32.5856\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 944us/step - loss: -32.5196\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8781\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8557\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.8351\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.8240\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1930\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.1549\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0324\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 931us/step - loss: -0.0323\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5380\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5412\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.2276e-04\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 967us/step - loss: -5.3142e-04\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9204\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.3922\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.3907\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0347\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0354\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.7091\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.7045\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7191\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -37.0993\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -37.0781\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1285\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1042\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -23.3474\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -23.3342\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9217\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9181\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.1800\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.1747\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.8884\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8731\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1165\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1165\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2143\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2004\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0106\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0106\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3480\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3552\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4630\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.4628\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6894\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6885\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.6915\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.6906\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.8624\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8619\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -19.0423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -19.0395\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4118\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.4056\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -41.8652\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -41.8606\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9050\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8533\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.3078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.3063\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0054\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9880\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.3232\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 952us/step - loss: -0.3231\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0342\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.0273\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0361\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0361\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8302\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8381\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 982us/step - loss: -0.0795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 1.2978\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3006\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.3114\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.3112\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7341\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7223\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -16.5366\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -16.5361\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5873\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5866\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -51.7514\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -51.7503\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3240\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3202\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -18.5878\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 938us/step - loss: -18.5873\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6148\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.6511\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 960us/step - loss: -1.6511\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7789\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7850\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0791\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 980us/step - loss: -0.0791\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 1.9686\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9679\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0168\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0168\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8454\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.8324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.7520\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.7520\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9682\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9529\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -13.2717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -13.2716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3952\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -28.0069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 930us/step - loss: -28.0068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0501\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0744\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -54.9306\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -54.9304\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9346\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9170\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -19.6475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -19.6474\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.6613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6593\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1308\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1308\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5776\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5778\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.4924e-04\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.4924e-04\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8364\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8352\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1336\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1336\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4346\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4341\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.5314\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.5314\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.7785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7788\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.9124\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 924us/step - loss: -5.9123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5745\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5742\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -42.8991\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -42.8991\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7150\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7098\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -41.7349\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 936us/step - loss: -41.7348\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3538\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3462\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.2200\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -7.2200\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 1.0246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0147\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1101\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3211\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3265\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0180\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0180\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3735\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3732\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9179\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.9016\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.3923\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.3923\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4140\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.2289\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.2289\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0808\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0479\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -20.1886\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -20.1886\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2981\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2799\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -57.6186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 905us/step - loss: -57.6186\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0372\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0399\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -11.6785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -11.6785\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3269\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.3116\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.3116\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8860\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8940\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0889\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0889\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9992\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.9924\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0012\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0012\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4891\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4903\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4267\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 960us/step - loss: -0.4267\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6213\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6112\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.0541\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.0541\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.4658\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 6.3324\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.9842\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 941us/step - loss: -3.9842\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6950\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7199\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -15.9329\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 983us/step - loss: -15.9329\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1345\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.1305\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -114.0419\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -114.0419\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 9.8947\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8600\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -92.1995\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -92.1995\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3033\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.4692\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.5522\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.5522\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9384\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8593\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4969\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 955us/step - loss: -0.4969\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5060\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4653\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4259\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 987us/step - loss: -0.4259\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.3925\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3635\n",
      "5/5 [==============================] - 0s 514us/step - loss: 68.1817\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49548916.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 47666792.0000\n",
      "Success in episode 15 at time step 860\n",
      "Episode 16\n",
      "[-0.4815293  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5634\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.4357\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0921\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1043\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 2.2237\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3242\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0686\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0566\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.9666\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.8492\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.7768\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 0.5584\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5899\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.7370\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.4966\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7576\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7680\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.4423\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 873us/step - loss: -5.9954\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3551\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.3632\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.8282\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.4340\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 1.9333\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9606\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.2892\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.2686\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 11ms/step - kl_loss: 2.0387\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0297\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0246\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0262\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 1.9746\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9262\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0502\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0480\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 3.1177\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0381\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4259\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.4146\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.9080\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.8749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.9145\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.8591\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 10ms/step - kl_loss: 2.7694\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6934\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.9973\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.9418\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 7ms/step - kl_loss: 1.0279\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0233\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.1487\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.0650\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 1.4169\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3832\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1384\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1363\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.4857\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4613\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0971\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 923us/step - loss: -0.0976\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6827\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.6049\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0025\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0024\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6844\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6824\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0832\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0824\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5516\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5441\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.7295\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 999us/step - loss: -2.7172\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0603\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0148\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.8916\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.8641\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3748\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3741\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.2612\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.2420\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9137\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9291\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1288\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1281\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5909\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5829\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0244\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0244\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1535\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1567\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0150\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0150\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2810\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.2822\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1194\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7295\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7281\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.4552\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.4522\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6785\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6597\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.3747\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.3716\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8094\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8156\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -9.5244\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 934us/step - loss: -9.5178\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2835\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2723\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.7748\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.7729\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7292\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.7289\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1901\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1900\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7502\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7511\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0197\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0197\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0706\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0599\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1095\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 985us/step - loss: -0.1094\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3042\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.2578\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.8354\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 940us/step - loss: -0.8352\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4613\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4644\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.9037\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.9031\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1504\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.7512\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.7491\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4972\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4900\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.5359\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.5352\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5566\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5534\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.7186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.7185\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5379\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5351\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0475\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2433\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.2462\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0333\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0333\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0133\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.0119\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.2065\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 945us/step - loss: -0.2065\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1137\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1142\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.5787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1000us/step - loss: -1.5786\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4959\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 4.4437\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.7990\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.7985\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5196\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5551\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -12.0071\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -12.0068\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0146\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0114\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.8176\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -1.8175\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2130\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2139\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0953\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0953\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0124\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0057\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1083\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 959us/step - loss: -0.1083\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3080\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3104\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0115\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0115\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1172\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1123\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1154\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5869\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5736\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.6309\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 986us/step - loss: -1.6309\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3350\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3304\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -8.1787\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -8.1786\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6222\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5968\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -6.3618\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.3618\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6754\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6833\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.1795\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 993us/step - loss: -4.1794\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4615\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4577\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1388\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1388\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2897\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2895\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0900\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 995us/step - loss: -0.0900\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5485\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5295\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1162\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1162\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6420\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6112\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0040\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0040\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.7223\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 12.6556\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.2809\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.2809\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2815\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 5.0649\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0043\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0043\n",
      "training on full data\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 1.9933\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9766\n",
      "4/4 [==============================] - 0s 566us/step - loss: 83.8953\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5410777.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5236186.0000\n",
      "Success in episode 16 at time step 699\n",
      "Episode 17\n",
      "[-0.5495009  0.       ]\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1898\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1798\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0380\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0404\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6500\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.6749\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0025\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 923us/step - loss: -0.0029\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6101\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6019\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0757\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0728\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2486\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2879\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -3.2569\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -3.1748\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3151\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3154\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -7.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -6.8392\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9572\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9221\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.9357\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.8605\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3802\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3613\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.4172\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.4062\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0709\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1488\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1501\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3853\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.3795\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0331\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0333\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4717\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4732\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0055\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0054\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1680\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 1.1551\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.5792\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.5754\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8665\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7981\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -5.6819\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.6526\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8475\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 2.8914\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.3843\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.3574\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8560\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8576\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.6124\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.6080\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5929\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - kl_loss: 0.5924\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0798\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.0800\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8404\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8238\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1078\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 958us/step - loss: -0.1078\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2996\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2996\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.0039\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 920us/step - loss: -0.0039\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8154\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8032\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -0.1711\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -0.1708\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2135\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2223\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -2.5792\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -2.5766\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0690\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0354\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -4.7186\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -4.7129\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0700\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0712\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.6720\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: -5.6660\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9558\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9467\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 2ms/step - loss: -1.2065\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 973us/step - loss: -1.2054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "daifa, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=50, render_env=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.11605959 -0.1665335 ]\n",
      " [ 0.1302168  -0.14987439]\n",
      " [ 0.14439172 -0.13310179]\n",
      " [ 0.15857817 -0.11622441]\n",
      " [ 0.1727699  -0.09925119]\n",
      " [ 0.1869605  -0.08219142]\n",
      " [ 0.20114371 -0.06505438]\n",
      " [ 0.21531293 -0.04784992]\n",
      " [ 0.22946163 -0.0305879 ]\n",
      " [ 0.24358335 -0.0132783 ]\n",
      " [ 0.2576715   0.00406859]\n",
      " [ 0.2717195   0.0214424 ]\n",
      " [ 0.28572083  0.03883282]\n",
      " [ 0.29966882  0.05622922]\n",
      " [ 0.31355703  0.07362098]\n",
      " [ 0.32737905  0.09099756]\n",
      " [ 0.34112844  0.10834828]\n",
      " [ 0.35479873  0.12566257]\n",
      " [ 0.36838382  0.14292988]\n",
      " [ 0.38187745  0.16013968]\n",
      " [ 0.39527366  0.1772816 ]\n",
      " [ 0.40856645  0.19434541]\n",
      " [ 0.42174998  0.2113209 ]\n",
      " [ 0.43481866  0.22819829]\n",
      " [ 0.44776687  0.24496765]\n",
      " [ 0.46058938  0.26161957]\n",
      " [ 0.47328103  0.27814472]\n",
      " [ 0.4858367   0.29453406]\n",
      " [ 0.4982517   0.3107789 ]\n",
      " [ 0.51052135  0.32687074]\n",
      " [ 0.5226414   0.34280145]\n",
      " [ 0.5346076   0.35856324]\n",
      " [ 0.5464159   0.37414864]\n",
      " [ 0.55806273  0.3895505 ]\n",
      " [ 0.5695444   0.4047622 ]\n",
      " [ 0.58085775  0.4197773 ]\n",
      " [ 0.59199977  0.4345898 ]\n",
      " [ 0.6029676   0.44919413]\n",
      " [ 0.6137587   0.46358517]\n",
      " [ 0.62437075  0.47775805]\n",
      " [ 0.6348018   0.4917083 ]\n",
      " [ 0.6450497   0.50543207]\n",
      " [ 0.65511316  0.51892567]\n",
      " [ 0.6649906   0.53218585]\n",
      " [ 0.67468107  0.54521   ]\n",
      " [ 0.6841835   0.5579955 ]\n",
      " [ 0.69349736  0.5705403 ]\n",
      " [ 0.7026221   0.5828428 ]\n",
      " [ 0.71155745  0.5949017 ]\n",
      " [ 0.72030354  0.606716  ]\n",
      " [ 0.72886044  0.618285  ]\n",
      " [ 0.7372286   0.6296086 ]\n",
      " [ 0.74540854  0.6406868 ]\n",
      " [ 0.753401    0.6515198 ]\n",
      " [ 0.761207    0.6621084 ]\n",
      " [ 0.7688276   0.6724535 ]\n",
      " [ 0.77626413  0.6825564 ]\n",
      " [ 0.78351796  0.6924184 ]\n",
      " [ 0.7905908   0.7020412 ]\n",
      " [ 0.7974843   0.711427  ]\n",
      " [ 0.8042003   0.7205778 ]\n",
      " [ 0.8107407   0.72949594]\n",
      " [ 0.8171079   0.73818403]\n",
      " [ 0.82330376  0.7466448 ]\n",
      " [ 0.829331    0.7548811 ]\n",
      " [ 0.83519167  0.762896  ]\n",
      " [ 0.8408883   0.77069277]\n",
      " [ 0.84642386  0.7782746 ]\n",
      " [ 0.8518004   0.78564495]\n",
      " [ 0.85702115  0.79280734]\n",
      " [ 0.8620887   0.7997655 ]\n",
      " [ 0.86700565  0.8065229 ]\n",
      " [ 0.871775    0.81308347]\n",
      " [ 0.8763997   0.819451  ]\n",
      " [ 0.88088274  0.82562923]\n",
      " [ 0.8852268   0.8316222 ]\n",
      " [ 0.88943523  0.8374337 ]\n",
      " [ 0.8935107   0.8430679 ]\n",
      " [ 0.8974563   0.8485285 ]\n",
      " [ 0.901275    0.85381943]\n",
      " [ 0.90496975  0.8589451 ]\n",
      " [ 0.90854365  0.86390895]\n",
      " [ 0.91199946  0.86871517]\n",
      " [ 0.9153404   0.87336755]\n",
      " [ 0.91856927  0.8778701 ]\n",
      " [ 0.92168885  0.88222665]\n",
      " [ 0.9247021   0.8864409 ]\n",
      " [ 0.92761207  0.89051676]\n",
      " [ 0.9304214   0.8944579 ]\n",
      " [ 0.9331328   0.8982681 ]\n",
      " [ 0.9357492   0.9019509 ]\n",
      " [ 0.9382732   0.9055099 ]\n",
      " [ 0.94070745  0.9089486 ]\n",
      " [ 0.94305474  0.9122705 ]\n",
      " [ 0.94531757  0.9154791 ]\n",
      " [ 0.9474984   0.91857755]\n",
      " [ 0.9495999   0.9215693 ]\n",
      " [ 0.9516243   0.9244575 ]\n",
      " [ 0.95357406  0.9272451 ]\n",
      " [ 0.95545167  0.9299355 ]], shape=(100, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x2bf326250>,\n <matplotlib.lines.Line2D at 0x2bf326190>]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "utils = daifa.prior_model(obs_pos)\n",
    "print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.5438074  0.00475725]\n",
      " [0.5473314  0.01857028]\n",
      " [0.5508701  0.03239663]\n",
      " [0.5544231  0.04623089]\n",
      " [0.5579896  0.06006796]\n",
      " [0.56156933 0.07390229]\n",
      " [0.5651616  0.08772871]\n",
      " [0.56876594 0.10154166]\n",
      " [0.5723817  0.11533588]\n",
      " [0.57600844 0.12910606]\n",
      " [0.5796455  0.14284676]\n",
      " [0.58329237 0.15655273]\n",
      " [0.58694845 0.17021869]\n",
      " [0.5906131  0.18383937]\n",
      " [0.5942856  0.19740961]\n",
      " [0.5979656  0.21092437]\n",
      " [0.6016524  0.22437844]\n",
      " [0.6053452  0.23776689]\n",
      " [0.6090436  0.25108495]\n",
      " [0.61274683 0.26432762]\n",
      " [0.6164542  0.27749032]\n",
      " [0.6201652  0.29056838]\n",
      " [0.6238792  0.3035573 ]\n",
      " [0.6275954  0.31645274]\n",
      " [0.6313133  0.3292504 ]\n",
      " [0.63503206 0.34194615]\n",
      " [0.63875115 0.354536  ]\n",
      " [0.64246994 0.36701602]\n",
      " [0.64618754 0.37938258]\n",
      " [0.64990354 0.391632  ]\n",
      " [0.65361696 0.4037609 ]\n",
      " [0.6573276  0.41576606]\n",
      " [0.66103435 0.42764425]\n",
      " [0.6647368  0.43939248]\n",
      " [0.66843414 0.45100802]\n",
      " [0.67212576 0.4624882 ]\n",
      " [0.67581093 0.4738305 ]\n",
      " [0.6794891  0.4850326 ]\n",
      " [0.6831596  0.49609223]\n",
      " [0.6868216  0.5070076 ]\n",
      " [0.69047457 0.51777667]\n",
      " [0.694118   0.52839786]\n",
      " [0.697751   0.53886956]\n",
      " [0.70137304 0.5491904 ]\n",
      " [0.70498335 0.5593593 ]\n",
      " [0.7085816  0.56937516]\n",
      " [0.712167   0.57923704]\n",
      " [0.7157389  0.58894414]\n",
      " [0.7192967  0.5984958 ]\n",
      " [0.7228397  0.60789174]\n",
      " [0.7263676  0.6171315 ]\n",
      " [0.7298795  0.626215  ]\n",
      " [0.73337495 0.635142  ]\n",
      " [0.7368535  0.6439129 ]\n",
      " [0.74031454 0.65252763]\n",
      " [0.7437573  0.6609865 ]\n",
      " [0.7471814  0.6692901 ]\n",
      " [0.7505865  0.677439  ]\n",
      " [0.7539718  0.6854337 ]\n",
      " [0.7573369  0.69327515]\n",
      " [0.7606813  0.700964  ]\n",
      " [0.7640045  0.70850146]\n",
      " [0.7673061  0.7158884 ]\n",
      " [0.77058566 0.72312605]\n",
      " [0.7738425  0.7302156 ]\n",
      " [0.7770764  0.7371585 ]\n",
      " [0.7802871  0.7439559 ]\n",
      " [0.78347373 0.7506093 ]\n",
      " [0.7866363  0.7571204 ]\n",
      " [0.78977424 0.76349044]\n",
      " [0.7928873  0.7697213 ]\n",
      " [0.795975   0.77581465]\n",
      " [0.79903704 0.7817722 ]\n",
      " [0.8020732  0.7875955 ]\n",
      " [0.8050829  0.7932866 ]\n",
      " [0.8080661  0.7988473 ]\n",
      " [0.8110225  0.8042795 ]\n",
      " [0.81395185 0.8095851 ]\n",
      " [0.81685376 0.8147658 ]\n",
      " [0.8197279  0.8198239 ]\n",
      " [0.8225744  0.8247612 ]\n",
      " [0.8253927  0.8295797 ]\n",
      " [0.8281828  0.83428144]\n",
      " [0.8309444  0.8388684 ]\n",
      " [0.8336774  0.8433426 ]\n",
      " [0.83638173 0.84770614]\n",
      " [0.8390569  0.8519609 ]\n",
      " [0.8417033  0.8561091 ]\n",
      " [0.8443204  0.86015254]\n",
      " [0.8469083  0.8640934 ]\n",
      " [0.8494668  0.8679337 ]\n",
      " [0.8519959  0.8716754 ]\n",
      " [0.85449547 0.87532043]\n",
      " [0.8569656  0.87887084]\n",
      " [0.85940605 0.8823285 ]\n",
      " [0.86181694 0.88569564]\n",
      " [0.86419827 0.88897395]\n",
      " [0.86654997 0.8921654 ]\n",
      " [0.86887205 0.89527184]\n",
      " [0.87116444 0.89829534]], shape=(100, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x2b81e59a0>,\n <matplotlib.lines.Line2D at 0x2b81e50a0>]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAeAUlEQVR4nO3dd5yU5b3+8c+XFVQ6CILSVeyFyKrEXqICFjCKigXB9kNj2jFGo6aZ5PxMYjzJSTAEsYEFCwYRKXZRCchCpAekCUtdBOltd7/nj3swk3VhZ3dn5plyvV+vfTHlmZnL2eHy5p7neW5zd0REJPvViTqAiIgkhwpdRCRHqNBFRHKECl1EJEeo0EVEcsR+Ub1wixYtvGPHjlG9vIhIVpo2bdo6d29Z2X2RFXrHjh0pKiqK6uVFRLKSmX2+t/s05SIikiNU6CIiOUKFLiKSI1ToIiI5QoUuIpIjVOgiIjlChS4ikiMi2w9dRCSv7NwCqz6F4qlwSBc4/Lykv4QKXUQk2dxh/WJY/gkUfwLLp8LaOeDl4f4zf6hCFxHJSKW7YNUMWPYPWD4l/GwtCfft3xjadIWz74E2heFyg4NSEkOFLiJSXbu2hqmTzyeFn+IiKN0e7mvWCY74FrQ7Fdp1g5ZHQZ2CtMRSoYuIVGXnFlg+GZZ+DEs/gpXTobwUrA60PgG69ocO3wwF3qhVZDFV6CIiFe3eEea+l0yEJR/CiqJQ4HX2g0NPhtO/Cx3ODKPwAxpHnfYrKnSRCn75+hwAfn7ZcREnkbQpL4fVM2Hxe7D4fVg2GUp3gBXAod8IBd7xzDAC379h1Gn3SoUuUsHclZuijiDpsGklLHoPFr0TSnzbF+H2lsdA1wFw2DnQ4YyMGoFXRYUuIvmhdFeYB//sLVj4TtiNEKBhKzjiwrAb4WHnQqPWkcasDRW6iOSuTatg4Vvw2Zuw6H3YtRnq1A1fYH7rl3DEBdDqeDCLOmlSqNBFJHe4h/3BF4yH+ePCkZkAjdvACVdC54ug09mwf6NIY6aKCl1EslvpLlg6Ef41NhT5phWAQdtT4IKfQeeLodVxOTMK3xcVuohkn52bw1z4v94I0yk7N0Hd+nD4+XD+g2Ek3qBF1CnTToUuItlh+4YwjTJ3NCx6F8p2Qv0WcGwvOPrS8IVm3QOiThkpFbqIZK7tG8IofM6osGth+W5o3BYKb4ZjLoX230zbYfXZQIUuIpllx8ZYif897Cdevhuatodud8CxvaHNyXkxH14TKnQRid7u7eELzVmvhLnxsp3QpB10GwjHfTscrakSr5IKXUSiUV4GSz6AmS/DvNfDPuINW0HhADj+yrCXikq8WlToIpJeq2fDjBfCaHzL6nC+8ON6wQl9oONZmhOvBRW6iKTelhKY9RJ8+gKsmRXOWtj5Yjjxajiye97vnZIsCRW6mXUH/gQUAEPd/eEK9zcBngXax57zEXd/KslZRSSblO0O+4j/89nwZ3lpOPVsj9+HKZUUrdqTz6osdDMrAAYBFwLFwFQzG+3uc+M2+w4w190vM7OWwHwze87dd6UktYhkrnWfwfRhYVpla0mYF+92J3S5Hg4+Oup0OS2REfqpwEJ3XwxgZiOAXkB8oTvQyMwMaAisB0qTnFVEMtXu7WFf8enDYNmkMKVyZHf4xo1hObYCze6mQyLvchtgedz1YuC0Ctv8BRgNrAQaAde471ne+t/M7HbgdoD27dvXJK+IZJK186DoKZg5Iuw/3vzwcBbDk/pGuhRbvkqk0Cvbb8grXL8Y+BQ4HzgceMvMPnT3/1gpwN2HAEMACgsLKz6HiGSD0p3h8PuiJ8Iq9wX14JjLw7qaHc/UroYRSqTQi4F2cdfbEkbi8QYAD7u7AwvNbAlwNPBJUlKKSPQ2FkPRkzDtGdi2Lqxuf+FD0OUGfcGZIRIp9KlAZzPrBKwArgWuq7DNMuAC4EMzawUcBSxOZlARiYB7WOV+ymCYPzbcdmR3OOUWOOx8qFMn2nzyH6osdHcvNbO7gAmE3RafdPc5ZjYwdv9g4FfA02Y2izBFc6+7r0thbhFJpd3bYeZLMOVvYam2A5vD6d8LRd5U339lqoS+enb3scDYCrcNjru8ErgoudFEJO02rYSpQ8MXndvXQ6sToNegsN943QOjTidV0L5EIgKrZsI//gKzR4ZzrBx9STi7YYcz9CVnFlGhi+Qrd1j4Dkz633CSrLoN4JTb4LT/B807RZ1OakCFLpJvynaHE2NN+nOYH290SNh3vGt/OLBp1OmkFlToIvli11aYPjwU+aZiaHkM9P4rHH8V7Fcv6nSSBCp0kVy3fQN8MhQmPxa+6Gx/Olz6aFhIWfPjOUWFLpKrtpSELzqnPhEWj+h8MZz1X9C+W9TJJEVU6CK5ZtOq8EVn0VNQugOOuyIUeesTok4mKaZCF8kVm1bCR/8TDs0vL4UTr4Gz7oYWR0SdTNJEhS6S7b4q8qfBy8OZDs+6W7se5iEVuki22rwmFHnRk+Bl0OW6UOTNOkadTCKiQhfJNtvWhyL/5HEo2wVd+sLZ96jIRYUukjV2bAq7Hk76C+zaEhZYPudeOOjwqJNJhlChi2S63TvCYhITHwn7kR9zGZz3ABx8TNTJJMOo0EUyVXkZzBgB7/13OLLzsPPggp9Bm5OjTiYZSoUukmnc4bM34e1fwNq50KYr9H4MDjsn6mSS4VToIplkxXR486fw+UdhweU+z8CxvXSIviREhS6SCb5cBu88BLNehvotoOcj4eyHBXWjTiZZRIUuEqUdm+CjR+Efj4VR+Fk/gjO+Dwc0jjqZZCEVukgUystg+jB47zewtQROvBYu+Ck0aRt1MsliKnSRdFsyEcb/BNbMDqeyve4l7bkiSaFCF0mXDZ/Dmw/AvNehSXt94SlJp0IXSbVd2+DjP8LHfwKrA+c9CKffBXUPjDqZ5BgVukiquIfR+IT7YePysNTbhQ9BkzZRJ5McpUIXSYV1n8HYe2Dxe9DqePj2EOhwetSpJMep0EWSadfWcM6VSX+GuvWhx++h8GYo0F81ST19ykSSwR3mj4Vx94bplZOuC9MrDVtGnUzyiApdpLa+XAZjfwwLxsHBx8KAcZpekUio0EVqqmx3OD/5+w+H6xc+BN3u1OH6EhkVukhNrJgGo78Pa2bBkT2g5++gafuoU0meU6GLVMfOzfDub2DKYGjUGq4eHhac0MFBkgFU6CKJWvAmjPkhbFoBp9wazr1yQJOoU4l8RYUuUpWt62D8feHUti2PhlvehHanRp1K5GtU6CJ74w6zR8K4H4fT3J77Ezjzh7Df/lEnE6mUCl2kEs3K1sELfcOuiG26Qq9BWpRZMp4KXSSeO+dse5ObNv0N1pfBRb+BbndAnYKok4lUqU4iG5lZdzObb2YLzey+vWxzrpl9amZzzOyD5MYUSYNNK+H5q7lz46Ms268j3DEpnBVRZS5ZosoRupkVAIOAC4FiYKqZjXb3uXHbNAUeA7q7+zIzOzhFeUWSzx1mjAiH7Zft4qnGA5lQ/3JGHHR41MlEqiWREfqpwEJ3X+zuu4ARQK8K21wHvOruywDcfW1yY4qkyOY1MOI6GDUwzJHf8THjG/TGLaF/vIpklEQ+tW2A5XHXi2O3xTsSaGZm75vZNDPrV9kTmdntZlZkZkUlJSU1SyySLHNfg8e6wcJ34KJfw4CxoFG5ZLFEvhSt7BA4r+R5ugIXAAcC/zCzye6+4D8e5D4EGAJQWFhY8TlE0mP7l2FXxJkvwiFd4Iq/wcFHR51KpNYSKfRioF3c9bbAykq2WefuW4GtZjYROAlYgEgmWfwBjLoDNq8O+5WfdbdOpiU5I5Epl6lAZzPrZGb1gGuB0RW2eQ04y8z2M7P6wGnAvORGFamF3Ttg/P0w7PKw8MStb8G596nMJadUOUJ391IzuwuYABQAT7r7HDMbGLt/sLvPM7PxwEygHBjq7rNTGVwkYWvmwMhbYe1cOOW2cJrbevWjTiWSdAkdWOTuY4GxFW4bXOH674HfJy+aSC2Vl4ezIr79czigKVz/CnS+MOpUIimjI0UlN21eE3ZFXPRuOF95r79AgxZRpxJJKRW65J4FE2DUnWHB5kseDYs063zlkgdU6JI7du8I0ytTBkOr4+HKJ7Q7ouQVFbrkhnWfwSsDYPUsOG0gfOuXUPeAqFOJpJUKXbKbO8x4Ad74UThPed8X4ajuUacSiYQKXbLXzi3wxt0wcwR0OBOufBwaHxp1KpHIqNAlO62eBS/3h/WL4dz74ewf6TS3kvdU6JJd3GHa0+FUtwc2g36jodNZSX2JYw9tnNTnE0kXFbpkj52b4fUfwOxX4PDz4Yoh0LBl0l/m55cdl/TnFEkHFbpkhzVz4KWbYP0iOP9BOPNuqKNzlovEU6FL5vv0eRjzX3BA45RMsYjkChW6ZK7d28N5y6cPg45nhQOFGrWKOpVIxlKhS2ZavwRe6gerZ4Zzlp97PxTo4yqyL/obIplnwQR49bZwWQcKiSRMhS6Zo7wM3n8YJv4OWp8AVw+H5p2iTiWSNVTokhm2rQ+j8oVvQ5fr4ZI/QN0Do04lklVU6BK9VTPhxRtg00q49H+g6wCd7lakBlToEq2ZL8Po74ajPm8eD20Lo04kkrVU6BKNslJ462cweRB0OAP6PA0ND446lUhWU6FL+m39Al6+CZZ+GM5dftGvoaBu1KlEsp4KXdJr1UwYcT1sWQO9B0OXvlEnEskZKnRJn9kjYdR3oH7zMF/e5uSoE4nkFBW6pF55Obz7K/joUWj/Tbh6mObLRVJAhS6ptWNT2L98wXjo2h96/B72qxd1KpGcpEKX1PliEbzQN5zy9pI/wCm3Rp1IJKep0CU1Fn8Q9mQBuPHv0OnsaPOI5AGtECDJN3UoDL8CGraC295TmYukiUbokjxlpTD+Ppj6OHS+GK4cGhalEJG0UKFLcmz/El7uD4vfg9O/C9/6JdQpiDqVSF5RoUvtrV8Mz18TFqW4/C9w8o1RJxLJSyp0qZ2lH4czJeLQbxR0PDPqRCJ5S1+KSs19+jwM6wX1D4Jb31GZi0RMI3SpvvJyeO/X8OEfoNM5cPUz4fS3IhIpFbpUz+7tMOpOmPMqnNwPLnlUZ0oUyRAqdEnclhIY0ReKi+DCh+D072llIZEMktAcupl1N7P5ZrbQzO7bx3anmFmZmV2VvIiSEUoWwNALYPXscHKtM76vMhfJMFWO0M2sABgEXAgUA1PNbLS7z61ku98CE1IRVCK09KNwDvOCutD/DWjbNepEIlKJREbopwIL3X2xu+8CRgC9Ktnuu8BIYG0S80nUZr4Ew3qH093e+rbKXCSDJVLobYDlcdeLY7d9xczaAFcAg/f1RGZ2u5kVmVlRSUlJdbNKOrnDxEfCqW/bnQa3vAnNOkadSkT2IZFCr2yi1Ctc/yNwr7uX7euJ3H2Iuxe6e2HLli0TjChpV1YKr38/LEpxQh+48VXtliiSBRLZy6UYaBd3vS2wssI2hcAIC1+StQB6mlmpu49KRkhJo51bwjlZFr4FZ90N5/9UX36KZIlECn0q0NnMOgErgGuB6+I3cPdOey6b2dPAGJV5Ftq8Bp7vA6tnwaV/hMIBUScSkWqostDdvdTM7iLsvVIAPOnuc8xsYOz+fc6bS5YoWQDPXQlb10HfEXDkxVEnEpFqSujAIncfC4ytcFulRe7u/WsfS9Jq2RR44RqwAug/BtpoTxaRbKSTc+W7eWNg2OXhS89b31KZi2QxFXo+m/oEvHQjtDoObnkLmh8WdSIRqQWdyyUfucN7/w0TfxeWiuvzFNRrEHUqEaklFXq+KSuFN34I04fBN26AS/8EBfoYiOQC/U3OJ7u2wchbYP5YOPseOO8B7WMukkNU6Pli23p4oS8snwI9H4FTb4s6kYgkmQo9H2xcAc9eCesXQZ+n4bjeUScSkRRQoee6kgUw/ArYsRFuGAmdzo46kYikiAo9lxVPg+eugjoFMOANOOSkqBOJSAppP/RctehdeOYy2L8R3DxBZS6SB1TouWj2q/Dc1dC8UziP+UGHR51IRNJAhZ5rpg6FV26GtoVhubhGraNOJCJpojn0XOEOHz4C7/4ajuwOVz0F9epHnUpE0kiFngvKy+HNB2HyIDjxGug1KCzoLCJ5RYWe7cpK4fXvwafPwWkD4eL/D3U0kyaSj1To2Wz3jnAo/7/GwLn3wzk/1qH8InlMhZ6tdm6BEX1hyUTo/lvoNjDqRCISMRV6Ntq2Hp7rAyv/Cb0HQ5e+UScSkQygQs82m1eHQ/m/WAjXDIejL4k6kYhkCBV6NtnwOQzrBVvWwvUvw2HnRp1IRDKICj1blCwIZb57G9w0Ohw4JCISR4WeDVbNgOHfBqsTjv5sfXzUiUQkA2mH5Uy3/BN4+jLY7wAYME5lLiJ7pULPZIvfh2G9oUELuHk8tDgi6kQiksFU6Jlq/rhwxsRmHcPIvGm7qBOJSIZToWei2SPhxRug1XHQfww0ahV1IhHJAir0TPPPZ2HkrdD2VOj3GtRvHnUiEckSKvRMMmUIvPadsH/5DSPhgMZRJxKRLKJCzxQf/RHG3QNHXQJ9R+hc5iJSbdoPPWru8P7D8MHDcPyVcMXfdC5zEakRFXqU3OHtn8PHf4Iu18Plf4Y6BVGnEpEspUKPSnk5jL8XPhkChbdAz0e0MIWI1IoKPQrlZTDmBzB9GHzzLrjo11qYQkRqTYWebmWl8NqdMPNFOPseOO8BlbmIJIUKPZ3Kdod9zOeOgvMfDIUuIpIkKvR0Kd0JLw+A+W/ARb+B0++KOpGI5JiEvoUzs+5mNt/MFprZfZXcf72ZzYz9TDKzk5IfNYvt3g4jrg9l3vMRlbmIpESVI3QzKwAGARcCxcBUMxvt7nPjNlsCnOPuG8ysBzAEOC0VgbPOrq3wQmwx58v/DCf3izqRiOSoREbopwIL3X2xu+8CRgC94jdw90nuviF2dTLQNrkxs9TOzWEx56UfQu+/qsxFJKUSKfQ2wPK468Wx2/bmFmBcZXeY2e1mVmRmRSUlJYmnzEY7NoZVhpZNhm8/Dl36Rp1IRHJcIoVe2T51XumGZucRCv3eyu539yHuXujuhS1btkw8ZbbZviEsTLFyOvR5Ck64KupEIpIHEtnLpRiIX12hLbCy4kZmdiIwFOjh7l8kJ14W2rY+LOa8dh5cPRyO7hl1IhHJE4mM0KcCnc2sk5nVA64FRsdvYGbtgVeBG919QfJjZomt6+DpS6FkPlz7vMpcRNKqyhG6u5ea2V3ABKAAeNLd55jZwNj9g4GfAQcBj1k46rHU3QtTFzsDbVkLz1wOG5bAdSPg8POjTiQiecbcK50OT7nCwkIvKiqK5LWTbvNqeOYy2FgM170Inc6OOpGI5Cgzm7a3AbOOFK2tTStDmW9aBde/Ah3PiDqRiOQpFXptbCwOc+Zb18GNr0L7blEnEpE8pkKvqS+XhTLfvgFu/Du0OyXqRCKS51ToNbFhKTx9GezcCP1GQZuuUScSEVGhV9v6xaHMd22BfqPh0C5RJxIRAVTo1fPFovAF6O7tcNPrcMiJUScSEfmKCj1R6z4LZV62K5R56+OjTiQi8h9U6IkoWQDPXBrWAr1pDLQ6NupEIiJfo0KvSsn8sDcLQP834OCjo80jIrIXKvR9WTsvTLNYnTAyb3lk1IlERPYqoSXo8tKauWFkbgVhZK4yF5EMp0KvzJo5Yc68oG4o8xado04kIlIlFXpFq2eFkXnB/rEyPyLqRCIiCdEcerxVM2HY5VC3PvQfA80PizqRiEjCNELfY9WM8AVo3QYqcxHJShqhA6z8NCwbt3+jcNBQ805RJxIRqTaN0Ff+M0yz7N8oNjJXmYtIdsrvQl8xPTYybxK+AG3WMepEIiI1lr9TLiumwbAr4MBYmTdtH3UiEZFayc8RevGeMm+qMheRnJF/hV48DYb3hvrNVOYiklPya8qluAiGXwH1m4cyb9I26kQiIkmTPyP05VNhWG+of5DKXERyUn4U+vJPwsi8YUuVuYjkrNyfclk2BZ698t9l3vjQqBOJiKREbo/Ql02BZ78NDQ9WmYtIzsvdQl82OVbmrcIRoCpzEclxuTnl8vk/4LmroFHrsNJQ40OiTiQiknK5N0L/fFKYM1eZi0ieya0R+tKP4bk+YXql/5hQ6iIieSJ3Cn1PmTdpE0bmjVpFnUhEJK1yY8pl6UdhzrxJW5W5iOSt7B+hL/kQnr86nJPlptfDLooiInkou0foSyaGaZamHVTmIpL3sneEvvgDeP6asMJQv9HhSFARkTyW0AjdzLqb2XwzW2hm91Vyv5nZ/8bun2lmJyc/apzF74dpluaHxUbmKnMRkSoL3cwKgEFAD+BYoK+ZHVthsx5A59jP7cBfk5zz374amR8ON42GBi1S9lIiItkkkRH6qcBCd1/s7ruAEUCvCtv0AoZ5MBloamapOaKn0SHQ4YwwMleZi4h8JZFCbwMsj7teHLututtgZrebWZGZFZWUlFQ3a9DySLjxVWhwUM0eLyKSoxIpdKvkNq/BNrj7EHcvdPfCli017y0ikkyJFHox0C7ueltgZQ22ERGRFEqk0KcCnc2sk5nVA64FRlfYZjTQL7a3Szdgo7uvSnJWERHZhyr3Q3f3UjO7C5gAFABPuvscMxsYu38wMBboCSwEtgEDUhdZREQqk9CBRe4+llDa8bcNjrvswHeSG01ERKojuw/9FxGRr6jQRURyhApdRCRHWJj+juCFzUqAz2v48BbAuiTGSZZMzQWZm025qke5qicXc3Vw90oP5Ims0GvDzIrcvTDqHBVlai7I3GzKVT3KVT35lktTLiIiOUKFLiKSI7K10IdEHWAvMjUXZG425aoe5aqevMqVlXPoIiLyddk6QhcRkQpU6CIiOSJjC93M+pjZHDMrN7O97t6zt/VOzay5mb1lZp/F/myWpFxVPq+ZHWVmn8b9bDKzH8Tu+4WZrYi7r2e6csW2W2pms2KvXVTdx6cil5m1M7P3zGxe7Hf+/bj7kvp+1WZ93Koem+Jc18fyzDSzSWZ2Utx9lf5O05TrXDPbGPf7+Vmij01xrnviMs02szIzax67L5Xv15NmttbMZu/l/tR+vtw9I3+AY4CjgPeBwr1sUwAsAg4D6gEzgGNj9/0OuC92+T7gt0nKVa3njWVcTTgYAOAXwI9S8H4llAtYCrSo7X9XMnMBhwAnxy43AhbE/R6T9n7t6/MSt01PYBxh0ZZuwJREH5viXKcDzWKXe+zJta/faZpynQuMqcljU5mrwvaXAe+m+v2KPffZwMnA7L3cn9LPV8aO0N19nrvPr2Kzfa132gt4Jnb5GaB3kqJV93kvABa5e02Pik1Ubf97I3u/3H2Vu0+PXd4MzKOSJQyToDbr4yby2JTlcvdJ7r4hdnUyYRGZVKvNf3Ok71cFfYEXkvTa++TuE4H1+9gkpZ+vjC30BO1rLdNWHltkI/bnwUl6zeo+77V8/cN0V+yfW08ma2qjGrkceNPMppnZ7TV4fKpyAWBmHYFvAFPibk7W+1Wb9XETWjc3hbni3UIY5e2xt99punJ908xmmNk4Mzuumo9NZS7MrD7QHRgZd3Oq3q9EpPTzldD50FPFzN4GWldy1wPu/loiT1HJbbXeD3Nfuar5PPWAy4GfxN38V+BXhJy/Av4A3JzGXGe4+0ozOxh4y8z+FRtV1FgS36+GhL94P3D3TbGba/x+VfYSldyW6Pq4KfmsVfGaX9/Q7DxCoZ8Zd3PSf6fVyDWdMJ24Jfb9xiigc4KPTWWuPS4DPnb3+FFzqt6vRKT08xVpobv7t2r5FPtay3SNmR3i7qti/6RZm4xcZlad5+0BTHf3NXHP/dVlM3scGJPOXO6+MvbnWjP7O+GfehOJ+P0ys7qEMn/O3V+Ne+4av1+VqM36uPUSeGwqc2FmJwJDgR7u/sWe2/fxO015rrj/8eLuY83sMTNrkchjU5krztf+hZzC9ysRKf18ZfuUy77WOx0N3BS7fBOQyIg/EdV53q/N3cVKbY8rgEq/DU9FLjNrYGaN9lwGLop7/cjeLzMz4Algnrs/WuG+ZL5ftVkfN5HHpiyXmbUHXgVudPcFcbfv63eajlytY78/zOxUQqd8kchjU5krlqcJcA5xn7kUv1+JSO3nKxXf9Cbjh/CXtxjYCawBJsRuPxQYG7ddT8JeEYsIUzV7bj8IeAf4LPZn8yTlqvR5K8lVn/DBblLh8cOBWcDM2C/skHTlInyDPiP2MydT3i/C9IHH3pNPYz89U/F+VfZ5AQYCA2OXDRgUu38WcXtY7e2zlqT3qapcQ4ENce9PUVW/0zTluiv2ujMIX9aengnvV+x6f2BEhcel+v16AVgF7Cb01y3p/Hzp0H8RkRyR7VMuIiISo0IXEckRKnQRkRyhQhcRyREqdBGRHKFCFxHJESp0EZEc8X+5wZvLDPlpyQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "utils = daifa.prior_model(vel_pos)\n",
    "print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.99570507]\n",
      " [0.9954051 ]\n",
      " [0.9950845 ]\n",
      " [0.99474144]\n",
      " [0.99437445]\n",
      " [0.993982  ]\n",
      " [0.9935624 ]\n",
      " [0.99311346]\n",
      " [0.9926334 ]\n",
      " [0.99211997]\n",
      " [0.99157095]\n",
      " [0.99098384]\n",
      " [0.9903559 ]\n",
      " [0.98968464]\n",
      " [0.9889669 ]\n",
      " [0.98819935]\n",
      " [0.98737884]\n",
      " [0.9865017 ]\n",
      " [0.985564  ]\n",
      " [0.9845617 ]\n",
      " [0.9834904 ]\n",
      " [0.98234546]\n",
      " [0.9811217 ]\n",
      " [0.97981423]\n",
      " [0.978417  ]\n",
      " [0.9769243 ]\n",
      " [0.97532946]\n",
      " [0.9738083 ]\n",
      " [0.972214  ]\n",
      " [0.97052413]\n",
      " [0.9687331 ]\n",
      " [0.96673006]\n",
      " [0.9645839 ]\n",
      " [0.96230185]\n",
      " [0.95987594]\n",
      " [0.95729727]\n",
      " [0.95455664]\n",
      " [0.95164454]\n",
      " [0.94855064]\n",
      " [0.94526446]\n",
      " [0.9417745 ]\n",
      " [0.93812364]\n",
      " [0.93432546]\n",
      " [0.9304661 ]\n",
      " [0.92638874]\n",
      " [0.92244214]\n",
      " [0.91895354]\n",
      " [0.91617835]\n",
      " [0.91331244]\n",
      " [0.91035324]\n",
      " [0.90729773]\n",
      " [0.90414345]\n",
      " [0.90088737]\n",
      " [0.89757824]\n",
      " [0.89404684]\n",
      " [0.8900676 ]\n",
      " [0.8867023 ]\n",
      " [0.88460135]\n",
      " [0.882464  ]\n",
      " [0.8802897 ]\n",
      " [0.87807745]\n",
      " [0.8766761 ]\n",
      " [0.8773687 ]\n",
      " [0.87869334]\n",
      " [0.8813048 ]\n",
      " [0.88394856]\n",
      " [0.8866979 ]\n",
      " [0.89066225]\n",
      " [0.8947397 ]\n",
      " [0.8986731 ]\n",
      " [0.90246725]\n",
      " [0.9061263 ]\n",
      " [0.9096546 ]\n",
      " [0.91305625]\n",
      " [0.9163356 ]\n",
      " [0.91949636]\n",
      " [0.9225425 ]\n",
      " [0.9254779 ]\n",
      " [0.9283062 ]\n",
      " [0.9310309 ]\n",
      " [0.93365586]\n",
      " [0.936184  ]\n",
      " [0.93861896]\n",
      " [0.9409638 ]\n",
      " [0.9432218 ]\n",
      " [0.94539577]\n",
      " [0.9474887 ]\n",
      " [0.94950354]\n",
      " [0.9514431 ]\n",
      " [0.9533098 ]\n",
      " [0.95510644]\n",
      " [0.9568355 ]\n",
      " [0.95849925]\n",
      " [0.96008235]\n",
      " [0.9615605 ]\n",
      " [0.9629849 ]\n",
      " [0.9643575 ]\n",
      " [0.96568   ]\n",
      " [0.96695435]\n",
      " [0.9681821 ]], shape=(100, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x2b81e5fd0>,\n <matplotlib.lines.Line2D at 0x2b631db20>]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_pos = np.vstack([np.linspace(-1, 1, 100), np.zeros(100)]).T\n",
    "\n",
    "utils = daifa.habit_action_model(obs_pos)\n",
    "print(utils)\n",
    "\n",
    "plt.plot(obs_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.7664637 ]\n",
      " [0.7629652 ]\n",
      " [0.7594779 ]\n",
      " [0.7564916 ]\n",
      " [0.7534736 ]\n",
      " [0.7504233 ]\n",
      " [0.74734086]\n",
      " [0.7442258 ]\n",
      " [0.741078  ]\n",
      " [0.7378974 ]\n",
      " [0.7346837 ]\n",
      " [0.7314366 ]\n",
      " [0.728156  ]\n",
      " [0.72484165]\n",
      " [0.7214934 ]\n",
      " [0.7181112 ]\n",
      " [0.7146947 ]\n",
      " [0.7112436 ]\n",
      " [0.70775807]\n",
      " [0.7042376 ]\n",
      " [0.7006822 ]\n",
      " [0.69709164]\n",
      " [0.6936469 ]\n",
      " [0.69127923]\n",
      " [0.68889654]\n",
      " [0.68662775]\n",
      " [0.68441373]\n",
      " [0.68266547]\n",
      " [0.68109655]\n",
      " [0.6795213 ]\n",
      " [0.6818516 ]\n",
      " [0.6851011 ]\n",
      " [0.6883238 ]\n",
      " [0.6915193 ]\n",
      " [0.6968533 ]\n",
      " [0.7033259 ]\n",
      " [0.7097156 ]\n",
      " [0.7154631 ]\n",
      " [0.72111547]\n",
      " [0.7306858 ]\n",
      " [0.751118  ]\n",
      " [0.7702059 ]\n",
      " [0.7880071 ]\n",
      " [0.804904  ]\n",
      " [0.8233792 ]\n",
      " [0.8419486 ]\n",
      " [0.8610158 ]\n",
      " [0.8766547 ]\n",
      " [0.8906372 ]\n",
      " [0.90311646]\n",
      " [0.9142363 ]\n",
      " [0.9240808 ]\n",
      " [0.9325456 ]\n",
      " [0.93972516]\n",
      " [0.9460026 ]\n",
      " [0.9516425 ]\n",
      " [0.9567066 ]\n",
      " [0.9613373 ]\n",
      " [0.9655321 ]\n",
      " [0.96927905]\n",
      " [0.9726245 ]\n",
      " [0.9756189 ]\n",
      " [0.978326  ]\n",
      " [0.9807357 ]\n",
      " [0.9828797 ]\n",
      " [0.98482823]\n",
      " [0.9866199 ]\n",
      " [0.98820126]\n",
      " [0.98959666]\n",
      " [0.9908525 ]\n",
      " [0.99196625]\n",
      " [0.99294496]\n",
      " [0.9938047 ]\n",
      " [0.99456006]\n",
      " [0.9952235 ]\n",
      " [0.9958062 ]\n",
      " [0.9963237 ]\n",
      " [0.9968021 ]\n",
      " [0.99722433]\n",
      " [0.99759084]\n",
      " [0.9979092 ]\n",
      " [0.9981854 ]\n",
      " [0.99842525]\n",
      " [0.9986332 ]\n",
      " [0.99881387]\n",
      " [0.99897075]\n",
      " [0.9991067 ]\n",
      " [0.9992247 ]\n",
      " [0.9993272 ]\n",
      " [0.9994161 ]\n",
      " [0.99949324]\n",
      " [0.99956036]\n",
      " [0.99961853]\n",
      " [0.99966884]\n",
      " [0.9997127 ]\n",
      " [0.9997507 ]\n",
      " [0.99978375]\n",
      " [0.9998123 ]\n",
      " [0.9998371 ]\n",
      " [0.9998587 ]], shape=(100, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x29a311bb0>,\n <matplotlib.lines.Line2D at 0x29a311ac0>]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vel_pos = np.vstack([np.zeros(100), np.linspace(-1, 1, 100)]).T\n",
    "\n",
    "utils = daifa.habit_action_model(vel_pos)\n",
    "print(utils)\n",
    "\n",
    "plt.plot(vel_pos, utils)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "success\n",
      "80.97179339907119\n",
      "227\n",
      "success\n",
      "81.30992979189202\n",
      "305\n",
      "success\n",
      "75.2456179705443\n",
      "161\n",
      "success\n",
      "86.26218154792542\n",
      "143\n",
      "success\n",
      "87.73278121613515\n",
      "246\n",
      "success\n",
      "79.16369505197055\n",
      "228\n",
      "success\n",
      "80.7827699923531\n",
      "226\n",
      "success\n",
      "81.08836495084566\n",
      "229\n",
      "success\n",
      "80.96468977926372\n",
      "233\n",
      "success\n",
      "80.62362825383977\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "obs_stddev = [0.05, 0.05]\n",
    "# obs_stddev = [0, 0]\n",
    "\n",
    "\n",
    "t_max = 999\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    done = False\n",
    "    rewards = []\n",
    "\n",
    "    t = 0\n",
    "    while not done:\n",
    "\n",
    "        obs = obs.reshape(1, obs.shape[0])\n",
    "        obs = transform_observations(obs, observation_max, observation_min, obs_stddev)\n",
    "\n",
    "        # print(obs)\n",
    "\n",
    "        action = daifa.habit_action_model(obs)\n",
    "        action = action.numpy()\n",
    "\n",
    "        for k in range(daifa.agent_time_ratio):\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            env.render()\n",
    "\n",
    "            # print(obs)\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "            t += 1\n",
    "\n",
    "            if t == t_max:\n",
    "                done = True\n",
    "                break\n",
    "            elif done:\n",
    "                break\n",
    "\n",
    "    print(t)\n",
    "    if t < t_max:\n",
    "        print(\"success\")\n",
    "    else:\n",
    "        print(\"Failure\")\n",
    "        print(\"max obs\", obs)\n",
    "\n",
    "    print(np.sum(rewards))\n",
    "    # print(rewards)\n",
    "\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8813, 2) (8813, 1)\n"
     ]
    }
   ],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 500\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "actions = []\n",
    "\n",
    "pre_obs = []\n",
    "post_obs = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "    actions.append(a)\n",
    "\n",
    "    pre_obs.append(o[:-1])\n",
    "    post_obs.append(o[1:])\n",
    "\n",
    "\n",
    "    observations.append(o)\n",
    "    # ob_seqs.append(train)\n",
    "    # next_obs.append(test)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "actions = np.vstack(actions)\n",
    "\n",
    "pre_obs = np.vstack(pre_obs)\n",
    "post_obs = np.vstack(post_obs)\n",
    "\n",
    "\n",
    "print(pre_obs.shape, actions.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.habit_action_model(pre_obs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = agent.model_vae(observations)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m, s, z_pre = agent.model_vae.encoder(pre_obs)\n",
    "m, s, z_post = agent.model_vae.encoder(post_obs)\n",
    "z_pre"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_plus_a = np.concatenate([z_pre, actions], axis=1)\n",
    "z_plus_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((z, np.zeros_like(z)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test without the replay training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2,  [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 20, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=5,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=40, action_repeats=6, num_actions_to_execute=2, train_on_full_data=False, show_replay_training=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = agent.model_vae(observations)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with the prior model FEEF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2, [0, 0], [0.3, 0.3], llik_scaling=1, recon_stddev=0.05)\n",
    "\n",
    "pl_hoz = 5\n",
    "latent_dim = 2\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 2*latent_dim*pl_hoz, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(2)\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           None,\n",
    "                           train_prior_model=True,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=40, action_repeats=6, num_actions_to_execute=5, train_on_full_data=False, show_replay_training=True, train_during_episode=True, train_vae=True, train_tran=True, train_prior=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the models produced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.model_vae(observations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.reduce_mean(agent.model_vae.compute_loss(observations))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.prior_model(observations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.prior_model.extrinsic_kl(observations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "pl_hoz = 5\n",
    "latent_dim = 2\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 2*pl_hoz*latent_dim, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(2)\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "# without prior model\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "# with prior model\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           None,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, action_repeats=10, num_actions_to_execute=2, train_on_full_data=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Test the models produced\n",
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.model_vae(observations)\n",
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Identity VAE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = agent.tran((ob_seqs[0:1], None))\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = ob_seqs[0:1, -1].reshape(1,1,3)\n",
    "h = out[3]\n",
    "h = h[0, -2, :]\n",
    "h = h.numpy().reshape(1,30)\n",
    "h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((t, h))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ob_seqs[0:1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test to see how the agent trains on standard observation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, vae_train_epochs=1, tran_train_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "success, agent, t, pre_obs, post_obs, acts = run_episode(env, daifa, observation_max, observation_min, observation_noise_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_np = np.array(pre_obs)\n",
    "a = np.array(acts)\n",
    "a.shape\n",
    "pre_a = np.concatenate([pre_np, a], axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(a.max(), a.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict = np.array(post_obs)[:, 14, :]\n",
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((pre_a, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine training the model on the observation data\n",
    "\n",
    "Does it eventually converge to a good model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_runs = 1\n",
    "for i in range(num_train_runs):\n",
    "\n",
    "    for j in range(len(pre)):\n",
    "        pre = pre_obs[j]\n",
    "        post = post_obs[j]\n",
    "        actions = acts[j]\n",
    "\n",
    "        daifa.train(pre, post, actions, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation(np.array([0.5, 0.1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the FEEF computations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, planning_horizon=15, n_policy_candidates=70, n_policies=1500, n_cem_policy_iterations=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_policy(agent, env, policy, action_repeats):\n",
    "\n",
    "    observation = env.reset()\n",
    "    obs = transform_observations(observation, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    z_t_minus_1 = obs\n",
    "    p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "    p\n",
    "    print(obs)\n",
    "    print(p)\n",
    "\n",
    "    for action in p:\n",
    "        for t in range(action_repeats):\n",
    "            res = env.step(np.array([action]))\n",
    "            print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([0, 0])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p\n",
    "\n",
    "agent.forward_policies(p, z_t_minus_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "test_policy(agent, env, p.numpy(), 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([-0.27691475,  0.01688306])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 given_prior_mean,\n",
    "                 given_prior_stddev,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 tran_train_epochs=1,\n",
    "                 vae_train_epochs=1,\n",
    "                 agent_time_ratio=6,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        self.prior_model = prior_model\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        self.vae_train_epochs = vae_train_epochs\n",
    "        self.tran_train_epochs = tran_train_epochs\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.model_vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        # transition\n",
    "        # takes action plus last state and outputs next latent state\n",
    "        self.tran = tran\n",
    "        self.tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        self.hidden_state = None\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(observation)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    def train(self, pre_observations_raw, post_observations_raw, actions_complete, rewards, verbose=0):\n",
    "\n",
    "        # compress the observations based on the agents time compression factor\n",
    "        # pre_observations = pre_observations_raw[::self.agent_time_ratio]  # for example take every 6th element\n",
    "        # post_observations = np.array([post_observations_raw[i] for i in range(len(post_observations_raw)) if i % self.agent_time_ratio == self.agent_time_ratio - 1])\n",
    "        #\n",
    "        # print(pre_observations_raw)\n",
    "        # print(pre_observations)\n",
    "        # print(post_observations_raw)\n",
    "        # print(post_observations)\n",
    "\n",
    "        pre_observations = pre_observations_raw\n",
    "        post_observations = post_observations_raw\n",
    "\n",
    "        # only look at the first n actions that we took\n",
    "        actions = actions_complete[0: len(pre_observations)]\n",
    "\n",
    "        num_observations = pre_observations.shape[0]\n",
    "        observation_dim = pre_observations.shape[1]\n",
    "        action_dim = actions.shape[1]\n",
    "        # action_dim = 1  # TODO fix this to allow different actions\n",
    "\n",
    "        # find the actual observed latent states using the vae\n",
    "        pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "        post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "        # set up the input training data that we use to train the transition model\n",
    "        z_train = np.concatenate([np.array(pre_latent_mean), np.array(actions)], axis=1)\n",
    "\n",
    "        # we use the sequence to find the right hidden states to use as input\n",
    "        z_train_seq = z_train.reshape((1, num_observations, observation_dim + action_dim))\n",
    "        z_train_singles = z_train.reshape(num_observations, 1, observation_dim + action_dim)\n",
    "\n",
    "        # the previous hidden state is the memory after observing some sequences but it might be None\n",
    "        if self.hidden_state is None:\n",
    "            self.hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "        if self.train_tran:\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.hidden_state))\n",
    "\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran_train_epochs, verbose=verbose)\n",
    "\n",
    "        # now find the new predicted hidden state that we will use for finding the policy\n",
    "        # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "        _, _, final_hidden_state, _ = self.tran((z_train_seq, self.hidden_state))\n",
    "        # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "        self.hidden_state = final_hidden_state\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            self.model_vae.fit(post_observations, epochs=self.vae_train_epochs, verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by one to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        print(z_t_minus_one)\n",
    "        print(mean_best_policies)\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape [self.n_policies, len(z_t_minus_one)]\n",
    "        prev_latent_mean = np.stack([z_t_minus_one]*self.n_policies)\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "            likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "            if self.prior_model is None:\n",
    "\n",
    "                # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                # create the prior distribution\n",
    "                prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "            # TODO Fix the learned prior model\n",
    "            else:\n",
    "                prior_dist = self.prior_model()\n",
    "\n",
    "            kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "            predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "            kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    def EFE(self, policy_posteriors, predicted_likelihood, predicted_posterior):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing with a pretrained transition model\n",
    "\n",
    "This works well! So the problem can't lie with the transition model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0.07]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, train_vae=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 200\n",
    "seq_length = 500\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "next_obs_stddev = []\n",
    "actions = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.2)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    # train = np.concatenate([o[:-1], a], axis=1)\n",
    "    train = o[:-1]\n",
    "    test = o[1:]\n",
    "\n",
    "    actions.append(a)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "    ob_seqs_stddev = np.ones_like(train)\n",
    "    next_stddev = np.ones_like(test)\n",
    "\n",
    "    next_obs_stddev.append(next_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(num_seqs):\n",
    "\n",
    "    pre = ob_seqs[i]\n",
    "    next = next_obs[i]\n",
    "    acts = actions[i]\n",
    "\n",
    "    next_sd = next_obs_stddev[i]\n",
    "\n",
    "    daifa.train(pre, next, acts, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 150\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    test = o[-1]\n",
    "\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "ob_seqs = np.array(ob_seqs)[:, -5:, :]\n",
    "next_obs = np.array(next_obs)\n",
    "ob_seqs.shape\n",
    "\n",
    "ob_seqs_stddev = np.ones_like(ob_seqs)\n",
    "next_obs_stddev = np.ones_like(next_obs)\n",
    "\n",
    "ob_seqs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.tran((ob_seqs, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_obs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks fantastic!!! With enough data the transition model is training very well"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.hidden_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([0.4, 0.5])\n",
    "daifa.hidden_state = None\n",
    "p, s = daifa.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.tran((np.array([[[0.4, 0.5, 1]]]), None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "daifa.train_tran = False\n",
    "daifa.train_vae = False\n",
    "\n",
    "daifa.hidden_state = None\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}