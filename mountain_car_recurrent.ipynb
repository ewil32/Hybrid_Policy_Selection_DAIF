{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run The Agent on Mountain Car"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from vae_recurrent import VAE, create_decoder, create_encoder\n",
    "from transition_gru import TransitionGRU\n",
    "from recurrent_agent import DAIFAgentRecurrent\n",
    "from prior_model import PriorModelBellman"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from util import random_observation_sequence, transform_observations\n",
    "from train_agent import train_single_agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from identity_vae import IdentityVAE, identity_encoder, identity_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the agent do?\n",
    "- The agent plans using a policy then executes that policy for 12 simulation timesteps, the first two actions of the policy are executed for 6 steps each\n",
    "\n",
    "What data does it accumulate?\n",
    "- It accumulates 12 observation actions pairs\n",
    "\n",
    "How is it trained?\n",
    "- VAE is trained to reproduce observations using the latent states\n",
    "- Transition is trained by taking previous hidden state and previous latent state and trying to predict the next latent state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Online learning For all tasks, we initialize all the agents with random weights and learn online only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ). In contrast, previous approaches using active inference [Ueltzh√∂ffer, 2018, Tschantz et al., 2019, 2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (offline) policy replay and typically need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge, this is the first model-based RL method to learn online using neural network representations. This is afforded by the high sample efficiency of the FEEF, which directs exploration towards states that are uncertain for both the encoder and transition models.\n",
    "\n",
    "\n",
    "Why this is true?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with no prior model FEEF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "# from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 given_prior_mean,\n",
    "                 given_prior_stddev,\n",
    "                 agent_time_ratio=6,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 tran_train_epochs=1,\n",
    "                 vae_train_epochs=1,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True,\n",
    "                 train_prior_model=False,\n",
    "                 use_kl_extrinsic=True,\n",
    "                 use_kl_intrinsic=True,\n",
    "                 use_FEEF=True,\n",
    "                 show_vae_training=False,\n",
    "                 show_tran_training=False,\n",
    "                 show_prior_training=False):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        self.vae_train_epochs = vae_train_epochs\n",
    "        self.tran_train_epochs = tran_train_epochs\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "\n",
    "        # do we use the kl divergence for extrinsic vs intrinsic\n",
    "        self.use_kl_intrinsic = use_kl_intrinsic\n",
    "        self.use_kl_extrinsic = use_kl_extrinsic\n",
    "\n",
    "        # do we use the FEEF or EFE?\n",
    "        self.use_FEEF = use_FEEF\n",
    "\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.model_vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "        self.show_vae_training = show_vae_training\n",
    "\n",
    "        # transition\n",
    "        # takes action plus last state and outputs next latent state\n",
    "        self.tran = tran\n",
    "        self.tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "        self.show_tran_training = show_tran_training\n",
    "        # track the hidden state of the transition gru model\n",
    "        self.hidden_state = None\n",
    "\n",
    "        # Prior model\n",
    "        self.prior_model = prior_model\n",
    "        self.train_prior = train_prior_model\n",
    "        self.show_prior_training = show_prior_training\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "\n",
    "\n",
    "    def train(self, pre_observations_raw, post_observations_raw, actions_complete, rewards, verbose=0):\n",
    "\n",
    "        # compress the observations based on the agents time compression factor\n",
    "        pre_observations = pre_observations_raw[::self.agent_time_ratio]  # for example take every 6th element\n",
    "        post_observations = np.array([post_observations_raw[i] for i in range(len(post_observations_raw)) if i %self.agent_time_ratio == self.agent_time_ratio - 1])\n",
    "        # post_observations = np.flip(np.flip(post_observations_raw)[::self.agent_time_ratio])  # ends up at element\n",
    "\n",
    "        # print(post_observations_a, post_observations)\n",
    "        # post_observations = np.array([post_observations_raw[i] for i in range(len(post_observations_raw)) if i % self.agent_time_ratio == self.agent_time_ratio - 1])\n",
    "        #\n",
    "        # print(pre_observations_raw)\n",
    "        # print(pre_observations)\n",
    "        # print(post_observations_raw)\n",
    "        # print(post_observations)\n",
    "\n",
    "        # pre_observations = pre_observations_raw\n",
    "        # post_observations = post_observations_raw\n",
    "\n",
    "        #### TRAIN THE TRANSITION MODEL ####\n",
    "        if self.train_tran:\n",
    "            # only look at the first n actions that we took\n",
    "            actions = actions_complete[0: len(pre_observations)]\n",
    "\n",
    "            num_observations = pre_observations.shape[0]\n",
    "            observation_dim = pre_observations.shape[1]\n",
    "            action_dim = actions.shape[1]\n",
    "            latent_dim = self.model_vae.latent_dim\n",
    "\n",
    "            # action_dim = 1  # TODO fix this to allow different actions\n",
    "\n",
    "            # find the actual observed latent states using the vae\n",
    "            pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "            post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "            # set up the input training data that we use to train the transition model\n",
    "            z_train = np.concatenate([np.array(pre_latent_mean), np.array(actions)], axis=1)\n",
    "\n",
    "            # we use the sequence to find the right hidden states to use as input\n",
    "            z_train_seq = z_train.reshape((1, num_observations, latent_dim + action_dim))\n",
    "            z_train_singles = z_train.reshape(num_observations, 1, latent_dim + action_dim)\n",
    "\n",
    "            # the previous hidden state is the memory after observing some sequences but it might be None\n",
    "            if self.hidden_state is None:\n",
    "                self.hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.hidden_state))\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran_train_epochs, verbose=self.show_tran_training)\n",
    "\n",
    "            # now find the new predicted hidden state that we will use for finding the policy\n",
    "            # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "            _, _, final_hidden_state, _ = self.tran((z_train_seq, self.hidden_state))\n",
    "            # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "            self.hidden_state = final_hidden_state\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            # self.model_vae.fit(pre_observations_raw, epochs=self.vae_train_epochs, verbose=self.show_vae_training)\n",
    "            self.model_vae.fit(pre_observations, epochs=self.vae_train_epochs, verbose=self.show_vae_training)\n",
    "\n",
    "            # print(\"true\", pre_observations)\n",
    "            # print(\"pred\", self.model_vae(pre_observations))\n",
    "\n",
    "        #### TRAIN THE PRIOR MODEL ####\n",
    "        if self.train_prior:\n",
    "            # self.prior_model.train(post_observations, rewards, verbose=self.show_prior_training)\n",
    "            self.prior_model.train(post_observations_raw, rewards, verbose=self.show_prior_training)\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "\n",
    "        # TODO do you take the mean or that latent here?\n",
    "        # get the latent state from this observation\n",
    "        _,  _, latent_state = self.model_vae.encoder(observation.reshape(1, observation.shape[0]))\n",
    "        # latent_state = latent_state.numpy().reshape((1, latent_state.shape[0]))\n",
    "\n",
    "        # print(latent_state)\n",
    "        # select the policy\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(latent_state)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by one to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape (self.n_policies, latent_dim) when z_t_minus is tensor with shape (1, latent_dim\n",
    "        prev_latent_mean = tf.squeeze(tf.stack([z_t_minus_one]*self.n_policies, axis=1))\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # print(prev_latent_mean)\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            # print(tran_input)\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        if self.use_FEEF:\n",
    "            return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "        else:\n",
    "            return self.EFE(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "                # Compute the extrinisc approximation with the prior model\n",
    "                else:\n",
    "                    kl_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    kl_extrinsic = tf.reduce_sum(kl_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                kl_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # print(\"Extrinsic\", kl_extrinsic)\n",
    "            # print(\"Intrinsic\", kl_intrinsic)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    # TODO Find out how this works with the log probability extrinsic term\n",
    "    def EFE(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        EFEs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "\n",
    "            if self.use_kl_extrinsic:\n",
    "                likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "                if self.prior_model is None:\n",
    "\n",
    "                    # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                    # create the prior distribution\n",
    "                    prior_preferences_mean = tf.convert_to_tensor(np.stack(self.given_prior_mean), dtype=\"float32\")\n",
    "                    prior_preferences_stddev = tf.convert_to_tensor(np.stack(self.given_prior_stddev), dtype=\"float32\")\n",
    "\n",
    "                    prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "                    # compute extrinsic prior preferences term\n",
    "                    efe_extrinsic = -1 * tf.math.log(prior_dist.prob(predicted_likelihood))\n",
    "\n",
    "                # TODO Can I use the learned prior model here?\n",
    "                else:\n",
    "                    efe_extrinsic = self.prior_model.extrinsic_kl(predicted_likelihood)\n",
    "                    efe_extrinsic = tf.reduce_sum(efe_extrinsic, axis=-1)\n",
    "\n",
    "            # if we don't use extrinsic set it to zero\n",
    "            else:\n",
    "                efe_extrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            if self.use_kl_intrinsic:\n",
    "\n",
    "                policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "                predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "                kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            else:\n",
    "                kl_intrinsic = tf.zeros(self.n_policies, dtype=\"float\")\n",
    "\n",
    "            EFE = efe_extrinsic - kl_intrinsic\n",
    "\n",
    "            EFEs.append(EFE)\n",
    "\n",
    "        return EFEs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.83333333, 0.        ])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2,  [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 20, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=5,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=False,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False,\n",
    "                           n_policies=50,\n",
    "                           n_policy_candidates=10,\n",
    "                           show_tran_training=True)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.5799777  0.       ]\n",
      "1/1 [==============================] - 1s 709ms/step - kl_loss: 0.1208\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0539\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0306\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0287\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0216\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0408\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0832\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1093\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0269\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0252\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.0247\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0157\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0414\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0526\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0271\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0080\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0197\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0247\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0132\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0298\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0274\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0157\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.0174\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0315\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0077\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0187\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0306\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0219\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0062\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0242\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0299\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0173\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0263\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0850\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.0273\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.0077\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0307\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0228\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0100\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0197\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0617\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0125\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0226\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0294\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0379\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0083\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0175\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0387\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0058\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0193\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0306\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0101\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0131\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0329\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.0131\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0143\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0400\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0105\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0079\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0269\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0272\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0069\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0148\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0114\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0092\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0216\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0514\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0316\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0142\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.0368\n",
      "1/1 [==============================] - 0s 4ms/step - kl_loss: 0.0348\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0221\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0178\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0442\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0999\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0239\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0532\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0582\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0396\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0170\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0671\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0577\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0309\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 460us/step - loss: 26.5445 - reconstruction_loss: 19.6540 - kl_loss: 4.5007\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 448us/step - loss: 20.3505 - reconstruction_loss: 16.8518 - kl_loss: 2.9208\n",
      "No Success\n",
      "Episode 2\n",
      "[-0.5256398  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5518\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5589\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3478\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 2.4350\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1059\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6502\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5313\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0926\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1068\n",
      "[0.75210952 0.22941477]\n",
      "tf.Tensor([0.59155244 0.536587   0.68349344 0.7316333  0.3089003 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "4/4 [==============================] - 0s 712us/step - loss: 52.3548 - reconstruction_loss: 49.5605 - kl_loss: 2.3692\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 0s 651us/step - loss: 50.6568 - reconstruction_loss: 47.4722 - kl_loss: 2.3312\n",
      "Success in episode 2 at time step 119\n",
      "Episode 3\n",
      "[-0.59040123  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9757\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4061\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0991\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0582\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1281\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4603\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4085\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4045\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0880\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0241\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0124\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0621\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2057\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3378\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1372\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0675\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0461\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0461\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0234\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2163\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2172\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1239\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0795\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0968\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0608\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0668\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1582\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0936\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1334\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1594\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1443\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0825\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0840\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1438\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0654\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0797\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1707\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2250\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1873\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0810\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1953\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1696\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0615\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1695\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1701\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1885\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1410\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0704\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2073\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1327\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0918\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1148\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1461\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0928\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0549\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2423\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2373\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1778\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1308\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1223\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1414\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0593\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0646\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2356\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1790\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1346\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1340\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2389\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1993\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2788\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2205\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2248\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1084\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0528\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2748\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3502\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2146\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1719\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1003\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1496\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2380\n",
      "[0.79094043 0.21069209]\n",
      "tf.Tensor([0.55465704 0.75864    0.6583992  0.41081533 0.495261  ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 435us/step - loss: 37.7353 - reconstruction_loss: 34.0520 - kl_loss: 3.1138\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 434us/step - loss: 34.4661 - reconstruction_loss: 30.2727 - kl_loss: 2.8767\n",
      "Success in episode 3 at time step 973\n",
      "Episode 4\n",
      "[-0.58327556  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5589\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6582\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2892\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1485\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2581\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5622\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5849\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4620\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1348\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1073\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1614\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5622\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.4973\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 18.2565\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0177\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2353\n",
      "[0.10746188 0.8053876 ]\n",
      "tf.Tensor([ 0.7038759   0.9216539   0.84462947  0.47890925 -0.02008794], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 799us/step - loss: 43.4020 - reconstruction_loss: 38.9175 - kl_loss: 3.2298\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 810us/step - loss: 33.6270 - reconstruction_loss: 31.1509 - kl_loss: 3.6296\n",
      "Success in episode 4 at time step 204\n",
      "Episode 5\n",
      "[-0.43877652  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3228\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1138\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1232\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1414\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1451\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1658\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2905\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2426\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1557\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2407\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2085\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2321\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4340\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5681\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2297\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1473\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2710\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1677\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1393\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1894\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3865\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2124\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1332\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1848\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0817\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1875\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0813\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1319\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0801\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1526\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1480\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0389\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0053\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0831\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0335\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0920\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1757\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1975\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2081\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1788\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1222\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0997\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1004\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2113\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2651\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3106\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1124\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1003\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0597\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2010\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1144\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2416\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2029\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3356\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0740\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0782\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1317\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3847\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4132\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2819\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2854\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0500\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1385\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3320\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4085\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1257\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1769\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1970\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0571\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2244\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1481\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3347\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1893\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5042\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1527\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0514\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1321\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2169\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2821\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8463\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0937\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0297\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0558\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 438us/step - loss: 8.9678 - reconstruction_loss: 7.4541 - kl_loss: 0.9446\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 443us/step - loss: 6.7478 - reconstruction_loss: 4.9362 - kl_loss: 1.2887\n",
      "No Success\n",
      "Episode 6\n",
      "[-0.49880037  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.3971\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1211\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0666\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1673\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3094\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7582\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5233\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3387\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7593\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5899\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4879\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3569\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5973\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3901\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9685\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1215\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3443\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9112\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3053\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5991\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0249\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2018\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1920\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0898\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6151\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3909\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3598\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2334\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2044\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5652\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5275\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5780\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8711\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9953\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3346\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2102\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3774\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6843\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8997\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2210\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0382\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1696\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7085\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8320\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7945\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4371\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5871\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0755\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1106\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1730\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3084\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2972\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8599\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7454\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7049\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7831\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8037\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2591\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3876\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1770\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4275\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2613\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4728\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2060\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4712\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0781\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6077\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5246\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3570\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2435\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1134\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8900\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7927\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5239\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8398\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2820\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3083\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2089\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5847\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4210\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1304\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4632\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 433us/step - loss: 6.1732 - reconstruction_loss: 3.5396 - kl_loss: 2.3112\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 444us/step - loss: 5.2684 - reconstruction_loss: 2.2732 - kl_loss: 2.7194\n",
      "No Success\n",
      "Episode 7\n",
      "[-0.52635854  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6503\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0247\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5218\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1639\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7768\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8949\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7847\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5875\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2179\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9252\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5662\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5837\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9702\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7959\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7525\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2395\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8085\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4572\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9319\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.6328\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1332\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3486\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1869\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7674\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5420\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1522\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1741\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4603\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6714\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4779\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9079\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6117\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3468\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5561\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5668\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4791\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9012\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1496\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6117\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5933\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2959\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6963\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6496\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9018\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5765\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9333\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3992\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7952\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7193\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1783\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5136\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2913\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4815\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2612\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0943\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0291\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7316\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7776\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1220\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7559\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9388\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5738\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4622\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9345\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6824\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2460\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5949\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8962\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6334\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5309\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7770\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4891\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4752\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9061\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0798\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6913\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1762\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3110\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3361\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1987\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0984\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7861\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2480\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 446us/step - loss: 4.1840 - reconstruction_loss: 1.5763 - kl_loss: 2.4886\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 441us/step - loss: 3.7821 - reconstruction_loss: 1.3245 - kl_loss: 2.5351\n",
      "No Success\n",
      "Episode 8\n",
      "[-0.5733375  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 19.3588\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1783\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0249\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2642\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8133\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6538\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6794\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5379\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4818\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1658\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5352\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1421\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8682\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.5199\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9043\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9930\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3158\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8367\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3733\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6465\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4193\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9839\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0250\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7504\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7298\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0545\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2784\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3076\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9890\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3742\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3265\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7456\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7390\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1803\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 7.6935\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0607\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3822\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3716\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5185\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3873\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2965\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7343\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1740\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0623\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2163\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0416\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3100\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4856\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4069\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5453\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9872\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6520\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3711\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4164\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7708\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5587\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4765\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3293\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6613\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4312\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5396\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2228\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6764\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4819\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3436\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6033\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8235\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6003\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5764\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2825\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7070\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4549\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1008\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6561\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3746\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3705\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0628\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7733\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2340\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5660\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3024\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.4592\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4215\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 430us/step - loss: 4.4466 - reconstruction_loss: 1.3320 - kl_loss: 3.1033\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 448us/step - loss: 4.5268 - reconstruction_loss: 1.3784 - kl_loss: 3.0679\n",
      "No Success\n",
      "Episode 9\n",
      "[-0.5329699  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3542\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5882\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3475\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4084\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3863\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1077\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8086\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4348\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2295\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3971\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5988\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2085\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7973\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2124\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4661\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0424\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1334\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1900\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2832\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2721\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1129\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5280\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2004\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7622\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7116\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7609\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2207\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0960\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5976\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5736\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7798\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7209\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7734\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8079\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3860\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1912\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6623\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2418\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0941\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3458\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0328\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4318\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6957\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7899\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1763\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4868\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5026\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2360\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7083\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7831\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1604\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0865\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3468\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5617\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3351\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0810\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6048\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 11.9219\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8586\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2190\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8883\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3152\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2457\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9307\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4405\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3107\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3627\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0452\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5768\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5482\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5876\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4805\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0256\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1183\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4596\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7421\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3590\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7945\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4909\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7015\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1047\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7847\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1498\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 451us/step - loss: 2.9592 - reconstruction_loss: 1.0438 - kl_loss: 1.9570\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 438us/step - loss: 2.9429 - reconstruction_loss: 1.0090 - kl_loss: 1.9655\n",
      "No Success\n",
      "Episode 10\n",
      "[-0.4046037  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4324\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5743\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2501\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6589\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9073\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1543\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0538\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2808\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9429\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6048\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4820\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5388\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1589\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4648\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2492\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1977\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3178\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2802\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3238\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2852\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3395\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3920\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2058\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1940\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7471\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2218\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7044\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8806\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5011\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7942\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2980\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8440\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1865\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5286\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2201\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1569\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5525\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2323\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5532\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0190\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6040\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5600\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1731\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5908\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3828\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6999\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8149\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2713\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0706\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4108\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5253\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9244\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2125\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0253\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4188\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8557\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2931\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3931\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2658\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6708\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6694\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5870\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3908\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4883\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7486\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0940\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7537\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2898\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7280\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7449\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7693\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4063\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1600\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1901\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1256\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5841\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9228\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2000\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.0406\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8693\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1056\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5282\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8450\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 432us/step - loss: 2.9874 - reconstruction_loss: 1.0042 - kl_loss: 1.9792\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 443us/step - loss: 2.9628 - reconstruction_loss: 0.9979 - kl_loss: 1.9646\n",
      "No Success\n",
      "Episode 11\n",
      "[-0.40494072  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3316\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1261\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1298\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6112\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6903\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8526\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0344\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6698\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9601\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5871\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5520\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2998\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3964\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6870\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7848\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4528\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9650\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2415\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3977\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0518\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5556\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3082\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7580\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 1.2802\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4963\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3078\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7583\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6539\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1422\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4804\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7048\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6650\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9458\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1080\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4403\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8068\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6407\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2731\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3314\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4445\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4095\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2731\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7817\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1589\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7059\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0919\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2031\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7884\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3584\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2186\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2962\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5027\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4163\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2939\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0008\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1521\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8701\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4550\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7922\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2944\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2846\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4138\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9525\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1272\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6109\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3001\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3777\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5402\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4433\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9808\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8826\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5812\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3457\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6094\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9386\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8831\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1890\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8367\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0187\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1198\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4765\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2683\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 449us/step - loss: 2.3769 - reconstruction_loss: 0.8745 - kl_loss: 1.5147\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 449us/step - loss: 2.3713 - reconstruction_loss: 0.9030 - kl_loss: 1.4694\n",
      "No Success\n",
      "Episode 12\n",
      "[-0.54285055  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7177\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3282\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5277\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6877\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3214\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3080\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3957\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2223\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8913\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3694\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8115\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6930\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6542\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4196\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4508\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8186\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2572\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6691\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2121\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6356\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5207\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4330\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9045\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4416\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8605\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0024\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1234\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9435\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8914\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2701\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1384\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1196\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4311\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8858\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0160\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4322\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4275\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2581\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9177\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5692\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4132\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9415\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4138\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4196\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4080\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1094\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5657\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9608\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3025\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5759\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3746\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0373\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7952\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0677\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7308\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1918\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1624\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4445\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1709\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0998\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2294\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9648\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2800\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8575\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1883\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4953\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7168\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7280\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5525\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6470\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3523\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7465\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4214\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7791\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9523\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5022\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3501\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2018\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4853\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7802\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2444\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9466\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5705\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 439us/step - loss: 2.7785 - reconstruction_loss: 1.0466 - kl_loss: 1.7260\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 451us/step - loss: 2.8568 - reconstruction_loss: 1.1002 - kl_loss: 1.7350\n",
      "No Success\n",
      "Episode 13\n",
      "[-0.56634915  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8035\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4487\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4796\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8052\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4553\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8002\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5702\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8626\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2883\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3486\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8436\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0661\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.7446\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2001\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6108\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5100\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3641\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3610\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4205\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8339\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3791\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9782\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1606\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6123\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1759\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7208\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4531\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4760\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4338\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1051\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6943\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2373\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5103\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6606\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6919\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1274\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1356\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2194\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9367\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2418\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5042\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2613\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9452\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1919\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4756\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7156\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8830\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5000\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6661\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5433\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0828\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6528\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8216\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6203\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1453\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5149\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6515\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2296\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5748\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9452\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3443\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4142\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7663\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2129\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9185\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2570\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3137\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6731\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1060\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9556\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.5992\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6148\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7684\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8752\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3591\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2880\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5213\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4493\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8056\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4506\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9518\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2720\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7244\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 464us/step - loss: 3.1050 - reconstruction_loss: 1.0305 - kl_loss: 2.0994\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 441us/step - loss: 3.1639 - reconstruction_loss: 1.0853 - kl_loss: 2.0821\n",
      "No Success\n",
      "Episode 14\n",
      "[-0.55298096  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2489\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5487\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2053\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8399\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4177\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4729\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5423\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5876\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7661\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9516\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4192\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2660\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9990\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8190\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1894\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6500\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0828\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0759\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6394\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8729\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7169\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8472\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5583\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6225\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1898\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5215\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2533\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4709\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9233\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5420\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0128\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8370\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0580\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3147\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7181\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4352\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2037\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6333\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3351\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8344\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5606\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8302\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8418\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5572\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6443\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7824\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6291\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7913\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1644\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8081\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5346\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9963\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7375\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7172\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5136\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7166\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3400\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3758\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9473\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1272\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4759\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5408\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6486\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6406\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4323\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7486\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0058\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2036\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8722\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9880\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2246\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4622\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8006\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3775\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9067\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7375\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8122\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4200\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8324\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6518\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8206\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 442us/step - loss: 3.1072 - reconstruction_loss: 1.0788 - kl_loss: 1.9993\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 445us/step - loss: 3.1005 - reconstruction_loss: 1.1063 - kl_loss: 2.0216\n",
      "No Success\n",
      "Episode 15\n",
      "[-0.489303  0.      ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.4212\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3102\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5970\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7846\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6510\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0608\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3404\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0483\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3871\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5684\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8183\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0417\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4526\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5512\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7376\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5920\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2311\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3259\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5651\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3869\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5694\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2964\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8038\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9517\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4230\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4710\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7729\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6908\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2760\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6409\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9693\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7652\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9323\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9643\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5442\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9113\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5201\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3344\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7312\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4646\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2289\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6823\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4792\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3833\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5591\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4031\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1692\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2794\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8695\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4742\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6244\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1873\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5424\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9142\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3265\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4224\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6333\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6041\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6464\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3556\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4106\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2037\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9323\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7447\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1886\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1857\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3823\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0782\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4471\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6032\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8245\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3686\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0685\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7290\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2079\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5551\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6670\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1201\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8165\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1913\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0673\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9727\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9816\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 452us/step - loss: 3.3180 - reconstruction_loss: 1.0302 - kl_loss: 2.3141\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 416us/step - loss: 3.3063 - reconstruction_loss: 1.0597 - kl_loss: 2.2748\n",
      "No Success\n",
      "Episode 16\n",
      "[-0.54885054  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.2370\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0407\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0267\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4156\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8804\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4914\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3114\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6675\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9788\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9360\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0621\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1650\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8718\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6512\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8871\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7583\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2638\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3783\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0643\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0613\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8363\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3863\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9014\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1369\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3720\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3661\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8063\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0336\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9706\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7116\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8600\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2915\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6159\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6117\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4266\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6065\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0396\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0915\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3386\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6405\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4477\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8091\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5196\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9653\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8637\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9622\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5837\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2085\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7216\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2947\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4489\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2212\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4103\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9691\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8008\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0414\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4725\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4267\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5858\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9279\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0307\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1906\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6737\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5686\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4585\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8553\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4033\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7187\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5623\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1533\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2176\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2868\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5963\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2010\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5157\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4838\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2133\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6091\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3723\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4308\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1675\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0919\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2784\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 437us/step - loss: 2.7519 - reconstruction_loss: 0.9708 - kl_loss: 1.8057\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 435us/step - loss: 2.7462 - reconstruction_loss: 0.9764 - kl_loss: 1.7772\n",
      "No Success\n",
      "Episode 17\n",
      "[-0.52620816  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.7260\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2395\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9219\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1935\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5092\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4374\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7792\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9166\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2717\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6367\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6336\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1954\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6940\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5918\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9216\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6961\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6703\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2538\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3092\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6774\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4206\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1282\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6357\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5742\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5400\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3005\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5372\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6808\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8038\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7113\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3354\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4967\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4715\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7784\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0947\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3499\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1300\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6872\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8378\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6843\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4003\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6097\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2057\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1315\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3591\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.0961\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1199\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5125\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7115\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3418\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6621\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4537\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9995\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7794\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3634\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4693\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1365\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5736\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0798\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7032\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3859\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1191\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6873\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2234\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1422\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7729\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5089\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8311\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5735\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2731\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4470\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6645\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3561\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0618\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1689\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5622\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3082\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9578\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4315\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8244\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4702\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7380\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0240\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 431us/step - loss: 3.0827 - reconstruction_loss: 1.0688 - kl_loss: 1.9950\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 437us/step - loss: 3.0911 - reconstruction_loss: 1.0630 - kl_loss: 2.0300\n",
      "No Success\n",
      "Episode 18\n",
      "[-0.45781144  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.3841\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8857\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8649\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6500\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6448\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7978\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2752\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7473\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7888\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2948\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6066\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2581\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9955\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6414\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3724\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5275\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5310\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0908\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1463\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2398\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7907\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8987\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2956\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6742\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6293\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2543\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0859\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7133\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2453\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1207\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4484\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2988\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7873\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8032\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1809\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3459\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9675\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2111\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5301\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8957\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6380\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8640\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1649\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2303\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6874\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4982\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5864\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3584\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2353\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4835\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4234\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1827\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7585\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4161\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9741\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5373\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6628\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5897\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8216\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1695\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2542\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7308\n",
      "1/1 [==============================] - 0s 5ms/step - kl_loss: 0.4154\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5208\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2359\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5949\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7361\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1530\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6058\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0074\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7891\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9366\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1306\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2275\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4342\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5434\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9529\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1695\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3516\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9748\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5727\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5824\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7284\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 442us/step - loss: 3.3858 - reconstruction_loss: 1.0287 - kl_loss: 2.2657\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 442us/step - loss: 3.3180 - reconstruction_loss: 1.0816 - kl_loss: 2.2299\n",
      "No Success\n",
      "Episode 19\n",
      "[-0.5327499  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.1753\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7328\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6004\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0067\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1596\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4118\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1007\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8343\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2877\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5695\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3348\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9786\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8896\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5706\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1796\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0907\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4835\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2968\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3512\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1134\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5585\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4173\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4761\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2439\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4136\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6824\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8127\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3076\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9500\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2932\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1665\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3030\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7301\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4035\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4824\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3732\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5392\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9683\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6009\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9880\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9679\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2629\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0908\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0415\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7592\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4968\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1747\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4813\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6967\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7024\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3370\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0463\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3031\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2054\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5500\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5444\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4282\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6228\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1302\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2444\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9112\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5208\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6564\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6471\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2180\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6079\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9482\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5737\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2887\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9482\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9197\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0947\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9051\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8860\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2221\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0375\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6997\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2200\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6197\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3107\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4964\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2015\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3696\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 448us/step - loss: 3.5595 - reconstruction_loss: 1.1142 - kl_loss: 2.4371\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 436us/step - loss: 3.5199 - reconstruction_loss: 1.0811 - kl_loss: 2.4469\n",
      "No Success\n",
      "Episode 20\n",
      "[-0.40392867  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4119\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3836\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1883\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7402\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3218\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9493\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7444\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4449\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8633\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9278\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9478\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8092\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5117\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8892\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3208\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9240\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1452\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3160\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9218\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5529\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7113\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6471\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6066\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7468\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6558\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5623\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2822\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8389\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6099\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8859\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0487\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7071\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8570\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1224\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0046\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5921\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.4551\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1749\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8778\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2764\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4576\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7090\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7764\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1109\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4505\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7399\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9053\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6464\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2985\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8640\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8472\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4853\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6271\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3175\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9602\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9156\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0394\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1433\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3016\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8510\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4511\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8967\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6648\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4933\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0657\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1548\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6690\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3869\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7282\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2345\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2145\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3867\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1939\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0873\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5204\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2570\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6384\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3496\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2897\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7009\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8880\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8753\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1798\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 450us/step - loss: 2.6222 - reconstruction_loss: 0.9419 - kl_loss: 1.6862\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 423us/step - loss: 2.6025 - reconstruction_loss: 0.9909 - kl_loss: 1.6458\n",
      "No Success\n",
      "Episode 21\n",
      "[-0.59367776  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3194\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6891\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4550\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8354\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4471\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7883\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4798\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6032\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9431\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8961\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7156\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2052\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2365\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6841\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0058\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7181\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1124\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4101\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2218\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3778\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7840\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3173\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2578\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2042\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4291\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4831\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3216\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4272\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4233\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3072\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0269\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7229\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4199\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4444\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6254\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8144\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4002\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6234\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4350\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3428\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9488\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3903\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5301\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6502\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4630\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8807\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7372\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3469\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4602\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9241\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1461\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6279\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5091\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4466\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4685\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5969\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9646\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3319\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5002\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3684\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8737\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0392\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0613\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9848\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0437\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7620\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0965\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8726\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3837\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2914\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5359\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6959\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3786\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3790\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1510\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6183\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5422\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6949\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2312\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0889\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1413\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7587\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 436us/step - loss: 3.8431 - reconstruction_loss: 1.1355 - kl_loss: 2.6364\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 431us/step - loss: 3.7968 - reconstruction_loss: 1.1808 - kl_loss: 2.6241\n",
      "No Success\n",
      "Episode 22\n",
      "[-0.4277265  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2782\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.3981\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1530\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7116\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4803\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2027\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3627\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7619\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0123\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6378\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7155\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3898\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8622\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0800\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3105\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8750\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1136\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1244\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4871\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1267\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0698\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0830\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6747\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6038\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3756\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3096\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0646\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5069\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8937\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3446\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4116\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9629\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4703\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8064\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7084\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9529\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8554\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0525\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9284\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9616\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7239\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8009\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4635\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0619\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4370\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8899\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2962\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1213\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5065\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2941\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3865\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4493\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4096\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4287\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2351\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9914\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3765\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8549\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3691\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3860\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4915\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1146\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8348\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3212\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5099\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3521\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3663\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1326\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2799\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6800\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8756\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2938\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6540\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2977\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6797\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9727\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0669\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7554\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4916\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3423\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0106\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9796\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 464us/step - loss: 2.4683 - reconstruction_loss: 0.8184 - kl_loss: 1.5918\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 411us/step - loss: 2.3495 - reconstruction_loss: 0.9139 - kl_loss: 1.4311\n",
      "No Success\n",
      "Episode 23\n",
      "[-0.46055615  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.8245\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3575\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6300\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6240\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0426\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5865\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5245\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2340\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8078\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1359\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6454\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7764\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8190\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6976\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2533\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4348\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2845\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6390\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0082\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3265\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3378\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6528\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9068\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8461\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2758\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6898\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9333\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2196\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0191\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2469\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5833\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8309\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4948\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5699\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0939\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1392\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5237\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2198\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4066\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9211\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1865\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4377\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6547\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2750\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2323\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9214\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3754\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9703\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2770\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4430\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2928\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3319\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4240\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3322\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5778\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3769\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1874\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6770\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1911\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6881\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6617\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3936\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5983\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5660\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8663\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3992\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6935\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0317\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8570\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0343\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3211\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5055\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0568\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6504\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4694\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6173\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0424\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3903\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2152\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4879\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8979\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2431\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 481us/step - loss: 2.3364 - reconstruction_loss: 0.9867 - kl_loss: 1.3581\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 430us/step - loss: 2.2662 - reconstruction_loss: 1.0262 - kl_loss: 1.2896\n",
      "No Success\n",
      "Episode 24\n",
      "[-0.51853985  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.8493\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7219\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1479\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1247\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4762\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4424\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6452\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9539\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1745\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9893\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6703\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4071\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6993\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5139\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6337\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0356\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3754\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9140\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2658\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4137\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9335\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0682\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0379\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0660\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5721\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2458\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5446\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8418\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4670\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1844\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4424\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5985\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5220\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8459\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3945\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7494\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0826\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7143\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2135\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8889\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1271\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8134\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9847\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4463\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5873\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8218\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7362\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3152\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4392\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0490\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4531\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3377\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8380\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2198\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7862\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2859\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8734\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3709\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2078\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8963\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8345\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5195\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1606\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3757\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4847\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8318\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8230\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8581\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6545\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0327\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7993\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0664\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7390\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1836\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2715\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0710\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1162\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1051\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3851\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3226\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1978\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2466\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 441us/step - loss: 2.4890 - reconstruction_loss: 1.1249 - kl_loss: 1.3862\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 427us/step - loss: 2.4735 - reconstruction_loss: 1.0529 - kl_loss: 1.4497\n",
      "No Success\n",
      "Episode 25\n",
      "[-0.43491998  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2536\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2492\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8146\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1864\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2042\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3154\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3283\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5122\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9615\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4135\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4642\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0555\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0643\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6860\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2930\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4731\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9748\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2607\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5466\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6429\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3540\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1098\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4575\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2848\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6878\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6684\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6353\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6527\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5082\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3546\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6874\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1290\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7957\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9411\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3216\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7518\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1967\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8422\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0046\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0049\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7223\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0067\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0675\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6609\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0253\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5394\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6539\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7966\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2430\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4994\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3162\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0164\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6683\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7642\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1689\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8669\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7942\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1581\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8925\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1895\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8592\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7706\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9620\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3973\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7880\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6981\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0971\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5479\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4771\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7990\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9856\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0195\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0678\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4746\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4856\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2737\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6706\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8796\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7772\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3916\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5192\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9585\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4968\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 488us/step - loss: 2.0064 - reconstruction_loss: 0.8770 - kl_loss: 1.1484\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 440us/step - loss: 1.9592 - reconstruction_loss: 0.9174 - kl_loss: 1.0256\n",
      "No Success\n",
      "Episode 26\n",
      "[-0.5123773  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1968\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2504\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3997\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6751\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6983\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2694\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1789\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5279\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5566\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7548\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1222\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0966\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7537\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0910\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6142\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3901\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1390\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7035\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7740\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9366\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3862\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7128\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2755\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5356\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3463\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2992\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5896\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6083\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1592\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8897\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2385\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1731\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6115\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5599\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3140\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3437\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7543\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3524\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6401\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2147\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2257\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3286\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3961\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3800\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4990\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0792\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3717\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4532\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3858\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3043\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7202\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1785\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3771\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2337\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1303\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5829\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9933\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3390\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0620\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0279\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1873\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8268\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2664\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2684\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5392\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6734\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1186\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4962\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2740\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3221\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7787\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2677\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8397\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0220\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0547\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1560\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4083\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2434\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5819\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0394\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5275\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8058\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1427\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 448us/step - loss: 3.0481 - reconstruction_loss: 1.3124 - kl_loss: 1.7271\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 439us/step - loss: 2.9590 - reconstruction_loss: 1.0903 - kl_loss: 1.8922\n",
      "No Success\n",
      "Episode 27\n",
      "[-0.55333036  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 10.6725\n",
      "1/1 [==============================] - 0s 18ms/step - kl_loss: 0.9145\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3107\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9859\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1883\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9814\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5530\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6787\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4261\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9173\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2501\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9746\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0597\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5101\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3175\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6287\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0972\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3129\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6748\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5027\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2144\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9954\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0819\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7190\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5070\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5340\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9924\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2821\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1634\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6924\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1567\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9944\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3892\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7886\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0620\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6090\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8175\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4926\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8584\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9658\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0585\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5112\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2676\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4304\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9552\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2606\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2161\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5616\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3923\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1147\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7981\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8016\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7534\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5635\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1155\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2364\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6848\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0088\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6191\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7314\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3940\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4276\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1198\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9943\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8458\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7287\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1141\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8806\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.1533\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3863\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0629\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8483\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6306\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6387\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6975\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4451\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4815\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5659\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6225\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1280\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3977\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2590\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7652\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 470us/step - loss: 2.9925 - reconstruction_loss: 1.0272 - kl_loss: 1.9492\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 462us/step - loss: 3.0179 - reconstruction_loss: 1.1055 - kl_loss: 1.9225\n",
      "No Success\n",
      "Episode 28\n",
      "[-0.4799117  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4727\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6964\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2485\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4876\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9683\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9748\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8486\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7720\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2890\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4958\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9915\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8645\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.8966\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.2592\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4842\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2118\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6968\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2333\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1026\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5288\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3946\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4766\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5662\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1613\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6572\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8970\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3853\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5496\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7296\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2000\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4064\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7230\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3437\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2573\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1166\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6496\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0568\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3408\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6184\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9786\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5869\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8806\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6711\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9244\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6882\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5538\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4708\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8802\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2215\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4732\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6335\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2291\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4860\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4846\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4796\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3642\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4191\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4497\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5589\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9245\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5869\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0101\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0885\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6572\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8973\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3216\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7768\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8041\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9504\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4562\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9200\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6459\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8461\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2515\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7504\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0597\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7643\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4474\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8447\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3239\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3605\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5131\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6523\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 456us/step - loss: 3.8346 - reconstruction_loss: 1.0872 - kl_loss: 2.7442\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 440us/step - loss: 3.7256 - reconstruction_loss: 1.1710 - kl_loss: 2.5694\n",
      "No Success\n",
      "Episode 29\n",
      "[-0.47592092  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 12.3914\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2826\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1051\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5568\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3998\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6351\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2880\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1078\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9769\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7382\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6828\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9836\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3214\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6184\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3469\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8430\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8083\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6226\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4537\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7228\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.5800\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3851\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3672\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2515\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8903\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8532\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1216\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3839\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.3508\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3817\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8606\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3425\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5715\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6187\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1879\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0737\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2970\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2909\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4965\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5539\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3110\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7046\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1588\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7629\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7751\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7711\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6114\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5349\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5504\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1762\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4170\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9586\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5263\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2383\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4646\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7497\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5504\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6607\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9963\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2156\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2918\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3627\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1491\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1234\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1442\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4514\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3449\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8072\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6529\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2816\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8703\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9344\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6349\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1361\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8412\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6473\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2889\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2763\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5214\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9611\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7668\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2188\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 447us/step - loss: 2.5661 - reconstruction_loss: 0.9653 - kl_loss: 1.6219\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 429us/step - loss: 2.4641 - reconstruction_loss: 0.9688 - kl_loss: 1.5429\n",
      "No Success\n",
      "Episode 30\n",
      "[-0.5757761  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9327\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4670\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0771\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6684\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3823\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3369\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4681\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2761\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9336\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5067\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7235\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6885\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9009\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5860\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7139\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0457\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8927\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3023\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1832\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2664\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6357\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2301\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4786\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5931\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3649\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3484\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2662\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5954\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6669\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0510\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3883\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2061\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7767\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4935\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2303\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4671\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7954\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0859\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4525\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6532\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5640\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7183\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1234\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0933\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3836\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3553\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7324\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4129\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8115\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0290\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2800\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8090\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.2312\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9837\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0107\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2108\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7446\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1609\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2696\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8301\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1433\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0056\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5851\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0938\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3455\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2158\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6665\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6428\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5511\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3176\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 5.7357\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5976\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7898\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1903\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4991\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1322\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2511\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8529\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8470\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2087\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5730\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8509\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1467\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 428us/step - loss: 3.9261 - reconstruction_loss: 1.1478 - kl_loss: 2.7399\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 426us/step - loss: 3.8150 - reconstruction_loss: 1.0707 - kl_loss: 2.6784\n",
      "No Success\n",
      "Episode 31\n",
      "[-0.5618337  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.7824\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6890\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6683\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.1259\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5462\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7167\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3243\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1403\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6388\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8218\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1313\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2685\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3682\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0796\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0005\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0595\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0885\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8288\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4580\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0019\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0831\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1253\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0236\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5486\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7838\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4524\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1554\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8771\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7714\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4850\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9124\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8443\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6202\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6270\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3773\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7081\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0577\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5741\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7871\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7100\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8625\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2000\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2014\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3098\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2837\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1270\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8283\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1439\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0587\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5774\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1271\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0927\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8100\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3843\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9419\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3330\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6528\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4135\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4385\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7207\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7400\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4354\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9957\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5771\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0351\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6223\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1146\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4820\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6874\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2533\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8622\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4377\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3416\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8223\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1532\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2171\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2642\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9627\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9395\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1142\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3114\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3663\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2820\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 448us/step - loss: 3.5428 - reconstruction_loss: 1.1208 - kl_loss: 2.4516\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 437us/step - loss: 3.4909 - reconstruction_loss: 1.0142 - kl_loss: 2.4902\n",
      "No Success\n",
      "Episode 32\n",
      "[-0.54802537  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 13.2537\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2164\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2548\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9673\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4436\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0609\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5994\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1618\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0133\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0225\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5983\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1413\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4792\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2092\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2205\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9028\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1593\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1732\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8217\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9935\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6892\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3679\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7382\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4940\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3912\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0795\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3654\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6496\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9905\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1454\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7674\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4101\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6201\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2863\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.2911\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1328\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1714\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4868\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1098\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4725\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0532\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7128\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2130\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5333\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8302\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0986\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8004\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7938\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9615\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0519\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3615\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7257\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6682\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3766\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1343\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8036\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8562\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0979\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7010\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1545\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9643\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0136\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7464\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3983\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3634\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6369\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3613\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4856\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4280\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0262\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3221\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5979\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5278\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0387\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2434\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6407\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8543\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8211\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3645\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0319\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0021\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7135\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8445\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 445us/step - loss: 2.7709 - reconstruction_loss: 0.9806 - kl_loss: 1.8198\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 434us/step - loss: 2.7450 - reconstruction_loss: 0.9450 - kl_loss: 1.7779\n",
      "No Success\n",
      "Episode 33\n",
      "[-0.42911306  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.0770\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5478\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8397\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6510\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3171\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4707\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7362\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6195\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7938\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2687\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8725\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6085\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4940\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0132\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1594\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3547\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3401\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4337\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5669\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8310\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3315\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1883\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6061\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3254\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3627\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8360\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5125\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4989\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9547\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2860\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1787\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1471\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1060\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5905\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5336\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2321\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2084\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4624\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3237\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2654\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6557\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5938\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5715\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8785\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3687\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3746\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5994\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6711\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4355\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8643\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5614\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4729\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6243\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5092\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1094\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4683\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7370\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4140\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2938\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4626\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5049\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8753\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5433\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4046\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9779\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4207\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4682\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6250\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6575\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3887\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3107\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4355\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3734\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3613\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9361\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0309\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8789\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7824\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8694\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9543\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7955\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1220\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 439us/step - loss: 2.1932 - reconstruction_loss: 0.8401 - kl_loss: 1.3178\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 423us/step - loss: 2.0603 - reconstruction_loss: 0.9099 - kl_loss: 1.0714\n",
      "No Success\n",
      "Episode 34\n",
      "[-0.43709233  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5083\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8876\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8089\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8746\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5811\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5885\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3629\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9758\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3516\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8859\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1808\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4993\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0122\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6262\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0647\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3840\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3409\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5275\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6354\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0214\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0585\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3607\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1933\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4702\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5279\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2076\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4667\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2807\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6607\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5039\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5149\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6246\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4202\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8471\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1260\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2993\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3678\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3923\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2240\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0221\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7031\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3529\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5700\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4807\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2640\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9751\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1200\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2878\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6867\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5912\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2870\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6144\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3107\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5012\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4591\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6885\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4248\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3755\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9763\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5581\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6726\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7743\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4811\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3890\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3976\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6631\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4517\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4258\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5306\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1049\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6391\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7494\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6803\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6201\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2200\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2744\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1534\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7836\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0850\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9355\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2626\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8954\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5264\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 438us/step - loss: 1.7854 - reconstruction_loss: 0.9874 - kl_loss: 0.7829\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 415us/step - loss: 1.7397 - reconstruction_loss: 1.0625 - kl_loss: 0.6504\n",
      "No Success\n",
      "Episode 35\n",
      "[-0.48201695  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3974\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0844\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2449\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7129\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0999\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4475\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0960\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2388\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1588\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5259\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2537\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8752\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8034\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1481\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7303\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1601\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1383\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6619\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4374\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5087\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5193\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5792\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8664\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4607\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0920\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7977\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1940\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4903\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8356\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2403\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4539\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9415\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2796\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5121\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5518\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2591\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8343\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0883\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4185\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3965\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1905\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3120\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8591\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2256\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0263\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1920\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1455\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7086\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0664\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4086\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1684\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7068\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3791\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8704\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9380\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1861\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4590\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3628\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2130\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0177\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1151\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1094\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2887\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1559\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3296\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2029\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2230\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9782\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0891\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3200\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4709\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6801\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1460\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6594\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3399\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0740\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1199\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3512\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6067\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4258\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6157\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2991\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 440us/step - loss: 2.0236 - reconstruction_loss: 1.3231 - kl_loss: 0.6734\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 426us/step - loss: 1.9230 - reconstruction_loss: 1.2511 - kl_loss: 0.7244\n",
      "No Success\n",
      "Episode 36\n",
      "[-0.5243529  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2526\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6580\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0808\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4890\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1382\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2519\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8144\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9855\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2971\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5663\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8948\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5671\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5393\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8881\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2109\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6083\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7836\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4149\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6698\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3756\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6851\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1166\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5721\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1147\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9049\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1941\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4198\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0237\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5074\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1721\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6343\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4750\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1421\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1529\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1707\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8763\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8603\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2674\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8367\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3256\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5927\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7023\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5532\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5705\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2754\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6634\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7578\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5746\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7978\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8420\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3047\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6440\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2515\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3404\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1235\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3841\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0547\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4659\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6742\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5695\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3015\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8798\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1331\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5765\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3416\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2382\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3006\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1816\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3901\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4296\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2626\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3240\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2160\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7092\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6071\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2087\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7224\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2213\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3394\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4300\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5242\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3031\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4795\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 439us/step - loss: 3.2851 - reconstruction_loss: 1.4246 - kl_loss: 1.7522\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 445us/step - loss: 3.1301 - reconstruction_loss: 1.0071 - kl_loss: 2.0114\n",
      "No Success\n",
      "Episode 37\n",
      "[-0.5229194  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 8.0470\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0393\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3747\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8859\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4696\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.5246\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0077\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5937\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6280\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2571\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9905\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1090\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7578\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3552\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8992\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4814\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5401\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7224\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9524\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0349\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7029\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6607\n",
      "1/1 [==============================] - 0s 6ms/step - kl_loss: 2.0959\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8932\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9847\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6377\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5760\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2178\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4256\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6471\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.4280\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5112\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4234\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8540\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2389\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8421\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3533\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2881\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9356\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6346\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1270\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3771\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4535\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8571\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7445\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5741\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9431\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6061\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7452\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8284\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1017\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2288\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6413\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4687\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0319\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8545\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2217\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7279\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7319\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3581\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1046\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6078\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0267\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5480\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4450\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0912\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1430\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2631\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3447\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1411\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2040\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9195\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5146\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2131\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7989\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5768\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9121\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8178\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9326\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0970\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5557\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7438\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5556\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 454us/step - loss: 3.0929 - reconstruction_loss: 1.0755 - kl_loss: 2.0118\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 426us/step - loss: 3.0086 - reconstruction_loss: 0.9975 - kl_loss: 2.0119\n",
      "No Success\n",
      "Episode 38\n",
      "[-0.47522986  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.6734\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9580\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0646\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9098\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6682\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 4.6356\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0114\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.1308\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4143\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2323\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0439\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6964\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5576\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1639\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5205\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5302\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0172\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3287\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4036\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8242\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1500\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4731\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7623\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5828\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9980\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2787\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5520\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2674\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0850\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2059\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7831\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0915\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1460\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8187\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2176\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7498\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4830\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8940\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5454\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4883\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3612\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3524\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7593\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8071\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1057\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5156\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7356\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6656\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8694\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1947\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5790\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4301\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0640\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3101\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0181\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3224\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2121\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8524\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7143\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4633\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9126\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.3603\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4005\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0936\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6691\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2348\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0161\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.7231\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1920\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9228\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0983\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0181\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3192\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8355\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7767\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8464\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5287\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0625\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6421\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2949\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1922\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5853\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2870\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 436us/step - loss: 2.7993 - reconstruction_loss: 0.9997 - kl_loss: 1.8247\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 420us/step - loss: 2.8402 - reconstruction_loss: 1.0031 - kl_loss: 1.8144\n",
      "No Success\n",
      "Episode 39\n",
      "[-0.5156579  0.       ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 6.1262\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6315\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3452\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7971\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4857\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8638\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4626\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7557\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8871\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8366\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2710\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8127\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6887\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7829\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9763\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1737\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3655\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9114\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5081\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8082\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9067\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9732\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1941\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.6709\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0849\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8389\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4117\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8817\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9407\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9716\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6472\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7974\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5745\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7617\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7896\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4674\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1232\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1737\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.5360\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6273\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8379\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1527\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5467\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8591\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0996\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5181\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5486\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5644\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4873\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8463\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2318\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1785\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.8258\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1473\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4961\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3709\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.0387\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2030\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6442\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2632\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3193\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9813\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0285\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3809\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8556\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2801\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1850\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8662\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0185\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6908\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2416\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1742\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1199\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3381\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0305\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.4565\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.0251\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9715\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7673\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2373\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.9062\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5173\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6443\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 450us/step - loss: 3.2144 - reconstruction_loss: 1.0534 - kl_loss: 2.1453\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 503us/step - loss: 3.2312 - reconstruction_loss: 1.0536 - kl_loss: 2.1679\n",
      "No Success\n",
      "Episode 40\n",
      "[-0.41738033  0.        ]\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4312\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7714\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7467\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2547\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3945\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5780\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4164\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.9551\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8266\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2302\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5857\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1020\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 3.7283\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7038\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.3083\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6149\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2914\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.2629\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0875\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4849\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3219\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3471\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7364\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0337\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0057\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2270\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2215\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0566\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6637\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7326\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8802\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3061\n",
      "1/1 [==============================] - 0s 3ms/step - kl_loss: 0.2688\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0223\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4903\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2142\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.0620\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4893\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3279\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0024\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0505\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 2.0604\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4171\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8936\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4506\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5929\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1059\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3914\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5960\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9801\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.7306\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.1525\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.5215\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.9340\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1387\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4391\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8592\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.8301\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2849\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.6982\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4502\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1502\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5759\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5129\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3881\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8957\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.9566\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1417\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3416\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3378\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.5554\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6278\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.4215\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4371\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.7027\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.3976\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.2720\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.4830\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 1.8208\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.3003\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.2127\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.6463\n",
      "1/1 [==============================] - 0s 2ms/step - kl_loss: 0.1363\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 0s 447us/step - loss: 2.1513 - reconstruction_loss: 0.8433 - kl_loss: 1.2522\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 475us/step - loss: 1.9351 - reconstruction_loss: 0.9103 - kl_loss: 0.9596\n",
      "No Success\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=40, action_repeats=6, num_actions_to_execute=2, train_on_full_data=True, show_replay_training=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12651, 2) (12651, 1)\n"
     ]
    }
   ],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 500\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "actions = []\n",
    "\n",
    "pre_obs = []\n",
    "post_obs = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "    actions.append(a)\n",
    "\n",
    "    pre_obs.append(o[:-1])\n",
    "    post_obs.append(o[1:])\n",
    "\n",
    "\n",
    "    observations.append(o)\n",
    "    # ob_seqs.append(train)\n",
    "    # next_obs.append(test)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "actions = np.vstack(actions)\n",
    "\n",
    "pre_obs = np.vstack(pre_obs)\n",
    "post_obs = np.vstack(post_obs)\n",
    "\n",
    "\n",
    "print(pre_obs.shape, actions.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.25044785,  0.03038468],\n       [-0.19236082,  0.06095558],\n       [-0.24001593,  0.03986869],\n       ...,\n       [-0.42658775,  0.59245269],\n       [-0.4112652 ,  0.65043436],\n       [-0.35537884,  0.62402917]])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(12671, 2), dtype=float32, numpy=\narray([[-0.26148793,  0.03625497],\n       [-0.14099051,  0.08441392],\n       [-0.21544485,  0.0564904 ],\n       ...,\n       [-0.36538196,  0.6092098 ],\n       [-0.386122  ,  0.5987153 ],\n       [-0.32891673,  0.5738698 ]], dtype=float32)>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = agent.model_vae(observations)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(12651, 2), dtype=float32, numpy=\narray([[-0.18604551,  0.03065864],\n       [-0.05877355, -0.03745528],\n       [-0.13142759,  0.01794346],\n       ...,\n       [-0.7772355 , -0.49976996],\n       [-0.585173  , -0.5954007 ],\n       [-0.6535084 , -0.52987987]], dtype=float32)>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, s, z_pre = agent.model_vae.encoder(pre_obs)\n",
    "m, s, z_post = agent.model_vae.encoder(post_obs)\n",
    "z_pre"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2 and the array at index 1 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [47]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m z_plus_a \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactions\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m z_plus_a\n",
      "File \u001B[0;32m<__array_function__ internals>:5\u001B[0m, in \u001B[0;36mconcatenate\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2 and the array at index 1 has size 1"
     ]
    }
   ],
   "source": [
    "z_plus_a = np.concatenate([z, actions], axis=0)\n",
    "z_plus_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"transition_gru_5\" (type TransitionGRU).\n\nInput 1 of layer \"transition\" is incompatible with the layer: expected shape=(None, 20), found shape=(10371, 2)\n\nCall arguments received:\n  ‚Ä¢ inputs=('tf.Tensor(shape=(10371, 2), dtype=float32)', 'tf.Tensor(shape=(10371, 2), dtype=float32)')\n  ‚Ä¢ training=None\n  ‚Ä¢ mask=None",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [38]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtran\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf_daif/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/Documents/Masters/Thesis/DAIF_Agents/transition_gru.py:50\u001B[0m, in \u001B[0;36mTransitionGRU.call\u001B[0;34m(self, inputs, training, mask)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m initial_state \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     49\u001B[0m     initial_state \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_units))  \u001B[38;5;66;03m# start as zeros with number of examples times hidden dimension\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransition_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: Exception encountered when calling layer \"transition_gru_5\" (type TransitionGRU).\n\nInput 1 of layer \"transition\" is incompatible with the layer: expected shape=(None, 20), found shape=(10371, 2)\n\nCall arguments received:\n  ‚Ä¢ inputs=('tf.Tensor(shape=(10371, 2), dtype=float32)', 'tf.Tensor(shape=(10371, 2), dtype=float32)')\n  ‚Ä¢ training=None\n  ‚Ä¢ mask=None"
     ]
    }
   ],
   "source": [
    "agent.tran((z, np.zeros_like(z)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test without the replay training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.83333333, 0.        ])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2,  [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 20, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=5,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.5452394  0.       ]\n",
      "WARNING:tensorflow:From /Users/Ethan/miniconda3/envs/tf_daif_car_race/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 14:32:26.876947: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80860598 0.09458183]\n",
      "tf.Tensor([ 0.9830013   0.7846042   0.3220308  -0.01687961  0.16555098], shape=(5,), dtype=float32)\n",
      "Success in episode 1 at time step 369\n",
      "Episode 2\n",
      "[-0.57064813  0.        ]\n",
      "[0.47878008 0.40288668]\n",
      "tf.Tensor([ 0.957456    0.79769796  0.22657327 -0.27285808 -0.09558801], shape=(5,), dtype=float32)\n",
      "Success in episode 2 at time step 106\n",
      "Episode 3\n",
      "[-0.4439096  0.       ]\n",
      "[0.55124285 0.51222228]\n",
      "tf.Tensor([ 0.9207449  -0.5341525  -0.88466763 -0.8933735  -0.15289223], shape=(5,), dtype=float32)\n",
      "Success in episode 3 at time step 92\n",
      "Episode 4\n",
      "[-0.55050933  0.        ]\n",
      "No Success\n",
      "Episode 5\n",
      "[-0.53231597  0.        ]\n",
      "[0.7296093  0.15947908]\n",
      "tf.Tensor([ 0.9820634   0.93507713  0.9540813   0.42379785 -0.84602374], shape=(5,), dtype=float32)\n",
      "Success in episode 5 at time step 125\n",
      "Episode 6\n",
      "[-0.4112631  0.       ]\n",
      "No Success\n",
      "Episode 7\n",
      "[-0.53678113  0.        ]\n",
      "No Success\n",
      "Episode 8\n",
      "[-0.42946008  0.        ]\n",
      "[0.74362838 0.34122923]\n",
      "tf.Tensor([0.7967912  0.9772121  0.9674017  0.93885607 0.19883062], shape=(5,), dtype=float32)\n",
      "Success in episode 8 at time step 758\n",
      "Episode 9\n",
      "[-0.5263604  0.       ]\n",
      "[0.84034789 0.53523016]\n",
      "tf.Tensor([ 0.9681933   0.97307837  0.94899935  0.8507929  -0.10709003], shape=(5,), dtype=float32)\n",
      "Success in episode 9 at time step 122\n",
      "Episode 10\n",
      "[-0.5852645  0.       ]\n",
      "[0.69803108 0.45851984]\n",
      "tf.Tensor([ 0.93200725  0.07335937  0.35393855 -0.43221903 -0.99461627], shape=(5,), dtype=float32)\n",
      "Success in episode 10 at time step 438\n",
      "Episode 11\n",
      "[-0.42456153  0.        ]\n",
      "[0.90410014 0.03039351]\n",
      "tf.Tensor([0.955181   0.9668999  0.94788754 0.9243209  0.03093042], shape=(5,), dtype=float32)\n",
      "Success in episode 11 at time step 196\n",
      "Episode 12\n",
      "[-0.5621345  0.       ]\n",
      "[0.60916363 0.64281688]\n",
      "tf.Tensor([0.9673787  0.9515966  0.9198448  0.83164215 0.37397212], shape=(5,), dtype=float32)\n",
      "Success in episode 12 at time step 353\n",
      "Episode 13\n",
      "[-0.44907644  0.        ]\n",
      "[0.6238851  0.56028498]\n",
      "tf.Tensor([0.962473   0.96590596 0.91697454 0.80474585 0.36682007], shape=(5,), dtype=float32)\n",
      "Success in episode 13 at time step 245\n",
      "Episode 14\n",
      "[-0.5084155  0.       ]\n",
      "[0.52400454 0.49420985]\n",
      "tf.Tensor([0.96537584 0.95639193 0.8497883  0.8272771  0.53330594], shape=(5,), dtype=float32)\n",
      "Success in episode 14 at time step 139\n",
      "Episode 15\n",
      "[-0.4963757  0.       ]\n",
      "[0.5967929  0.78715146]\n",
      "tf.Tensor([0.9737183  0.96602696 0.92889744 0.8075658  0.27599594], shape=(5,), dtype=float32)\n",
      "Success in episode 15 at time step 472\n",
      "Episode 16\n",
      "[-0.41137737  0.        ]\n",
      "[0.64697548 0.55997122]\n",
      "tf.Tensor([ 0.7319554   0.5848996   0.45162955 -0.20630382 -0.1557222 ], shape=(5,), dtype=float32)\n",
      "Success in episode 16 at time step 114\n",
      "Episode 17\n",
      "[-0.47954446  0.        ]\n",
      "[0.66216164 0.5763151 ]\n",
      "tf.Tensor([0.9872321 0.9339481 0.8323261 0.7743125 0.7955457], shape=(5,), dtype=float32)\n",
      "Success in episode 17 at time step 126\n",
      "Episode 18\n",
      "[-0.54798496  0.        ]\n",
      "[0.80955268 0.84861332]\n",
      "tf.Tensor([0.9511565  0.91198766 0.914686   0.92453223 0.4987105 ], shape=(5,), dtype=float32)\n",
      "Success in episode 18 at time step 109\n",
      "Episode 19\n",
      "[-0.41204917  0.        ]\n",
      "[0.76391228 0.86990232]\n",
      "tf.Tensor([0.9856899  0.91792905 0.67700845 0.79191905 0.81273013], shape=(5,), dtype=float32)\n",
      "Success in episode 19 at time step 122\n",
      "Episode 20\n",
      "[-0.5644128  0.       ]\n",
      "[0.67947636 0.68230617]\n",
      "tf.Tensor([ 0.9875462   0.9684505   0.82187605  0.0949681  -0.51842016], shape=(5,), dtype=float32)\n",
      "Success in episode 20 at time step 135\n",
      "Episode 21\n",
      "[-0.5304834  0.       ]\n",
      "[0.0453475 0.9325196]\n",
      "tf.Tensor([ 0.99650794  0.9938319   0.9598182   0.10676017 -0.1436152 ], shape=(5,), dtype=float32)\n",
      "Success in episode 21 at time step 120\n",
      "Episode 22\n",
      "[-0.58897793  0.        ]\n",
      "[0.32071005 0.86789848]\n",
      "tf.Tensor([ 0.96506894  0.97593784  0.9567569   0.37466732 -0.29258507], shape=(5,), dtype=float32)\n",
      "Success in episode 22 at time step 105\n",
      "Episode 23\n",
      "[-0.4831836  0.       ]\n",
      "[0.55355523 0.84974087]\n",
      "tf.Tensor([ 0.8452918  -0.92526656 -0.96639925 -0.9039103  -0.2935575 ], shape=(5,), dtype=float32)\n",
      "Success in episode 23 at time step 90\n",
      "Episode 24\n",
      "[-0.4490086  0.       ]\n",
      "[0.15431216 0.96833963]\n",
      "tf.Tensor([ 0.9827694   0.66276896 -0.89383835 -0.9488957  -0.9110807 ], shape=(5,), dtype=float32)\n",
      "Success in episode 24 at time step 94\n",
      "Episode 25\n",
      "[-0.51117045  0.        ]\n",
      "[0.42419586 0.93457236]\n",
      "tf.Tensor([-0.8502137  -0.9579167  -0.9561652  -0.8854989  -0.45486552], shape=(5,), dtype=float32)\n",
      "Success in episode 25 at time step 105\n",
      "Episode 26\n",
      "[-0.47662842  0.        ]\n",
      "[0.23855552 0.93985884]\n",
      "tf.Tensor([ 0.0436843  -0.96799046 -0.9648949  -0.92606574 -0.81029797], shape=(5,), dtype=float32)\n",
      "Success in episode 26 at time step 96\n",
      "Episode 27\n",
      "[-0.5792641  0.       ]\n",
      "[0.39285086 0.89285321]\n",
      "tf.Tensor([-0.8838425  -0.9465538  -0.9562758  -0.8997264  -0.53602725], shape=(5,), dtype=float32)\n",
      "Success in episode 27 at time step 117\n",
      "Episode 28\n",
      "[-0.4025429  0.       ]\n",
      "[0.16296143 0.88732952]\n",
      "tf.Tensor([ 0.96141434 -0.7195983  -0.9567334  -0.9061281  -0.70640635], shape=(5,), dtype=float32)\n",
      "Success in episode 28 at time step 84\n",
      "Episode 29\n",
      "[-0.45679152  0.        ]\n",
      "[0.75470664 0.31699938]\n",
      "tf.Tensor([-0.97266483 -0.9755431  -0.9096328  -0.63750523 -0.53149885], shape=(5,), dtype=float32)\n",
      "Success in episode 29 at time step 89\n",
      "Episode 30\n",
      "[-0.5462031  0.       ]\n",
      "[0.34965257 0.95319361]\n",
      "tf.Tensor([ 0.30918306 -0.9515482  -0.9745215  -0.934501   -0.7999848 ], shape=(5,), dtype=float32)\n",
      "Success in episode 30 at time step 140\n",
      "Episode 31\n",
      "[-0.41845554  0.        ]\n",
      "[0.42680894 0.5955394 ]\n",
      "tf.Tensor([ 0.0122298  -0.97020054 -0.97598225 -0.92446953 -0.71551466], shape=(5,), dtype=float32)\n",
      "Success in episode 31 at time step 273\n",
      "Episode 32\n",
      "[-0.41770574  0.        ]\n",
      "[0.77922881 0.63037632]\n",
      "tf.Tensor([-0.9758603  -0.988826   -0.92702734  0.09244861  0.27263066], shape=(5,), dtype=float32)\n",
      "Success in episode 32 at time step 253\n",
      "Episode 33\n",
      "[-0.463659  0.      ]\n",
      "[0.62340402 0.5435404 ]\n",
      "tf.Tensor([-0.9704191  -0.9395351  -0.89592385 -0.80561024 -0.7685855 ], shape=(5,), dtype=float32)\n",
      "Success in episode 33 at time step 283\n",
      "Episode 34\n",
      "[-0.47814003  0.        ]\n",
      "[0.79361985 0.38159733]\n",
      "tf.Tensor([-0.9939903  -0.8851764   0.33940306 -0.06241485 -0.8044955 ], shape=(5,), dtype=float32)\n",
      "Success in episode 34 at time step 219\n",
      "Episode 35\n",
      "[-0.5698738  0.       ]\n",
      "[0.80840458 0.25253609]\n",
      "tf.Tensor([-0.65526855 -0.10981813  0.9148883  -0.66293246 -0.93880355], shape=(5,), dtype=float32)\n",
      "Success in episode 35 at time step 436\n",
      "Episode 36\n",
      "[-0.4417323  0.       ]\n",
      "No Success\n",
      "Episode 37\n",
      "[-0.5901701  0.       ]\n",
      "[0.73845693 0.1457434 ]\n",
      "tf.Tensor([0.50353736 0.82554203 0.9732157  0.9508254  0.50749767], shape=(5,), dtype=float32)\n",
      "Success in episode 37 at time step 238\n",
      "Episode 38\n",
      "[-0.45247665  0.        ]\n",
      "[0.7372389  0.34390254]\n",
      "tf.Tensor([-0.30627385 -0.20624106  0.9204775   0.93034166 -0.22478874], shape=(5,), dtype=float32)\n",
      "Success in episode 38 at time step 471\n",
      "Episode 39\n",
      "[-0.41200373  0.        ]\n",
      "[0.70348916 0.06563696]\n",
      "tf.Tensor([ 0.9063062   0.90392214  0.8661124  -0.79284    -0.9465599 ], shape=(5,), dtype=float32)\n",
      "Success in episode 39 at time step 576\n",
      "Episode 40\n",
      "[-0.57113284  0.        ]\n",
      "[0.74960369 0.26518259]\n",
      "tf.Tensor([ 0.8939229  -0.29449376  0.42528403 -0.7624439  -0.9759877 ], shape=(5,), dtype=float32)\n",
      "Success in episode 40 at time step 229\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=40, action_repeats=6, num_actions_to_execute=2, train_on_full_data=False, show_replay_training=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'observations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mobservations\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'observations' is not defined"
     ]
    }
   ],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'observations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m res \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mmodel_vae(\u001B[43mobservations\u001B[49m)\n\u001B[1;32m      2\u001B[0m res\n",
      "\u001B[0;31mNameError\u001B[0m: name 'observations' is not defined"
     ]
    }
   ],
   "source": [
    "res = agent.model_vae(observations)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with the prior model FEEF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, 2, [0, 0], [0.3, 0.3], llik_scaling=1, recon_stddev=0.05)\n",
    "\n",
    "pl_hoz = 5\n",
    "latent_dim = 2\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 2*latent_dim*pl_hoz, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(2)\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "# observation_noise_stddev = [0, 0]\n",
    "observation_noise_stddev = [0.05, 0.05]\n",
    "\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           None,\n",
    "                           train_prior_model=True,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=True,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[-0.5484944  0.       ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 599us/step - loss: 58.4202 - reconstruction_loss: 45.0543 - kl_loss: 6.8987\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 627us/step - loss: 36.1541 - reconstruction_loss: 31.3868 - kl_loss: 5.3182\n",
      "No Success\n",
      "Episode 2\n",
      "[-0.518806  0.      ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 514us/step - loss: 23.7571 - reconstruction_loss: 19.5673 - kl_loss: 3.8580\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 470us/step - loss: 22.4032 - reconstruction_loss: 18.4973 - kl_loss: 2.9210\n",
      "No Success\n",
      "Episode 3\n",
      "[-0.48708177  0.        ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 488us/step - loss: 44.9584 - reconstruction_loss: 41.6020 - kl_loss: 2.8819\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 453us/step - loss: 37.5611 - reconstruction_loss: 33.4581 - kl_loss: 3.1555\n",
      "No Success\n",
      "Episode 4\n",
      "[-0.59569204  0.        ]\n",
      "[0.43393912 0.60661473]\n",
      "tf.Tensor([ 0.53942364 -0.80796605 -0.9376626  -0.9712407  -0.96946347], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "19/19 [==============================] - 0s 583us/step - loss: 28.1456 - reconstruction_loss: 25.0616 - kl_loss: 2.8645\n",
      "Epoch 2/2\n",
      "19/19 [==============================] - 0s 588us/step - loss: 23.4992 - reconstruction_loss: 20.2202 - kl_loss: 3.0532\n",
      "Success in episode 4 at time step 610\n",
      "Episode 5\n",
      "[-0.52210057  0.        ]\n",
      "[0.43684663 0.46498674]\n",
      "tf.Tensor([ 0.92546123  0.68108666 -0.6756315  -0.9598786  -0.95649064], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 0s 475us/step - loss: 24.0181 - reconstruction_loss: 15.6585 - kl_loss: 6.4340\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 0s 469us/step - loss: 17.0986 - reconstruction_loss: 9.3317 - kl_loss: 7.0323\n",
      "Success in episode 5 at time step 639\n",
      "Episode 6\n",
      "[-0.5314198  0.       ]\n",
      "[-0.64987119  0.68371874]\n",
      "tf.Tensor([ 0.90175134  0.9434608  -0.32570428 -0.9429536  -0.9643723 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 753us/step - loss: 8.7762 - reconstruction_loss: 5.5566 - kl_loss: 3.6357\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 686us/step - loss: 8.4378 - reconstruction_loss: 4.6241 - kl_loss: 3.6613\n",
      "Success in episode 6 at time step 179\n",
      "Episode 7\n",
      "[-0.4605715  0.       ]\n",
      "[-0.00353998  0.9392916 ]\n",
      "tf.Tensor([ 0.68352056 -0.5560999  -0.9425656  -0.9672263  -0.95395947], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 13.1592 - reconstruction_loss: 5.6714 - kl_loss: 7.1696\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 792us/step - loss: 12.0877 - reconstruction_loss: 4.8470 - kl_loss: 7.1424\n",
      "Success in episode 7 at time step 105\n",
      "Episode 8\n",
      "[-0.5544191  0.       ]\n",
      "[0.07817301 0.7160106 ]\n",
      "tf.Tensor([ 0.5129768  -0.8201981  -0.95326155 -0.9582234  -0.9605208 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - 0s 523us/step - loss: 20.3771 - reconstruction_loss: 9.2486 - kl_loss: 11.3828\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 0s 499us/step - loss: 19.7152 - reconstruction_loss: 8.1764 - kl_loss: 11.3159\n",
      "Success in episode 8 at time step 377\n",
      "Episode 9\n",
      "[-0.45428526  0.        ]\n",
      "[-0.71513816  0.66045801]\n",
      "tf.Tensor([-0.310609    0.9450641   0.69586194 -0.9487002  -0.9586462 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 0s 477us/step - loss: 18.8558 - reconstruction_loss: 7.1300 - kl_loss: 11.9460\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 0s 460us/step - loss: 17.5985 - reconstruction_loss: 5.9947 - kl_loss: 10.9378\n",
      "Success in episode 9 at time step 660\n",
      "Episode 10\n",
      "[-0.5995278  0.       ]\n",
      "[0.6379365  0.49749467]\n",
      "tf.Tensor([-0.95766777  0.3941255  -0.37302512 -0.70233625 -0.48940507], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "11/11 [==============================] - 0s 508us/step - loss: 12.0112 - reconstruction_loss: 4.9172 - kl_loss: 6.8543\n",
      "Epoch 2/2\n",
      "11/11 [==============================] - 0s 509us/step - loss: 10.5292 - reconstruction_loss: 3.8701 - kl_loss: 6.7893\n",
      "Success in episode 10 at time step 347\n",
      "Episode 11\n",
      "[-0.55990255  0.        ]\n",
      "[-0.60410505  0.60038473]\n",
      "tf.Tensor([ 0.4334197   0.89764506  0.9182931  -0.89183265 -0.94067025], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "15/15 [==============================] - 0s 611us/step - loss: 6.4572 - reconstruction_loss: 3.3261 - kl_loss: 3.0807\n",
      "Epoch 2/2\n",
      "15/15 [==============================] - 0s 581us/step - loss: 6.1785 - reconstruction_loss: 3.2316 - kl_loss: 3.1090\n",
      "Success in episode 11 at time step 476\n",
      "Episode 12\n",
      "[-0.5676446  0.       ]\n",
      "[0.78900151 0.01502983]\n",
      "tf.Tensor([ 0.9525033  -0.5266449  -0.90737504 -0.9485466  -0.9685088 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "4/4 [==============================] - 0s 756us/step - loss: 7.7062 - reconstruction_loss: 3.8728 - kl_loss: 4.3860\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 0s 908us/step - loss: 7.3972 - reconstruction_loss: 3.1801 - kl_loss: 4.4344\n",
      "Success in episode 12 at time step 125\n",
      "Episode 13\n",
      "[-0.4126645  0.       ]\n",
      "[0.6035577  0.39746463]\n",
      "tf.Tensor([ 0.94505864  0.8305527  -0.84912413 -0.9223308  -0.91720754], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 781us/step - loss: 8.1907 - reconstruction_loss: 2.4208 - kl_loss: 5.7350\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.4131 - reconstruction_loss: 2.0390 - kl_loss: 5.7346\n",
      "Success in episode 13 at time step 101\n",
      "Episode 14\n",
      "[-0.57494265  0.        ]\n",
      "[0.05225397 0.75246499]\n",
      "tf.Tensor([ 0.9236813   0.88682705 -0.8156639  -0.9238964  -0.8818617 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 896us/step - loss: 10.7618 - reconstruction_loss: 3.4984 - kl_loss: 6.9304\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 692us/step - loss: 10.3217 - reconstruction_loss: 3.4507 - kl_loss: 6.9022\n",
      "Success in episode 14 at time step 199\n",
      "Episode 15\n",
      "[-0.4938598  0.       ]\n",
      "[-0.53897305  0.59568419]\n",
      "tf.Tensor([ 0.6923169  0.9413038  0.9228984 -0.9531834 -0.9176845], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "11/11 [==============================] - 0s 750us/step - loss: 11.6503 - reconstruction_loss: 3.0920 - kl_loss: 8.6630\n",
      "Epoch 2/2\n",
      "11/11 [==============================] - 0s 661us/step - loss: 12.0253 - reconstruction_loss: 2.8702 - kl_loss: 8.5575\n",
      "Success in episode 15 at time step 359\n",
      "Episode 16\n",
      "[-0.58367604  0.        ]\n",
      "[-0.22943367  0.84849797]\n",
      "tf.Tensor([ 0.887139    0.9392145  -0.5319459  -0.94324166 -0.92820805], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 670us/step - loss: 8.8991 - reconstruction_loss: 2.4107 - kl_loss: 6.2945\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 659us/step - loss: 8.4455 - reconstruction_loss: 2.4163 - kl_loss: 6.2123\n",
      "Success in episode 16 at time step 200\n",
      "Episode 17\n",
      "[-0.4923572  0.       ]\n",
      "[0.48803502 0.54621775]\n",
      "tf.Tensor([ 0.95160884 -0.59898406 -0.9622428  -0.9601188  -0.7568512 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 794us/step - loss: 8.7988 - reconstruction_loss: 2.3113 - kl_loss: 6.6859\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 741us/step - loss: 9.3523 - reconstruction_loss: 2.6106 - kl_loss: 6.6324\n",
      "Success in episode 17 at time step 166\n",
      "Episode 18\n",
      "[-0.4676559  0.       ]\n",
      "[0.55960618 0.32915288]\n",
      "tf.Tensor([ 0.9601318   0.76323575 -0.8307218  -0.9234122  -0.86427575], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 616us/step - loss: 9.1163 - reconstruction_loss: 2.5926 - kl_loss: 6.2077\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 654us/step - loss: 7.9493 - reconstruction_loss: 1.8056 - kl_loss: 6.1592\n",
      "Success in episode 18 at time step 158\n",
      "Episode 19\n",
      "[-0.55600256  0.        ]\n",
      "[0.78381184 0.43816678]\n",
      "tf.Tensor([ 0.94960546  0.93194854 -0.7137635  -0.91298646 -0.8813957 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "7/7 [==============================] - 0s 669us/step - loss: 7.3899 - reconstruction_loss: 2.3247 - kl_loss: 5.0661\n",
      "Epoch 2/2\n",
      "7/7 [==============================] - 0s 774us/step - loss: 7.0945 - reconstruction_loss: 2.2097 - kl_loss: 5.0609\n",
      "Success in episode 19 at time step 214\n",
      "Episode 20\n",
      "[-0.43542972  0.        ]\n",
      "[-0.77829979  0.38725577]\n",
      "tf.Tensor([ 0.67858493  0.937512    0.96516037  0.54972255 -0.9547544 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "4/4 [==============================] - 0s 756us/step - loss: 5.8008 - reconstruction_loss: 1.5050 - kl_loss: 4.2047\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 5.9445 - reconstruction_loss: 1.8158 - kl_loss: 4.1713\n",
      "Success in episode 20 at time step 150\n",
      "Episode 21\n",
      "[-0.55849165  0.        ]\n",
      "[-0.33124466  0.86650486]\n",
      "tf.Tensor([ 0.9371447   0.94848025  0.8995507  -0.7638814  -0.8855443 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 836us/step - loss: 6.6571 - reconstruction_loss: 1.6020 - kl_loss: 5.1269\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.5744 - reconstruction_loss: 1.5278 - kl_loss: 5.1054\n",
      "Success in episode 21 at time step 109\n",
      "Episode 22\n",
      "[-0.4310394  0.       ]\n",
      "[-0.03981547  0.61139527]\n",
      "tf.Tensor([ 0.9779087  0.9653669  0.7095495 -0.6561639 -0.8852587], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 876us/step - loss: 5.3512 - reconstruction_loss: 1.3716 - kl_loss: 4.0622\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 897us/step - loss: 6.0304 - reconstruction_loss: 1.9029 - kl_loss: 4.0604\n",
      "Success in episode 22 at time step 112\n",
      "Episode 23\n",
      "[-0.5496855  0.       ]\n",
      "[-0.44668815  0.56953237]\n",
      "tf.Tensor([ 0.96393096  0.95517796  0.9484314  -0.02067748 -0.93797463], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - 0s 680us/step - loss: 9.4566 - reconstruction_loss: 2.6261 - kl_loss: 6.7383\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 0s 645us/step - loss: 8.8589 - reconstruction_loss: 2.1317 - kl_loss: 6.8827\n",
      "Success in episode 23 at time step 384\n",
      "Episode 24\n",
      "[-0.5740722  0.       ]\n",
      "[0.76662139 0.44852311]\n",
      "tf.Tensor([ 0.96741486  0.92653435 -0.6716142  -0.9289489  -0.81193256], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "13/13 [==============================] - 0s 504us/step - loss: 5.4406 - reconstruction_loss: 1.7470 - kl_loss: 3.6913\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 0s 539us/step - loss: 5.2857 - reconstruction_loss: 1.5390 - kl_loss: 3.6619\n",
      "Success in episode 24 at time step 391\n",
      "Episode 25\n",
      "[-0.47093832  0.        ]\n",
      "[0.34536269 0.62146267]\n",
      "tf.Tensor([ 0.982638   0.9438238 -0.2913136 -0.9403439 -0.8967593], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 622us/step - loss: 7.5697 - reconstruction_loss: 2.1730 - kl_loss: 5.3664\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 638us/step - loss: 6.7834 - reconstruction_loss: 1.7488 - kl_loss: 5.3519\n",
      "Success in episode 25 at time step 159\n",
      "Episode 26\n",
      "[-0.4050906  0.       ]\n",
      "[0.52380358 0.55880912]\n",
      "tf.Tensor([ 0.97054344  0.91447043 -0.82860225 -0.93091756 -0.85550815], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 695us/step - loss: 6.9088 - reconstruction_loss: 1.5761 - kl_loss: 5.1277\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 667us/step - loss: 6.9870 - reconstruction_loss: 1.7676 - kl_loss: 5.1074\n",
      "Success in episode 26 at time step 157\n",
      "Episode 27\n",
      "[-0.5262916  0.       ]\n",
      "[0.60023659 0.53565799]\n",
      "tf.Tensor([ 0.96012986 -0.0096777  -0.94669604 -0.948937   -0.85035473], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "7/7 [==============================] - 0s 828us/step - loss: 7.6614 - reconstruction_loss: 2.0994 - kl_loss: 5.5271\n",
      "Epoch 2/2\n",
      "7/7 [==============================] - 0s 672us/step - loss: 7.2678 - reconstruction_loss: 1.8238 - kl_loss: 5.5268\n",
      "Success in episode 27 at time step 215\n",
      "Episode 28\n",
      "[-0.47990215  0.        ]\n",
      "[-0.69654455  0.47187729]\n",
      "tf.Tensor([ 0.8633668   0.9329309   0.9395837  -0.7648681  -0.92263836], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "8/8 [==============================] - 0s 578us/step - loss: 5.7185 - reconstruction_loss: 1.4130 - kl_loss: 4.4546\n",
      "Epoch 2/2\n",
      "8/8 [==============================] - 0s 682us/step - loss: 5.8526 - reconstruction_loss: 1.4291 - kl_loss: 4.4411\n",
      "Success in episode 28 at time step 270\n",
      "Episode 29\n",
      "[-0.47733557  0.        ]\n",
      "[0.72197514 0.24306388]\n",
      "tf.Tensor([-0.29253593 -0.960111   -0.96267974 -0.95452476 -0.83690506], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 638us/step - loss: 8.2226 - reconstruction_loss: 1.9444 - kl_loss: 6.2108\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 622us/step - loss: 8.2735 - reconstruction_loss: 1.7529 - kl_loss: 6.1656\n",
      "Success in episode 29 at time step 184\n",
      "Episode 30\n",
      "[-0.5131528  0.       ]\n",
      "[-0.14345096  0.80473539]\n",
      "tf.Tensor([ 0.9552281   0.9496022   0.01079097 -0.94117475 -0.94041866], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "24/24 [==============================] - 0s 542us/step - loss: 3.7798 - reconstruction_loss: 1.2655 - kl_loss: 2.5184\n",
      "Epoch 2/2\n",
      "24/24 [==============================] - 0s 510us/step - loss: 3.7290 - reconstruction_loss: 1.2098 - kl_loss: 2.5490\n",
      "Success in episode 30 at time step 768\n",
      "Episode 31\n",
      "[-0.46932176  0.        ]\n",
      "[0.39546656 0.66208924]\n",
      "tf.Tensor([ 0.9668821  -0.04248648 -0.93477327 -0.94676405 -0.8885452 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 851us/step - loss: 6.7260 - reconstruction_loss: 1.5653 - kl_loss: 5.2926\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 784us/step - loss: 6.4667 - reconstruction_loss: 1.3425 - kl_loss: 5.2772\n",
      "Success in episode 31 at time step 99\n",
      "Episode 32\n",
      "[-0.5092919  0.       ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 507us/step - loss: 7.0663 - reconstruction_loss: 1.4210 - kl_loss: 5.6891\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 431us/step - loss: 6.9905 - reconstruction_loss: 1.5410 - kl_loss: 5.4485\n",
      "No Success\n",
      "Episode 33\n",
      "[-0.541249  0.      ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 600us/step - loss: 4.6203 - reconstruction_loss: 1.3204 - kl_loss: 3.2881\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 550us/step - loss: 4.4882 - reconstruction_loss: 1.1874 - kl_loss: 3.3315\n",
      "No Success\n",
      "Episode 34\n",
      "[-0.42544827  0.        ]\n",
      "[-0.0741573   0.94230029]\n",
      "tf.Tensor([ 0.73010695 -0.5771215  -0.9590358  -0.973611   -0.9504563 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "14/14 [==============================] - 0s 634us/step - loss: 7.4225 - reconstruction_loss: 1.1921 - kl_loss: 6.1865\n",
      "Epoch 2/2\n",
      "14/14 [==============================] - 0s 652us/step - loss: 7.2913 - reconstruction_loss: 1.2582 - kl_loss: 6.0974\n",
      "Success in episode 34 at time step 438\n",
      "Episode 35\n",
      "[-0.48182368  0.        ]\n",
      "[-0.54787218  0.75677811]\n",
      "tf.Tensor([ 0.76725364  0.93586665 -0.04508896 -0.95973086 -0.95905685], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 902us/step - loss: 6.1304 - reconstruction_loss: 1.3611 - kl_loss: 4.7358\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 729us/step - loss: 5.8535 - reconstruction_loss: 1.3062 - kl_loss: 4.6978\n",
      "Success in episode 35 at time step 204\n",
      "Episode 36\n",
      "[-0.45295024  0.        ]\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 530us/step - loss: 5.7639 - reconstruction_loss: 1.3331 - kl_loss: 4.5020\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 536us/step - loss: 5.7395 - reconstruction_loss: 1.2709 - kl_loss: 4.5028\n",
      "No Success\n",
      "Episode 37\n",
      "[-0.44485533  0.        ]\n",
      "[0.44641612 0.37350669]\n",
      "tf.Tensor([ 0.9651801   0.9173072  -0.9163242  -0.9164517  -0.61982554], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "7/7 [==============================] - 0s 630us/step - loss: 6.5600 - reconstruction_loss: 1.1956 - kl_loss: 5.5112\n",
      "Epoch 2/2\n",
      "7/7 [==============================] - 0s 588us/step - loss: 6.7416 - reconstruction_loss: 1.1619 - kl_loss: 5.4893\n",
      "Success in episode 37 at time step 226\n",
      "Episode 38\n",
      "[-0.57085204  0.        ]\n",
      "[0.48467547 0.29315904]\n",
      "tf.Tensor([ 0.9787136   0.9768982  -0.49663472 -0.94532764 -0.6978502 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - 0s 597us/step - loss: 6.4877 - reconstruction_loss: 1.1846 - kl_loss: 5.2208\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 0s 575us/step - loss: 6.3580 - reconstruction_loss: 1.2805 - kl_loss: 5.1756\n",
      "Success in episode 38 at time step 380\n",
      "Episode 39\n",
      "[-0.44514108  0.        ]\n",
      "[-0.76237081  0.42460001]\n",
      "tf.Tensor([ 0.89983445  0.949412    0.91550064  0.5047957  -0.9490105 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "4/4 [==============================] - 0s 757us/step - loss: 4.4097 - reconstruction_loss: 1.2832 - kl_loss: 3.1965\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 0s 775us/step - loss: 4.6610 - reconstruction_loss: 1.3046 - kl_loss: 3.2002\n",
      "Success in episode 39 at time step 149\n",
      "Episode 40\n",
      "[-0.45099804  0.        ]\n",
      "[0.49815578 0.49676182]\n",
      "tf.Tensor([ 0.9588886   0.94861776 -0.69989896 -0.9204575  -0.84000653], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5.0808 - reconstruction_loss: 0.9404 - kl_loss: 4.2115\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 832us/step - loss: 5.5306 - reconstruction_loss: 1.1334 - kl_loss: 4.2071\n",
      "Success in episode 40 at time step 99\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_single_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=40, action_repeats=6, num_actions_to_execute=5, train_on_full_data=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the models produced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ethan/python_repos/gym/gym/core.py:330: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ethan/python_repos/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": "(10740, 2)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.10825755,  0.05147636],\n       [-0.15054854, -0.03395247],\n       [-0.16731468, -0.08817429],\n       ...,\n       [ 0.83856054,  0.35450622],\n       [ 0.77111532,  0.34275715],\n       [ 0.85586775,  0.30161132]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=9.67474>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(agent.model_vae.compute_loss(observations))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Test the models produced\n",
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape\n",
    "observations\n",
    "agent.model_vae(observations)\n",
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.83333333, 0.        ])"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = create_encoder(2, 2, [20])\n",
    "dec = create_decoder(2, 2, [20])\n",
    "vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=1)\n",
    "\n",
    "pl_hoz = 5\n",
    "latent_dim = 2\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 2*pl_hoz*latent_dim, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_model = PriorModelBellman(2)\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "# without prior model\n",
    "daifa = DAIFAgentRecurrent(None,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           scaled_prior_mean,\n",
    "                           prior_stddev,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "# with prior model\n",
    "daifa = DAIFAgentRecurrent(prior_model,\n",
    "                           vae,\n",
    "                           tran,\n",
    "                           None,\n",
    "                           None,\n",
    "                           planning_horizon=pl_hoz,\n",
    "                           use_kl_extrinsic=True,\n",
    "                           use_kl_intrinsic=True,\n",
    "                           use_FEEF=False,\n",
    "                           vae_train_epochs=1,\n",
    "                           tran_train_epochs=1,\n",
    "                           show_vae_training=False)\n",
    "\n",
    "scaled_prior_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[0.32907545 0.81204165]\n",
      "tf.Tensor([0.93988144 0.9176549  0.9576125  0.92440426 0.50135976], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - 0s 647us/step - loss: 79.1247 - reconstruction_loss: 66.5922 - kl_loss: 6.1959\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 0s 544us/step - loss: 56.3993 - reconstruction_loss: 49.0732 - kl_loss: 5.4928\n",
      "Success in episode 1 at time step 389\n",
      "Episode 2\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 438us/step - loss: 70.1578 - reconstruction_loss: 65.3552 - kl_loss: 3.8945\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 442us/step - loss: 62.2310 - reconstruction_loss: 58.4824 - kl_loss: 3.3635\n",
      "No Success\n",
      "Episode 3\n",
      "[0.69670431 0.33783742]\n",
      "tf.Tensor([ 0.9314696  -0.9435271   0.9645834   0.8415556   0.56516707], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 0s 606us/step - loss: 52.3677 - reconstruction_loss: 45.6785 - kl_loss: 3.5939\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 0s 501us/step - loss: 46.2414 - reconstruction_loss: 41.3540 - kl_loss: 3.5405\n",
      "Success in episode 3 at time step 306\n",
      "Episode 4\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 490us/step - loss: 17.5091 - reconstruction_loss: 12.6776 - kl_loss: 1.7703\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 470us/step - loss: 9.0485 - reconstruction_loss: 6.6526 - kl_loss: 1.8981\n",
      "No Success\n",
      "Episode 5\n",
      "[0.49734786 0.77757351]\n",
      "tf.Tensor([ 0.98152953 -0.9801048   0.346932    0.32506007 -0.20486811], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "14/14 [==============================] - 0s 522us/step - loss: 51.1933 - reconstruction_loss: 45.8139 - kl_loss: 2.3554\n",
      "Epoch 2/2\n",
      "14/14 [==============================] - 0s 481us/step - loss: 39.7211 - reconstruction_loss: 36.3924 - kl_loss: 2.5372\n",
      "Success in episode 5 at time step 426\n",
      "Episode 6\n",
      "[-0.08916565  0.69457532]\n",
      "tf.Tensor([ 0.86979437  0.9163867   0.45106322 -0.9567182  -0.96376824], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "17/17 [==============================] - 0s 620us/step - loss: 30.2887 - reconstruction_loss: 26.7621 - kl_loss: 1.6930\n",
      "Epoch 2/2\n",
      "17/17 [==============================] - 0s 506us/step - loss: 24.3142 - reconstruction_loss: 21.4270 - kl_loss: 2.6086\n",
      "Success in episode 6 at time step 560\n",
      "Episode 7\n",
      "[0.7740761  0.43105478]\n",
      "tf.Tensor([-0.8406548   0.22958903 -0.86530244  0.16205291  0.9585254 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "8/8 [==============================] - 0s 502us/step - loss: 51.1668 - reconstruction_loss: 44.3108 - kl_loss: 4.4013\n",
      "Epoch 2/2\n",
      "8/8 [==============================] - 0s 642us/step - loss: 43.8689 - reconstruction_loss: 40.3531 - kl_loss: 4.6428\n",
      "Success in episode 7 at time step 242\n",
      "Episode 8\n",
      "[0.60109328 0.73302718]\n",
      "tf.Tensor([-0.9034789  -0.93242174 -0.9142734  -0.9228895  -0.82804745], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "30/30 [==============================] - 0s 599us/step - loss: 26.6581 - reconstruction_loss: 23.5741 - kl_loss: 3.7185\n",
      "Epoch 2/2\n",
      "30/30 [==============================] - 0s 631us/step - loss: 25.3964 - reconstruction_loss: 22.4093 - kl_loss: 3.5270\n",
      "Success in episode 8 at time step 945\n",
      "Episode 9\n",
      "[0.58582333 0.70690177]\n",
      "tf.Tensor([-0.7801693  -0.82969654 -0.92583454 -0.9501423  -0.8357265 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "14/14 [==============================] - 0s 526us/step - loss: 37.6207 - reconstruction_loss: 32.8728 - kl_loss: 5.0782\n",
      "Epoch 2/2\n",
      "14/14 [==============================] - 0s 585us/step - loss: 35.9430 - reconstruction_loss: 31.7605 - kl_loss: 4.9600\n",
      "Success in episode 9 at time step 446\n",
      "Episode 10\n",
      "Epoch 1/2\n",
      "31/31 [==============================] - 0s 468us/step - loss: 30.1776 - reconstruction_loss: 25.5544 - kl_loss: 3.9946\n",
      "Epoch 2/2\n",
      "31/31 [==============================] - 0s 503us/step - loss: 26.9137 - reconstruction_loss: 18.8948 - kl_loss: 4.4838\n",
      "No Success\n",
      "Episode 11\n",
      "[-0.13406156  0.91068   ]\n",
      "tf.Tensor([0.9825429  0.450113   0.8094051  0.96760887 0.9726515 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "15/15 [==============================] - 0s 635us/step - loss: 19.5421 - reconstruction_loss: 12.5481 - kl_loss: 6.4961\n",
      "Epoch 2/2\n",
      "15/15 [==============================] - 0s 612us/step - loss: 15.8965 - reconstruction_loss: 7.9619 - kl_loss: 6.8670\n",
      "Success in episode 11 at time step 495\n",
      "Episode 12\n",
      "[-0.26456655  0.80451077]\n",
      "tf.Tensor([0.9721597  0.9052161  0.9302739  0.83734506 0.75346595], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 580us/step - loss: 12.4143 - reconstruction_loss: 5.5323 - kl_loss: 7.1294\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 607us/step - loss: 11.0802 - reconstruction_loss: 3.7463 - kl_loss: 6.8431\n",
      "Success in episode 12 at time step 818\n",
      "Episode 13\n",
      "[0.11189345 0.84674417]\n",
      "tf.Tensor([ 0.9897314   0.5497189  -0.90938026  0.67785156  0.9646587 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 638us/step - loss: 8.1991 - reconstruction_loss: 3.2571 - kl_loss: 4.5878\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 620us/step - loss: 7.5046 - reconstruction_loss: 2.8409 - kl_loss: 4.5775\n",
      "Success in episode 13 at time step 192\n",
      "Episode 14\n",
      "[0.50627537 0.85736673]\n",
      "tf.Tensor([-0.95708704 -0.93841213 -0.939945   -0.30885416  0.9591544 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 0s 524us/step - loss: 12.8243 - reconstruction_loss: 3.9857 - kl_loss: 8.7120\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 0s 645us/step - loss: 11.9485 - reconstruction_loss: 3.3041 - kl_loss: 8.4940\n",
      "Success in episode 14 at time step 307\n",
      "Episode 15\n",
      "[0.40779439 0.87726324]\n",
      "tf.Tensor([-0.89853    -0.92443955 -0.9282573   0.4178334   0.95529646], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "16/16 [==============================] - 0s 541us/step - loss: 6.3207 - reconstruction_loss: 1.3305 - kl_loss: 5.0124\n",
      "Epoch 2/2\n",
      "16/16 [==============================] - 0s 582us/step - loss: 6.2795 - reconstruction_loss: 1.4447 - kl_loss: 4.9020\n",
      "Success in episode 15 at time step 509\n",
      "Episode 16\n",
      "[-0.12792678  0.90065215]\n",
      "tf.Tensor([ 0.97663975  0.85461074 -0.6157359  -0.80765086  0.89361745], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 784us/step - loss: 9.5037 - reconstruction_loss: 3.4068 - kl_loss: 6.1400\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 830us/step - loss: 9.5505 - reconstruction_loss: 2.9323 - kl_loss: 6.2233\n",
      "Success in episode 16 at time step 175\n",
      "Episode 17\n",
      "[0.34461867 0.76592857]\n",
      "tf.Tensor([-0.71878654 -0.7026098  -0.9614596   0.92464733  0.9633947 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 0s 636us/step - loss: 10.0144 - reconstruction_loss: 2.2789 - kl_loss: 7.3443\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 0s 611us/step - loss: 9.1990 - reconstruction_loss: 2.2846 - kl_loss: 7.2655\n",
      "Success in episode 17 at time step 195\n",
      "Episode 18\n",
      "[0.22763537 0.87905425]\n",
      "tf.Tensor([-0.50013196 -0.8199611  -0.9188756  -0.9499112   0.9782911 ], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "14/14 [==============================] - 0s 474us/step - loss: 6.2820 - reconstruction_loss: 1.0543 - kl_loss: 5.1769\n",
      "Epoch 2/2\n",
      "14/14 [==============================] - 0s 524us/step - loss: 6.0942 - reconstruction_loss: 1.0928 - kl_loss: 5.0239\n",
      "Success in episode 18 at time step 434\n",
      "Episode 19\n",
      "[0.82519335 0.04704075]\n",
      "tf.Tensor([ 0.9419391  -0.95463955 -0.9360158  -0.943113    0.44610092], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "8/8 [==============================] - 0s 587us/step - loss: 9.0316 - reconstruction_loss: 2.9396 - kl_loss: 6.2023\n",
      "Epoch 2/2\n",
      "8/8 [==============================] - 0s 541us/step - loss: 8.5861 - reconstruction_loss: 2.1607 - kl_loss: 6.2835\n",
      "Success in episode 19 at time step 242\n",
      "Episode 20\n",
      "[0.56574249 0.72304362]\n",
      "tf.Tensor([-0.9569217  -0.91001046 -0.92691606 -0.58486927  0.96289206], shape=(5,), dtype=float32)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 598us/step - loss: 6.3744 - reconstruction_loss: 1.6380 - kl_loss: 4.6993\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 674us/step - loss: 6.1429 - reconstruction_loss: 1.4956 - kl_loss: 4.6901\n",
      "Success in episode 20 at time step 147\n"
     ]
    }
   ],
   "source": [
    "# train the agent on the env\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent, results = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=20, action_repeats=10, num_actions_to_execute=2, train_on_full_data=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "(12806, 2)"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test the models produced\n",
    "num_seqs = 20\n",
    "seq_length = 300\n",
    "ob_dim = 2\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "observations = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, 1000, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, observation_noise_stddev)\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    # train = o[:-1]\n",
    "    test = o[-1]\n",
    "\n",
    "    observations.append(o)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "# ob_seqs = np.array(ob_seqs)\n",
    "# next_obs = np.array(next_obs)\n",
    "\n",
    "observations = np.vstack(observations)\n",
    "# observations = observations.reshape((num_seqs*(seq_length+1), ob_dim))\n",
    "\n",
    "# ob_seqs_stddev = np.ones_like(ob_seqs_flat)\n",
    "# next_obs_stddev = np.ones_like(ob_seqs_flat)\n",
    "\n",
    "observations.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.27734923,  0.        ],\n       [-0.27835019, -0.01286914],\n       [-0.28034457, -0.02564205],\n       ...,\n       [ 0.78566315,  0.35808382],\n       [ 0.81395161,  0.36370895],\n       [ 0.8428795 ,  0.37193014]])"
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(12806, 2), dtype=float32, numpy=\narray([[-0.26162374, -0.03878228],\n       [-0.27726045, -0.06711522],\n       [-0.24541792, -0.04369228],\n       ...,\n       [ 0.7668527 ,  0.39962938],\n       [ 0.93341327,  0.4722877 ],\n       [ 0.8112131 ,  0.41610983]], dtype=float32)>"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model_vae(observations)\n",
    "## Test EFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Identity VAE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev, num_episodes=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = agent.tran((ob_seqs[0:1], None))\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = ob_seqs[0:1, -1].reshape(1,1,3)\n",
    "h = out[3]\n",
    "h = h[0, -2, :]\n",
    "h = h.numpy().reshape(1,30)\n",
    "h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((t, h))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ob_seqs[0:1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test to see how the agent trains on standard observation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "hidden_size = 2*2*15  # 2*latent_dim * planning_size\n",
    "tran = TransitionGRU(2, 1, 12, hidden_size, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.45, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0,0])  # no noise on prior\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, vae_train_epochs=1, tran_train_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "success, agent, t, pre_obs, post_obs, acts = run_episode(env, daifa, observation_max, observation_min, observation_noise_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_np = np.array(pre_obs)\n",
    "a = np.array(acts)\n",
    "a.shape\n",
    "pre_a = np.concatenate([pre_np, a], axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(a.max(), a.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict = np.array(post_obs)[:, 14, :]\n",
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.tran((pre_a, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_obs_to_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine training the model on the observation data\n",
    "\n",
    "Does it eventually converge to a good model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_runs = 1\n",
    "for i in range(num_train_runs):\n",
    "\n",
    "    for j in range(len(pre)):\n",
    "        pre = pre_obs[j]\n",
    "        post = post_obs[j]\n",
    "        actions = acts[j]\n",
    "\n",
    "        daifa.train(pre, post, actions, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation(np.array([0.5, 0.1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daifa.cem_policy_optimisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the FEEF computations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  0.5]\n"
     ]
    }
   ],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, planning_horizon=15, n_policy_candidates=70, n_policies=1500, n_cem_policy_iterations=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "WARNING:tensorflow:From /Users/Ethan/miniconda3/envs/tf_daif/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-06 15:23:48.777636: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_policy(agent, env, policy, action_repeats):\n",
    "\n",
    "    observation = env.reset()\n",
    "    obs = transform_observations(observation, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    z_t_minus_1 = obs\n",
    "    p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "    p\n",
    "    print(obs)\n",
    "    print(p)\n",
    "\n",
    "    for action in p:\n",
    "        for t in range(action_repeats):\n",
    "            res = env.step(np.array([action]))\n",
    "            print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([0, 0])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p\n",
    "\n",
    "agent.forward_policies(p, z_t_minus_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "test_policy(agent, env, p.numpy(), 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_t_minus_1 = np.array([-0.27691475,  0.01688306])\n",
    "p, s = agent.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from vae_recurrent import VAE\n",
    "\n",
    "\n",
    "class DAIFAgentRecurrent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_model,\n",
    "                 vae,\n",
    "                 tran,\n",
    "                 given_prior_mean,\n",
    "                 given_prior_stddev,\n",
    "                 planning_horizon=15,\n",
    "                 n_policies=1500,\n",
    "                 n_cem_policy_iterations=2,\n",
    "                 n_policy_candidates=70,\n",
    "                 tran_train_epochs=1,\n",
    "                 vae_train_epochs=1,\n",
    "                 agent_time_ratio=6,\n",
    "                 train_vae=True,\n",
    "                 train_tran=True):\n",
    "\n",
    "        super(DAIFAgentRecurrent, self).__init__()\n",
    "\n",
    "        self.prior_model = prior_model\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.n_policy_candidates = n_policy_candidates\n",
    "        self.n_policies = n_policies\n",
    "        self.n_cem_policy_iterations = n_cem_policy_iterations\n",
    "\n",
    "        self.vae_train_epochs = vae_train_epochs\n",
    "        self.tran_train_epochs = tran_train_epochs\n",
    "        self.train_vae = train_vae\n",
    "        self.train_tran = train_tran\n",
    "\n",
    "        self.given_prior_mean = given_prior_mean\n",
    "        self.given_prior_stddev = given_prior_stddev\n",
    "\n",
    "        # full vae\n",
    "        self.model_vae = vae\n",
    "        self.model_vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        # transition\n",
    "        # takes action plus last state and outputs next latent state\n",
    "        self.tran = tran\n",
    "        self.tran.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "        self.hidden_state = None\n",
    "\n",
    "        # how much is the agents planning time compressed compared to the simulation time\n",
    "        self.agent_time_ratio = agent_time_ratio\n",
    "\n",
    "\n",
    "    def select_policy(self, observation):\n",
    "\n",
    "        policy_mean, policy_stddev = self.cem_policy_optimisation(observation)\n",
    "\n",
    "        # return a distribution that we can sample from\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=policy_mean, scale_diag=policy_stddev)\n",
    "\n",
    "\n",
    "    def train(self, pre_observations_raw, post_observations_raw, actions_complete, rewards, verbose=0):\n",
    "\n",
    "        # compress the observations based on the agents time compression factor\n",
    "        # pre_observations = pre_observations_raw[::self.agent_time_ratio]  # for example take every 6th element\n",
    "        # post_observations = np.array([post_observations_raw[i] for i in range(len(post_observations_raw)) if i % self.agent_time_ratio == self.agent_time_ratio - 1])\n",
    "        #\n",
    "        # print(pre_observations_raw)\n",
    "        # print(pre_observations)\n",
    "        # print(post_observations_raw)\n",
    "        # print(post_observations)\n",
    "\n",
    "        pre_observations = pre_observations_raw\n",
    "        post_observations = post_observations_raw\n",
    "\n",
    "        # only look at the first n actions that we took\n",
    "        actions = actions_complete[0: len(pre_observations)]\n",
    "\n",
    "        num_observations = pre_observations.shape[0]\n",
    "        observation_dim = pre_observations.shape[1]\n",
    "        action_dim = actions.shape[1]\n",
    "        # action_dim = 1  # TODO fix this to allow different actions\n",
    "\n",
    "        # find the actual observed latent states using the vae\n",
    "        pre_latent_mean, pre_latent_stddev, pre_latent = self.model_vae.encoder(pre_observations)\n",
    "        post_latent_mean, post_latent_stddev, post_latent = self.model_vae.encoder(post_observations)\n",
    "\n",
    "        # set up the input training data that we use to train the transition model\n",
    "        z_train = np.concatenate([np.array(pre_latent_mean), np.array(actions)], axis=1)\n",
    "\n",
    "        # we use the sequence to find the right hidden states to use as input\n",
    "        z_train_seq = z_train.reshape((1, num_observations, observation_dim + action_dim))\n",
    "        z_train_singles = z_train.reshape(num_observations, 1, observation_dim + action_dim)\n",
    "\n",
    "        # the previous hidden state is the memory after observing some sequences but it might be None\n",
    "        if self.hidden_state is None:\n",
    "            self.hidden_state = np.zeros((1, self.tran.hidden_units))\n",
    "\n",
    "        if self.train_tran:\n",
    "            # find the hidden states at t=0, t=1, t=2, ..., t=num_observations - 1\n",
    "            _, _, _, h_states = self.tran((z_train_seq, self.hidden_state))\n",
    "\n",
    "\n",
    "            # squeeze so we make the shape [num_observations, hidden_units]\n",
    "            h_states = tf.squeeze(h_states)\n",
    "\n",
    "            # exclude the last state as this will become the hidden state later on. next hidden state will become our new memory\n",
    "            h_states_for_training = h_states[:-1]\n",
    "            # next_hidden_state = h_states[-1]\n",
    "\n",
    "            # add the current hidden state we saved to the start. This has h0, h1, h2, .. h=num_observations - 1\n",
    "            h_states_for_training = tf.concat([self.hidden_state, h_states_for_training], axis=0)\n",
    "\n",
    "\n",
    "            # use the hidden states with the pre and post observations to train transition model\n",
    "            self.tran.fit((z_train_singles, h_states_for_training), (post_latent_mean, post_latent_stddev), epochs=self.tran_train_epochs, verbose=verbose)\n",
    "\n",
    "        # now find the new predicted hidden state that we will use for finding the policy\n",
    "        # TODO not sure if I should pass the old hidden state or reset it to 0\n",
    "        _, _, final_hidden_state, _ = self.tran((z_train_seq, self.hidden_state))\n",
    "        # _, _, final_hidden_state, _ = self.tran((z_train_seq, None))\n",
    "\n",
    "        self.hidden_state = final_hidden_state\n",
    "\n",
    "        #### TRAIN THE VAE ####\n",
    "        if self.train_vae:\n",
    "            # train the vae model on post_observations because these are all new\n",
    "            self.model_vae.fit(post_observations, epochs=self.vae_train_epochs, verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def cem_policy_optimisation(self, z_t_minus_one):\n",
    "\n",
    "        # need to change these two if the policy dimension changes\n",
    "        mean_best_policies = tf.zeros(self.planning_horizon)\n",
    "        std_best_policies = tf.ones(self.planning_horizon)\n",
    "\n",
    "        for i in range(self.n_cem_policy_iterations):\n",
    "            policy_distr = tfp.distributions.MultivariateNormalDiag(loc=mean_best_policies, scale_diag=std_best_policies)\n",
    "            policies = policy_distr.sample([self.n_policies])\n",
    "            policies = tf.clip_by_value(policies, clip_value_min=-1, clip_value_max=1)\n",
    "\n",
    "            # project trajectory into the future using transition model and calculate FEEF for each policy\n",
    "            policy_results = self.forward_policies(policies.numpy(), z_t_minus_one)\n",
    "            FEEFs = self.evaluate_policy(*policy_results)\n",
    "\n",
    "            FEEFs = tf.convert_to_tensor(FEEFs)\n",
    "\n",
    "            # sum over the timesteps to get the FEEF for each policy\n",
    "            FEEFs_sum = tf.reduce_sum(FEEFs, axis=0)\n",
    "\n",
    "            # multiply by one to find largest value which is euqivalent to smallest FEEF with top_k\n",
    "            neg_FEEF_sum = -1*FEEFs_sum\n",
    "\n",
    "            result = tf.math.top_k(neg_FEEF_sum, self.n_policy_candidates, sorted=False)\n",
    "            min_FEEF_indices = result.indices\n",
    "\n",
    "            # update the policy distributions\n",
    "            mean_best_policies = tf.reduce_mean(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "            std_best_policies = tf.math.reduce_std(tf.gather(policies, min_FEEF_indices), axis=0)\n",
    "\n",
    "\n",
    "        # TODO not sure why we need all of this is with the x means? I think it's for training but maybe not\n",
    "\n",
    "        # One last forward pass to gather the stats of the policy mean\n",
    "        #FEEFs, next_x_means, next_x_stds = self._forward_policies(mean_best_policies.unsqueeze(1))\n",
    "        # return mean_best_policies, std_best_policies, FEEFs.detach().squeeze(1), next_x_means.detach().squeeze(1), next_x_stds.detach().squeeze(1)\n",
    "\n",
    "        print(z_t_minus_one)\n",
    "        print(mean_best_policies)\n",
    "        return mean_best_policies, std_best_policies\n",
    "\n",
    "\n",
    "    def forward_policies(self, policies, z_t_minus_one):\n",
    "        \"\"\"\n",
    "        Forward propogate a policy and compute the FEEF of each policy\n",
    "        :param z_t_minus_one:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # stack up the new observation to have shape [self.n_policies, len(z_t_minus_one)]\n",
    "        prev_latent_mean = np.stack([z_t_minus_one]*self.n_policies)\n",
    "\n",
    "        policy_posteriors = []\n",
    "        policy_sds = []\n",
    "        likelihoods = []\n",
    "        z_means = []\n",
    "        z_sds = []\n",
    "\n",
    "        # get the starting hidden state that coressponds to the memory stored by the previous sequences. Should have shape (1, self.tran.num_hidden_units) for the observed sequence\n",
    "        # extend the current hidden state to the number of policies present\n",
    "        if self.hidden_state is None:\n",
    "            cur_hidden_state = np.zeros((self.n_policies, self.tran.hidden_units))\n",
    "        else:\n",
    "            cur_hidden_state = np.vstack([self.hidden_state]*self.n_policies)\n",
    "\n",
    "        # print(cur_hidden_state)\n",
    "\n",
    "        # find the predicted latent states from the transition model\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            ob_plus_action = np.concatenate([prev_latent_mean, policies[:, t].reshape(self.n_policies, 1)], axis=1)\n",
    "            tran_input = ob_plus_action.reshape((self.n_policies, 1, ob_plus_action.shape[1]))  # reshape to pass to GRU\n",
    "\n",
    "            next_latent_mean, next_latent_sd, next_hidden_state, _ = self.tran((tran_input, cur_hidden_state))  # shape = [num policies, latent dim\n",
    "\n",
    "            # update the hidden state for use with the next policies\n",
    "            cur_hidden_state = next_hidden_state\n",
    "\n",
    "            policy_posteriors.append(next_latent_mean)\n",
    "            policy_sds.append(next_latent_sd)\n",
    "\n",
    "            next_likelihoods = self.model_vae.decoder(next_latent_mean)\n",
    "            likelihoods.append(next_likelihoods)\n",
    "\n",
    "            next_posterior_means, next_posteriors_sds, next_posteriors_z = self.model_vae.encoder(next_likelihoods)\n",
    "            z_means.append(next_posterior_means)\n",
    "            z_sds.append(next_posteriors_sds)\n",
    "\n",
    "            prev_latent_mean = next_latent_mean\n",
    "\n",
    "        return policy_posteriors, policy_sds, likelihoods, z_means, z_sds\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd):\n",
    "\n",
    "        return self.FEEF(policy_posteriors, policy_sd, predicted_likelihood, predicted_posterior, predicted_posterior_sd)\n",
    "\n",
    "\n",
    "    def FEEF(self, policy_posteriors_list, policy_sd_list, predicted_likelihood_list, predicted_posterior_list, predicted_posterior_sd_list):\n",
    "        \"\"\"\n",
    "        Compute the FEEF for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        FEEFs = []\n",
    "\n",
    "        for t in range(self.planning_horizon):\n",
    "\n",
    "            # extract the values for each time step\n",
    "            predicted_likelihood = predicted_likelihood_list[t]\n",
    "            policy_posteriors = policy_posteriors_list[t]\n",
    "            policy_sd = policy_sd_list[t]\n",
    "            predicted_posterior = predicted_posterior_list[t]\n",
    "            predicted_posterior_sd = predicted_posterior_sd_list[t]\n",
    "\n",
    "            # !!!! evaluate the EXTRINSIC KL divergence !!!!\n",
    "\n",
    "            # convert to normal distributions\n",
    "            # TODO Why is the stddev 1s here? I think because we assume it is on the true state of the world.\n",
    "            likelihood_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_likelihood, scale_diag=np.ones_like(predicted_likelihood))\n",
    "\n",
    "            if self.prior_model is None:\n",
    "\n",
    "                # TODO how exactly is the prior defined? After you apply transformations what is the prior\n",
    "                # create the prior distribution\n",
    "                prior_preferences_mean = tf.convert_to_tensor(np.stack([self.given_prior_mean]*self.n_policies), dtype=\"float32\")\n",
    "                prior_preferences_stddev = tf.convert_to_tensor(np.stack([self.given_prior_stddev]*self.n_policies), dtype=\"float32\")\n",
    "\n",
    "                prior_dist = tfp.distributions.MultivariateNormalDiag(loc=prior_preferences_mean, scale_diag=prior_preferences_stddev)\n",
    "\n",
    "            # TODO Fix the learned prior model\n",
    "            else:\n",
    "                prior_dist = self.prior_model()\n",
    "\n",
    "            kl_extrinsic = tfp.distributions.kl_divergence(likelihood_dist, prior_dist)\n",
    "\n",
    "            # !!!! evaluate the KL INTRINSIC part !!!!\n",
    "            policy_posteriors_dist = tfp.distributions.MultivariateNormalDiag(loc=policy_posteriors, scale_diag=policy_sd)\n",
    "            predicted_posterior_dist = tfp.distributions.MultivariateNormalDiag(loc=predicted_posterior, scale_diag=predicted_posterior_sd)\n",
    "\n",
    "            kl_intrinsic = tfp.distributions.kl_divergence(predicted_posterior_dist, policy_posteriors_dist)\n",
    "\n",
    "            FEEF = kl_extrinsic - kl_intrinsic\n",
    "\n",
    "            FEEFs.append(FEEF)\n",
    "\n",
    "        return FEEFs\n",
    "\n",
    "\n",
    "    def EFE(self, policy_posteriors, predicted_likelihood, predicted_posterior):\n",
    "        \"\"\"\n",
    "        Compute the EFE for policy selection\n",
    "        :param policy_posteriors:\n",
    "        :param predicted_likelihood:\n",
    "        :param predicted_posterior:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing with a pretrained transition model\n",
    "\n",
    "This works well! So the problem can't lie with the transition model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# enc = create_encoder(2, 2, [20])\n",
    "# dec = create_decoder(2, 2, [20])\n",
    "# vae = VAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "enc = identity_encoder\n",
    "dec = identity_decoder\n",
    "idvae = IdentityVAE(enc, dec, [0, 0], [0.3, 0.3], llik_scaling=100)\n",
    "\n",
    "tran = TransitionGRU(2, 1, 12, 60, 2)\n",
    "\n",
    "# unscaled prior mean and prior stddev\n",
    "prior_mean = [0.6, 0.07]\n",
    "prior_stddev = [1, 1]\n",
    "\n",
    "observation_max = np.array([0.6, 0.07])\n",
    "observation_min = np.array([-1.2, -0.07])\n",
    "\n",
    "observation_noise_stddev = [0, 0]\n",
    "\n",
    "scaled_prior_mean = transform_observations(prior_mean, observation_max, observation_min, [0, 0])  # no noise on prior\n",
    "\n",
    "print(scaled_prior_mean)\n",
    "\n",
    "daifa = DAIFAgentRecurrent(None, idvae, tran, scaled_prior_mean, prior_stddev, train_vae=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "num_seqs = 200\n",
    "seq_length = 500\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "next_obs_stddev = []\n",
    "actions = []\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.2)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    # train = np.concatenate([o[:-1], a], axis=1)\n",
    "    train = o[:-1]\n",
    "    test = o[1:]\n",
    "\n",
    "    actions.append(a)\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "    ob_seqs_stddev = np.ones_like(train)\n",
    "    next_stddev = np.ones_like(test)\n",
    "\n",
    "    next_obs_stddev.append(next_stddev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 1ms/step - kl_loss: 0.1993\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0497\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0132\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0103\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 0.0097\n",
      "6/6 [==============================] - 0s 1ms/step - kl_loss: 0.0108\n",
      "16/16 [==============================] - 0s 967us/step - kl_loss: 0.0029\n",
      "16/16 [==============================] - 0s 940us/step - kl_loss: 0.0020\n",
      "15/15 [==============================] - 0s 882us/step - kl_loss: 0.0047\n",
      "16/16 [==============================] - 0s 853us/step - kl_loss: 0.0054\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 0.0074\n",
      "16/16 [==============================] - 0s 943us/step - kl_loss: 0.0113\n",
      "8/8 [==============================] - 0s 920us/step - kl_loss: 0.0256\n",
      "16/16 [==============================] - 0s 891us/step - kl_loss: 0.0049\n",
      "16/16 [==============================] - 0s 923us/step - kl_loss: 0.0037\n",
      "16/16 [==============================] - 0s 905us/step - kl_loss: 0.0038\n",
      "16/16 [==============================] - 0s 860us/step - kl_loss: 0.0033\n",
      "14/14 [==============================] - 0s 896us/step - kl_loss: 0.0033\n",
      "16/16 [==============================] - 0s 928us/step - kl_loss: 0.0012\n",
      "7/7 [==============================] - 0s 964us/step - kl_loss: 0.0033\n",
      "16/16 [==============================] - 0s 853us/step - kl_loss: 6.2965e-04\n",
      "16/16 [==============================] - 0s 895us/step - kl_loss: 3.8473e-04\n",
      "16/16 [==============================] - 0s 886us/step - kl_loss: 9.0204e-04\n",
      "16/16 [==============================] - 0s 866us/step - kl_loss: 3.5106e-04\n",
      "7/7 [==============================] - 0s 948us/step - kl_loss: 0.0014\n",
      "16/16 [==============================] - 0s 921us/step - kl_loss: 0.0011\n",
      "16/16 [==============================] - 0s 897us/step - kl_loss: 2.8698e-04\n",
      "16/16 [==============================] - 0s 918us/step - kl_loss: 3.7622e-04\n",
      "12/12 [==============================] - 0s 979us/step - kl_loss: 7.3435e-04\n",
      "16/16 [==============================] - 0s 872us/step - kl_loss: 6.9894e-04\n",
      "16/16 [==============================] - 0s 845us/step - kl_loss: 3.9676e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 9.3522e-04\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 5.7943e-04\n",
      "16/16 [==============================] - 0s 861us/step - kl_loss: 3.5893e-04\n",
      "16/16 [==============================] - 0s 927us/step - kl_loss: 4.9170e-04\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 2.7267e-04\n",
      "16/16 [==============================] - 0s 838us/step - kl_loss: 1.5856e-04\n",
      "6/6 [==============================] - 0s 1ms/step - kl_loss: 9.0418e-04\n",
      "16/16 [==============================] - 0s 886us/step - kl_loss: 5.7081e-04\n",
      "12/12 [==============================] - 0s 907us/step - kl_loss: 4.9615e-04\n",
      "16/16 [==============================] - 0s 878us/step - kl_loss: 6.9546e-04\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 2.8895e-04\n",
      "13/13 [==============================] - 0s 899us/step - kl_loss: 7.5522e-04\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 6.0688e-04\n",
      "11/11 [==============================] - 0s 921us/step - kl_loss: 5.8398e-04\n",
      "9/9 [==============================] - 0s 976us/step - kl_loss: 0.0012\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 6.1929e-04\n",
      "16/16 [==============================] - 0s 836us/step - kl_loss: 2.2836e-04\n",
      "10/10 [==============================] - 0s 916us/step - kl_loss: 5.1460e-04\n",
      "16/16 [==============================] - 0s 867us/step - kl_loss: 4.9099e-04\n",
      "16/16 [==============================] - 0s 858us/step - kl_loss: 3.6082e-04\n",
      "16/16 [==============================] - 0s 889us/step - kl_loss: 3.1299e-04\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 8.5156e-04\n",
      "16/16 [==============================] - 0s 921us/step - kl_loss: 0.0018\n",
      "16/16 [==============================] - 0s 852us/step - kl_loss: 0.0030\n",
      "10/10 [==============================] - 0s 915us/step - kl_loss: 0.0085\n",
      "16/16 [==============================] - 0s 897us/step - kl_loss: 0.0071\n",
      "16/16 [==============================] - 0s 917us/step - kl_loss: 0.0035\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 0.0035\n",
      "16/16 [==============================] - 0s 890us/step - kl_loss: 0.0026\n",
      "16/16 [==============================] - 0s 882us/step - kl_loss: 0.0020\n",
      "7/7 [==============================] - 0s 966us/step - kl_loss: 0.0028\n",
      "16/16 [==============================] - 0s 896us/step - kl_loss: 0.0013\n",
      "12/12 [==============================] - 0s 911us/step - kl_loss: 0.0014\n",
      "14/14 [==============================] - 0s 918us/step - kl_loss: 0.0015\n",
      "16/16 [==============================] - 0s 880us/step - kl_loss: 8.1093e-04\n",
      "16/16 [==============================] - 0s 859us/step - kl_loss: 5.0124e-04\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 2.6348e-04\n",
      "8/8 [==============================] - 0s 976us/step - kl_loss: 5.0341e-04\n",
      "10/10 [==============================] - 0s 871us/step - kl_loss: 4.9221e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 2.3029e-04\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 6.5155e-05\n",
      "10/10 [==============================] - 0s 918us/step - kl_loss: 5.9321e-04\n",
      "16/16 [==============================] - 0s 912us/step - kl_loss: 3.2684e-04\n",
      "16/16 [==============================] - 0s 874us/step - kl_loss: 1.7592e-04\n",
      "16/16 [==============================] - 0s 863us/step - kl_loss: 1.6582e-04\n",
      "16/16 [==============================] - 0s 864us/step - kl_loss: 1.8564e-04\n",
      "16/16 [==============================] - 0s 925us/step - kl_loss: 1.4003e-04\n",
      "16/16 [==============================] - 0s 915us/step - kl_loss: 1.7503e-04\n",
      "12/12 [==============================] - 0s 975us/step - kl_loss: 2.7620e-04\n",
      "16/16 [==============================] - 0s 935us/step - kl_loss: 2.7183e-04\n",
      "11/11 [==============================] - 0s 942us/step - kl_loss: 4.2113e-04\n",
      "16/16 [==============================] - 0s 867us/step - kl_loss: 4.4761e-04\n",
      "12/12 [==============================] - 0s 916us/step - kl_loss: 2.7003e-04\n",
      "13/13 [==============================] - 0s 886us/step - kl_loss: 5.0503e-04\n",
      "12/12 [==============================] - 0s 891us/step - kl_loss: 4.9012e-04\n",
      "16/16 [==============================] - 0s 888us/step - kl_loss: 2.4893e-04\n",
      "16/16 [==============================] - 0s 1ms/step - kl_loss: 1.1987e-04\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 2.8243e-04\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 1.1809e-04\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 7.0822e-05\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 6.8286e-05\n",
      "16/16 [==============================] - 0s 900us/step - kl_loss: 2.7148e-04\n",
      "15/15 [==============================] - 0s 960us/step - kl_loss: 1.1328e-04\n",
      "16/16 [==============================] - 0s 859us/step - kl_loss: 1.6294e-04\n",
      "16/16 [==============================] - 0s 899us/step - kl_loss: 9.3267e-05\n",
      "13/13 [==============================] - 0s 930us/step - kl_loss: 2.7863e-04\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 3.4847e-04\n",
      "16/16 [==============================] - 0s 909us/step - kl_loss: 6.9290e-05\n",
      "16/16 [==============================] - 0s 902us/step - kl_loss: 2.5452e-04\n",
      "16/16 [==============================] - 0s 904us/step - kl_loss: 2.0722e-04\n",
      "16/16 [==============================] - 0s 903us/step - kl_loss: 1.6057e-04\n",
      "16/16 [==============================] - 0s 996us/step - kl_loss: 1.6325e-04\n",
      "10/10 [==============================] - 0s 981us/step - kl_loss: 2.9550e-04\n",
      "16/16 [==============================] - 0s 905us/step - kl_loss: 2.2208e-04\n",
      "16/16 [==============================] - 0s 880us/step - kl_loss: 1.4792e-04\n",
      "14/14 [==============================] - 0s 876us/step - kl_loss: 2.6079e-04\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 2.2629e-04\n",
      "16/16 [==============================] - 0s 927us/step - kl_loss: 1.7176e-04\n",
      "16/16 [==============================] - 0s 863us/step - kl_loss: 7.7746e-05\n",
      "16/16 [==============================] - 0s 907us/step - kl_loss: 4.9998e-05\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 9.7333e-05\n",
      "16/16 [==============================] - 0s 875us/step - kl_loss: 9.3879e-06\n",
      "16/16 [==============================] - 0s 871us/step - kl_loss: 3.5369e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 6.1430e-05\n",
      "16/16 [==============================] - 0s 913us/step - kl_loss: 3.4472e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 4.7960e-05\n",
      "11/11 [==============================] - 0s 988us/step - kl_loss: 1.9969e-04\n",
      "16/16 [==============================] - 0s 886us/step - kl_loss: 1.3500e-04\n",
      "9/9 [==============================] - 0s 959us/step - kl_loss: 1.3400e-04\n",
      "15/15 [==============================] - 0s 927us/step - kl_loss: 1.8833e-04\n",
      "16/16 [==============================] - 0s 889us/step - kl_loss: 1.6533e-04\n",
      "16/16 [==============================] - 0s 855us/step - kl_loss: 4.5978e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 6.0070e-05\n",
      "16/16 [==============================] - 0s 878us/step - kl_loss: 1.2162e-04\n",
      "16/16 [==============================] - 0s 942us/step - kl_loss: 9.9278e-05\n",
      "16/16 [==============================] - 0s 919us/step - kl_loss: 6.8983e-05\n",
      "16/16 [==============================] - 0s 935us/step - kl_loss: 3.9110e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 3.2605e-05\n",
      "16/16 [==============================] - 0s 908us/step - kl_loss: 5.6698e-05\n",
      "16/16 [==============================] - 0s 912us/step - kl_loss: 4.7928e-05\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 1.7173e-04\n",
      "10/10 [==============================] - 0s 866us/step - kl_loss: 1.5600e-04\n",
      "16/16 [==============================] - 0s 848us/step - kl_loss: 1.7683e-04\n",
      "10/10 [==============================] - 0s 940us/step - kl_loss: 1.8164e-04\n",
      "10/10 [==============================] - 0s 920us/step - kl_loss: 2.5693e-04\n",
      "16/16 [==============================] - 0s 887us/step - kl_loss: 1.4039e-04\n",
      "16/16 [==============================] - 0s 869us/step - kl_loss: 3.1678e-04\n",
      "16/16 [==============================] - 0s 877us/step - kl_loss: 2.0544e-04\n",
      "16/16 [==============================] - 0s 910us/step - kl_loss: 1.3164e-04\n",
      "16/16 [==============================] - 0s 920us/step - kl_loss: 3.7877e-05\n",
      "16/16 [==============================] - 0s 881us/step - kl_loss: 1.7913e-05\n",
      "16/16 [==============================] - 0s 896us/step - kl_loss: 8.9225e-06\n",
      "16/16 [==============================] - 0s 894us/step - kl_loss: 2.8919e-05\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 8.2512e-05\n",
      "16/16 [==============================] - 0s 852us/step - kl_loss: 6.1667e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 6.0950e-05\n",
      "16/16 [==============================] - 0s 906us/step - kl_loss: 4.6845e-05\n",
      "16/16 [==============================] - 0s 872us/step - kl_loss: 1.2996e-04\n",
      "16/16 [==============================] - 0s 934us/step - kl_loss: 8.1739e-05\n",
      "11/11 [==============================] - 0s 936us/step - kl_loss: 1.4202e-04\n",
      "16/16 [==============================] - 0s 919us/step - kl_loss: 1.3208e-04\n",
      "16/16 [==============================] - 0s 857us/step - kl_loss: 2.5416e-05\n",
      "16/16 [==============================] - 0s 879us/step - kl_loss: 2.3686e-04\n",
      "16/16 [==============================] - 0s 893us/step - kl_loss: 8.3754e-05\n",
      "16/16 [==============================] - 0s 910us/step - kl_loss: 5.5621e-05\n",
      "16/16 [==============================] - 0s 908us/step - kl_loss: 4.7069e-05\n",
      "16/16 [==============================] - 0s 863us/step - kl_loss: 3.4773e-05\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 6.9179e-05\n",
      "16/16 [==============================] - 0s 892us/step - kl_loss: 5.2090e-05\n",
      "14/14 [==============================] - 0s 910us/step - kl_loss: 2.1028e-04\n",
      "9/9 [==============================] - 0s 919us/step - kl_loss: 3.0514e-04\n",
      "16/16 [==============================] - 0s 907us/step - kl_loss: 1.1390e-04\n",
      "16/16 [==============================] - 0s 879us/step - kl_loss: 2.2734e-05\n",
      "16/16 [==============================] - 0s 883us/step - kl_loss: 2.9648e-05\n",
      "16/16 [==============================] - 0s 841us/step - kl_loss: 4.2551e-04\n",
      "16/16 [==============================] - 0s 858us/step - kl_loss: 8.8995e-05\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 2.1523e-04\n",
      "16/16 [==============================] - 0s 878us/step - kl_loss: 2.3479e-04\n",
      "16/16 [==============================] - 0s 884us/step - kl_loss: 3.5033e-05\n",
      "16/16 [==============================] - 0s 893us/step - kl_loss: 2.2248e-05\n",
      "16/16 [==============================] - 0s 862us/step - kl_loss: 2.9066e-05\n",
      "16/16 [==============================] - 0s 865us/step - kl_loss: 2.1744e-05\n",
      "11/11 [==============================] - 0s 894us/step - kl_loss: 6.2629e-05\n",
      "16/16 [==============================] - 0s 853us/step - kl_loss: 2.1039e-04\n",
      "16/16 [==============================] - 0s 911us/step - kl_loss: 7.2563e-05\n",
      "16/16 [==============================] - 0s 911us/step - kl_loss: 7.8947e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 1.6788e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 3.6060e-04\n",
      "16/16 [==============================] - 0s 871us/step - kl_loss: 7.9603e-05\n",
      "16/16 [==============================] - 0s 888us/step - kl_loss: 3.5523e-05\n",
      "16/16 [==============================] - 0s 905us/step - kl_loss: 1.2547e-04\n",
      "16/16 [==============================] - 0s 884us/step - kl_loss: 2.6244e-04\n",
      "8/8 [==============================] - 0s 905us/step - kl_loss: 1.9605e-04\n",
      "16/16 [==============================] - 0s 880us/step - kl_loss: 7.6105e-05\n",
      "16/16 [==============================] - 0s 904us/step - kl_loss: 1.9218e-04\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 1.0097e-04\n",
      "16/16 [==============================] - 0s 898us/step - kl_loss: 1.8222e-04\n",
      "16/16 [==============================] - 0s 913us/step - kl_loss: 5.0225e-05\n",
      "16/16 [==============================] - 0s 873us/step - kl_loss: 1.4844e-04\n",
      "12/12 [==============================] - 0s 916us/step - kl_loss: 7.4900e-05\n",
      "16/16 [==============================] - 0s 885us/step - kl_loss: 8.2255e-05\n",
      "7/7 [==============================] - 0s 998us/step - kl_loss: 1.0713e-04\n",
      "16/16 [==============================] - 0s 877us/step - kl_loss: 8.7325e-05\n",
      "16/16 [==============================] - 0s 884us/step - kl_loss: 2.1384e-05\n",
      "16/16 [==============================] - 0s 890us/step - kl_loss: 6.9913e-05\n",
      "16/16 [==============================] - 0s 861us/step - kl_loss: 4.8478e-05\n",
      "16/16 [==============================] - 0s 858us/step - kl_loss: 5.1530e-05\n",
      "16/16 [==============================] - 0s 928us/step - kl_loss: 9.6592e-06\n",
      "13/13 [==============================] - 0s 938us/step - kl_loss: 9.6443e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_seqs):\n",
    "\n",
    "    pre = ob_seqs[i]\n",
    "    next = next_obs[i]\n",
    "    acts = actions[i]\n",
    "\n",
    "    next_sd = next_obs_stddev[i]\n",
    "\n",
    "    daifa.train(pre, next, acts, None, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "data": {
      "text/plain": "(20, 5, 3)"
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seqs = 20\n",
    "seq_length = 150\n",
    "ob_seqs = []\n",
    "next_obs = []\n",
    "\n",
    "for i in range(num_seqs):\n",
    "    o, a, r = random_observation_sequence(env, seq_length, epsilon=0.1)\n",
    "\n",
    "    o = transform_observations(o, observation_max, observation_min, [0, 0])\n",
    "\n",
    "    train = np.concatenate([o[:-1], a], axis=1)\n",
    "    test = o[-1]\n",
    "\n",
    "    ob_seqs.append(train)\n",
    "    next_obs.append(test)\n",
    "\n",
    "ob_seqs = np.array(ob_seqs)[:, -5:, :]\n",
    "next_obs = np.array(next_obs)\n",
    "ob_seqs.shape\n",
    "\n",
    "ob_seqs_stddev = np.ones_like(ob_seqs)\n",
    "next_obs_stddev = np.ones_like(next_obs)\n",
    "\n",
    "ob_seqs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: shape=(20, 2), dtype=float32, numpy=\n array([[0.42757505, 0.6529479 ],\n        [0.38936174, 0.5224733 ],\n        [0.5177275 , 0.5719767 ],\n        [0.32569912, 0.5357033 ],\n        [0.5789795 , 0.37268484],\n        [0.50462687, 0.16914466],\n        [0.10576638, 0.2804633 ],\n        [0.50895584, 0.26160803],\n        [0.84278893, 0.7405859 ],\n        [0.41768757, 0.8168086 ],\n        [0.6252694 , 0.524398  ],\n        [0.29302755, 0.6640839 ],\n        [0.7982165 , 0.4708082 ],\n        [0.0011582 , 0.43203047],\n        [0.15381938, 0.3239351 ],\n        [0.51816386, 0.7722647 ],\n        [0.4246617 , 0.18837008],\n        [0.62402654, 0.48184252],\n        [0.6900874 , 0.6458197 ],\n        [0.48874134, 0.24390756]], dtype=float32)>,\n <tf.Tensor: shape=(20, 2), dtype=float32, numpy=\n array([[1.0015444 , 1.0040803 ],\n        [1.0029435 , 1.0013554 ],\n        [1.00249   , 1.0037054 ],\n        [1.0025867 , 1.0013175 ],\n        [1.0037783 , 1.0024655 ],\n        [1.0045325 , 0.9992334 ],\n        [1.0020174 , 1.0017799 ],\n        [1.0041207 , 1.0012053 ],\n        [1.0033425 , 1.0089743 ],\n        [1.0012552 , 1.0062354 ],\n        [1.0033436 , 1.0037761 ],\n        [1.0008956 , 1.0037417 ],\n        [1.0051221 , 1.0040122 ],\n        [1.0008185 , 1.0032822 ],\n        [1.0018778 , 1.0018895 ],\n        [0.99936193, 1.0098908 ],\n        [1.0041473 , 1.001115  ],\n        [1.0043585 , 1.0022985 ],\n        [1.0025626 , 1.0072029 ],\n        [1.0040324 , 1.0017579 ]], dtype=float32)>,\n <tf.Tensor: shape=(20, 60), dtype=float32, numpy=\n array([[ 0.15026245,  0.02147635,  0.08431074, ..., -0.10982125,\n          0.05334226,  0.06565161],\n        [ 0.1892961 ,  0.0171166 ,  0.14129707, ..., -0.02967911,\n          0.19479504,  0.10307508],\n        [ 0.1276658 ,  0.0259374 ,  0.09705178, ..., -0.09186851,\n          0.05516341,  0.06683648],\n        ...,\n        [ 0.12049596,  0.0347067 ,  0.12938446, ..., -0.06724519,\n          0.1028895 ,  0.08167858],\n        [ 0.067205  ,  0.0587694 ,  0.05941856, ..., -0.19308257,\n         -0.05885605,  0.03636646],\n        [-0.04927346,  0.04336642,  0.01616201, ..., -0.09991126,\n         -0.13079813,  0.00147478]], dtype=float32)>,\n <tf.Tensor: shape=(20, 5, 60), dtype=float32, numpy=\n array([[[ 0.04525499, -0.01716381,  0.02350051, ..., -0.04238965,\n           0.01428884,  0.0045097 ],\n         [ 0.09317333, -0.00512476,  0.05376579, ..., -0.07720339,\n           0.02760326,  0.0271852 ],\n         [ 0.12649374,  0.00985319,  0.0731228 , ..., -0.09722769,\n           0.03932348,  0.04809814],\n         [ 0.14352977,  0.01830245,  0.08136631, ..., -0.10614912,\n           0.04794318,  0.06033898],\n         [ 0.15026245,  0.02147635,  0.08431074, ..., -0.10982125,\n           0.05334226,  0.06565161]],\n \n        [[ 0.06360728, -0.0279705 ,  0.04693291, ...,  0.01876127,\n           0.07270339,  0.02625261],\n         [ 0.12016988, -0.01528889,  0.09293082, ...,  0.00722509,\n           0.12378819,  0.05685403],\n         [ 0.15799734,  0.00201168,  0.12204836, ..., -0.00917845,\n           0.15883154,  0.08118919],\n         [ 0.17884886,  0.0125097 ,  0.13581392, ..., -0.02153258,\n           0.1813325 ,  0.09584883],\n         [ 0.1892961 ,  0.0171166 ,  0.14129707, ..., -0.02967911,\n           0.19479504,  0.10307508]],\n \n        [[ 0.01899024, -0.00537003,  0.01712187, ..., -0.05791149,\n          -0.00749379, -0.00342179],\n         [ 0.05159665,  0.01008601,  0.04413131, ..., -0.09687613,\n          -0.01088502,  0.01538653],\n         [ 0.09476812,  0.01297409,  0.07409859, ..., -0.08821176,\n           0.01896192,  0.04428795],\n         [ 0.11822307,  0.02161739,  0.09018441, ..., -0.08948812,\n           0.0409235 ,  0.05975568],\n         [ 0.1276658 ,  0.0259374 ,  0.09705178, ..., -0.09186851,\n           0.05516341,  0.06683648]],\n \n        ...,\n \n        [[-0.01287574,  0.00913438,  0.01037965, ..., -0.07211688,\n          -0.03120288, -0.01136945],\n         [ 0.04945046, -0.00161371,  0.06758605, ..., -0.04407822,\n           0.02909646,  0.0303868 ],\n         [ 0.09147461,  0.0191132 ,  0.10595157, ..., -0.05436005,\n           0.06481314,  0.0583218 ],\n         [ 0.11274984,  0.03069214,  0.12332863, ..., -0.0628503 ,\n           0.08860222,  0.07462525],\n         [ 0.12049596,  0.0347067 ,  0.12938446, ..., -0.06724519,\n           0.1028895 ,  0.08167858]],\n \n        [[ 0.01436141, -0.00646611,  0.01604711, ..., -0.07796118,\n          -0.01446355, -0.00974533],\n         [ 0.05152211,  0.0144128 ,  0.04881622, ..., -0.12986496,\n          -0.0225791 ,  0.01168774],\n         [ 0.08065747,  0.03350948,  0.06986689, ..., -0.15249343,\n          -0.02346765,  0.03437518],\n         [ 0.09257582,  0.04136415,  0.07716379, ..., -0.15684438,\n          -0.02214691,  0.04622274],\n         [ 0.067205  ,  0.0587694 ,  0.05941856, ..., -0.19308257,\n          -0.05885605,  0.03636646]],\n \n        [[-0.04124852,  0.02623192, -0.00106705, ..., -0.07540748,\n          -0.05910962, -0.01571499],\n         [-0.0516727 ,  0.0423548 ,  0.00782672, ..., -0.10877656,\n          -0.10001399, -0.01105171],\n         [-0.05289169,  0.04889357,  0.01271122, ..., -0.11655216,\n          -0.12273175, -0.00386084],\n         [-0.04969812,  0.04598803,  0.01593486, ..., -0.10666266,\n          -0.12809363,  0.0011813 ],\n         [-0.04927346,  0.04336642,  0.01616201, ..., -0.09991126,\n          -0.13079813,  0.00147478]]], dtype=float32)>]"
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daifa.tran((ob_seqs, None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.41424149, 0.64817536],\n       [0.37549612, 0.51434882],\n       [0.50395287, 0.56238058],\n       [0.31282919, 0.53150581],\n       [0.56830029, 0.36537232],\n       [0.49751557, 0.15729226],\n       [0.10200014, 0.28023718],\n       [0.49930485, 0.25297206],\n       [0.83264549, 0.76498072],\n       [0.40183171, 0.8195579 ],\n       [0.61205382, 0.51735449],\n       [0.28334383, 0.66585968],\n       [0.78955368, 0.48945753],\n       [0.        , 0.5       ],\n       [0.15014838, 0.32678897],\n       [0.5064109 , 0.78005801],\n       [0.41774712, 0.18253206],\n       [0.61022634, 0.47224011],\n       [0.67659513, 0.6480953 ],\n       [0.48181082, 0.23722683]])"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_obs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks fantastic!!! With enough data the transition model is training very well"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 60), dtype=float32, numpy=\narray([[ 0.1668222 ,  0.05080901,  0.20015422,  0.38899958, -0.08546768,\n        -0.04040675,  0.03512116,  0.00171472,  0.03852477,  0.20108685,\n        -0.03330585,  0.01087453, -0.03944991, -0.23539615,  0.19884759,\n         0.15129937,  0.08765514,  0.15757117, -0.16009761, -0.02254741,\n        -0.17335685, -0.09706004,  0.05607434,  0.03711884, -0.0560054 ,\n        -0.27313083,  0.02705026,  0.14458522, -0.25310335, -0.08086976,\n        -0.10635097, -0.28293777,  0.00502296,  0.2793439 ,  0.07475004,\n        -0.09199525, -0.23762226,  0.05454395, -0.07554322, -0.06423084,\n         0.11491245,  0.03344171, -0.03258195,  0.04890673,  0.07888647,\n         0.11464167,  0.31568897,  0.01460155, -0.23916677,  0.24096602,\n         0.1589966 ,  0.0215495 , -0.38883814,  0.2073881 ,  0.17495394,\n         0.30218056, -0.14856315, -0.09490789,  0.20044254,  0.12068783]],\n      dtype=float32)>"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daifa.hidden_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.5]\n",
      "tf.Tensor(\n",
      "[ 0.7089493   0.76423895  0.8047202   0.8124183   0.73879427  0.76978123\n",
      "  0.61311316  0.5821078   0.4306626   0.40520254  0.17977683  0.27913105\n",
      " -0.00298302  0.03768509  0.0850338 ], shape=(15,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(15,), dtype=float32, numpy=\narray([ 0.7089493 ,  0.76423895,  0.8047202 ,  0.8124183 ,  0.73879427,\n        0.76978123,  0.61311316,  0.5821078 ,  0.4306626 ,  0.40520254,\n        0.17977683,  0.27913105, -0.00298302,  0.03768509,  0.0850338 ],\n      dtype=float32)>"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_t_minus_1 = np.array([0.4, 0.5])\n",
    "daifa.hidden_state = None\n",
    "p, s = daifa.cem_policy_optimisation(z_t_minus_1)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.17485519, 0.28052184]], dtype=float32)>,\n <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.998175 , 0.9848511]], dtype=float32)>,\n <tf.Tensor: shape=(1, 60), dtype=float32, numpy=\n array([[ 0.06734794, -0.03086371,  0.04919352,  0.06776053,  0.05020184,\n         -0.02855486,  0.05532712,  0.00621268,  0.03898622,  0.06404883,\n         -0.06092996,  0.02080428, -0.03182369, -0.09051668,  0.02938318,\n          0.02920253,  0.09898859,  0.04242412, -0.0937261 , -0.07689307,\n         -0.04255384, -0.01035933,  0.0217123 ,  0.0560016 , -0.06113841,\n         -0.08259731, -0.04008626,  0.02852438, -0.09232952, -0.09844272,\n         -0.09578703, -0.0808928 ,  0.07631816,  0.10538951,  0.03266784,\n         -0.06981869, -0.13518177, -0.00443301, -0.0766279 ,  0.03137437,\n          0.10195947,  0.03933517,  0.01060682, -0.02867689,  0.05144734,\n          0.02094595,  0.07122529,  0.09150924, -0.11927285,  0.11849919,\n          0.06474774, -0.00642365, -0.15364884,  0.10189001,  0.09677684,\n          0.10298578,  0.00337658,  0.01931385,  0.07793737,  0.02707184]],\n       dtype=float32)>,\n <tf.Tensor: shape=(1, 1, 60), dtype=float32, numpy=\n array([[[ 0.06734794, -0.03086371,  0.04919352,  0.06776053,\n           0.05020184, -0.02855486,  0.05532712,  0.00621268,\n           0.03898622,  0.06404883, -0.06092996,  0.02080428,\n          -0.03182369, -0.09051668,  0.02938318,  0.02920253,\n           0.09898859,  0.04242412, -0.0937261 , -0.07689307,\n          -0.04255384, -0.01035933,  0.0217123 ,  0.0560016 ,\n          -0.06113841, -0.08259731, -0.04008626,  0.02852438,\n          -0.09232952, -0.09844272, -0.09578703, -0.0808928 ,\n           0.07631816,  0.10538951,  0.03266784, -0.06981869,\n          -0.13518177, -0.00443301, -0.0766279 ,  0.03137437,\n           0.10195947,  0.03933517,  0.01060682, -0.02867689,\n           0.05144734,  0.02094595,  0.07122529,  0.09150924,\n          -0.11927285,  0.11849919,  0.06474774, -0.00642365,\n          -0.15364884,  0.10189001,  0.09677684,  0.10298578,\n           0.00337658,  0.01931385,  0.07793737,  0.02707184]]],\n       dtype=float32)>]"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daifa.tran((np.array([[[0.4, 0.5, 1]]]), None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "[0.3531864 0.5      ]\n",
      "tf.Tensor(\n",
      "[0.75776505 0.808785   0.823482   0.7384236  0.66807634 0.75550365\n",
      " 0.65706307 0.57430935 0.44249344 0.50059706 0.30967107 0.3025037\n",
      " 0.23872639 0.13167822 0.03006317], shape=(15,), dtype=float32)\n",
      "[0.41072467 0.60575192]\n",
      "tf.Tensor(\n",
      "[0.80091256 0.831186   0.80336916 0.743643   0.67225486 0.7388898\n",
      " 0.6710419  0.49252346 0.45496187 0.39570916 0.33172902 0.4691094\n",
      " 0.18269321 0.02243686 0.13316137], shape=(15,), dtype=float32)\n",
      "[0.52321638 0.6161014 ]\n",
      "tf.Tensor(\n",
      "[0.7582315  0.7428887  0.78753793 0.70738727 0.6904973  0.6947737\n",
      " 0.7184091  0.63121426 0.51972836 0.44251725 0.4724567  0.35061508\n",
      " 0.20868196 0.29411447 0.1608929 ], shape=(15,), dtype=float32)\n",
      "[0.59332169 0.53282979]\n",
      "tf.Tensor(\n",
      "[ 0.8261601   0.7788568   0.7498009   0.68705684  0.71283436  0.7625619\n",
      "  0.6251273   0.59106356  0.47288027  0.37912145  0.37760547  0.27614397\n",
      "  0.17472365  0.0352636  -0.06415275], shape=(15,), dtype=float32)\n",
      "[0.57653528 0.43853916]\n",
      "tf.Tensor(\n",
      "[ 0.84993833  0.838954    0.836631    0.8009051   0.6381009   0.67451525\n",
      "  0.66360396  0.7313162   0.44090325  0.61107373  0.4429256   0.2499801\n",
      "  0.18311256  0.00455586 -0.03917548], shape=(15,), dtype=float32)\n",
      "[0.48622649 0.38311274]\n",
      "tf.Tensor(\n",
      "[0.8081694  0.80967754 0.7922341  0.7968606  0.7375868  0.6537518\n",
      " 0.64383316 0.62586427 0.55786794 0.3814907  0.40052238 0.48054183\n",
      " 0.37376764 0.13650063 0.09636682], shape=(15,), dtype=float32)\n",
      "[0.38618095 0.41815763]\n",
      "tf.Tensor(\n",
      "[0.808529   0.80657184 0.7916462  0.70533156 0.7616304  0.6811054\n",
      " 0.6398279  0.49607593 0.48863018 0.42573503 0.4255617  0.5032834\n",
      " 0.25781015 0.1669854  0.03199732], shape=(15,), dtype=float32)\n",
      "[0.36487361 0.53167849]\n",
      "tf.Tensor(\n",
      "[ 0.80258495  0.86818594  0.71748924  0.8466585   0.7656668   0.6802341\n",
      "  0.7175385   0.55456597  0.56965184  0.48494464  0.4133271   0.35727996\n",
      "  0.29771766  0.2542887  -0.04310569], shape=(15,), dtype=float32)\n",
      "[0.44369857 0.61764689]\n",
      "tf.Tensor(\n",
      "[0.8110079  0.6923667  0.79028475 0.6758299  0.71477115 0.7232198\n",
      " 0.6768123  0.6062628  0.5514061  0.47747606 0.4313433  0.3517048\n",
      " 0.39273486 0.17666398 0.08394014], shape=(15,), dtype=float32)\n",
      "[0.54803135 0.58811021]\n",
      "tf.Tensor(\n",
      "[0.77069455 0.7971718  0.76023203 0.76838154 0.70611453 0.7311224\n",
      " 0.6250644  0.6438176  0.47762975 0.5243256  0.41966495 0.36352235\n",
      " 0.40499055 0.01003213 0.061273  ], shape=(15,), dtype=float32)\n",
      "[0.58816123 0.5017748 ]\n",
      "tf.Tensor(\n",
      "[ 0.82042825  0.80002797  0.74095577  0.71816117  0.73239034  0.7092903\n",
      "  0.72302485  0.62576604  0.5700318   0.4367064   0.43498516  0.47891188\n",
      "  0.16538338  0.18202068 -0.19701585], shape=(15,), dtype=float32)\n",
      "[0.54586323 0.41759907]\n",
      "tf.Tensor(\n",
      "[0.7925538  0.797785   0.7362157  0.7193825  0.72075266 0.6847715\n",
      " 0.63142014 0.52450913 0.6858361  0.4741538  0.46578348 0.35045347\n",
      " 0.4875199  0.10972673 0.30347246], shape=(15,), dtype=float32)\n",
      "[0.44605739 0.38614578]\n",
      "tf.Tensor(\n",
      "[0.7413631  0.7655986  0.7876091  0.7940088  0.70736146 0.6543615\n",
      " 0.6855993  0.6544356  0.58999455 0.48650116 0.347798   0.12228234\n",
      " 0.24543785 0.20780389 0.03425851], shape=(15,), dtype=float32)\n",
      "[0.36494746 0.45144458]\n",
      "tf.Tensor(\n",
      "[0.8217085  0.73286784 0.7572981  0.7622988  0.7463897  0.71150637\n",
      " 0.5872628  0.6521302  0.51973563 0.41864827 0.4010417  0.27743676\n",
      " 0.38145977 0.16709988 0.10859277], shape=(15,), dtype=float32)\n",
      "[0.3799972  0.56624843]\n",
      "tf.Tensor(\n",
      "[0.7157361  0.8330341  0.7970764  0.7716476  0.79019064 0.6825232\n",
      " 0.58022153 0.49948868 0.45807794 0.5443946  0.4094528  0.3813852\n",
      " 0.3188499  0.09375567 0.2158043 ], shape=(15,), dtype=float32)\n",
      "[0.47352085 0.61658074]\n",
      "tf.Tensor(\n",
      "[0.7780647  0.80061483 0.722193   0.74317825 0.7437629  0.68641585\n",
      " 0.5962243  0.50690097 0.65318114 0.52816546 0.38895717 0.27855322\n",
      " 0.25044137 0.2727542  0.06080603], shape=(15,), dtype=float32)\n",
      "[0.56438515 0.56986869]\n",
      "tf.Tensor(\n",
      "[0.7811205  0.7339901  0.81370497 0.72263205 0.73430645 0.629454\n",
      " 0.6017497  0.6197657  0.56409466 0.43045133 0.26490685 0.2498877\n",
      " 0.31837606 0.00660494 0.18230386], shape=(15,), dtype=float32)\n",
      "[0.58373286 0.47584327]\n",
      "tf.Tensor(\n",
      "[ 0.786635    0.7676723   0.78109056  0.7734856   0.7349278   0.6434932\n",
      "  0.64729017  0.6477715   0.46817616  0.5103125   0.3484181   0.32719758\n",
      "  0.19347914  0.08783682 -0.12831853], shape=(15,), dtype=float32)\n",
      "[0.51849457 0.39698744]\n",
      "tf.Tensor(\n",
      "[0.7013822  0.82110566 0.8151903  0.83248204 0.69641477 0.7037499\n",
      " 0.65221685 0.62713647 0.6174364  0.46683326 0.47702065 0.45662794\n",
      " 0.22590521 0.12989476 0.18857364], shape=(15,), dtype=float32)\n",
      "[0.41009074 0.39329548]\n",
      "tf.Tensor(\n",
      "[0.78286713 0.75951535 0.79978824 0.7267528  0.71680486 0.65614986\n",
      " 0.6204901  0.54773957 0.50284386 0.5718347  0.25679275 0.19043566\n",
      " 0.2571739  0.09063831 0.13465643], shape=(15,), dtype=float32)\n",
      "[0.35496967 0.49114117]\n",
      "tf.Tensor(\n",
      "[ 0.7816021   0.81464404  0.8433688   0.76073045  0.763372    0.6750622\n",
      "  0.5641183   0.69435006  0.5063368   0.4080222   0.47917065  0.31884998\n",
      "  0.17305967 -0.06161011 -0.04073815], shape=(15,), dtype=float32)\n",
      "[0.40579482 0.60065321]\n",
      "tf.Tensor(\n",
      "[0.79529333 0.750566   0.79003304 0.76391315 0.73424697 0.6420629\n",
      " 0.6682913  0.6523948  0.5369906  0.5469905  0.33981997 0.26553097\n",
      " 0.0638757  0.28567824 0.23673962], shape=(15,), dtype=float32)\n",
      "[0.51507599 0.61200315]\n",
      "tf.Tensor(\n",
      "[ 0.79337245  0.7404581   0.7697803   0.73482007  0.70173734  0.73203075\n",
      "  0.5884999   0.60060364  0.46503568  0.4337764   0.4391953   0.34379748\n",
      "  0.26070306  0.07910137 -0.0782097 ], shape=(15,), dtype=float32)\n",
      "[0.58594846 0.53615845]\n",
      "tf.Tensor(\n",
      "[0.77882797 0.69763243 0.7354432  0.78261834 0.7548013  0.7395214\n",
      " 0.6853571  0.57952696 0.5142892  0.46434534 0.52490604 0.3411709\n",
      " 0.2709834  0.09081004 0.05274038], shape=(15,), dtype=float32)\n",
      "[0.57014214 0.43662953]\n",
      "tf.Tensor(\n",
      "[ 0.8492138   0.7675102   0.8595176   0.78523564  0.7543218   0.66763735\n",
      "  0.5548783   0.62393606  0.48151332  0.5518069   0.38802665  0.3985188\n",
      "  0.3526202   0.23939711 -0.12900242], shape=(15,), dtype=float32)\n",
      "[0.47905944 0.38159515]\n",
      "tf.Tensor(\n",
      "[ 0.82852745  0.81620026  0.76462984  0.6585787   0.680708    0.5028422\n",
      "  0.67517024  0.60897154  0.4863976   0.62219536  0.49263546  0.44589314\n",
      "  0.28722784  0.23585846 -0.02223484], shape=(15,), dtype=float32)\n",
      "[0.38228903 0.42519666]\n",
      "tf.Tensor(\n",
      "[0.79549915 0.73700064 0.78116643 0.7397113  0.7940053  0.78030664\n",
      " 0.5730892  0.49749538 0.5726207  0.3887233  0.46988285 0.3230513\n",
      " 0.03857424 0.06511506 0.09033238], shape=(15,), dtype=float32)\n",
      "[0.36676927 0.53437901]\n",
      "tf.Tensor(\n",
      "[0.8047022  0.75837153 0.8462696  0.7162242  0.72801363 0.65123004\n",
      " 0.59589547 0.54535824 0.6131784  0.4514287  0.6247042  0.36965016\n",
      " 0.20724636 0.18744104 0.0636511 ], shape=(15,), dtype=float32)\n",
      "[0.44490315 0.61066828]\n",
      "tf.Tensor(\n",
      "[0.81024164 0.7460855  0.7998578  0.74819976 0.7318434  0.7199478\n",
      " 0.6176514  0.6539627  0.45442376 0.54600036 0.46350846 0.27041775\n",
      " 0.35329205 0.08493165 0.01173883], shape=(15,), dtype=float32)\n",
      "[0.54393757 0.58586972]\n",
      "tf.Tensor(\n",
      "[0.83877844 0.7512684  0.8179611  0.7089987  0.62314284 0.6872689\n",
      " 0.666742   0.58867216 0.43242267 0.48708692 0.5365295  0.36782113\n",
      " 0.2323856  0.22591136 0.1930209 ], shape=(15,), dtype=float32)\n",
      "[0.58560245 0.50302988]\n",
      "tf.Tensor(\n",
      "[ 0.81615007  0.8337144   0.7280479   0.83652335  0.6593734   0.6673092\n",
      "  0.644081    0.6065941   0.49902397  0.597173    0.33554184  0.29067463\n",
      "  0.29533008  0.15974145 -0.04347519], shape=(15,), dtype=float32)\n",
      "[0.54541119 0.42172624]\n",
      "tf.Tensor(\n",
      "[0.77859366 0.7399454  0.80361915 0.8222891  0.6997945  0.7333289\n",
      " 0.5931555  0.5250654  0.38968724 0.4366026  0.41840667 0.2858042\n",
      " 0.20093226 0.11056146 0.28115508], shape=(15,), dtype=float32)\n",
      "[0.44752685 0.38479307]\n",
      "tf.Tensor(\n",
      "[ 0.82429093  0.7936777   0.8563681   0.7860239   0.7406905   0.5799222\n",
      "  0.7315543   0.5343847   0.460609    0.4562714   0.35923907  0.36962327\n",
      "  0.18014123  0.23509723 -0.02257505], shape=(15,), dtype=float32)\n",
      "[0.36856751 0.45480162]\n",
      "tf.Tensor(\n",
      "[0.8142052  0.7555646  0.8146456  0.76780176 0.7035508  0.6438452\n",
      " 0.6299063  0.6259845  0.56285286 0.4900472  0.4291155  0.2972585\n",
      " 0.23083456 0.0416389  0.11854338], shape=(15,), dtype=float32)\n",
      "[0.38437616 0.56566065]\n",
      "tf.Tensor(\n",
      "[0.8717742  0.79311955 0.8349144  0.7581415  0.6517145  0.639386\n",
      " 0.60070837 0.52159375 0.42648387 0.45765376 0.37017617 0.3854718\n",
      " 0.27083525 0.08882858 0.03711194], shape=(15,), dtype=float32)\n",
      "[0.48105746 0.61698049]\n",
      "tf.Tensor(\n",
      "[0.7816412  0.8154942  0.7475302  0.7307145  0.67497694 0.65167934\n",
      " 0.6135576  0.52580523 0.50448567 0.5064087  0.28986984 0.33856618\n",
      " 0.2111372  0.04163319 0.06080171], shape=(15,), dtype=float32)\n",
      "[0.56957929 0.56592761]\n",
      "tf.Tensor(\n",
      "[0.7415218  0.8276692  0.76559246 0.7420308  0.6718621  0.6524063\n",
      " 0.74649006 0.6595096  0.57205176 0.42472783 0.46960062 0.40188935\n",
      " 0.3381443  0.31225252 0.12252428], shape=(15,), dtype=float32)\n",
      "[0.58403423 0.47394873]\n",
      "tf.Tensor(\n",
      "[ 0.7820771   0.76585364  0.77992487  0.7481109   0.71760786  0.7291078\n",
      "  0.73734087  0.65609974  0.58259207  0.44035995  0.39198986  0.0941532\n",
      "  0.22869183 -0.15184657  0.09070029], shape=(15,), dtype=float32)\n",
      "[0.51688021 0.39510235]\n",
      "tf.Tensor(\n",
      "[0.7352284  0.75400406 0.78572464 0.801884   0.6663553  0.7583888\n",
      " 0.67039853 0.5376294  0.50376385 0.5414519  0.5258707  0.35912326\n",
      " 0.2996345  0.05289698 0.06059019], shape=(15,), dtype=float32)\n",
      "[0.40798668 0.39102628]\n",
      "tf.Tensor(\n",
      "[0.7611072  0.8020517  0.78023297 0.6550679  0.66480124 0.76778185\n",
      " 0.6363506  0.5958002  0.47454938 0.5027793  0.5354361  0.33032587\n",
      " 0.32072702 0.1383539  0.08724254], shape=(15,), dtype=float32)\n",
      "[0.3520312  0.49354863]\n",
      "tf.Tensor(\n",
      "[0.79392415 0.7739514  0.8184119  0.79044056 0.6628201  0.73477334\n",
      " 0.6351135  0.6286006  0.4333898  0.42271107 0.5196934  0.10632459\n",
      " 0.33844072 0.25389937 0.09108002], shape=(15,), dtype=float32)\n",
      "[0.40609846 0.60283605]\n",
      "tf.Tensor(\n",
      "[ 0.7884622   0.82729053  0.8360866   0.7258866   0.64380354  0.7157528\n",
      "  0.5779476   0.5710402   0.47314978  0.5921881   0.3813641   0.29816145\n",
      "  0.16894156  0.27616373 -0.02353296], shape=(15,), dtype=float32)\n",
      "[0.51798088 0.61750049]\n",
      "tf.Tensor(\n",
      "[ 0.81359965  0.763978    0.76793635  0.7462814   0.70802116  0.68962127\n",
      "  0.58806014  0.5863664   0.46981502  0.5895484   0.37318972  0.36630228\n",
      "  0.25067335  0.17080323 -0.03141759], shape=(15,), dtype=float32)\n",
      "[0.5938309  0.54126586]\n",
      "tf.Tensor(\n",
      "[0.8113294  0.768809   0.6856807  0.73540765 0.72331357 0.6783503\n",
      " 0.5949049  0.583399   0.61106104 0.5376641  0.33809215 0.40843654\n",
      " 0.18948479 0.06254447 0.08455604], shape=(15,), dtype=float32)\n",
      "[0.58345062 0.44372764]\n",
      "tf.Tensor(\n",
      "[0.8009942  0.8070976  0.7275021  0.69663674 0.71600336 0.70018804\n",
      " 0.65069294 0.66603565 0.7022384  0.45402578 0.34891218 0.22229828\n",
      " 0.2553366  0.20484611 0.02414317], shape=(15,), dtype=float32)\n",
      "[0.49264736 0.37778779]\n",
      "tf.Tensor(\n",
      "[0.74953175 0.75108176 0.7682707  0.7941111  0.6321828  0.7890561\n",
      " 0.59978455 0.5697439  0.5587085  0.5677614  0.34142888 0.45922023\n",
      " 0.34311283 0.18927541 0.04024025], shape=(15,), dtype=float32)\n",
      "[0.38186869 0.40307761]\n",
      "tf.Tensor(\n",
      "[0.8401727  0.83660525 0.7682112  0.7321585  0.7469177  0.6507884\n",
      " 0.6442639  0.64668775 0.45425525 0.34885836 0.329645   0.31632945\n",
      " 0.17799746 0.10585867 0.17687285], shape=(15,), dtype=float32)\n",
      "[0.35298689 0.53085447]\n",
      "tf.Tensor(\n",
      "[0.78200173 0.7954946  0.79973924 0.76843774 0.76992786 0.64292175\n",
      " 0.6476148  0.5449644  0.6215908  0.56584924 0.37136087 0.37221712\n",
      " 0.19877341 0.05788508 0.09599178], shape=(15,), dtype=float32)\n",
      "[0.43535255 0.62311142]\n",
      "tf.Tensor(\n",
      "[0.7988832  0.81995803 0.79670495 0.75617784 0.7537437  0.72877586\n",
      " 0.6120326  0.6481022  0.528509   0.44550523 0.54102105 0.4045983\n",
      " 0.20502405 0.10994437 0.01944509], shape=(15,), dtype=float32)\n",
      "[0.54961391 0.6057297 ]\n",
      "tf.Tensor(\n",
      "[0.82574815 0.8087345  0.79786146 0.726313   0.6651899  0.7448676\n",
      " 0.6447881  0.57326835 0.49473333 0.34929955 0.18304539 0.3073979\n",
      " 0.30910343 0.2195527  0.08602198], shape=(15,), dtype=float32)\n",
      "[0.60715892 0.51897679]\n",
      "tf.Tensor(\n",
      "[0.73939127 0.7683055  0.77412814 0.7428036  0.71010387 0.734558\n",
      " 0.6276515  0.5459412  0.57485867 0.42185855 0.32804263 0.21587849\n",
      " 0.03043543 0.26203704 0.07618994], shape=(15,), dtype=float32)\n",
      "[0.571388   0.41616956]\n",
      "tf.Tensor(\n",
      "[0.8084016  0.79118896 0.7125908  0.78489465 0.7554889  0.70314\n",
      " 0.6542661  0.6204031  0.5129748  0.53307694 0.3740126  0.1729145\n",
      " 0.2167828  0.13401434 0.3829881 ], shape=(15,), dtype=float32)\n",
      "[0.46168311 0.36681081]\n",
      "tf.Tensor(\n",
      "[ 0.80106664  0.8936466   0.76621354  0.7754078   0.77766734  0.63762605\n",
      "  0.644117    0.4937583   0.3810801   0.40484437  0.45562983  0.39988062\n",
      "  0.14110254  0.12532564 -0.0749072 ], shape=(15,), dtype=float32)\n",
      "[0.36223823 0.43672092]\n",
      "tf.Tensor(\n",
      "[ 0.8590739   0.85995823  0.7432653   0.7503428   0.80756474  0.72860914\n",
      "  0.69533503  0.65018004  0.58915186  0.49469632  0.26144204  0.31654248\n",
      "  0.1668773   0.16982254 -0.07862785], shape=(15,), dtype=float32)\n",
      "[0.37103283 0.57032034]\n",
      "tf.Tensor(\n",
      "[0.7883463  0.7421475  0.7198873  0.8076494  0.73724467 0.7076293\n",
      " 0.55192846 0.49001226 0.5639493  0.52365774 0.33413577 0.2639018\n",
      " 0.14376952 0.13097861 0.12782432], shape=(15,), dtype=float32)\n",
      "[0.47393083 0.62502963]\n",
      "tf.Tensor(\n",
      "[ 0.78464913  0.7832258   0.81469816  0.7778928   0.73397785  0.6671145\n",
      "  0.56024915  0.63776195  0.52743816  0.43552956  0.38730374  0.34354916\n",
      "  0.14792752 -0.00164434  0.15573315], shape=(15,), dtype=float32)\n",
      "[0.57156853 0.57477212]\n",
      "tf.Tensor(\n",
      "[0.8371124  0.72853917 0.8691501  0.73401505 0.6498376  0.73067075\n",
      " 0.63454086 0.55254984 0.5146528  0.4891952  0.43070522 0.43262103\n",
      " 0.1375717  0.04265443 0.3134485 ], shape=(15,), dtype=float32)\n",
      "[0.59580991 0.47936026]\n",
      "tf.Tensor(\n",
      "[ 0.81873614  0.83532137  0.7040841   0.7788191   0.6197398   0.61335325\n",
      "  0.68482035  0.5888923   0.61703384  0.4427971   0.6005854   0.20061466\n",
      "  0.14735143  0.22884987 -0.0335655 ], shape=(15,), dtype=float32)\n",
      "[0.5329938  0.39897234]\n",
      "tf.Tensor(\n",
      "[0.8065402  0.7822272  0.76538205 0.7681339  0.7753727  0.7742152\n",
      " 0.69478977 0.57195425 0.4704853  0.49332318 0.31115493 0.395126\n",
      " 0.28096202 0.04866501 0.1842206 ], shape=(15,), dtype=float32)\n",
      "[0.42374525 0.38484408]\n",
      "tf.Tensor(\n",
      "[0.85375875 0.85130006 0.82338494 0.7543496  0.5935664  0.68263274\n",
      " 0.6397417  0.6747055  0.4713774  0.47407013 0.29585087 0.24762736\n",
      " 0.16460706 0.19996603 0.0240241 ], shape=(15,), dtype=float32)\n",
      "[0.35941744 0.48232772]\n",
      "tf.Tensor(\n",
      "[0.8330857  0.7875589  0.7354576  0.7439057  0.73607373 0.72214407\n",
      " 0.57638764 0.573217   0.5422481  0.41029024 0.3359802  0.30563188\n",
      " 0.10840655 0.21389659 0.00672198], shape=(15,), dtype=float32)\n",
      "[0.40275122 0.59225201]\n",
      "tf.Tensor(\n",
      "[ 0.82999897  0.86564636  0.7875039   0.81777596  0.70565     0.67825115\n",
      "  0.61548394  0.5552539   0.42982295  0.41920453  0.4481033   0.38862363\n",
      "  0.21809898  0.06253474 -0.07581279], shape=(15,), dtype=float32)\n",
      "[0.51037026 0.6184226 ]\n",
      "tf.Tensor(\n",
      "[0.7676647  0.82916576 0.7767916  0.7539434  0.705458   0.64423805\n",
      " 0.69134396 0.44446015 0.62188345 0.49696687 0.4288368  0.4260155\n",
      " 0.28188017 0.08480137 0.05216508], shape=(15,), dtype=float32)\n",
      "[0.58867452 0.54802307]\n",
      "tf.Tensor(\n",
      "[0.8200514  0.8098292  0.80813813 0.74955344 0.6526869  0.66327983\n",
      " 0.609704   0.53753686 0.49178484 0.45022994 0.4564877  0.33983544\n",
      " 0.3784235  0.15109646 0.04174715], shape=(15,), dtype=float32)\n",
      "[0.58638319 0.45443259]\n",
      "tf.Tensor(\n",
      "[0.7696876  0.7509072  0.7973766  0.7715203  0.7091568  0.6614527\n",
      " 0.5852787  0.6455517  0.45499912 0.40769756 0.4184576  0.47384334\n",
      " 0.33827722 0.06488781 0.05048884], shape=(15,), dtype=float32)\n",
      "[0.50134118 0.37832446]\n",
      "tf.Tensor(\n",
      "[0.8192935  0.7438691  0.8174823  0.7081893  0.73018426 0.7355522\n",
      " 0.59525925 0.5514984  0.5196334  0.38062623 0.40469217 0.34332815\n",
      " 0.40626797 0.17307793 0.15640299], shape=(15,), dtype=float32)\n",
      "[0.38968613 0.39832184]\n",
      "tf.Tensor(\n",
      "[ 0.8166617   0.8328098   0.79810065  0.78592587  0.77027655  0.6313598\n",
      "  0.5805887   0.6444551   0.6290719   0.4495581   0.3667833   0.28382215\n",
      "  0.2628423  -0.02390542 -0.15235856], shape=(15,), dtype=float32)\n",
      "[0.35192616 0.51947195]\n",
      "tf.Tensor(\n",
      "[0.78434974 0.81409925 0.8017937  0.74562925 0.6691752  0.718595\n",
      " 0.60850894 0.50091076 0.5456703  0.45592985 0.43720937 0.29297298\n",
      " 0.25382063 0.32232195 0.18150227], shape=(15,), dtype=float32)\n",
      "[0.42643755 0.61913247]\n",
      "tf.Tensor(\n",
      "[0.7865445  0.8380168  0.7752155  0.7637406  0.7297546  0.70171916\n",
      " 0.6920485  0.54565537 0.52947325 0.47661933 0.4009594  0.3770377\n",
      " 0.32883    0.16308029 0.02899723], shape=(15,), dtype=float32)\n",
      "[0.54147191 0.61092481]\n",
      "tf.Tensor(\n",
      "[0.78888315 0.7469997  0.67738193 0.7490696  0.66438204 0.69212526\n",
      " 0.6257292  0.5625566  0.64628035 0.53416157 0.4505727  0.4250792\n",
      " 0.22938962 0.04641598 0.08701646], shape=(15,), dtype=float32)\n",
      "[0.60302623 0.52109476]\n",
      "tf.Tensor(\n",
      "[0.78477997 0.81664246 0.84776825 0.7173021  0.6917322  0.72397983\n",
      " 0.64939183 0.47706875 0.5227951  0.51427835 0.32041466 0.2011926\n",
      " 0.26265493 0.08005637 0.13914573], shape=(15,), dtype=float32)\n",
      "[0.57279901 0.42512558]\n",
      "tf.Tensor(\n",
      "[0.8355642  0.81322175 0.8219691  0.64820415 0.6749398  0.6151745\n",
      " 0.6576263  0.528807   0.5957073  0.31524938 0.5663655  0.27555418\n",
      " 0.14108284 0.23462236 0.08474787], shape=(15,), dtype=float32)\n",
      "[0.47154124 0.37436967]\n",
      "tf.Tensor(\n",
      "[ 0.77912587  0.8206091   0.8392971   0.6829742   0.81002873  0.71247864\n",
      "  0.6173976   0.53924984  0.5642729   0.44932613  0.42185163  0.31431144\n",
      "  0.1367163   0.162648   -0.05937104], shape=(15,), dtype=float32)\n",
      "[0.37085166 0.42617494]\n",
      "tf.Tensor(\n",
      "[ 0.7898823   0.7594188   0.79121584  0.78815037  0.7210286   0.71293074\n",
      "  0.6105649   0.70436573  0.4780255   0.39781225  0.34089234  0.3126574\n",
      "  0.20237492 -0.04093971  0.1548445 ], shape=(15,), dtype=float32)\n",
      "[0.36229389 0.54699585]\n",
      "tf.Tensor(\n",
      "[ 0.82355934  0.7344039   0.8172188   0.73402274  0.74502105  0.6534147\n",
      "  0.6435986   0.7128599   0.49431676  0.48380378  0.39066416  0.20980707\n",
      "  0.12757571  0.13026321 -0.0428597 ], shape=(15,), dtype=float32)\n",
      "[0.45297857 0.62130913]\n",
      "tf.Tensor(\n",
      "[ 0.7381621   0.71149874  0.7293028   0.7196442   0.6499199   0.7436134\n",
      "  0.6323719   0.70429844  0.6065198   0.37744427  0.3373286   0.3274003\n",
      "  0.21847181 -0.0329199   0.06734627], shape=(15,), dtype=float32)\n",
      "[0.55327263 0.58079277]\n",
      "tf.Tensor(\n",
      "[0.7875618  0.77709216 0.83968383 0.7608554  0.6851276  0.7055456\n",
      " 0.7077723  0.7634509  0.49213797 0.47112393 0.40849224 0.30878276\n",
      " 0.23586862 0.19032483 0.104571  ], shape=(15,), dtype=float32)\n",
      "[0.58605266 0.49302219]\n",
      "tf.Tensor(\n",
      "[ 0.77135384  0.77240855  0.76240754  0.7110033   0.7368906   0.73387945\n",
      "  0.7226573   0.69948393  0.53461146  0.4671657   0.29715195  0.20918204\n",
      "  0.28141868  0.21912421 -0.05751437], shape=(15,), dtype=float32)\n",
      "[0.53415447 0.40774971]\n",
      "tf.Tensor(\n",
      "[0.8164045  0.7889183  0.7453783  0.7584136  0.790255   0.69659835\n",
      " 0.55485296 0.5503677  0.4235517  0.59859073 0.27493244 0.30828068\n",
      " 0.2731018  0.03790416 0.07867999], shape=(15,), dtype=float32)\n",
      "[0.43198381 0.39000767]\n",
      "tf.Tensor(\n",
      "[ 0.84704113  0.84706455  0.757801    0.8020106   0.7303452   0.7478122\n",
      "  0.66020226  0.51668745  0.36881408  0.40496936  0.41804153  0.26775324\n",
      "  0.29054877  0.09941671 -0.02390235], shape=(15,), dtype=float32)\n",
      "[0.36699534 0.47671569]\n",
      "tf.Tensor(\n",
      "[0.777419   0.8151371  0.75728863 0.82728755 0.7308517  0.66778195\n",
      " 0.64413494 0.6297771  0.5729615  0.43044972 0.48582512 0.2614562\n",
      " 0.19151399 0.0993737  0.01213963], shape=(15,), dtype=float32)\n",
      "[0.40010066 0.58109861]\n",
      "tf.Tensor(\n",
      "[0.8289774  0.84249175 0.7708501  0.7566673  0.75550324 0.6980426\n",
      " 0.6896482  0.68317795 0.41909352 0.3745316  0.4521397  0.201769\n",
      " 0.19879925 0.13832897 0.14238796], shape=(15,), dtype=float32)\n",
      "[0.49982994 0.61265017]\n",
      "tf.Tensor(\n",
      "[0.78668094 0.8483236  0.76588315 0.74072427 0.69789535 0.76625866\n",
      " 0.5793121  0.65345925 0.49243197 0.5656529  0.30996072 0.2837328\n",
      " 0.3334923  0.18241735 0.21956131], shape=(15,), dtype=float32)\n",
      "[0.57826431 0.55251938]\n",
      "tf.Tensor(\n",
      "[0.7769138  0.7900274  0.787963   0.7225514  0.7607372  0.6719565\n",
      " 0.6733853  0.5608764  0.48230317 0.5253998  0.28176615 0.37010866\n",
      " 0.1746049  0.23498641 0.1128964 ], shape=(15,), dtype=float32)\n",
      "No Success\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "daifa.train_tran = False\n",
    "daifa.train_vae = False\n",
    "\n",
    "daifa.hidden_state = None\n",
    "\n",
    "agent, succeeded, time_to_success = train_agent(env, daifa, observation_max, observation_min, observation_noise_stddev,\n",
    "                                                num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}